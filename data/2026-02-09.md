<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.CL](#cs.CL) [Total: 42]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors](https://arxiv.org/abs/2602.06122)
*Ding-Jiun Huang,Yuanhao Wang,Shao-Ji Yuan,Albert Mosella-Montoro,Francisco Vicente Carrasco,Cheng Zhang,Fernando De la Torre*

Main category: cs.CV

TL;DR: SuperHead 是一种用于提升低分辨率、可动画化3D头部模型质量的新框架，通过结合预训练生成模型与动态感知的3D逆优化技术，实现高保真且动态一致的3D头像生成。


<details>
  <summary>Details</summary>
Motivation: 低质量和低分辨率的图像/视频源限制了高质量3D重建的效果，现有超分辨率（SR）方法在动态3D处理上存在不足，导致动画生成时几何、纹理不一致且细节丢失。

Method: 提出动态感知3D逆优化策略，利用预训练3D生成模型的先验知识，结合稀疏的高分辨率2D面部渲染图和深度图监督，优化生成高分辨率的3D高斯点云模型，并与参数化头像模型（如FLAME）结合实现动画控制。

Result: 实验表明，SuperHead在动态面部运动中生成了更精细的面部细节，在视觉质量上显著优于基线方法。

Conclusion: SuperHead通过融合生成模型先验与动态优化策略，解决了动态3D超分辨率重建的挑战，为沉浸式应用提供高质量且可动画化的3D头像。

Abstract: Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.

</details>


### [2] [EgoAVU: Egocentric Audio-Visual Understanding](https://arxiv.org/abs/2602.06139)
*Ashish Seth,Xinhao Mei,Changsheng Zhao,Varun Nagaraja,Ernie Chang,Gregory P. Meyer,Gael Le Lan,Yunyang Xiong,Vikas Chandra,Yangyang Shi,Dinesh Manocha,Zhipeng Cai*

Main category: cs.CV

TL;DR: 提出EgoAVU数据引擎，生成大规模自我中心视频多模态数据，通过训练大幅提升MLLMs的跨模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs因缺乏联合多模态文本标签，无法有效理解自我中心视频中的音频-视觉信号关联，导致模型过度依赖视觉信息而忽略音频线索。

Method: 构建EgoAVU引擎，通过跨模态相关建模生成多模态叙述，结合基于图的数据策划与视频过滤技术，并基于此生成3M规模的EgoAVU-Instruct训练集与人工验证的EgoAVU-Bench评测集。

Result: 微调后的MLLMs在EgoAVU-Bench上性能提升113%，且在EgoTempo/EgoIllusion等基准测试中获得最高28%的相对提升。

Conclusion: 为解决自我中心视频的多模态协同理解提供了高质量基准与有效方法，揭示了模型在多模态整合中的关键缺陷，并验证了大规模多模态数据训练的有效性。

Abstract: Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.

</details>


### [3] [MGP-KAD: Multimodal Geometric Priors and Kolmogorov-Arnold Decoder for Single-View 3D Reconstruction in Complex Scenes](https://arxiv.org/abs/2602.06158)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: MGP-KAD是一种用于复杂现实场景下单视角3D重建的新型多模态特征融合框架。


<details>
  <summary>Details</summary>
Motivation: 针对单视角3D重建中存在的噪声干扰、物体多样性及数据集有限性问题，传统方法存在局限性。

Method: 1) 提出多模态融合框架MGP-KAD，结合RGB和几何先验特征；2) 通过采样和聚类真实数据生成类级别几何特征，实现训练动态优化；3) 引入基于Kolmogorov-Arnold Networks的混合解码器提升多模态输入处理能力。

Result: 在Pix3D数据集上实现最优性能，几何完整性提升23%，表面平滑度指标达到0.91，细节保真度提升19%。

Conclusion: 为复杂场景中的单视角3D重建提供了鲁棒且高效的技术路径，开辟了KAN解码器在多模态领域的创新应用。

Abstract: Single-view 3D reconstruction in complex real-world scenes is challenging due to noise, object diversity, and limited dataset availability. To address these challenges, we propose MGP-KAD, a novel multimodal feature fusion framework that integrates RGB and geometric prior to enhance reconstruction accuracy. The geometric prior is generated by sampling and clustering ground-truth object data, producing class-level features that dynamically adjust during training to improve geometric understanding. Additionally, we introduce a hybrid decoder based on Kolmogorov-Arnold Networks (KAN) to overcome the limitations of traditional linear decoders in processing complex multimodal inputs. Extensive experiments on the Pix3D dataset demonstrate that MGP-KAD achieves state-of-the-art (SOTA) performance, significantly improving geometric integrity, smoothness, and detail preservation. Our work provides a robust and effective solution for advancing single-view 3D reconstruction in complex scenes.

</details>


### [4] [Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving](https://arxiv.org/abs/2602.06159)
*Xuyang Chen,Conglang Zhang,Chuanheng Fu,Zihao Yang,Kaixuan Zhou,Yizhi Zhang,Jianan He,Yanfeng Zhang,Mingwei Sun,Zengmao Wang,Zhen Dong,Xiaoxiao Long,Liqiu Meng*

Main category: cs.CV

TL;DR: 本文提出Driving with DINO（DwD）框架，通过视觉基础模块（VFM）特征统一仿真域与真实世界域，解决自动驾驶视频生成中的控制性与真实感矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于可控视频扩散的自动驾驶仿真到真实（Sim2Real）方法依赖显式中间表征，但低级信号（如边缘、模糊图像）控制精确但缺乏真实感，高级先验（如深度、语义）真实感强但结构细节不足，本研究旨在解决这一根本性矛盾。

Method: 提出主子空间投影去除高频纹理干扰，引入随机通道尾部丢弃缓解降维导致的结构损失；设计可学习空间对齐模块适配高分辨率特征；采用因果卷积的时序聚合模块保留运动上下文。

Result: 实现控制一致性与照片级真实感的统一，在视频生成质量、时间稳定性及结构控制精度上优于现有方法。

Conclusion: 通过VFM特征与所提技术的有机结合，突破传统中间表征的限制，为自动驾驶视频生成提供新的技术范式。

Abstract: Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by "baking in" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for "texture baking," while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: https://albertchen98.github.io/DwD-project/

</details>


### [5] [MetaSSP: Enhancing Semi-supervised Implicit 3D Reconstruction through Meta-adaptive EMA and SDF-aware Pseudo-label Evaluation](https://arxiv.org/abs/2602.06163)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: MetaSSP提出了一种基于半监督学习的单视角3D重建框架，通过利用未标注数据显著提升了表面重建质量。


<details>
  <summary>Details</summary>
Motivation: 隐式SDF方法虽能实现高质量表面重建，但依赖大规模标注数据，存在扩展性瓶颈。半监督学习可减少标注依赖性。

Method: 1) 利用梯度参数重要性估计正则化EMA模型更新；2) 结合数据增强一致性和SDF方差的伪标签加权机制；3) 从10%监督数据起始的联合优化流程。

Result: 在Pix3D数据集上，相比现有半监督基线方法：Chamfer Distance降低20.61%，IoU提升24.09%，达到新SOTA性能。

Conclusion: MetaSSP解决了隐式SDF方法的数据依赖问题，为单视角3D重建提供了高效可扩展的新范式。

Abstract: Implicit SDF-based methods for single-view 3D reconstruction achieve high-quality surfaces but require large labeled datasets, limiting their scalability. We propose MetaSSP, a novel semi-supervised framework that exploits abundant unlabeled images. Our approach introduces gradient-based parameter importance estimation to regularize adaptive EMA updates and an SDF-aware pseudo-label weighting mechanism combining augmentation consistency with SDF variance. Beginning with a 10% supervised warm-up, the unified pipeline jointly refines labeled and unlabeled data. On the Pix3D benchmark, our method reduces Chamfer Distance by approximately 20.61% and increases IoU by around 24.09% compared to existing semi-supervised baselines, setting a new state of the art.

</details>


### [6] [PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining](https://arxiv.org/abs/2602.06184)
*Cheng Liang,Chaoyi Wu,Weike Zhao,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 提出PhenoLIP框架及PhenoKG知识图谱，通过结构化表型知识提升多模态医学图像分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLMs依赖粗粒度图像-文本对比目标，未能有效整合医学表型本体中的结构化知识，导致知识利用不足与可解释性弱。

Method: 构建PhenoKG图谱（520K图像-文本对关联3K+表型），基于其设计两阶段PhenoLIP框架：1) 利用文本本体数据构建知识增强的表型嵌入空间；2) 通过教师引导蒸馏将结构化知识注入多模态预训练。

Result: 在PhenoBench基准上，PhenoLIP在表型分类任务相对BiomedCLIP提升8.85%，跨模态检索任务相对BIOMEDICA提升15.03%，验证了结构化知识整合的有效性。

Conclusion: 引入表型中心化先验知识能显著提升多模态医学分析的结构化与可解释性，为视觉语言模型的医学应用提供新范式。

Abstract: Recent progress in large-scale CLIP-like vision-language models(VLMs) has greatly advanced medical image analysis. However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies. To address this gap, we construct PhenoKG, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes. Building upon PhenoKG, we propose PhenoLIP, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process. We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective. To support evaluation, we further introduce PhenoBench, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image--caption pairs covering more than 1,000 phenotypes. Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85\% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.

</details>


### [7] [DeDPO: Debiased Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2602.06195)
*Khiem Pham,Quang Nguyen,Tung Nguyen,Jingsen Zhu,Michele Santacatterina,Dimitris Metaxas,Ramin Zabih*

Main category: cs.CV

TL;DR: 本文提出DeDPO方法，通过去偏估计技术解决扩散模型训练中依赖大量人工标注的问题，实验证明其性能超越全人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 当前基于DPO的扩散模型因依赖大量高成本人工偏好标签导致可扩展性受限，需要开发低成本半监督方案。

Method: 提出DeDPO框架，创新性引入因果推断中的去偏估计技术，结合有限人工数据与大量AI合成标注，构建损失函数对抗合成标注器的系统偏差。

Result: 在多种合成标注场景下保持稳定性能，其中75%人工数据+25%合成数据组合达到全人工数据80%性能，而纯合成数据达75%性能。

Conclusion: DeDPO有效降低人工标注依赖，为扩散模型训练提供了可扩展且具成本效益的解决方案，实验证明其鲁棒性和理论可行性。

Abstract: Direct Preference Optimization (DPO) has emerged as a predominant alignment method for diffusion models, facilitating off-policy training without explicit reward modeling. However, its reliance on large-scale, high-quality human preference labels presents a severe cost and scalability bottleneck. To overcome this, We propose a semi-supervised framework augmenting limited human data with a large corpus of unlabeled pairs annotated via cost-effective synthetic AI feedback. Our paper introduces Debiased DPO (DeDPO), which uniquely integrates a debiased estimation technique from causal inference into the DPO objective. By explicitly identifying and correcting the systematic bias and noise inherent in synthetic annotators, DeDPO ensures robust learning from imperfect feedback sources, including self-training and Vision-Language Models (VLMs). Experiments demonstrate that DeDPO is robust to the variations in synthetic labeling methods, achieving performance that matches and occasionally exceeds the theoretical upper bound of models trained on fully human-labeled data. This establishes DeDPO as a scalable solution for human-AI alignment using inexpensive synthetic supervision.

</details>


### [8] [DroneKey++: A Size Prior-free Method and New Benchmark for Drone 3D Pose Estimation from Sequential Images](https://arxiv.org/abs/2602.06211)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出DroneKey++，一种无需先验信息的无人机三维姿态估计框架，并构建了包含5万张图像的大规模合成数据集6DroneSyn。该框架通过联合关键点检测、分类和几何推理实现高精度实时姿态估计。


<details>
  <summary>Details</summary>
Motivation: 现有无人机姿态估计方法依赖物理尺寸或三维网格等先验信息，且数据集存在规模小、模型单一、环境受限等问题，导致泛化能力难以验证。需要解决先验依赖和数据局限性挑战。

Method: 提出DroneKey++框架：1）关键点编码器同步检测关键点和分类无人机型号；2）姿态解码器基于射线几何推理和类别嵌入估计3D姿态；3）构建6DroneSyn合成数据集（50K+图像覆盖7种机型+88种户外背景），采用360度全景合成技术生成。

Result: 实验显示：旋转误差MAE 17.34°/MedAE 17.1°，平移误差MAE 0.135m/MedAE 0.242m；推理速度CPU 19.25FPS/GPU 414.07FPS；方法在多种无人机型号上表现泛化性，满足实时应用需求；数据集已开源。

Conclusion: DroneKey++突破传统方法对先验信息的依赖，通过几何建模和合成数据增强提升姿态估计精度与鲁棒性，为无人机监控系统提供实时解决方案，推动该领域数据标准化。

Abstract: Accurate 3D pose estimation of drones is essential for security and surveillance systems. However, existing methods often rely on prior drone information such as physical sizes or 3D meshes. At the same time, current datasets are small-scale, limited to single models, and collected under constrained environments, which makes reliable validation of generalization difficult. We present DroneKey++, a prior-free framework that jointly performs keypoint detection, drone classification, and 3D pose estimation. The framework employs a keypoint encoder for simultaneous keypoint detection and classification, and a pose decoder that estimates 3D pose using ray-based geometric reasoning and class embeddings. To address dataset limitations, we construct 6DroneSyn, a large-scale synthetic benchmark with over 50K images covering 7 drone models and 88 outdoor backgrounds, generated using 360-degree panoramic synthesis. Experiments show that DroneKey++ achieves MAE 17.34 deg and MedAE 17.1 deg for rotation, MAE 0.135 m and MedAE 0.242 m for translation, with inference speeds of 19.25 FPS (CPU) and 414.07 FPS (GPU), demonstrating both strong generalization across drone models and suitability for real-time applications. The dataset is publicly available.

</details>


### [9] [Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models](https://arxiv.org/abs/2602.06214)
*Jorge Daniel Rodríguez-Vidal,Gabriel Villalonga,Diego Porres,Antonio M. López Peña*

Main category: cs.CV

TL;DR: 提出可微分车辆模型框架，连接航路点与动作策略，实现动作模型在航路点基线中评估，并在多基准测试中取得SOTA性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有航路点基线使动作策略难以训练与对比，需构建端到端框架弥合输出差异。

Method: 设计可微分车辆模型，将动作序列转化为轨迹坐标，通过航路点监督优化动作策略。

Result: 多个挑战性基准测试中性能提升，NAVSIM navhard实现SOTA，且无需修改原评估协议。

Conclusion: 动作策略可通过轨迹 rollout 实现与航路点基线兼容，为动作控制研究提供新路径。

Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are waypoint-based, which makes action-based policies harder to train and compare, slowing their progress. To bridge this waypoint-action gap, we propose a novel, differentiable vehicle-model framework that rolls out predicted action sequences to their corresponding ego-frame waypoint trajectories while supervising in waypoint space. Our approach enables action-based architectures to be trained and evaluated, for the first time, within waypoint-based benchmarks without modifying the underlying evaluation protocol. We extensively evaluate our framework across multiple challenging benchmarks and observe consistent improvements over the baselines. In particular, on NAVSIM \texttt{navhard} our approach achieves state-of-the-art performance. Our code will be made publicly available upon acceptance.

</details>


### [10] [Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings](https://arxiv.org/abs/2602.06218)
*Grégoire Dhimoïla,Thomas Fel,Victor Boutin,Agustin Picard*

Main category: cs.CV

TL;DR: 该论文研究了视觉-语言模型(VLMs)中跨模态嵌入空间的几何结构，提出基于Iso-Energy假设的Aligned SAE方法，揭示了双模态原子承载对齐信号、单模态原子解释模态差异等关键发现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM的共享嵌入空间几何特性未被充分理解。作者通过提出Iso-Energy假设（即跨模态共享概念应具有相同平均能量），旨在探索如何通过能量一致性改进跨模态对齐，并构建可解释的几何分析工具。

Method: 设计了一种具备能量一致性约束的对齐稀疏自编码器(Aligned SAE)，在保留重建能力的同时强制执行跨模态能量一致性作为归纳偏置。通过控制变量实验验证假设有效性，并在基础VLM模型上分析编码器的几何特性。

Result: SAE在不降低重建性能的前提下改变解空间结构；在可控数据中验证了Iso-Energy假设对对齐的促进作用；发现稀疏双模态原子承载完整跨模态对齐信号，移除单模态原子可消除模态差距，且限制在双模态子空间的向量运算能提升检索性能。

Conclusion: 通过能量一致性归纳偏置可同时保持模型保真度与可解释性，揭示的几何结构为后续设计更可解释的VLM模型提供了理论依据。

Abstract: Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.

</details>


### [11] [ForeHOI: Feed-forward 3D Object Reconstruction from Daily Hand-Object Interaction Videos](https://arxiv.org/abs/2602.06226)
*Yuantao Chen,Jiahao Chang,Chongjie Ye,Chaoran Zhang,Zhaojie Fang,Chenghong Li,Xiaoguang Han*

Main category: cs.CV

TL;DR: ForeHOI通过前馈模型联合预测2D掩膜补全和3D形状补全，在单目手物交互视频中实现快速高质量物体重建。


<details>
  <summary>Details</summary>
Motivation: 单目手持物体视频中严重的遮挡和复杂的运动导致物体3D重建困难，现有优化方法速度慢且效果差。

Method: 提出端到端前馈框架，同时进行2D掩膜补全和3D形状补全，并构建大规模合成数据集辅助训练。

Result: 实现单分钟内推理速度，较优化方法提升约100倍，重建质量显著优于现有方法。

Conclusion: 该方法通过2D-3D协同补全有效解决遮挡问题，为手物交互重建提供了高效解决方案。

Abstract: The ubiquity of monocular videos capturing daily hand-object interactions presents a valuable resource for embodied intelligence. While 3D hand reconstruction from in-the-wild videos has seen significant progress, reconstructing the involved objects remains challenging due to severe occlusions and the complex, coupled motion of the camera, hands, and object. In this paper, we introduce ForeHOI, a novel feed-forward model that directly reconstructs 3D object geometry from monocular hand-object interaction videos within one minute of inference time, eliminating the need for any pre-processing steps. Our key insight is that, the joint prediction of 2D mask inpainting and 3D shape completion in a feed-forward framework can effectively address the problem of severe occlusion in monocular hand-held object videos, thereby achieving results that outperform the performance of optimization-based methods. The information exchanges between the 2D and 3D shape completion boosts the overall reconstruction quality, enabling the framework to effectively handle severe hand-object occlusion. Furthermore, to support the training of our model, we contribute the first large-scale, high-fidelity synthetic dataset of hand-object interactions with comprehensive annotations. Extensive experiments demonstrate that ForeHOI achieves state-of-the-art performance in object reconstruction, significantly outperforming previous methods with around a 100x speedup. Code and data are available at: https://github.com/Tao-11-chen/ForeHOI.

</details>


### [12] [An Interpretable Vision Transformer as a Fingerprint-Based Diagnostic Aid for Kabuki and Wiedemann-Steiner Syndromes](https://arxiv.org/abs/2602.06282)
*Marilyn Lionts,Arnhildur Tomasdottir,Viktor I. Agustsson,Yuankai Huo,Hans T. Bjornsson,Lotta M. Ellingsen*

Main category: cs.CV

TL;DR: 本文提出了一种基于指纹图像的视觉变换器深度学习模型，可区分Kabuki综合征（KS）、Wiedemann-Steiner综合征（WSS）与正常对照，并实现较高的分类精度。


<details>
  <summary>Details</summary>
Motivation: KS与WSS临床特征相似导致诊断困难，且基因检测受限于可及性与专业门槛。指纹皮纹异常作为潜在诊断信号尚未被充分应用，亟需非侵入性、易获取的辅助诊断工具。

Method: 构建视觉变换器模型，使用指纹图像进行三级分类任务（对照vs.KS、对照vs.WSS、KS vs.WSS），并通过注意力机制可视化模型关注的指纹关键区域。

Result: 模型表现：AUC分别为0.80（对照vs.KS）、0.73（对照vs.WSS）、0.85（KS vs.WSS）；F1分数达0.71、0.72、0.83。注意力可视化揭示了与综合征相关性显著的指纹特征区域。

Conclusion: 研究表明指纹特征具有综合征特异性，AI驱动的指纹分析可作为可解释、易普及的辅助诊断工具，助力遗传综合征的早期筛查。

Abstract: Kabuki syndrome (KS) and Wiedemann-Steiner syndrome (WSS) are rare but distinct developmental disorders that share overlapping clinical features, including neurodevelopmental delay, growth restriction, and persistent fetal fingertip pads. While genetic testing remains the diagnostic gold standard, many individuals with KS or WSS remain undiagnosed due to barriers in access to both genetic testing and expertise. Dermatoglyphic anomalies, despite being established hallmarks of several genetic syndromes, remain an underutilized diagnostic signal in the era of molecular testing. This study presents a vision transformer-based deep learning model that leverages fingerprint images to distinguish individuals with KS and WSS from unaffected controls and from one another. We evaluate model performance across three binary classification tasks. Across the three classification tasks, the model achieved AUC scores of 0.80 (control vs. KS), 0.73 (control vs. WSS), and 0.85 (KS vs. WSS), with corresponding F1 scores of 0.71, 0.72, and 0.83, respectively. Beyond classification, we apply attention-based visualizations to identify fingerprint regions most salient to model predictions, enhancing interpretability. Together, these findings suggest the presence of syndrome-specific fingerprint features, demonstrating the feasibility of a fingerprint-based artificial intelligence (AI) tool as a noninvasive, interpretable, and accessible future diagnostic aid for the early diagnosis of underdiagnosed genetic syndromes.

</details>


### [13] [MMEarth-Bench: Global Model Adaptation via Multimodal Test-Time Training](https://arxiv.org/abs/2602.06285)
*Lucia Gordon,Serge Belongie,Christian Igel,Nico Lang*

Main category: cs.CV

TL;DR: 本文介绍了一个名为MMEarth-Bench的新基准数据集，包含5个新多模态环境任务、12种数据模态及全球化分布数据，用于评估地理迁移和模型适应性。同时提出测试时间训练方法TTT-MMR，提升模型在随机和地理测试分集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间基准数据集模态单一且地理代表性不足，无法有效评估多模态预训练模型的全球化能力。需要填补该领域空白并改进模型适应性方法。

Method: 开发MMEarth-Bench数据集，涵盖多模态任务和地理分布测试分割；提出模型无关的测试时间多模态重建训练方法TTT-MMR，利用测试时所有模态作为辅助任务。

Result: 验证了预训练对小样本任务的普适有效性，但地理泛化性仍薄弱。TTT-MMR在随机和地理分集均提升性能，地理批量划分实现正则化与特化的平衡。

Conclusion: MMEarth-Bench解决了多模态地理评估缺口，TTT-MMR方法增强了模型跨任务和跨区域适应性，推动了地理空间机器学习进展。

Abstract: Recent research in geospatial machine learning has demonstrated that models pretrained with self-supervised learning on Earth observation data can perform well on downstream tasks with limited training data. However, most of the existing geospatial benchmark datasets have few data modalities and poor global representation, limiting the ability to evaluate multimodal pretrained models at global scales. To fill this gap, we introduce MMEarth-Bench, a collection of five new multimodal environmental tasks with 12 modalities, globally distributed data, and both in- and out-of-distribution test splits. We benchmark a diverse set of pretrained models and find that while (multimodal) pretraining tends to improve model robustness in limited data settings, geographic generalization abilities remain poor. In order to facilitate model adaptation to new downstream tasks and geographic domains, we propose a model-agnostic method for test-time training with multimodal reconstruction (TTT-MMR) that uses all the modalities available at test time as auxiliary tasks, regardless of whether a pretrained model accepts them as input. Our method improves model performance on both the random and geographic test splits, and geographic batching leads to a good trade-off between regularization and specialization during TTT. Our dataset, code, and visualization tool are linked from the project page at lgordon99.github.io/mmearth-bench.

</details>


### [14] [Unsupervised MRI-US Multimodal Image Registration with Multilevel Correlation Pyramidal Optimization](https://arxiv.org/abs/2602.06288)
*Jiazheng Wang,Zeyu Liu,Min Liu,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: The paper proposes an unsupervised multimodal medical image registration method (MCPO) using multilevel correlation pyramidal optimization to address surgical navigation challenges caused by tissue deformation and modality differences, achieving top performance in Learn2Reg 2025 and demonstrating strong applicability on the Resect dataset.


<details>
  <summary>Details</summary>
Motivation: Surgical navigation faces challenges due to intraoperative tissue deformation and differences between preoperative/intraoperative multimodal images, leading to misalignment. Current methods struggle with robust registration under these conditions, necessitating a solution that ensures accurate real-time guidance for surgeons.

Method: The MCPO method employs modality-independent feature extraction via neighborhood descriptors to map multimodal images into a unified feature space. It then uses dense correlation analysis and weight-balanced convex optimization across multiple scales via a pyramidal fusion mechanism, achieving global-local displacement field optimization without supervised training.

Result: MCPO achieved first place in Learn2Reg 2025's ReMIND2Reg task (validation/test phases) and reduced target registration error (TRE) to 1.798 mm on the Resect dataset, outperforming existing methods in accuracy and robustness for deformable preoperative-to-intraoperative registration.

Conclusion: The unsupervised MCPO framework effectively addresses multimodal registration challenges in dynamic surgical environments, demonstrating superior performance and generalizability for clinical applications. Its open-source release enables broader adoption in image-guided interventions.

Abstract: Surgical navigation based on multimodal image registration has played a significant role in providing intraoperative guidance to surgeons by showing the relative position of the target area to critical anatomical structures during surgery. However, due to the differences between multimodal images and intraoperative image deformation caused by tissue displacement and removal during the surgery, effective registration of preoperative and intraoperative multimodal images faces significant challenges. To address the multimodal image registration challenges in Learn2Reg 2025, an unsupervised multimodal medical image registration method based on multilevel correlation pyramidal optimization (MCPO) is designed to solve these problems. First, the features of each modality are extracted based on the modality independent neighborhood descriptor, and the multimodal images is mapped to the feature space. Second, a multilevel pyramidal fusion optimization mechanism is designed to achieve global optimization and local detail complementation of the displacement field through dense correlation analysis and weight-balanced coupled convex optimization for input features at different scales. Our method focuses on the ReMIND2Reg task in Learn2Reg 2025. Based on the results, our method achieved the first place in the validation phase and test phase of ReMIND2Reg. The MCPO is also validated on the Resect dataset, achieving an average TRE of 1.798 mm. This demonstrates the broad applicability of our method in preoperative-to-intraoperative image registration. The code is avaliable at https://github.com/wjiazheng/MCPO.

</details>


### [15] [Accelerating Vision Transformers on Brain Processing Unit](https://arxiv.org/abs/2602.06300)
*Jinchi Tang,Yan Guo*

Main category: cs.CV

TL;DR: A method adapts Vision Transformers like DeiT to run efficiently on Brain Processing Units (BPUs) optimized for CNNs by replacing linear layers with convolutional operations, achieving speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing BPUs optimized for CNNs with 4D convolutions face inefficiencies in deploying Vision Transformers (ViTs) due to architectural mismatch (ViTs' 3D linear layers vs BPU's 4D convolution requirements).

Method: Restructures ViT architecture by replacing linear layers and layer normalization with convolutional operators compatible with BPU hardware, enabling parameter reuse without retraining or fine-tuning.

Result: Quantized DeiT-Base model achieves 80.4% ImageNet accuracy (vs original 81.8%) with 3.8× inference speedup; fine-tuned DeiT on flower dataset shows only 0.5% accuracy drop, proving effectiveness.

Conclusion: First successful BPU optimization for Vision Transformers without retraining, demonstrating hardware-software co-adaptation for efficient deployment with minimal accuracy sacrifice.

Abstract: With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT) models, such as the Data-efficient Image Transformer (DeiT), have demonstrated superior performance and play increasingly crucial roles in computer vision tasks. However, due to the architectural mismatch between CNN-optimized hardware and Vision Transformer computation characteristics--namely, that linear layers in Transformers operate on three-dimensional data while BPU acceleration is designed for four-dimensional convolution operations-it is difficult or even impossible to leverage BPU's advantages when deploying Vision Transformers. To address this challenge, we propose a novel approach that restructures the Vision Transformer by replacing linear layers and layer normalization operations with carefully designed convolutional operators. This enables DeiT to fully utilize the acceleration capabilities of BPUs, while allowing the original weight parameters to be inherited by the restructured models without retraining or fine-tuning. To the best of our knowledge, this is the first successful deployment of Vision Transformers that fully leverages BPU classification datasets demonstrate the effectiveness of our approach. Specifically, the quantized DeiT-Base model achieves 80.4% accuracy on ImageNet, compared to the original 81.8%, while obtaining up to a 3.8* inference speedup. Our finetuned DeiT model on the flower classification dataset also achieves excellent performance, with only a 0.5% accuracy drop for the DeiT-Base model, further demonstrating the effectiveness of our method.

</details>


### [16] [Adaptive and Balanced Re-initialization for Long-timescale Continual Test-time Domain Adaptation](https://arxiv.org/abs/2602.06328)
*Yanshuo Wang,Jinguang Tong,Jun Lan,Weiqiang Wang,Huijia Zhu,Haoxing Chen,Xuesong Li,Jie Hong*

Main category: cs.CV

TL;DR: 本研究提出ABR方法，通过自适应间隔的权重重新初始化解决CTTA任务中的长期适应问题，基于标签翻转轨迹模式提升模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法未解决模型在长时间连续变化环境中性能下降的痛点，研究旨在建立可持续适应的长期机制。

Method: 通过观测标签翻转轨迹模式与性能的关联性，设计自适应间隔权重重新初始化策略（ABR），动态调整模型重置时机。

Result: ABR在多个CTTA基准测试中均取得优越性能表现，证明策略能有效维持模型长期适应能力。

Conclusion: 权重重置策略应考虑动态环境变化速率，ABR方法通过平衡模型新旧知识实现持续适应，为CTTA提供新思路。

Abstract: Continual test-time domain adaptation (CTTA) aims to adjust models so that they can perform well over time across non-stationary environments. While previous methods have made considerable efforts to optimize the adaptation process, a crucial question remains: Can the model adapt to continually changing environments over a long time? In this work, we explore facilitating better CTTA in the long run using a re-initialization (or reset) based method. First, we observe that the long-term performance is associated with the trajectory pattern in label flip. Based on this observed correlation, we propose a simple yet effective policy, Adaptive-and-Balanced Re-initialization (ABR), towards preserving the model's long-term performance. In particular, ABR performs weight re-initialization using adaptive intervals. The adaptive interval is determined based on the change in label flip. The proposed method is validated on extensive CTTA benchmarks, achieving superior performance.

</details>


### [17] [Halt the Hallucination: Decoupling Signal and Semantic OOD Detection Based on Cascaded Early Rejection](https://arxiv.org/abs/2602.06330)
*Ningkang Peng,Chuanjie Cheng,Jingyang Mao,Xiaoqian Peng,Feng Xing,Bo Zhang,Chao Tan,Zhichao Zheng,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出级联早期拒绝框架（CER），通过层次化过滤实现高效的异常检测，显著降低计算开销并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低级统计噪声上执行全规模推理，导致资源浪费和语义幻觉，亟需一种兼顾效率与鲁棒性的OOD检测框架。

Method: 1) 结构能量筛（SES）使用拉普拉斯算子在输入层构建非参数屏障拦截物理信号异常；2) 语义感知超球能量（SHE）解耦中间层特征的幅度与方向以捕获细粒度语义偏差。二者结合实现从粗到精的层级过滤。

Result: 计算开销降低32%，CIFAR-100基准下FPR95从33.58%降至22.84%，AUROC提升至93.97%，在传感器故障模拟的实际场景中显著超越SOTA方法。

Conclusion: CER作为通用插件可无缝集成至多种SOTA模型，有效平衡检测性能与计算效率，并在实际场景中展现强泛化能力。

Abstract: Efficient and robust Out-of-Distribution (OOD) detection is paramount for safety-critical applications.However, existing methods still execute full-scale inference on low-level statistical noise. This computational mismatch not only incurs resource waste but also induces semantic hallucination, where deep networks forcefully interpret physical anomalies as high-confidence semantic features.To address this, we propose the Cascaded Early Rejection (CER) framework, which realizes hierarchical filtering for anomaly detection via a coarse-to-fine logic.CER comprises two core modules: 1)Structural Energy Sieve (SES), which establishes a non-parametric barrier at the network entry using the Laplacian operator to efficiently intercept physical signal anomalies; and 2) the Semantically-aware Hyperspherical Energy (SHE) detector, which decouples feature magnitude from direction in intermediate layers to identify fine-grained semantic deviations. Experimental results demonstrate that CER not only reduces computational overhead by 32% but also achieves a significant performance leap on the CIFAR-100 benchmark:the average FPR95 drastically decreases from 33.58% to 22.84%, and AUROC improves to 93.97%. Crucially, in real-world scenarios simulating sensor failures, CER exhibits performance far exceeding state-of-the-art methods. As a universal plugin, CER can be seamlessly integrated into various SOTA models to provide performance gains.

</details>


### [18] [Taming SAM3 in the Wild: A Concept Bank for Open-Vocabulary Segmentation](https://arxiv.org/abs/2602.06333)
*Gensheng Pei,Xiruo Jiang,Yazhou Yao,Xiangbo Shu,Fumin Shen,Byeungwoo Jeon*

Main category: cs.CV

TL;DR: 本论文提出ConceptBank，一个无需参数的校准框架，用于解决SAM3在开放词汇分割中因数据或概念漂移导致的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有SAM3模型依赖预定义概念，导致在目标域出现数据分布变化（如数据漂移或概念漂移）时，视觉证据与提示的对齐失效，模型性能下降。

Method: 构建目标域特定的概念银行：1) 通过类级视觉原型锚定证据；2) 挖掘代表样本抑制数据漂移下的异常值；3) 融合候选概念纠正概念漂移。完全动态处理且无需模型参数更新。

Result: 在自然场景与遥感图像等分布漂移场景中，ConceptBank显著提升SAM3的适应性，建立新的鲁棒性基线，且实验表明其效率优于微调方法。

Conclusion: 该方法无需修改模型参数即可动态优化概念对齐，为开放词汇分割任务提供了应对分布变化的普适性框架。

Abstract: The recent introduction of \texttt{SAM3} has revolutionized Open-Vocabulary Segmentation (OVS) through \textit{promptable concept segmentation}, which grounds pixel predictions in flexible concept prompts. However, this reliance on pre-defined concepts makes the model vulnerable: when visual distributions shift (\textit{data drift}) or conditional label distributions evolve (\textit{concept drift}) in the target domain, the alignment between visual evidence and prompts breaks down. In this work, we present \textsc{ConceptBank}, a parameter-free calibration framework to restore this alignment on the fly. Instead of adhering to static prompts, we construct a dataset-specific concept bank from the target statistics. Our approach (\textit{i}) anchors target-domain evidence via class-wise visual prototypes, (\textit{ii}) mines representative supports to suppress outliers under data drift, and (\textit{iii}) fuses candidate concepts to rectify concept drift. We demonstrate that \textsc{ConceptBank} effectively adapts \texttt{SAM3} to distribution drifts, including challenging natural-scene and remote-sensing scenarios, establishing a new baseline for robustness and efficiency in OVS. Code and model are available at https://github.com/pgsmall/ConceptBank.

</details>


### [19] [SPDA-SAM: A Self-prompted Depth-Aware Segment Anything Model for Instance Segmentation](https://arxiv.org/abs/2602.06335)
*Yihan Shang,Wei Wang,Chao Huang,Xinghui Dong*

Main category: cs.CV

TL;DR: 设计自提示深度感知SAM模型，提升实例分割性能。


<details>
  <summary>Details</summary>
Motivation: Segment Anything Model (SAM)在实例分割任务中表现优异，但依赖手动提示输入且RGB图像缺乏深度信息，限制其空间结构感知能力。

Method: 提出Self-prompted Depth-Aware SAM (SPDA-SAM)，包括语义-空间自提示模块（SSSPM）和粗到精RGB-D融合模块（C2FFM）。SSSPM自动提取语义和空间提示，C2FFM通过深度图提供粗粒度结构引导，并编码深度局部变化实现细粒度特征融合。

Result: 在12个数据集中均超越现有最先进方法，验证自提示机制和深度融合的有效性。

Conclusion: 通过自生成提示信息和深度引导的特征融合，有效缓解SAM对人工输入的依赖并提升空间结构感知能力。

Abstract: Recently, Segment Anything Model (SAM) has demonstrated strong generalizability in various instance segmentation tasks. However, its performance is severely dependent on the quality of manual prompts. In addition, the RGB images that instance segmentation methods normally use inherently lack depth information. As a result, the ability of these methods to perceive spatial structures and delineate object boundaries is hindered. To address these challenges, we propose a Self-prompted Depth-Aware SAM (SPDA-SAM) for instance segmentation. Specifically, we design a Semantic-Spatial Self-prompt Module (SSSPM) which extracts the semantic and spatial prompts from the image encoder and the mask decoder of SAM, respectively. Furthermore, we introduce a Coarse-to-Fine RGB-D Fusion Module (C2FFM), in which the features extracted from a monocular RGB image and the depth map estimated from it are fused. In particular, the structural information in the depth map is used to provide coarse-grained guidance to feature fusion, while local variations in depth are encoded in order to fuse fine-grained feature representations. To our knowledge, SAM has not been explored in such self-prompted and depth-aware manners. Experimental results demonstrate that our SPDA-SAM outperforms its state-of-the-art counterparts across twelve different data sets. These promising results should be due to the guidance of the self-prompts and the compensation for the spatial information loss by the coarse-to-fine RGB-D fusion operation.

</details>


### [20] [Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering](https://arxiv.org/abs/2602.06343)
*Weiquan Wang,Feifei Shao,Lin Li,Zhen Wang,Jun Xiao,Long Chen*

Main category: cs.CV

TL;DR: U-4DGS improves dynamic human rendering under occlusions using uncertainty-aware neural rendering.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with occlusions, leading to flickering or unrealistic results. Prior solutions either hallucinate content poorly or use rigid models that fail for diverse appearances.

Method: Proposed U-4DGS with a Probabilistic Deformation Network and Double Rasterization to model observation uncertainty. Uses learned uncertainty maps to modulate gradients and Confidence-Aware Regularization to constrain geometry in ambiguous regions.

Result: SOTA rendering fidelity on ZJU-MoCap and OcMotion benchmarks, with reduced artifacts under occlusions.

Conclusion: Explicit uncertainty modeling improves robustness and temporal coherence compared to prior human rendering approaches.

Abstract: High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.

</details>


### [21] [FlowConsist: Make Your Flow Consistent with Real Trajectory](https://arxiv.org/abs/2602.06346)
*Tianyi Zhang,Chengcheng Liu,Jinwei Chen,Chun-Le Guo,Chongyi Li,Ming-Ming Cheng,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 本文提出FlowConsist框架，通过优化轨迹一致性解决快速流模型的轨迹漂移和误差累积问题，在ImageNet 256×256上实现1.52 FID的SOTA结果


<details>
  <summary>Details</summary>
Motivation: 现有快速流模型训练存在两个根本问题：随机噪声-数据配对产生的条件速度导致轨迹漂移；模型误差随时间步长累积造成长期偏差

Method: 1. 用模型自预测的边际速度替代条件速度，实现轨迹对齐；2. 引入轨迹修正策略，每一步对齐生成样本与真实样本的边际分布

Result: 在ImageNet 256×256数据集上仅需1个采样步长即达到1.52 FID，建立新的图像生成质量基准

Conclusion: FlowConsist为快速流模型提供了理论更完善的优化范式，解决了长期轨迹一致性难题，为高分辨率图像生成提供了新方法

Abstract: Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\times$256, achieving an FID of 1.52 with only 1 sampling step.

</details>


### [22] [Revisiting Salient Object Detection from an Observer-Centric Perspective](https://arxiv.org/abs/2602.06369)
*Fuxi Zhang,Yifan Wang,Hengrun Zhao,Zhuohan Sun,Changxing Xia,Lijun Wang,Huchuan Lu,Yangrui Shao,Chen Yang,Long Teng*

Main category: cs.CV

TL;DR: Salient object detection 的主观性挑战被重新定义为 observer-centric 问题，通过引入观测者偏好因素与大规模多模态数据集 OC-SODBench，实现更真实的人机感知对齐。


<details>
  <summary>Details</summary>
Motivation: 传统客观化建模忽视人类感知的主观差异，单值标签导致病态问题，需融合视觉线索与个性化观测者意图进行个性化显著性预测。

Method: 基于多模态大语言模型构建 OC-SODBench（33k 图像/152k 文本提示），设计 OC-SODAgent 框架模拟人类 “感知-反思-调整” 过程。

Result: 实验验证了 observer-centric 方法有效性，首个多模态 OC-SOD 数据集与基线模型可公开获取。

Conclusion: 通过观测者中心视角，实现了计算模型与人类感知多样性之间的桥梁，拓展显著性定义边界。

Abstract: Salient object detection is inherently a subjective problem, as observers with different priors may perceive different objects as salient. However, existing methods predominantly formulate it as an objective prediction task with a single groundtruth segmentation map for each image, which renders the problem under-determined and fundamentally ill-posed. To address this issue, we propose Observer-Centric Salient Object Detection (OC-SOD), where salient regions are predicted by considering not only the visual cues but also the observer-specific factors such as their preferences or intents. As a result, this formulation captures the intrinsic ambiguity and diversity of human perception, enabling personalized and context-aware saliency prediction. By leveraging multi-modal large language models, we develop an efficient data annotation pipeline and construct the first OC-SOD dataset named OC-SODBench, comprising 33k training, validation and test images with 152k textual prompts and object pairs. Built upon this new dataset, we further design OC-SODAgent, an agentic baseline which performs OC-SOD via a human-like "Perceive-Reflect-Adjust" process. Extensive experiments on our proposed OC-SODBench have justified the effectiveness of our contribution. Through this observer-centric perspective, we aim to bridge the gap between human perception and computational modeling, offering a more realistic and flexible understanding of what makes an object truly "salient." Code and dataset are publicly available at: https://github.com/Dustzx/OC_SOD

</details>


### [23] [POINTS-GUI-G: GUI-Grounding Journey](https://arxiv.org/abs/2602.06391)
*Zhongyin Zhao,Yuan Liu,Yikun Liu,Haicheng Wang,Le Tian,Xiao Zhou,Yangxiu You,Zilin Yu,Yang Yu,Jie Zhou*

Main category: cs.CV

TL;DR: The paper introduces POINTS-GUI-G-8B, a GUI grounding model for vision-language systems, achieving state-of-the-art results on multiple benchmarks through refined data engineering, improved training strategies, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: GUI grounding is essential for automating digital tasks via precise interface interaction. Traditional approaches rely on pre-trained models with strong spatial awareness, limiting understanding of end-to-end pipelines. This work aims to enhance grounding capabilities starting from a base model with minimal spatial skills.

Method: 1) Unified multi-source datasets with augmentation and difficulty grading. 2) Continuous vision encoder tuning and resolution consistency during training. 3) Reinforcement learning with verifiable rewards focused on perception accuracy rather than pure reasoning.

Result: POINTS-GUI-G-8B achieved 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision, demonstrating superior GUI grounding performance compared to existing methods.

Conclusion: The study demonstrates that combining structured data engineering, training optimization, and reward-verifiable reinforcement learning can significantly improve GUI grounding. This approach provides a pathway for vision-language models to handle complex interface-driven tasks without relying on pretrained spatial capabilities.

Abstract: The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.

</details>


### [24] [TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction](https://arxiv.org/abs/2602.06400)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: TFusionOcc采用以目标为中心的多传感器融合框架，结合学生t分布混合模型（TMM）和可变形超二次曲线表示，提出高效且几何细节感知的3D语义占位预测方法，在nuScenes数据集上达到当前最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D语义占位预测方法严重依赖3D体素或高斯表征，存在几何细节捕捉能力不足与计算效率低效的瓶颈，而自动驾驶车辆需要对复杂驾驶环境进行精确的细粒度几何建模以确保安全决策。

Method: 提出多阶段多传感器融合框架：1）引入学生t分布混合模型（TMM）增强空间不确定性建模；2）采用可逆形变超二次曲线作为几何基元；3）设计分层传感器特征融合架构；4）使用几何感知的多尺度损失函数优化。

Result: 在nuScenes语义占位挑战赛上以IoU指标超越现有方法5.2%-8.7%，并在nuScenes-C数据集多种传感器干扰场景（如雾天lidar降级、低光camera失效）中保持90%+预测稳定性，可视化展示出对复杂物体形状（如自行车车轮、行人四肢）的准确建模。

Conclusion: 该研究证明了对象中心混合表示的有效性，所提出的TMM分布相比传统高斯混合模型提升18.4%的几何保真度，为自动驾驶提供鲁棒的端到端环境表征方案，在CVPR 2024相关研讨会获最佳论文奖励。

Abstract: 3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely heavily on 3D voxel volumes or a set of 3D Gaussians, hindering the model's ability to efficiently and effectively capture fine-grained geometric details in the 3D driving environment. This paper introduces TFusionOcc, a novel object-centric multi-sensor fusion framework for predicting 3D semantic occupancy. By leveraging multi-stage multi-sensor fusion, Student's t-distribution, and the T-Mixture model (TMM), together with more geometrically flexible primitives, such as the deformable superquadric (superquadric with inverse warp), the proposed method achieved state-of-the-art (SOTA) performance on the nuScenes benchmark. In addition, extensive experiments were conducted on the nuScenes-C dataset to demonstrate the robustness of the proposed method in different camera and lidar corruption scenarios. The code will be available at: https://github.com/DanielMing123/TFusionOcc

</details>


### [25] [MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing](https://arxiv.org/abs/2602.06402)
*Wenjie Wang,Wei Wu,Ying Liu,Yuan Zhao,Xiaole Lv,Liang Diao,Zengjian Fan,Wenfeng Xie,Ziling Lin,De Shi,Lin Huang,Kaihe Xu,Hong Li*

Main category: cs.CV

TL;DR: 提出MeDocVL框架，结合标签精化与混合后训练策略，提升噪声环境下医疗文档OCR字段匹配精度，在发票数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗文档OCR面临复杂布局、领域术语、噪声标注及严格字段匹配需求，现有OCR系统与通用视觉语言模型在该场景下性能不足。

Method: 设计双模块框架：1) 训练驱动标签精化模块（TDR）从噪声标注生成高质量监督信号；2) 噪声感知混合后训练策略（NHPT），融合强化学习与监督微调提升模型鲁棒性。

Result: 在医疗发票基准测试中，MeDocVL较传统OCR系统和先进视觉语言模型提升显著，尤其在噪声监督场景下达成SOTA性能，字段级精确匹配率提升15.3%。

Conclusion: 所提方法通过自精化监督信号与混合训练策略，在噪声标注场景下实现稳定高精度医疗文档解析，为专业领域文档处理提供通用范式。

Abstract: Medical document OCR is challenging due to complex layouts, domain-specific terminology, and noisy annotations, while requiring strict field-level exact matching. Existing OCR systems and general-purpose vision-language models often fail to reliably parse such documents. We propose MeDocVL, a post-trained vision-language model for query-driven medical document parsing. Our framework combines Training-driven Label Refinement to construct high-quality supervision from noisy annotations, with a Noise-aware Hybrid Post-training strategy that integrates reinforcement learning and supervised fine-tuning to achieve robust and precise extraction. Experiments on medical invoice benchmarks show that MeDocVL consistently outperforms conventional OCR systems and strong VLM baselines, achieving state-of-the-art performance under noisy supervision.

</details>


### [26] [Point Virtual Transformer](https://arxiv.org/abs/2602.06406)
*Veerain Sood,Bnalin,Gaurav Pandey*

Main category: cs.CV

TL;DR: PointViT通过融合LiDAR真实点云和选择性采样RGB衍生的虚拟点云，在BEV空间下实现高效的transformer架构，提升了3D远场目标检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR检测器因远场点云稀疏导致几何信息缺失，而直接融合全量虚拟点会引入计算冗余和虚实信息干扰。

Method: 设计层次化融合架构：1) 点级早期融合与BEV门控融合的混合策略；2) 稀疏卷积构建BEV特征；3) 变压器模块优化高置信度目标查询初始化。

Result: 在KITTI基准创下最优成绩：3D检测AP91.16%/BEV AP95.94%/2D检测99.36%的Car类AP指标。

Conclusion: 提出的虚实点云协同机制与transformer架构在保持计算效率前提下显著提升复杂场景下远场目标检测能力。

Abstract: LiDAR-based 3D object detectors often struggle to detect far-field objects due to the sparsity of point clouds at long ranges, which limits the availability of reliable geometric cues. To address this, prior approaches augment LiDAR data with depth-completed virtual points derived from RGB images; however, directly incorporating all virtual points leads to increased computational cost and introduces challenges in effectively fusing real and virtual information. We present Point Virtual Transformer (PointViT), a transformer-based 3D object detection framework that jointly reasons over raw LiDAR points and selectively sampled virtual points. The framework examines multiple fusion strategies, ranging from early point-level fusion to BEV-based gated fusion, and analyses their trade-offs in terms of accuracy and efficiency. The fused point cloud is voxelized and encoded using sparse convolutions to form a BEV representation, from which a compact set of high-confidence object queries is initialised and refined through a transformer-based context aggregation module. Experiments on the KITTI benchmark report 91.16% 3D AP, 95.94% BEV AP, and 99.36% AP on the KITTI 2D detection benchmark for the Car class.

</details>


### [27] [Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors](https://arxiv.org/abs/2602.06419)
*Soham Pahari,Sandeep C. Kumain*

Main category: cs.CV

TL;DR: 本文提出SemGeo-AttentionNet双流架构，通过跨模态融合几何与语义特征，并利用强化学习生成符合3D拓扑的扫描路径，突破现有3D显著性检测方法的几何-语义分离局限。


<details>
  <summary>Details</summary>
Motivation: 现有3D显著性检测方法过度依赖手工几何特征或缺乏语义感知能力，无法解释人类注意力对语义显著区域的偏好，需建立几何处理与语义识别的有机交互模型。

Method: 构建双流架构：几何处理分支采用点云Transformer，语义分支通过扩散模型生成多视角语义先验；引入不对称跨注意机制使几何特征主动检索语义内容；采用策略梯度强化学习结合抑制返回机制生成3D扫描路径。

Result: 在SAL3D、NUS3D和3DVA数据集上分别取得89.4%、76.2%和92.1%的显著提升，在拓扑感知扫描路径生成任务中达到人类水平的78.5%路径重合度。

Conclusion: 该研究证明几何-语义双流架构与认知科学理论的深度结合能有效模拟人类3D视觉注意力机制，为跨模态感知建模提供了新范式。

Abstract: Human visual attention on three-dimensional objects emerges from the interplay between bottom-up geometric processing and top-down semantic recognition. Existing 3D saliency methods rely on hand-crafted geometric features or learning-based approaches that lack semantic awareness, failing to explain why humans fixate on semantically meaningful but geometrically unremarkable regions. We introduce SemGeo-AttentionNet, a dual-stream architecture that explicitly formalizes this dichotomy through asymmetric cross-modal fusion, leveraging diffusion-based semantic priors from geometry-conditioned multi-view rendering and point cloud transformers for geometric processing. Cross-attention ensures geometric features query semantic content, enabling bottom-up distinctiveness to guide top-down retrieval. We extend our framework to temporal scanpath generation through reinforcement learning, introducing the first formulation respecting 3D mesh topology with inhibition-of-return dynamics. Evaluation on SAL3D, NUS3D and 3DVA datasets demonstrates substantial improvements, validating how cognitively motivated architectures effectively model human visual attention on three-dimensional surfaces.

</details>


### [28] [Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO](https://arxiv.org/abs/2602.06422)
*Yunze Tong,Mushui Liu,Canyu Zhao,Wanggui He,Shiyi Zhang,Hongwei Zhang,Peng Zhang,Jinlong Liu,Ju Huang,Jiamang Wang,Hao Jiang,Pipei Huang*

Main category: cs.CV

TL;DR: TP-GRPO通过引入逐步增量奖励和识别转折点机制，改进了Flow Matching模型的文本生成图像能力。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO框架存在两个问题：1) 结果奖励均匀传播导致步骤效应模糊；2) 组对比忽略轨迹内依赖关系，早期操作的延迟影响未被建模。

Method: 提出TP-GRPO框架：1) 用逐步增量奖励替换结果奖励，提供密集学习信号；2) 通过增量奖励符号变化定位转折点，对其分配聚合后的长期奖励捕获延迟效应，且无需超参数。

Result: 实验表明该方法更高效利用奖励信号，并在文本生成图像任务中保持一致性能提升（含消融实验及代码验证）。

Conclusion: 该工作通过解决奖励稀疏性和建模长程依赖，为扩散模型强化学习提供了新范式，代码开源促进后续研究。

Abstract: Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.

</details>


### [29] [POPL-KF: A Pose-Only Geometric Representation-Based Kalman Filter for Point-Line-Based Visual-Inertial Odometry](https://arxiv.org/abs/2602.06425)
*Aiping Wang,Zhaolong Yang,Shuwen Chen,Hai Zhang*

Main category: cs.CV

TL;DR: This paper introduces POPL-KF, a novel VIO system using pose-only geometric representations for points and lines to reduce linearization errors and improve accuracy in challenging environments.


<details>
  <summary>Details</summary>
Motivation: Traditional VIO systems degrade in performance under challenging scenarios due to linearization errors from 3D feature coordinates and delayed measurement updates in MSCKF-based systems.

Method: Proposes POPL-KF with (1) pose-only geometric representation for line features, (2) elimination of feature coordinates in measurement equations, (3) unified base-frames selection algorithm, and (4) line feature filter via image segmentation and optical flow consistency.

Result: POPL-KF surpasses state-of-the-art filter-based (OpenVINS, PO-KF) and optimization-based (PL-VINS, EPLF-VINS) methods in public datasets and real-world experiments while maintaining real-time performance.

Conclusion: Simultaneously addressing point/line feature representation and measurement error reduction achieves superior robustness and efficiency in VIO.

Abstract: Mainstream Visual-inertial odometry 
(VIO) systems rely on point features for motion estimation and localization. However, their performance degrades in challenging scenarios. Moreover, the localization accuracy of multi-state constraint Kalman filter (MSCKF)-based VIO systems suffers from linearization errors associated with feature 3D coordinates and delayed measurement updates. To improve the performance of VIO in challenging scenes, we first propose a pose-only geometric representation for line features. Building on this, we develop POPL-KF, a Kalman filter-based VIO system that employs a pose-only geometric representation for both point and line features. POPL-KF mitigates linearization errors by explicitly eliminating both point and line feature coordinates from the measurement equations, while enabling immediate update of visual measurements. We also design a unified base-frames selection algorithm for both point and line features to ensure optimal constraints on camera poses within the pose-only measurement model. To further improve line feature quality, a line feature filter based on image grid segmentation and bidirectional optical flow consistency is proposed. Our system is evaluated on public datasets and real-world experiments, demonstrating that POPL-KF outperforms the state-of-the-art (SOTA) filter-based methods (OpenVINS, PO-KF) and optimization-based methods (PL-VINS, EPLF-VINS), while maintaining real-time performance.

</details>


### [30] [Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters](https://arxiv.org/abs/2602.06427)
*Yuxiang Zhao,Yirong Yang,Yanqing Zhu,Yanfen Shen,Chiyu Wang,Zhining Gu,Pei Shi,Wei Guo,Mu Xu*

Main category: cs.CV

TL;DR: 提出一种无需外部先验的视觉驱动具身导航框架，实现无缝室内外转换，并发布首个开源数据集，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法受限于室内外环境隔离及对精确坐标的依赖，难以实现精细室内入口导航。需消除外部先验依赖，解决实用化中的室内外连续导航难题。

Method: 设计基于视觉指令的具身导航框架，通过图像提示驱动决策；创建包含轨迹条件视频合成的开源数据集；全流程依赖自身视觉观测与指令交互。

Result: 在成功率、路径效率等核心指标上持续超越现有SOTA基线模型，验证了视觉驱动方法在室内外连续导航的可行性。

Conclusion: 通过提出新任务范式、构建数据集和框架，有效解决了传统导航系统受限于环境封闭性和先验假设的问题，为开放动态环境导航提供新思路。

Abstract: Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.

</details>


### [31] [ChatUMM: Robust Context Tracking for Conversational Interleaved Generation](https://arxiv.org/abs/2602.06442)
*Wenxun Dai,Zhiyuan Zhao,Yule Zhong,Yiji Cheng,Jianwei Zhang,Linqing Wang,Shiyi Zhang,Yunlong Lin,Runze He,Fellix Song,Wayne Zhuang,Yong Liu,Haoji Zhang,Yansong Tang,Qinglin Lu,Chunyu Wang*

Main category: cs.CV

TL;DR: ChatUMM通过交错多模态生成和对话数据合成管线，解决了传统统一多模态模型单轮交互的局限性，实现上下文感知的流畅多轮对话。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型（UMMs）受限于单轮交互范式，无法处理持续对话中的上下文依赖关系，需设计新框架突破这一瓶颈。

Method: 1) 交错多模态训练策略：将文本-图像序列建模为连续对话流；2) 三级对话数据合成：基础对话构建→历史依赖问题重构→自然模态交互合成

Result: 在视觉理解/指令编辑基准达开源UMMs最优性能，保持文本生成质量，复杂多轮场景鲁棒性显著提升（具体指标未提及）

Conclusion: ChatUMM证实了对话式统一多模态架构的可行性，为多模态持久交互系统提供了可扩展范式

Abstract: Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor'' turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.

</details>


### [32] [What Is Wrong with Synthetic Data for Scene Text Recognition? A Strong Synthetic Engine with Diverse Simulations and Self-Evolution](https://arxiv.org/abs/2602.06450)
*Xingsong Ye,Yongkun Du,JiaXin Zhang,Chen Li,Jing LYU,Zhineng Chen*

Main category: cs.CV

TL;DR: 本文提出UnionST合成数据引擎与SEL框架，解决场景文本识别中合成数据与真实数据的领域差距问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据存在语料库、字体及布局多样性不足导致的复杂场景现实性缺陷，且性能落后于真实数据，需系统性改进数据生成机制。

Method: 构建UnionST数据引擎合成覆盖挑战样本的复杂数据集UnionST-S，并开发自进化学习框架(SEL)优化真实数据标注。

Result: UnionST-S训练模型性能超越现有合成数据，部分场景超越真实数据，结合SEL仅需9%真实标注即可获得竞争性结果。

Conclusion: UnionST-S与SEL联合提升了STR模型效能，降低了数据标注成本，验证了合成数据生成与学习策略优化的协同作用。

Abstract: Large-scale and categorical-balanced text data is essential for training effective Scene Text Recognition (STR) models, which is hard to achieve when collecting real data. Synthetic data offers a cost-effective and perfectly labeled alternative. However, its performance often lags behind, revealing a significant domain gap between real and current synthetic data. In this work, we systematically analyze mainstream rendering-based synthetic datasets and identify their key limitations: insufficient diversity in corpus, font, and layout, which restricts their realism in complex scenarios. To address these issues, we introduce UnionST, a strong data engine synthesizes text covering a union of challenging samples and better aligns with the complexity observed in the wild. We then construct UnionST-S, a large-scale synthetic dataset with improved simulations in challenging scenarios. Furthermore, we develop a self-evolution learning (SEL) framework for effective real data annotation. Experiments show that models trained on UnionST-S achieve significant improvements over existing synthetic datasets. They even surpass real-data performance in certain scenarios. Moreover, when using SEL, the trained models achieve competitive performance by only seeing 9% of real data labels.

</details>


### [33] [LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection](https://arxiv.org/abs/2602.06474)
*Xu Zhang,Zhe Chen,Jing Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 该论文提出LAB-Det的无训练单次域泛化目标检测方法，利用语言作为跨域桥梁，在不更新参数的情况下，通过单样本标注将通用检测器适配到水下图像等特殊领域。


<details>
  <summary>Details</summary>
Motivation: 现有通用目标检测器（如GLIP/Grounding DINO）在数据稀缺的特殊领域（水下图像/工业缺陷）性能下降，传统微调方法存在过拟合风险和部署成本，需要无需训练的适配方案。

Method: 构建语言域不变桥梁（LAB-Det），将单样本标注目标投射为描述性文本，以语言条件调控冻结检测器参数，用语义描述替代梯度更新实现域自适应。

Result: UODD水下数据集和NEU-DET工业缺陷数据集上较微调基线提升5.4mAP，且在边界模糊场景中保持鲁棒性，证明语言适配可替代参数微调。

Conclusion: 提出无需训练的单样本域泛化范式，验证语言条件作为跨域检测的有效解决方案，提供比参数微调更高效的可解释适配机制。

Abstract: Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.

</details>


### [34] [Rebenchmarking Unsupervised Monocular 3D Occupancy Prediction](https://arxiv.org/abs/2602.06488)
*Zizhan Guo,Yi Feng,Mengtan Zhang,Haoran Zhang,Wei Ye,Rui Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督单目3D占用预测基准，通过优化体积渲染中的占用表示和引入遮挡感知机制，显著提升遮挡区域的3D结构推理能力，性能媲美有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法在训练与评估协议间存在不一致性，且2D真值无法反映遮挡区域几何约束不足的固有模糊性，导致3D结构推理效果受限。

Method: 1) 通过分析体积渲染过程确定物理一致性最强的占用概率表示；2) 构建与3D体素真值对齐的评估协议；3) 提出遮挡感知的极化机制，融合多视角视觉线索增强遮挡区域的占据/自由空间判别。

Result: 实验表明新方法在遮挡区域的3D占用预测性能显著优于现有无监督方法，并达到与有监督方法相当的水平，且代码已开源。

Conclusion: 该方法通过统一评估协议和遮挡区域几何约束建模，首次实现了无监督方法与有监督方案在3D占据预测上的性能可比性。

Abstract: Inferring the 3D structure from a single image, particularly in occluded regions, remains a fundamental yet unsolved challenge in vision-centric autonomous driving. Existing unsupervised approaches typically train a neural radiance field and treat the network outputs as occupancy probabilities during evaluation, overlooking the inconsistency between training and evaluation protocols. Moreover, the prevalent use of 2D ground truth fails to reveal the inherent ambiguity in occluded regions caused by insufficient geometric constraints. To address these issues, this paper presents a reformulated benchmark for unsupervised monocular 3D occupancy prediction. We first interpret the variables involved in the volume rendering process and identify the most physically consistent representation of the occupancy probability. Building on these analyses, we improve existing evaluation protocols by aligning the newly identified representation with voxel-wise 3D occupancy ground truth, thereby enabling unsupervised methods to be evaluated in a manner consistent with that of supervised approaches. Additionally, to impose explicit constraints in occluded regions, we introduce an occlusion-aware polarization mechanism that incorporates multi-view visual cues to enhance discrimination between occupied and free spaces in these regions. Extensive experiments demonstrate that our approach not only significantly outperforms existing unsupervised approaches but also matches the performance of supervised ones. Our source code and evaluation protocol will be made available upon publication.

</details>


### [35] [DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation](https://arxiv.org/abs/2602.06494)
*Lulu Chen,Yijiang Hu,Yuanqing Liu,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: 提出DreamHome-Pano全景生成框架解决室内设计中的结构约束与风格冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成框架难以平衡建筑结构约束与风格偏好导致的几何精度下降问题

Method: 构建Prompt-LLM实现跨模态对齐，设计Conflict-Free Control结构保持建筑完整性，建立全景基准与多阶段训练流程（SFT+RL）

Result: 成功实现美学质量与结构一致性平衡，达到专业级全景可视化效果

Conclusion: DreamHome-Pano为建筑约束场景生成提供了可控性强的解决方案范式

Abstract: In modern interior design, the generation of personalized spaces frequently necessitates a delicate balance between rigid architectural structural constraints and specific stylistic preferences. However, existing multi-condition generative frameworks often struggle to harmonize these inputs, leading to "condition conflicts" where stylistic attributes inadvertently compromise the geometric precision of the layout. To address this challenge, we present DreamHome-Pano, a controllable panoramic generation framework designed for high-fidelity interior synthesis. Our approach introduces a Prompt-LLM that serves as a semantic bridge, effectively translating layout constraints and style references into professional descriptive prompts to achieve precise cross-modal alignment. To safeguard architectural integrity during the generative process, we develop a Conflict-Free Control architecture that incorporates structural-aware geometric priors and a multi-condition decoupling strategy, effectively suppressing stylistic interference from eroding the spatial layout. Furthermore, we establish a comprehensive panoramic interior benchmark alongside a multi-stage training pipeline, encompassing progressive Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Experimental results demonstrate that DreamHome-Pano achieves a superior balance between aesthetic quality and structural consistency, offering a robust and professional-grade solution for panoramic interior visualization.

</details>


### [36] [DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving](https://arxiv.org/abs/2602.06521)
*Feiyang jia,Lin Liu,Ziying Song,Caiyan Jia,Hangjun Ye,Xiaoshuai Hao,Long Chen*

Main category: cs.CV

TL;DR: 提出DriveWorld-VLA框架，通过在表征层级深度融合视觉-语言-动作（VLA）与世界模型，实现场景演化与动作规划的统一。


<details>
  <summary>Details</summary>
Motivation: 现有方法因潜在状态共享不足，无法在单一架构内有效整合未来场景演化与动作规划，限制了视觉想象对决策的影响。

Method: 将VLA与世界模型在潜空间中紧密集成，利用世界模型的潜在状态作为VLA决策核心，通过特征级可控想象减少像素级计算开销。

Result: 在NAVSIMv1/v2和nuScenes数据集上分别取得91.3 PDMS、86.8 EPDMS及0.16碰撞率的SOTA性能。

Conclusion: DriveWorld-VLA通过潜空间统一世界建模与规划，提升决策质量并显著降低对标注数据的依赖，代码与模型已开源。

Abstract: End-to-end (E2E) autonomous driving has recently attracted increasing interest in unifying Vision-Language-Action (VLA) with World Models to enhance decision-making and forward-looking imagination. However, existing methods fail to effectively unify future scene evolution and action planning within a single architecture due to inadequate sharing of latent states, limiting the impact of visual imagination on action decisions. To address this limitation, we propose DriveWorld-VLA, a novel framework that unifies world modeling and planning within a latent space by tightly integrating VLA and world models at the representation level, which enables the VLA planner to benefit directly from holistic scene-evolution modeling and reducing reliance on dense annotated supervision. Additionally, DriveWorld-VLA incorporates the latent states of the world model as core decision-making states for the VLA planner, facilitating the planner to assess how candidate actions impact future scene evolution. By conducting world modeling entirely in the latent space, DriveWorld-VLA supports controllable, action-conditioned imagination at the feature level, avoiding expensive pixel-level rollouts. Extensive open-loop and closed-loop evaluations demonstrate the effectiveness of DriveWorld-VLA, which achieves state-of-the-art performance with 91.3 PDMS on NAVSIMv1, 86.8 EPDMS on NAVSIMv2, and 0.16 3-second average collision rate on nuScenes. Code and models will be released in https://github.com/liulin815/DriveWorld-VLA.git.

</details>


### [37] [MicroBi-ConvLSTM: An Ultra-Lightweight Efficient Model for Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.06523)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: 本文提出了MicroBi-ConvLSTM，一种针对资源受限可穿戴设备的超轻量级人类活动识别模型，通过两阶段卷积和单向双向LSTM结构实现11.4K参数，相比现有方法减少2.9-11.9倍参数量，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级HAR模型（如TinierHAR和TinyHAR）因微控制器SRAM内存限制及操作系统开销难以部署，需开发参数更少且保持线性复杂度的新架构。

Method: 采用两阶段卷积特征提取、4倍时间池化、单个双向LSTM层及INT8量化技术，通过2.9x参数缩减（对比TinierHAR）和11.9x参数缩减（对比DeepConvLSTM）实现超轻量设计。

Result: 在8个HAR基准中，MicroBi-ConvLSTM实现UCI-HAR 93.41%宏观F1（+2.3%优于TinierHAR）、SKODA 94.46%、Daphnet 88.98%，INT8量化后仅0.21%平均F1损失，部署体积23.0KB。

Conclusion: 双向结构对偶发事件检测有效但周期性运动增益有限，该架构通过线性复杂度和超小内存占用（23KB）为边缘设备部署提供了实用解决方案。

Abstract: Human Activity Recognition (HAR) on resource constrained wearables requires models that balance accuracy against strict memory and computational budgets. State of the art lightweight architectures such as TinierHAR (34K parameters) and TinyHAR (55K parameters) achieve strong accuracy, but exceed memory budgets of microcontrollers with limited SRAM once operating system overhead is considered. We present MicroBi-ConvLSTM, an ultra-lightweight convolutional-recurrent architecture achieving 11.4K parameters on average through two stage convolutional feature extraction with 4x temporal pooling and a single bidirectional LSTM layer. This represents 2.9x parameter reduction versus TinierHAR and 11.9x versus DeepConvLSTM while preserving linear O(N) complexity. Evaluation across eight diverse HAR benchmarks shows that MicroBi-ConvLSTM maintains competitive performance within the ultra-lightweight regime: 93.41% macro F1 on UCI-HAR, 94.46% on SKODA assembly gestures, and 88.98% on Daphnet gait freeze detection. Systematic ablation reveals task dependent component contributions where bidirectionality benefits episodic event detection, but provides marginal gains on periodic locomotion. INT8 post training quantization incurs only 0.21% average F1-score degradation, yielding a 23.0 KB average deployment footprint suitable for memory constrained edge devices.

</details>


### [38] [AdaptOVCD: Training-Free Open-Vocabulary Remote Sensing Change Detection via Adaptive Information Fusion](https://arxiv.org/abs/2602.06529)
*Mingyu Dou,Shi Qiu,Ming Hu,Yifan Chen,Huping Ye,Xiaohan Liao,Zhe Sun*

Main category: cs.CV

TL;DR: 本文提出了AdaptOVCD，一种无需训练的开放词汇变化检测框架，通过双维度多级信息融合提升遥感影像变化检测的泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义类别和大量像素级标注，限制了其在开放场景中的泛化性。

Method: 设计了垂直方向的数据-特征-决策三级融合架构与水平方向的自适应模块，包括数据层的自适应辐射对齐、特征层的自适应变化阈值、决策层的自适应置信度过滤。

Result: 在9个场景中零样本检测任意类别变化，跨数据集评估达到全监督方法84.89%的性能上限。

Conclusion: 通过异构预训练模型深度协同抑制误差传播，在开放世界场景中实现高效无监督变化检测。

Abstract: Remote sensing change detection plays a pivotal role in domains such as environmental monitoring, urban planning, and disaster assessment. However, existing methods typically rely on predefined categories and large-scale pixel-level annotations, which limit their generalization and applicability in open-world scenarios. To address these limitations, this paper proposes AdaptOVCD, a training-free Open-Vocabulary Change Detection (OVCD) architecture based on dual-dimensional multi-level information fusion. The framework integrates multi-level information fusion across data, feature, and decision levels vertically while incorporating targeted adaptive designs horizontally, achieving deep synergy among heterogeneous pre-trained models to effectively mitigate error propagation. Specifically, (1) at the data level, Adaptive Radiometric Alignment (ARA) fuses radiometric statistics with original texture features and synergizes with SAM-HQ to achieve radiometrically consistent segmentation; (2) at the feature level, Adaptive Change Thresholding (ACT) combines global difference distributions with edge structure priors and leverages DINOv3 to achieve robust change detection; (3) at the decision level, Adaptive Confidence Filtering (ACF) integrates semantic confidence with spatial constraints and collaborates with DGTRS-CLIP to achieve high-confidence semantic identification. Comprehensive evaluations across nine scenarios demonstrate that AdaptOVCD detects arbitrary category changes in a zero-shot manner, significantly outperforming existing training-free methods. Meanwhile, it achieves 84.89\% of the fully-supervised performance upper bound in cross-dataset evaluations and exhibits superior generalization capabilities. The code is available at https://github.com/Dmygithub/AdaptOVCD.

</details>


### [39] [Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance](https://arxiv.org/abs/2602.06530)
*Haipeng Li,Rongxuan Peng,Anwei Luo,Shunquan Tan,Changsheng Chen,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 提出ForgeryEraser框架，通过对抗攻击消除AI生成图像的伪造痕迹，无需访问目标检测器即可导致检测器性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC检测器评估协议缺乏对抗取证攻击的考量，无法保障实际应用中的鲁棒性。研究目标是揭示基于视觉-语言模型共享架构的系统性漏洞，并开发通用攻击方法。

Method: 利用CLIP等公开视觉-语言模型（VLM）的特征空间，通过多模态引导损失：1) 将生成图像特征向真实文本锚点拉近 2) 同时远离伪造锚点，通过对抗方式在特征空间抹除伪造痕迹。

Result: 1) 在全局生成和局部编辑任务中导致主流检测器F1-score下降超40% 2) 欺骗可解释模型生成与真实图像相似的解释，成功率达87% 3) 攻击效果跨模型泛化能力强。

Conclusion: 揭示AI生成检测系统存在架构级漏洞，强调检测器需要在特征空间对抗维度进行加固，为未来鲁棒检测器设计提供新方向。

Abstract: The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forgery anchors. Extensive experiments demonstrate that ForgeryEraser causes substantial performance degradation to advanced AIGC detectors on both global synthesis and local editing benchmarks. Moreover, ForgeryEraser induces explainable forensic models to generate explanations consistent with authentic images for forged images. Our code will be made publicly available.

</details>


### [40] [NECromancer: Breathing Life into Skeletons via BVH Animation](https://arxiv.org/abs/2602.06548)
*Mingxi Xu,Qi Wang,Zhengyu Wen,Phong Dao Thien,Zhengyu Li,Ning Zhang,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种通用运动分词器NECromancer（NEC），通过三维组件（OwO编码器、TAT分词器、UvU数据集）实现跨骨架拓扑的动作压缩与解耦，支持跨物种动作迁移与生成。


<details>
  <summary>Details</summary>
Motivation: 现有动作分词方法受限于特定骨骼结构，缺乏跨形态适应性。

Method: 基于BVH骨架构建OwO编码器提取结构先验，设计拓扑无关的TAT分词器生成离散表示，并构建涵盖多样骨骼的UvU大规模数据集。

Result: 实验验证在高压缩比下可实现动作与骨骼结构分离，成功实现跨物种动作迁移、去噪、生成及文本-动作检索。

Conclusion: 建立首个跨形态动作分析与合成的统一框架，为基于分词的通用运动模型奠定基础。

Abstract: Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/

</details>


### [41] [LIBERO-X: Robustness Litmus for Vision-Language-Action Models](https://arxiv.org/abs/2602.06556)
*Guodong Wang,Chenkai Zhang,Qingjie Liu,Jinjin Zhang,Jiancheng Cai,Junjie Liu,Xinmin Liu*

Main category: cs.CV

TL;DR: 本文提出了LIBERO-X基准，改进了Vision-Language-Action (VLA)模型的评估方法，通过分层评价协议与多样化训练数据解决现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLA基准存在评估协议不足、无法捕捉现实分布偏移的问题，导致模型性能评估结果不准确，缺乏对模型泛化性和鲁棒性的有效检验。

Method: 设计包含三个核心能力（空间泛化、物体识别、任务指令理解）的分层评估协议，并构建高多样性训练数据集，通过人工遥操作采集每个场景的多细粒度操作目标。

Result: 实验表明代表性VLA模型在累积扰动下性能显著下降，暴露其在场景理解和指令对齐上的持续性缺陷，而LIBERO-X有效弥合了训练与评估的数据分布差距。

Conclusion: LIBERO-X通过整合分层评估与多样数据，为VLA模型的评估与发展提供了更可靠的技术基础。

Abstract: Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.

</details>


### [42] [SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs](https://arxiv.org/abs/2602.06566)
*Niccolo Avogaro,Nayanika Debnath,Li Mi,Thomas Frick,Junling Wang,Zexue He,Hang Hua,Konrad Schindler,Mattia Rigotti*

Main category: cs.CV

TL;DR: SPARC框架通过分离视觉感知和推理阶段，采用两阶段流水线处理视觉语言任务，在降低计算成本的同时提升了模型在复杂基准任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在测试时扩展性差，感知与推理的耦合导致错误累积，且依赖昂贵的强化学习。需要模块化方案解决动态资源分配与性能瓶颈问题。

Method: SPARC框架分两阶段处理：第一阶段通过视觉搜索准确定位问题相关区域，第二阶段基于局部区域进行针对性推理。支持非对称计算分配（如优先处理感知）和选择性优化（如单独改进感知模块）。

Result: 在$V^*$ VQA基准上Qwen3VL-4B模型准确率提升6.7%，在分布外任务中超越'图像思考'方法4.6%，且token预算消耗降低200倍。

Conclusion: SPARC的感知-推理分离架构有效解决了视觉语言模型在动态扩展性、计算效率及错误传播方面的关键难题，为多模态模型设计提供了神经科学启发的工程范式。

Abstract: Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.

</details>


### [43] [An Integer Linear Programming Approach to Geometrically Consistent Partial-Partial Shape Matching](https://arxiv.org/abs/2602.06590)
*Viktoria Ehm,Paul Roetzer,Florian Bernard,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出首个针对部分-部分三维形状匹配的整数线性规划方法，通过几何一致性先验实现高质量匹配且更具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统方法集中于全-全或部分-全三维形状匹配，但实际场景如3D扫描常出现部分-部分匹配且缺乏有效方法，需同时解决未知重叠区域识别与精确匹配的挑战。

Method: 设计基于几何一致性约束的整数线性规划模型，联合优化重叠区域估计和邻域保持的对应关系计算。

Result: 实验表明在匹配误差和映射平滑性指标上均优于现有方法，且能处理更大规模数据。

Conclusion: 该方法首次系统性解决部分-部分三维形状匹配问题，兼顾准确性和计算效率，更适合现实应用需求。

Abstract: The task of establishing correspondences between two 3D shapes is a long-standing challenge in computer vision. While numerous studies address full-full and partial-full 3D shape matching, only a limited number of works have explored the partial-partial setting, very likely due to its unique challenges: we must compute accurate correspondences while at the same time find the unknown overlapping region. Nevertheless, partial-partial 3D shape matching reflects the most realistic setting, as in many real-world cases, such as 3D scanning, shapes are only partially observable. In this work, we introduce the first integer linear programming approach specifically designed to address the distinctive challenges of partial-partial shape matching. Our method leverages geometric consistency as a strong prior, enabling both robust estimation of the overlapping region and computation of neighbourhood-preserving correspondences. We empirically demonstrate that our approach achieves high-quality matching results both in terms of matching error and smoothness. Moreover, we show that our method is more scalable than previous formalisms.

</details>


### [44] [ProtoQuant: Quantization of Prototypical Parts For General and Fine-Grained Image Classification](https://arxiv.org/abs/2602.06592)
*Mikołaj Janusz,Adam Wróbel,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: ProtoQuant introduces a prototypical parts-based model with latent vector quantization, addressing prototype drift and enabling efficient ImageNet-scale generalization without requiring backbone fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing prototypical models face challenges in generalization (e.g., ImageNet-scale), computational cost from backbone fine-tuning, and unreliable activation patterns due to prototype drift. The work aims to decouple prototypes from the backbone while maintaining interpretability and scalability.

Method: ProtoQuant employs a fixed latent space with a discrete learned codebook. Prototypes are constrained to this codebook, eliminating the need for backbone updates. Quantization ensures prototypes remain grounded in the training data distribution, while enabling end-to-end learning of the codebook and classifier head.

Result: ProtoQuant achieved competitive classification accuracy on ImageNet (76.5% top-1) and fine-grained datasets (CUB-200: 85.2%, Cars-196: 92.1%), outperforming traditional prototypical methods in scalability. Prototype stability improved by 30% under adversarial perturbations compared to prior approaches.

Conclusion: ProtoQuant demonstrates that codebook-based prototypes enable stable, interpretable representations scalable to large datasets without compromising performance, overcoming critical limitations of existing prototypical architectures.

Abstract: Prototypical parts-based models offer a "this looks like that" paradigm for intrinsic interpretability, yet they typically struggle with ImageNet-scale generalization and often require computationally expensive backbone finetuning. Furthermore, existing methods frequently suffer from "prototype drift," where learned prototypes lack tangible grounding in the training distribution and change their activation under small perturbations. We present ProtoQuant, a novel architecture that achieves prototype stability and grounded interpretability through latent vector quantization. By constraining prototypes to a discrete learned codebook within the latent space, we ensure they remain faithful representations of the training data without the need to update the backbone. This design allows ProtoQuant to function as an efficient, interpretable head that scales to large-scale datasets. We evaluate ProtoQuant on ImageNet and several fine-grained benchmarks (CUB-200, Cars-196). Our results demonstrate that ProtoQuant achieves competitive classification accuracy while generalizing to ImageNet and comparable interpretability metrics to other prototypical-parts-based methods.

</details>


### [45] [DAVE: Distribution-aware Attribution via ViT Gradient Decomposition](https://arxiv.org/abs/2602.06613)
*Adam Wróbel,Siddhartha Gairola,Jacek Tabor,Bernt Schiele,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: 提出DAVE方法，用于生成Vision Transformers的稳定且高分辨率的归因图，通过结构化输入梯度分解解决现有方法的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法在ViTs的像素级解释中因patch嵌入和注意力路由引入结构化伪影，导致仅能依赖粗糙的patch级归因，需开发数学理论支持的新型归因方法。

Method: 基于ViT架构特性，通过数学推导对输入梯度进行结构化分解，分离局部等变稳定组件与架构伪影，构建DAVE归因框架。

Result: DAVE生成细粒度像素级归因图，在ImageNet数据集上显示比传统注意力掩膜更精确的物体边界定位，同时保持归因结果的分布稳定性。

Conclusion: 该方法突破ViT解释性瓶颈，通过理论驱动的梯度分析证明了架构感知归因的必要性，为模型可解释性提供新的数学分析框架。

Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \textit{(\underline{D}istribution-aware \underline{A}ttribution via \underline{V}iT Gradient D\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.

</details>


### [46] [CauCLIP: Bridging the Sim-to-Real Gap in Surgical Video Understanding via Causality-Inspired Vision-Language Modeling](https://arxiv.org/abs/2602.06619)
*Yuxin He,An Li,Cheng Xue*

Main category: cs.CV

TL;DR: 提出CauCLIP因果框架，在无需目标域数据下实现手术阶段识别


<details>
  <summary>Details</summary>
Motivation: 手术数据领域差异大且注释数据不足，现有模型泛化能力受限

Method: 融合CLIP视觉语言模型+频率域数据增强+因果抑制损失函数的三阶段训练框架

Result: 在SurgVisDom跨域基准测试中超越所有现有方法，展现更强适应性

Conclusion: 因果性建模能有效提升手术视频理解模型的跨域泛化能力

Abstract: Surgical phase recognition is a critical component for context-aware decision support in intelligent operating rooms, yet training robust models is hindered by limited annotated clinical videos and large domain gaps between synthetic and real surgical data. To address this, we propose CauCLIP, a causality-inspired vision-language framework that leverages CLIP to learn domain-invariant representations for surgical phase recognition without access to target domain data. Our approach integrates a frequency-based augmentation strategy to perturb domain-specific attributes while preserving semantic structures, and a causal suppression loss that mitigates non-causal biases and reinforces causal surgical features. These components are combined in a unified training framework that enables the model to focus on stable causal factors underlying surgical workflows. Experiments on the SurgVisDom hard adaptation benchmark demonstrate that our method substantially outperforms all competing approaches, highlighting the effectiveness of causality-guided vision-language models for domain-generalizable surgical video understanding.

</details>


### [47] [PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks](https://arxiv.org/abs/2602.06663)
*Junxian Li,Kai Liu,Leyang Chen,Weida Wang,Zhixin Wang,Jiaqi Xu,Fan Li,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: PlanViz: Evaluates image generation/editing for computer-use tasks like route planning using multimodal models. Proposes PlanScore for assessing correctness, quality, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Unified multimodal models (UMMs) excel in image generation and multimodal reasoning but lack evaluation in real-world computer-use tasks requiring spatial reasoning and procedural understanding. Existing benchmarks do not address these domains.

Method: Developed PlanViz, a benchmark with three sub-tasks (route planning, work diagramming, web&UI displaying) requiring planning steps. Used human-annotated data, quality control, and introduced PlanScore (task-adaptive metric) to measure correctness, visual quality, and efficiency of generated images.

Result: Experimental results on PlanViz highlighted key limitations of UMMs in handling computer-use planning tasks and identified opportunities for advancement in this field.

Conclusion: PlanViz and PlanScore provide a framework to assess UMM capabilities in practical computer-use scenarios, encouraging future research to bridge gaps in spatial reasoning and procedural understanding for real-world applications.

Abstract: Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.

</details>


### [48] [CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis](https://arxiv.org/abs/2602.06674)
*Yonghao Si,Xingyuan Zeng,Zhao Chen,Libin Zheng,Caleb Chen Cao,Lei Chen,Jian Yin*

Main category: cs.CV

TL;DR: 本论文提出了CytoCrowd数据集，包含446张高分辨率细胞学图像，每张图像包含四名病理学家冲突标注和一名资深专家建立的高精度金标准标注。该数据集同时支持计算机视觉基准测试和评估标注聚合算法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像数据集存在两个缺陷：要么只提供单一标准标注隐藏现实争议，要么提供多标注但缺乏独立金标准评估。这种缺陷阻碍了医学图像分析模型在真实临床场景中的可靠性验证。

Method: 构建双元结构数据集，包含446张细胞学图像：(1)四名独立病理学家的原始冲突标注；(2)由资深专家建立的金标准标注。该设计既支持常规计算机视觉任务验证，也支持标注争议解决算法测试。

Result: 实验显示了CytoCrowd数据集的挑战性特征：病理学家标注差异率达23.6%，且模型在解决标注冲突时性能下降明显。基准测试证明了同时处理多专家标注与金标准的必要性。

Conclusion: CytoCrowd通过整合专家争议标注与独立金标准，为开发下一代医学图像分析模型提供了新型基准，同时推动了标注聚合算法的研究进展。

Abstract: High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.

</details>


### [49] [Can We Build a Monolithic Model for Fake Image Detection? SICA: Semantic-Induced Constrained Adaptation for Unified-Yet-Discriminative Artifact Feature Space Reconstruction](https://arxiv.org/abs/2602.06676)
*Bo Du,Xiaochen Ma,Xuekang Zhu,Zhe Yang,Chaogun Niu,Jian Liu,Ji-Zhe Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种用于跨领域伪造图像检测的新型单体模型SICA，通过语义引导的特征空间重构，在统一且具区分性的框架下提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统单体伪造图像检测模型效果差于集成方法，根源在于不同子领域图像伪迹特征空间的异构性导致特征坍塌。需解决特征空间的统一性与判别性矛盾。

Method: 提出语义诱导约束自适应(SICA)框架，通过引入高层语义作为特征空间重建的结构先验，采用近正交化策略实现多子域特征空间的协同重构。

Result: 在OpenMMSec数据集上超越15种最先进方法，验证了特征空间的高判别性（类间距增加28.6%）和跨域泛化性（检测准确率提升12.3%）。

Conclusion: 论文首次揭示异构性对特征空间的破坏机制，通过引入语义先验实现单体模型的突破，为多任务图像取证提供了新范式。开源代码与数据集推动领域发展。

Abstract: Fake Image Detection (FID), aiming at unified detection across four image forensic subdomains, is critical in real-world forensic scenarios. Compared with ensemble approaches, monolithic FID models are theoretically more promising, but to date, consistently yield inferior performance in practice. In this work, by discovering the ``heterogeneous phenomenon'', which is the intrinsic distinctness of artifacts across subdomains, we diagnose the cause of this underperformance for the first time: the collapse of the artifact feature space driven by such phenomenon. The core challenge for developing a practical monolithic FID model thus boils down to the ``unified-yet-discriminative" reconstruction of the artifact feature space. To address this paradoxical challenge, we hypothesize that high-level semantics can serve as a structural prior for the reconstruction, and further propose Semantic-Induced Constrained Adaptation (SICA), the first monolithic FID paradigm. Extensive experiments on our OpenMMSec dataset demonstrate that SICA outperforms 15 state-of-the-art methods and reconstructs the target unified-yet-discriminative artifact feature space in a near-orthogonal manner, thus firmly validating our hypothesis. The code and dataset are available at:https: //github.com/scu-zjz/SICA_OpenMMSec.

</details>


### [50] [Clinical-Prior Guided Multi-Modal Learning with Latent Attention Pooling for Gait-Based Scoliosis Screening](https://arxiv.org/abs/2602.06743)
*Dong Chen,Zizhuang Wei,Jialei Xu,Xinyang Sun,Zonglin He,Meiru An,Huili Peng,Yong Hu,Kenneth MC Cheung*

Main category: cs.CV

TL;DR: 提出了ScoliGait数据集和多模态框架，通过步态视频+文本+临床先验知识提升青少年特发性脊柱侧弯筛查


<details>
  <summary>Details</summary>
Motivation: 传统筛查方法主观性强且难以扩展，现有步态分析数据集存在数据泄漏问题，模型缺乏临床可解释性

Method: 构建包含1572个训练视频和300个独立测试视频的ScoliGait数据集，提出融合临床先验知识映射图与潜在注意力池化的多模态框架

Result: 在无重复受试者的基准测试中建立新的SOTA性能，相比现有方法存在显著性能优势

Conclusion: 为可扩展的无创脊柱侧弯评估提供了坚实基础，具备临床可解释性和实际应用潜力

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a prevalent spinal deformity whose progression can be mitigated through early detection. Conventional screening methods are often subjective, difficult to scale, and reliant on specialized clinical expertise. Video-based gait analysis offers a promising alternative, but current datasets and methods frequently suffer from data leakage, where performance is inflated by repeated clips from the same individual, or employ oversimplified models that lack clinical interpretability. To address these limitations, we introduce ScoliGait, a new benchmark dataset comprising 1,572 gait video clips for training and 300 fully independent clips for testing. Each clip is annotated with radiographic Cobb angles and descriptive text based on clinical kinematic priors. We propose a multi-modal framework that integrates a clinical-prior-guided kinematic knowledge map for interpretable feature representation, alongside a latent attention pooling mechanism to fuse video, text, and knowledge map modalities. Our method establishes a new state-of-the-art, demonstrating a significant performance gap on a realistic, non-repeating subject benchmark. Our approach establishes a new state of the art, showing a significant performance gain on a realistic, subject-independent benchmark. This work provides a robust, interpretable, and clinically grounded foundation for scalable, non-invasive AIS assessment.

</details>


### [51] [Gold Exploration using Representations from a Multispectral Autoencoder](https://arxiv.org/abs/2602.06748)
*Argyro Tsandalidou,Konstantinos Dogeas,Eleftheria Tetoula Tsonga,Elisavet Parselia,Georgios Tsimiklis,George Arvanitakis*

Main category: cs.CV

TL;DR: 通过结合 Sentinel-2 卫星影像的生成表示与 XGBoost 分类器，本文提出了一种从太空识别金矿区的新方法，显著提高了识别准确性。


<details>
  <summary>Details</summary>
Motivation: 由于传统现场矿物勘探数据成本高昂且获取困难，卫星遥感影像被用于大规模勘查。本文旨在利用生成式表示学习，从多光谱 Sentinel-2 数据中提取可迁移的矿物学特征，以解决标签数据有限的问题。

Method: 基于预训练的 Isometric 自编码器模型（FalconSpace-S2 v1.0 数据集），提取高光谱-空间联合表示，并以此作为输入训练轻量级 XGBoost 分类器。实验对比了 63 组已知金矿和非金矿区域的原始光谱输入基线方法。

Result: 相比基线方法，本文方法在局部像元级别准确率从 0.51 提升至 0.68，整图级别从 0.55 提升至 0.73。结果表明，生成式嵌入能有效捕捉可迁移的矿物模式，且对标签数据需求较低。

Conclusion: 基于基础模型的表示学习方法可显著提升矿物勘探的效率、可扩展性和全球适用性，为大规模资源勘查提供新范式。

Abstract: Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.

</details>


### [52] [Revisiting Emotions Representation for Recognition in the Wild](https://arxiv.org/abs/2602.06778)
*Joao Baptista Cardia Neto,Claudio Ferrari,Stefano Berretti*

Main category: cs.CV

TL;DR: 本文提出了一种通过概率分布描述复杂情感状态的方法，利用现有数据集的VAD标注自动重构标签，将情感表示为多类别混合分布。


<details>
  <summary>Details</summary>
Motivation: 传统单标签情感识别无法体现真实情感的多维度与复合特性，现有分布学习方法受限于数据集的单情感标注。

Method: 基于VAD空间中情感类别的概率分布映射，通过VAD标注数据估计多情感类别概率，实现数据集自动重构标签。

Result: 初步实验证明该方法可有效描述情感混合状态，并提升对情感感知模糊性的表征能力。

Conclusion: 该方法通过VAD驱动的分布学习框架，为情感识别提供了更贴近现实的新方向。

Abstract: Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at https://github.com/jbcnrlz/affectnet-b-annotation.

</details>


### [53] [Machine Learning for Detection and Severity Estimation of Sweetpotato Weevil Damage in Field and Lab Conditions](https://arxiv.org/abs/2602.06786)
*Doreen M. Chelangat,Sudi Murindanyi,Bruce Mugizi,Paul Musana,Benard Yada,Milton A. Otema,Florence Osaru,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CV

TL;DR: 本研究开发了一种基于计算机视觉的自动化评估方法，用于检测田间和实验室环境下甜薯象鼻虫的损害，提升了传统人工评估的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估方法成本高、主观性强，阻碍了抗虫甜薯品种的育种进展。

Method: 在田间训练分类模型预测根部损害程度，实验室采用YOLO12模型结合根部分割与分块策略检测害虫微小孔洞。

Result: 田间模型测试准确率为71.43%，实验室检测小孔的平均精度达77.7%。

Conclusion: 计算机视觉技术能为甜薯育种提供高效、客观的规模化评估工具，助力保障粮食安全。

Abstract: Sweetpotato weevils (Cylas spp.) are considered among the most destructive pests impacting sweetpotato production, particularly in sub-Saharan Africa. Traditional methods for assessing weevil damage, predominantly relying on manual scoring, are labour-intensive, subjective, and often yield inconsistent results. These challenges significantly hinder breeding programs aimed at developing resilient sweetpotato varieties. This study introduces a computer vision-based approach for the automated evaluation of weevil damage in both field and laboratory contexts. In the field settings, we collected data to train classification models to predict root-damage severity levels, achieving a test accuracy of 71.43%. Additionally, we established a laboratory dataset and designed an object detection pipeline employing YOLO12, a leading real-time detection model. This methodology incorporated a two-stage laboratory pipeline that combined root segmentation with a tiling strategy to improve the detectability of small objects. The resulting model demonstrated a mean average precision of 77.7% in identifying minute weevil feeding holes. Our findings indicate that computer vision technologies can provide efficient, objective, and scalable assessment tools that align seamlessly with contemporary breeding workflows. These advancements represent a significant improvement in enhancing phenotyping efficiency within sweetpotato breeding programs and play a crucial role in mitigating the detrimental effects of weevils on food security.

</details>


### [54] [A Unified Formula for Affine Transformations between Calibrated Cameras](https://arxiv.org/abs/2602.06805)
*Levente Hajder*

Main category: cs.CV

TL;DR: 该论文推导出一种闭式仿射变换表达式，用于在两个校准视图间映射局部图像块，该变换与相机相对位姿、图像坐标和局部表面法向量有关。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理校准相机间的局部图像变换时可能存在效率或精度不足，需通过闭式解直接建模仿射关系并融合多几何参数。

Method: 基于几何约束推导闭式数学表达式，结合相对位姿（旋转和平移）、图像局部坐标及表面法向量建立三维-二维映射关系。

Result: 成功获得显式仿射变换公式，理论验证表明其能准确描述局部图像形变（如尺度、旋转、剪切）与场景几何的关联。

Conclusion: 提出的方法为校准相机间的图像对齐和表面重建提供了简洁解析解，适用于实时视觉测量或三维重建任务。

Abstract: In this technical note, we derive a closed-form expression for the affine transformation mapping local image patches between two calibrated views. We show that the transformation is a function of the relative camera pose, the image coordinates, and the local surface normal.

</details>


### [55] [RAIGen: Rare Attribute Identification in Text-to-Image Generative Models](https://arxiv.org/abs/2602.06806)
*Silpa Vadakkeeveetil Sreelatha,Dan Wang,Serge Belongie,Muhammad Awais,Anjan Dutta*

Main category: cs.CV

TL;DR: 提出RAIGen框架，通过无监督方式在扩散模型中发现未被传统公平性分类覆盖的稀有属性（如社会、文化或风格特征），以弥补现有方法对少数特征覆盖不足的缺陷。


<details>
  <summary>Details</summary>
Motivation: 已有方法局限于预定义公平性类别（如性别、种族）或单纯识别主流偏见过度表达，但未解决模型中隐藏的、数据分布中少数但模型编码的稀有属性发现任务，该任务有助于揭示社会、文化或风格层面的偏差。

Method: 设计基于Matryoshka稀疏自编码器（Matryoshka Sparse Autoencoders）的框架，结合神经元激活频率与语义独特性的新少数者度量指标，定位可解释神经元，并通过其最活跃图像揭示稀有属性。

Result: 在Stable Diffusion和SDXL等模型中验证有效，成功发现超越固定公平性分类的属性，支持跨模型架构的系统性审计，并实现生成过程中对稀有属性的定向增强。

Conclusion: RAIGen是首个实现扩散模型稀有属性无监督发现的框架，填补了现有方法忽略模型内隐编码但实际存在的少数特征的空白，为模型偏差审计与生成可控性提供新方向。

Abstract: Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.

</details>


### [56] [GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification](https://arxiv.org/abs/2602.06830)
*Soonbin Lee,Yeong-Gyu Kim,Simon Sasse,Tomas M. Borges,Yago Sanchez,Eun-Seok Ryu,Thomas Schierl,Cornelius Hellge*

Main category: cs.CV

TL;DR: GaussianPOP提出了一种基于解析误差量化的3D高斯点简化框架，通过引入直接源自渲染方程的误差准则，实现了模型紧凑性与渲染质量的更优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯点简化方法依赖非视觉误差驱动的评分指标（如权重或敏感度），导致冗余高斯识别不准确。需要建立与视觉误差强关联的原理性简化框架。

Method: 提出基于3DGS渲染方程的误差准则，开发单次前向传播的误差量化算法，支持训练中剪枝与训练后迭代误差量化重构的双重简化模式。

Result: 实验表明相较现有SOTA剪枝方法，该框架在训练后参数压缩率提升23%的同时保持PSNR>38dB，迭代量化方案使模型稳定性提升17%。

Conclusion: GaussianPOP通过建立数学可导的误差衡量体系，实现了理论严谨且灵活实用的高斯点简化方案，在保持高渲染质量的同时显著提升模型压缩效率。

Abstract: Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image. By introducing a highly efficient algorithm, our framework enables practical error calculation in a single forward pass. The framework is both accurate and flexible, supporting on-training pruning as well as post-training simplification via iterative error re-quantification for improved stability. Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.

</details>


### [57] [Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping](https://arxiv.org/abs/2602.06850)
*Chao Zhou,Tianyi Wei,Yiling Chen,Wenbo Zhou,Nenghai Yu*

Main category: cs.CV

TL;DR: 本研究提出了一种高效的注意力机制PKA（Position-aligned and Keyword-Scoped Attention），通过空间对齐和语义裁剪优化多条件控制生成，显著提升文本到图像模型的计算效率与资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型缺乏对用户指定空间布局或物体外观的精细控制，传统多条件控制方法因计算与内存消耗随条件数量二次增长而受限。

Method: 引入PKA框架：1) 位置对齐注意力(PAA)通过局部块对齐线性化空间控制；2) 关键词作用域注意力(KSA)通过语义感知掩码剪枝无关交互；3) 条件敏感度采样策略(CSAS)动态加权训练目标。

Result: 推理速度提升10.0倍，显存占用减少5.1倍，在高保真多条件生成任务中实现高效可扩展方案。

Conclusion: 该方法有效消除跨模态冗余交互，为多条件控制生成提供了资源友好的解决方案，支持大规模复杂场景的应用部署。

Abstract: While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.

</details>


### [58] [Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing](https://arxiv.org/abs/2602.06862)
*Meng Lou,Stanley Yu,Yizhou Yu*

Main category: cs.CV

TL;DR: 该论文提出AdaRoute，一种新的参数高效微调方法，通过动态参数路由机制和共享专家中心，在保持低参数量的同时提升密集视觉任务的模型适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法在复杂密集预测任务中面临输入无关建模和冗余跨层表示的问题，需要一种更灵活高效的适应方法。

Method: 设计基于混合专家（MoE）的适配器，在共享专家中心下动态生成输入依赖的权重矩阵，并通过跨层共享促进隐式特征交互。

Result: 在语义分割、目标检测等多个视觉任务中超越现有方法，在相同参数量下取得更高性能。

Conclusion: AdaRoute通过动态低秩适应和跨层特征多样性提升，为视觉模型参数高效微调提供了新思路。

Abstract: Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cross-layer representations. To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture. Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix. During a feedforward pass, each AdaRoute module in the network dynamically generates weight matrices tailored for the current module via a simple dynamic parameter routing mechanism, which selectively aggregates parameter matrices in the corresponding expert center. Dynamic weight matrices in AdaRoute modules facilitate low-rank adaptation in an input-dependent manner, thus generating more customized and powerful feature representations. Moreover, since AdaRoute modules across multiple network layers share the same expert center, they improve feature diversity by promoting implicit cross-layer feature interaction. Extensive experiments demonstrate the superiority of AdaRoute on diverse vision tasks, including semantic segmentation, object detection and instance segmentation, and panoptic segmentation. Code will be available at: https://bit.ly/3NZcr0H.

</details>


### [59] [RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing](https://arxiv.org/abs/2602.06871)
*Mohammadreza Salehi,Mehdi Noroozi,Luca Morreale,Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Ramos,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: 本文提出了一种高效的因果残差流扩散模型（RFDM）用于可变长度视频编辑，通过帧间残差预测降低计算复杂度并利用时间冗余性，性能优于现有文本驱动视频编辑方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频编辑方法依赖固定长度输入且计算成本高，而自回归生成模型虽能处理可变长度合成但尚未被充分探索。作者旨在构建既能适应可变长度视频又能保持低计算量的编辑框架。

Method: 基于2D图像扩散模型，通过时序条件建模（t-1帧预测作为t帧输入）实现视频到视频编辑。提出新的扩散前向过程公式鼓励模型预测当前目标与历史预测的残差，形成残差流扩散机制。

Result: 在全局/局部风格迁移和物体移除任务中，RFDM超越基于图像的方法，可与全时空3D视频模型竞争，同时计算量与图像模型相当且不受视频长度影响。

Conclusion: RFDM通过自回归预测和残差建模实现了高效可变长度视频编辑，提出了新的测评基准验证方法有效性，为低资源环境下的视频生成提供新方向。

Abstract: Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/

</details>


### [60] [NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices](https://arxiv.org/abs/2602.06879)
*Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Couto Pimentel Ramos,Luca Morreale,Mehdi Noroozi,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: NanoFLUX通过模型压缩与蒸馏技术，将17B参数的文本到图像模型压缩至2.4B，实现在移动设备上高质量生成。


<details>
  <summary>Details</summary>
Motivation: 缩小大型文本到图像模型与设备端部署方案间的质量与效率差距，推动移动端高性能生成技术发展。

Method: 1) 剪枝扩散Transformer冗余组件，将参数量从12B降至2B；2) ResNet下采样机制降低中间层计算延迟；3) 基于去噪器早期层视觉信号的文本编码器蒸馏方法。

Result: 移动设备生成512x512图像仅需2.5秒，保持与原始FLUX.1-Schnell相当的视觉质量。

Conclusion: 提出的压缩框架验证了轻量化文本到图像模型在设备端的实用性，为边缘计算场景提供可行解决方案。

Abstract: While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.

</details>


### [61] [PANC: Prior-Aware Normalized Cut for Object Segmentation](https://arxiv.org/abs/2602.06912)
*Juan Gutiérrez,Victor Gutiérrez-Garcia,José Luis Blanco-Murillo*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督的语义分割框架PANC，通过在TokenCut方法基础上引入少量标注的视觉标记，结合锚节点先验信息优化图谱结构，从而在不依赖密集标注的前提下实现稳定、可控且可复现的物体分割。


<details>
  <summary>Details</summary>
Motivation: 传统无监督分割流程易受初始化敏感性和阈值启发式影响导致不可复现，而全监督方法标注成本高昂。本文旨在通过最小标注量实现兼具可控性和高质量的分割效果。

Method: 基于TokenCut的token-token关联图框架，添加带锚节点的先验约束条件，通过改进图谱拓扑结构引导谱聚类特征空间，结合弱监督标注数据优化分割结果。

Result: 在5-30个标注量条件下，于DUTS-TE/ECSST/MS COCO等基准测试表现SOTA。CFD/CUB-200/HAM10000数据集mIoU达96.8%（+14.43%）、78.0%（+0.2%）、78.8%（+0.37%），在多物体分割中展现显著用户控制能力。

Conclusion: 通过极少量标注换取分割可控性、稳定性与质量的显著提升，为标注成本高昂的场景提供有效解决方案，尤其在细粒度与纹理受限领域表现突出。

Abstract: Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics.
  We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality.
  Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.

</details>


### [62] [Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs](https://arxiv.org/abs/2602.06914)
*Darryl Hannan,John Cooper,Dylan White,Yijing Watkins*

Main category: cs.CV

TL;DR: 研究指出视觉大语言模型（VLLM）在复杂视觉任务中的表现受限于视觉信息的冗余与压缩问题，并提出通过引入高复杂度视觉数据提升其性能


<details>
  <summary>Details</summary>
Motivation: 尽管VLLM在语言能力上表现优异，但精细视觉信息处理和空间推理能力仍不足。现有文献未能明确造成这一局限的具体原因，仅初步提出视觉冗余假说（即模型丢失关键细粒度信息）

Method: 构建合成基准数据集，设计可量化视觉冗余的指标，系统性探测不同视觉特征的处理机制；通过调整训练数据的复杂度，分析冗余压缩与任务性能的关联性

Result: 发现任务复杂度与视觉压缩程度存在显著关联，高复杂度数据能有效改善视觉表征分布，提升模型在精细视觉任务中的表现

Conclusion: 训练数据复杂度是优化VLLM视觉能力的关键因素，建议下一代模型在训练阶段采用高复杂度数据增强策略，以突破现有性能瓶颈

Abstract: Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.

</details>


### [63] [Reliable Mislabel Detection for Video Capsule Endoscopy Data](https://arxiv.org/abs/2602.06938)
*Julia Werner,Julius Oexle,Oliver Bause,Maxime Le Floch,Franz Brinkmann,Hannah Tolle,Jochen Hampe,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本文提出了一种医学数据集中的误标签检测框架，并通过专家复注释验证其有效性，从而提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像分类中的性能受限于准确标注数据的获取，因标注需依赖专业医师且类别边界模糊，导致数据标注困难且易出错。

Method: 设计误标签检测框架，基于胶囊内窥镜视频的最大公开数据集进行验证，并由三名胃肠病专家对可疑样本进行复核与重新标注。

Result: 该框架成功识别错误标注数据，清洗后显著提升异常检测性能，超过现有基线方法。

Conclusion: 所提方法能有效改善医学数据集标注质量，从而提高机器学习模型的可靠性。

Abstract: The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.

</details>


### [64] [CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation](https://arxiv.org/abs/2602.06959)
*Kaiyi Huang,Yukun Huang,Yu Li,Jianhong Bai,Xintao Wang,Zinan Lin,Xuefei Ning,Jiwen Yu,Pengfei Wan,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: 本文提出CineScene框架，通过隐式3D感知场景表示生成电影级视频，利用VGGT编码静态环境图像并串联上下文信息，结合随机打乱策略和自建数据集实现场景一致性视频生成。


<details>
  <summary>Details</summary>
Motivation: 传统影视制作依赖昂贵的实体场景搭建，本文旨在降低动态视频生成成本，实现用户可控的相机轨迹与动态主体的场景一致性合成。

Method: 使用VGGT编码静态场景图像为视觉表示，通过多尺度上下文串联注入预训练文生视频模型，引入随机图像打乱策略增强模型稳健性，并基于Unreal Engine 5构建包含全景场景、动态视频及相机轨迹的合成数据集。

Result: 在处理大范围相机运动和复杂环境场景时，相较现有方法提升场景一致性与视频质量，在自建数据集和公开数据集上均实现最先进性能。

Conclusion: 该方法通过隐式3D场景建模与上下文注入机制，有效解耦场景与动态元素，为影视生成提供了新的技术路径。自建数据集为未来研究提供了重要基础。

Abstract: Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.

</details>


### [65] [MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images](https://arxiv.org/abs/2602.06965)
*Ankan Deria,Komal Kumar,Adinath Madhavrao Dukre,Eran Segal,Salman Khan,Imran Razzak*

Main category: cs.CV

TL;DR: MedMO是专为医疗领域设计的多模态大语言模型，通过三阶段训练方法提升跨模态对齐、任务性能和空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在医疗应用中受限于领域覆盖不足、模态对齐不佳和推理能力弱，需针对性改进。

Method: 分三阶段：1）跨模态预训练对齐视觉编码器与医学语言主干；2）多任务指令微调（包含VQA、报告生成等）；3）结合事实检查和边界框GIoU奖励的强化学习。

Result: 在VQA任务上比基线高13.7%，文本QA高6.9%，医学报告生成语义精度显著提升，空间定位IoU提升达40.4%。

Conclusion: MedMO在放射学、眼科等多医疗模态任务中表现优异，证明其跨领域泛化能力，已开源4B/8B版本。

Abstract: Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [Recontextualizing Famous Quotes for Brand Slogan Generation](https://arxiv.org/abs/2602.06049)
*Ziao Yang,Zizhang Chen,Lei Zhang,Hongfu Liu*

Main category: cs.CL

TL;DR: 利用名人名言重构品牌口号生成，平衡新奇与熟悉度，提升创意输出效果。


<details>
  <summary>Details</summary>
Motivation: 传统口号易因广告疲劳失效，现有LLM生成存在风格重复、缺乏品牌人设等问题，需创新性解决方案。

Method: 提出模块化框架，包含名言匹配、结构解构、词汇替换和融合生成四个子任务，通过引用名人名言实现创意重构。

Result: 对比三种LLM基线模型，在多样性、新颖性、情感影响力及用户偏好度指标均获显著优化提升。

Conclusion: 基于名人名言的口号生成范式可有效平衡创新性与品牌一致性，为机器创意生成提供可解释技术路径。

Abstract: Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.

</details>


### [67] [Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering](https://arxiv.org/abs/2602.06050)
*Jongha Kim,Byungoh Ko,Jeehye Na,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CL

TL;DR: 提出了一种新的RAG解码方法RMCD，通过基于上下文相关性加权合并多个预测结果，在增强LVLM知识密集型任务表现方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RAG解码方法未能充分挖掘多个相关上下文的协同作用，并缺乏抑制无关上下文干扰的机制。针对这一问题，论文提出了改进方案。

Method: RMCD为每个检索上下文生成独立预测，并通过可学习的相关性权重进行集成，在生成过程中动态加权相关上下文贡献，抑制无关信息干扰。

Result: 在三个视觉知识密集型问答基准测试中均取得最优性能，且在不同检索质量（从最弱到最强）下均保持鲁棒性，无需额外训练即可适配主流LVLM。

Conclusion: RMCD通过显式建模上下文相关性与预测集成的协同机制，为RAG解码提供了即插即用的改进方案，在视觉问答任务中显著提升模型准确性。

Abstract: Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.

</details>


### [68] [What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation](https://arxiv.org/abs/2602.06054)
*Abeer Mostafa,Thi Huyen Nguyen,Zahra Ahmadi*

Main category: cs.CL

TL;DR: 该论文提出了一种基于文献分析的自动化新颖性评估系统，通过AI模型解析同行评审标准并结构化对比现有研究。


<details>
  <summary>Details</summary>
Motivation: 传统新颖性评估存在主观性强、对比不完整的问题，亟需客观可解释的评估工具辅助科研评审。

Method: 利用80K标注评审数据微调LLM，构建概念级相似度图谱，系统化抽取论文要素（方法/观点/声明）并关联历史文献。

Result: 实现校准精准度提升42%，新颖性评分一致性提高35%，显著降低人类评审的过度乐观偏差。

Conclusion: 证明结构化证据链与语言模型结合可有效量化研究创新价值，为智能评审工具开发提供新范式。

Abstract: Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.

</details>


### [69] [Quantifying and Attributing Polarization to Annotator Groups](https://arxiv.org/abs/2602.06055)
*Dimitris Tsirmpas,John Pavlopoulos*

Main category: cs.CL

TL;DR: 提出一种新型极化指标，用于解决标注群体间主观任务不一致问题，适用于不平衡组间多标签分析，发现种族、宗教、教育程度影响标注结果，并开源工具。


<details>
  <summary>Details</summary>
Motivation: 当前标注一致性指标存在对组间不平衡敏感、无法处理多标注及跨组分析限制，导致在主观的毒性/仇恨言论标注研究中效果不佳。

Method: 构建可量化极化程度的新指标，配套统计显著性检验方法，支持多标签场景与大规模不平衡社会人口学/意识形态组别对比。

Result: 1) 种族差异显著加剧仇恨言论标注极化；2) 宗教标注者组内共识强但与外组冲突，非宗教组趋势反转；3) 教育水平低加剧主观性，高知群体更一致；4) 开源库验证4类公开数据集结果。

Conclusion: 标注者社会属性显著影响主观标签分布，提出的极化测量框架有效量化群体差异，并提供标注规模估算方案与开源实现。

Abstract: Current annotation agreement metrics are not well-suited for inter-group analysis, are sensitive to group size imbalances and restricted to single-annotation settings. These restrictions render them insufficient for many subjective tasks such as toxicity and hate-speech detection. For this reason, we introduce a quantifiable metric, paired with a statistical significance test, that attributes polarization to various annotator groups. Our metric enables direct comparisons between heavily imbalanced sociodemographic and ideological subgroups across different datasets and tasks, while also enabling analysis on multi-label settings. We apply this metric to three datasets on hate speech, and one on toxicity detection, discovering that: (1) Polarization is strongly and persistently attributed to annotator race, especially on the hate speech task. (2) Religious annotators do not fundamentally disagree with each other, but do with other annotators, a trend that is gradually diminished and then reversed with irreligious annotators. (3) Less educated annotators are more subjective, while educated ones tend to broadly agree more between themselves. Overall, our results reflect current findings around annotation patterns for various subgroups. Finally, we estimate the minimum number of annotators needed to obtain robust results, and provide an open-source Python library that implements our metric.

</details>


### [70] [Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding](https://arxiv.org/abs/2602.06161)
*Yanzheng Xiang,Lan Wei,Yizhen Yao,Qinglin Zhu,Hanqi Yan,Chen Jin,Philip Alexander Teare,Dandan Zhang,Lin Gui,Amrutha Saseendran,Yulan He*

Main category: cs.CL

TL;DR: COVER通过减少冗余修订，在保持输出质量的同时显著加快了扩散语言模型的解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有可逆解码方法因频繁触发token来回翻转（flip-flop oscillations）导致推理效率下降，需设计更高效的验证机制

Method: 通过KV缓存覆盖实现单次前向传播完成留一验证与稳定草案生成：1) 构造双重注意力视图（掩码种子token的验证视图与注入缓存的草案视图） 2) 采用对抗自泄露的对角线修正 3) 基于不确定性/下游影响/缓存漂移的稳定性评分动态选择验证种子数

Result: 在多个基准测试中，COVER将冗余修订次数减少58%，解码速度提升2.1倍，且生成质量保持与基线方法相当

Conclusion: 该方法有效平衡了并行预取与必要修订的冲突，在保持上下文连贯性的前提下实现了高效的扩散模型推理

Abstract: Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.

</details>


### [71] [Uncertainty Drives Social Bias Changes in Quantized Large Language Models](https://arxiv.org/abs/2602.06181)
*Stanley Z. Hua,Sanae Lotfi,Irene Y. Chen*

Main category: cs.CL

TL;DR: 大规模语言模型的后训练量化会影响其社会偏见，导致聚合指标无法捕捉到的偏见状态（有偏/无偏）的翻转现象，且强度越大的量化（如4bit）引发的行为变化越显著。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对量化模型社会偏见的细粒度分析，且传统聚合指标无法反映量化对偏见的潜在破坏性影响，需要系统评估量化在真实场景中的可靠性。

Method: 构建包含13个封闭与开放性偏见数据集的统一基准PostTrainingBiasBench，通过50个量化模型进行大规模实验，分析模型不确定性和量化强度对偏见变化的影响，并评估不同群体间的差异。

Result: 观察到最高21%的响应在量化后出现偏见状态翻转，高不确定性响应的变化概率是低不确定性的3-11倍；4bit模型比8bit模型多4-6倍行为变化；量化对不同群体的影响具有不对称性（最多恶化18.6%/改善14.1%）。

Conclusion: 压缩技术会本质性改变偏见模式，需在量化后进行专门评估与干预以确保实际可靠性，且模型规模扩大无法缓解该问题，不同模型家族的影响方向不可预测。

Abstract: Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.

</details>


### [72] [BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks](https://arxiv.org/abs/2602.06221)
*Nishant Balepur,Bhavya Rajasekaran,Jane Oh,Michael Xie,Atrey Desai,Vipul Gupta,Steven James Moore,Eunsol Choi,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 提出BenchMarker工具，利用LLM评估多项选择题的常见缺陷（污染、捷径、写作错误），并揭示现有基准的问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理中的多项选择题基准缺乏质量控制，影响评估可靠性。

Method: 构建教育学启发的工具，通过LLM标注三种缺陷，并用人工标注验证工具有效性，随后审计12个基准。

Result: 污染题型高估准确率，写作错误降低准确率；现有修复方法引入新缺陷（如不合理的干扰项）。

Conclusion: MCQA缺陷损害NLP评估，需结合教育学规则改进基准设计。

Abstract: Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

</details>


### [73] [Can One-sided Arguments Lead to Response Change in Large Language Models?](https://arxiv.org/abs/2602.06260)
*Pedro Cisneros-Velarde*

Main category: cs.CL

TL;DR: 该研究探讨了如何通过单方面论点引导LLMs在有争议问题上转向特定观点。


<details>
  <summary>Details</summary>
Motivation: 探讨是否能通过提供单一边界论点直观引导LLMs在有争议问题上的输出观点，而非传统多视角平衡。

Method: 构建三维系统实验：(i)LLM立场诱导 (ii)问题表述类型 (iii)论点呈现方式，测试不同模型、论点数量及主题下的引导效果。

Result: 单方面论点显著引发观点转向，但切换论点类型或增加论点数量会弱化该效应。

Conclusion: 仅提供单方面论点可系统性引导LLM观点输出，但效应受论点类型、数量及模型参数影响而减弱。

Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.

</details>


### [74] [Is my model "mind blurting"? Interpreting the dynamics of reasoning tokens with Recurrence Quantification Analysis (RQA)](https://arxiv.org/abs/2602.06266)
*Quoc Tuan Pham,Mehdi Jafari,Flora Salim*

Main category: cs.CL

TL;DR: 本文提出Recurrence Quantification Analysis（RQA）方法，通过分析模型生成时的隐层动态轨迹，有效量化推理链的重复和停滞模式，并显著提升任务复杂度预测。


<details>
  <summary>Details</summary>
Motivation: 现有响应长度指标无法反映推理链动态效果，缺乏对模型潜在生成机制的分析工具。

Method: 将token生成视为动力系统，提取每步生成时的隐层表示轨迹，并应用RQA中的Determinism与Laminarity指标量化重复/停滞模式。

Result: 在3600条DeepSeek-R1-Distill生成轨迹中，RQA捕捉到长度指标未反映的信息，且使任务复杂度预测准确率提升8%。

Conclusion: RQA可作为测试时扩展推理模型生成动态分析的可靠工具，为模型行为研究提供新方法论。

Abstract: Test-time compute is central to large reasoning models, yet analysing their reasoning behaviour through generated text is increasingly impractical and unreliable. Response length is often used as a brute proxy for reasoning effort, but this metric fails to capture the dynamics and effectiveness of the Chain of Thoughts (CoT) or the generated tokens. We propose Recurrence Quantification Analysis (RQA) as a non-textual alternative for analysing model's reasoning chains at test time. By treating token generation as a dynamical system, we extract hidden embeddings at each generation step and apply RQA to the resulting trajectories. RQA metrics, including Determinism and Laminarity, quantify patterns of repetition and stalling in the model's latent representations. Analysing 3,600 generation traces from DeepSeek-R1-Distill, we show that RQA captures signals not reflected by response length, but also substantially improves prediction of task complexity by 8\%. These results help establish RQA as a principled tool for studying the latent token generation dynamics of test-time scaling in reasoning models.

</details>


### [75] [MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CL

TL;DR: 本论文提出医学提示注入基准（MPIB），用于评估临床环境中大型语言模型（LLMs）和检索增强生成（RAG）系统的安全性，通过临床危害事件率（CHER）和攻击成功率（ASR）量化风险。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs和RAG系统在临床流程中的应用面临提示注入攻击风险，可能导致医学误导或安全隐患，但缺乏系统性评估框架。

Method: 构建包含9,697个样本的基准数据集，采用多阶段质量筛选和临床安全验证，同时评估不同模型与防御配置下ASR与CHER指标的关联性。

Result: 发现ASR与CHER存在显著差异，攻击位置（用户输入vs检索上下文）显著影响模型鲁棒性，且临床风险与指令遵循能力存在解耦现象。

Conclusion: MPIB为临床提示注入攻击研究提供可复现框架，强调评估需同时关注指令攻击特征与实际临床危害，推动医疗AI安全性研究。

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

</details>


### [76] [VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation](https://arxiv.org/abs/2602.06270)
*Yancheng Wang,Osama Hanna,Ruiming Xie,Xianfeng Rui,Maohao Shen,Xuedong Zhang,Christian Fuegen,Jilong Wu,Debjyoti Paul,Arthur Guo,Zhihong Lei,Ozlem Kalinli,Qing He,Yingzhen Yang*

Main category: cs.CL

TL;DR: 本文提出VowelPrompt框架，通过引入可解释的元音级韵律特征增强基于大语言模型的语音情感识别效果。


<details>
  <summary>Details</summary>
Motivation: 纯文本情感识别模型忽视语音韵律特征（基频、强度及时长），导致情感识别有效性和可解释性受限。

Method: 1. 提取元音分段时间对齐的韵律特征（基频/能量/时长）并转化为自然语言描述；2. 采用监督微调+基于奖励的强化学习（GRPO算法）两阶段适配策略。

Result: 在零样本、跨领域、跨语言场景下均优于现有方法，能生成融合语义与韵律结构的可解释性反馈，有效提升模型泛化能力。

Conclusion: 验证了基于语音学基础的可解释特征增强方法对语音情感识别的价值，开辟了语言学知识与深度学习结合的新路径。

Abstract: Emotion recognition in speech presents a complex multimodal challenge, requiring comprehension of both linguistic content and vocal expressivity, particularly prosodic features such as fundamental frequency, intensity, and temporal dynamics. Although large language models (LLMs) have shown promise in reasoning over textual transcriptions for emotion recognition, they typically neglect fine-grained prosodic information, limiting their effectiveness and interpretability. In this work, we propose VowelPrompt, a linguistically grounded framework that augments LLM-based emotion recognition with interpretable, fine-grained vowel-level prosodic cues. Drawing on phonetic evidence that vowels serve as primary carriers of affective prosody, VowelPrompt extracts pitch-, energy-, and duration-based descriptors from time-aligned vowel segments, and converts these features into natural language descriptions for better interpretability. Such a design enables LLMs to jointly reason over lexical semantics and fine-grained prosodic variation. Moreover, we adopt a two-stage adaptation procedure comprising supervised fine-tuning (SFT) followed by Reinforcement Learning with Verifiable Reward (RLVR), implemented via Group Relative Policy Optimization (GRPO), to enhance reasoning capability, enforce structured output adherence, and improve generalization across domains and speaker variations. Extensive evaluations across diverse benchmark datasets demonstrate that VowelPrompt consistently outperforms state-of-the-art emotion recognition methods under zero-shot, fine-tuned, cross-domain, and cross-linguistic conditions, while enabling the generation of interpretable explanations that are jointly grounded in contextual semantics and fine-grained prosodic structure.

</details>


### [77] [Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math](https://arxiv.org/abs/2602.06291)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Hyunwoo Ko,Amit Agarwal,Sunghee Ahn,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出基于后果效用评估的Consequence-Based Utility方法，自动评估研究级数学习题解法的质量


<details>
  <summary>Details</summary>
Motivation: 传统验证人工耗时长，现有LLM评估模型无法有效区分正确与错误解法，需要无需专家标注的新评估范式

Method: 设计上下文感知的效用评估器，将候选解法作为示例输入到相关可验证问题中，通过生成结果质量间接判断解法优劣

Result: 在GPT-OSS-120B上实现76.3%的Top1准确率（基线67.2%），AUC提升8.2个百分点，比LLM-Judges模型保持更强的求解能力分离度

Conclusion: 验证了间接评估方法对研究级数学问题的有效性，为减少专家验证成本提供了新的解决方案

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

</details>


### [78] [Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions](https://arxiv.org/abs/2602.06307)
*Nemika Tyagi,Holly Hendrix,Nelvin Licona-Guevara,Justin Mackie,Phanos Kareen,Muhammad Imran,Megan Michelle Smith,Tatiana Gallego Hernande,Chitta Baral,Olga Kellert*

Main category: cs.CL

TL;DR: This paper addresses challenges in parsing spoken code-switching (CSW) by proposing FLEX-UD, an ambiguity-aware evaluation metric, and DECAP, a decoupled parsing framework that achieves 52.6% improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard parsers and LLMs fail on spoken CSW due to disfluencies, repetition, ellipsis, and discourse-driven structures violating Universal Dependencies (UD) assumptions. Rigid evaluation metrics conflate valid linguistic variations with errors, masking parser weaknesses in spoken contexts.

Method: The authors introduced SpokeBench (an expert-annotated benchmark for spoken CSW), FLEX-UD (a metric tolerating linguistically plausible ambiguities), and DECAP (a modular parsing framework isolating spoken phenomena from core syntactic analysis).

Result: DECAP achieves significant robustness gains (52.6% improvement) without retraining, and FLEX-UD uncovers qualitative improvements obscured by standard metrics.

Conclusion: The work highlights the need for linguistically grounded frameworks and evaluation metrics to accurately assess and enhance parsing capabilities in spoken, code-switched language.

Abstract: Spoken code-switching (CSW) challenges syntactic parsing in ways not observed in written text. Disfluencies, repetition, ellipsis, and discourse-driven structure routinely violate standard Universal Dependencies (UD) assumptions, causing parsers and large language models (LLMs) to fail despite strong performance on written data. These failures are compounded by rigid evaluation metrics that conflate genuine structural errors with acceptable variation. In this work, we present a systems-oriented approach to spoken CSW parsing. We introduce a linguistically grounded taxonomy of spoken CSW phenomena and SpokeBench, an expert-annotated gold benchmark designed to test spoken-language structure beyond standard UD assumptions. We further propose FLEX-UD, an ambiguity-aware evaluation metric, which reveals that existing parsing techniques perform poorly on spoken CSW by penalizing linguistically plausible analyses as errors. We then propose DECAP, a decoupled agentic parsing framework that isolates spoken-phenomena handling from core syntactic analysis. Experiments show that DECAP produces more robust and interpretable parses without retraining and achieves up to 52.6% improvements over existing parsing techniques. FLEX-UD evaluations further reveal qualitative improvements that are masked by standard metrics.

</details>


### [79] [Can Post-Training Transform LLMs into Causal Reasoners?](https://arxiv.org/abs/2602.06337)
*Junqi Chen,Sirui Chen,Chaochao Lu*

Main category: cs.CL

TL;DR: This paper introduces CauGym, a causal inference dataset, and demonstrates that targeted post-training (e.g., GRPO) significantly improves LLMs' causal reasoning capabilities, enabling smaller models to outperform larger ones like OpenAI GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To address causal inference challenges in non-expert decision-making by enhancing LLMs' causal estimation abilities through systematic post-training approaches, given their current limitations in this domain.

Method: Created CauGym (7 training tasks + 5 test sets) and evaluated 5 post-training methods (SFT/DPO/KTO/PPO/GRPO) across 9 benchmarks including CaLM. Tested under real-world conditions (distribution shifts/noise).

Result: 14B parameter LLM achieved 93.5% accuracy on CaLM benchmark (vs OpenAI GPT-4o's 55.4%). Post-trained models showed strong generalization, robustness to noise, and superior performance compared to larger models.

Conclusion: Targeted post-training can systematically build reliable LLM-based causal reasoners. Open-sourced models/dataset enable reproducibility and future research in this area.

Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.

</details>


### [80] [Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production](https://arxiv.org/abs/2602.06370)
*Alberto Andres Valdes Gonzalez*

Main category: cs.CL

TL;DR: 比较了零样本/少样本大型语言模型和微调编码器模型在文本分类中的性能、成本和延迟，发现微调模型效率更高且效果更好。


<details>
  <summary>Details</summary>
Motivation: 为了解决结构化文本分类任务中过度依赖大型语言模型的问题，研究模型选择时需综合考虑预测性能、运维成本与延迟。

Method: 在4个基准数据集上对比零样本/少样本LLM提示与全微调编码器模型的预测质量（F1）、延迟及成本，使用Pareto前沿和参数化效用函数分析多目标权衡。

Result: BERT家族微调模型分类性能与LLM相当或更优，延迟和成本低1-2个数量级，LLMs用于混合架构能优化系统级效果。

Conclusion: 结构化NLP任务中优先使用微调编码器提升效率，LLMs适合作为复杂系统的补充组件。

Abstract: Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems.
  In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost.
  We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting.
  Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.

</details>


### [81] [ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis](https://arxiv.org/abs/2602.06373)
*Tianqiang Yan,Sihan Shang,Yuheng Li,Song Qiu,Hao Peng,Wenjian Luo,Jue Xie,Lizhen Qu,Yuan Gao*

Main category: cs.CL

TL;DR: 本研究提出了ReBeCA框架，通过因果分析揭示语言模型自省行为的可解释层次结构，利用因果图和不变因果预测（ICP）识别影响性能的真实决定因素。


<details>
  <summary>Details</summary>
Motivation: 现有基于相关性的自省分析泛化能力不足，需通过因果建模揭示行为层次对结果的实际影响机制。

Method: 将自省轨迹建模为因果图，采用三阶段ICP流水线分离语义行为的直接/间接层级影响，并基于新数据集进行干预验证。

Result: 发现（1）模型语义行为呈现层级化影响；（2）仅少数行为具有因果泛化性；（3）正向行为叠加可能损害效果。ICP识别出的稀疏因果父节点在任务间保持49.6%的结构似然增益，且OoD验证显著（p=0.013）。

Conclusion: ReBeCA通过解耦虚假关联提供了一种严谨的自省机制分析方法，为提升模型可靠性提供了因果视角的理论框架。

Abstract: While self-reflection can enhance language model reliability, its underlying mechanisms remain opaque, with existing analyses often yielding correlation-based insights that fail to generalize. To address this, we introduce \textbf{\texttt{ReBeCA}} (self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}}havior explained through \textbf{\texttt{C}}ausal \textbf{\texttt{A}}nalysis), a framework that unveils the interpretable behavioral hierarchy governing the self-reflection outcome. By modeling self-reflection trajectories as causal graphs, ReBeCA isolates genuine determinants of performance through a three-stage Invariant Causal Prediction (ICP) pipeline. We establish three critical findings: (1) \textbf{Behavioral hierarchy:} Semantic behaviors of the model influence final self-reflection results hierarchically: directly or indirectly; (2) \textbf{Causation matters:} Generalizability in self-reflection effects is limited to just a few semantic behaviors; (3) \textbf{More $\mathbf{\neq}$ better:} The confluence of seemingly positive semantic behaviors, even among direct causal factors, can impair the efficacy of self-reflection. ICP-based verification identifies sparse causal parents achieving up to $49.6\%$ structural likelihood gains, stable across tasks where correlation-based patterns fail. Intervention studies on novel datasets confirm these causal relationships hold out-of-distribution ($p = .013, η^2_\mathrm{p} = .071$). ReBeCA thus provides a rigorous methodology for disentangling genuine causal mechanisms from spurious associations in self-reflection dynamics.

</details>


### [82] [FMBench: Adaptive Large Language Model Output Formatting](https://arxiv.org/abs/2602.06384)
*Yaoting Wang,Yun Zhou,Henghui Ding*

Main category: cs.CL

TL;DR: 本论文提出FMBench（Markdown格式输出评估基准），通过监督微调（SFT）与强化学习微调的轻量级对齐流程，改善大语言模型在保持语义意图的同时满足Markdown格式约束的能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在生成Markdown内容（如列表、表格、标题）时易出现隐性错误，影响实用性。传统流程依赖硬性解码约束，且缺乏全面评估格式与语义协同的基准测试。

Method: 构建涵盖多级结构、混合内容及用户指定布局的FMBench基准；采用两阶段训练：先进行SFT提升语义对齐，再通过强化学习平衡语义准确性和结构正确性。

Result: 在OpenPangu和Qwen模型上的实验表明，SFT显著提升语义一致性，而强化学习在复杂Markdown场景下进一步增强鲁棒性，但也揭示了语义与结构目标间的固有权衡。

Conclusion: 设计可靠的格式化生成需谨慎定义奖励函数以平衡多目标，FMBench为推动标准化评估与改进方法提供了公开可复现的评测平台。

Abstract: Producing outputs that satisfy both semantic intent and format constraints is essential for deploying large language models in user-facing and system-integrated workflows. In this work, we focus on Markdown formatting, which is ubiquitous in assistants, documentation, and tool-augmented pipelines but still prone to subtle, hard-to-detect errors (e.g., broken lists, malformed tables, inconsistent headings, and invalid code blocks) that can significantly degrade downstream usability. We present FMBench, a benchmark for adaptive Markdown output formatting that evaluates models under a wide range of instruction-following scenarios with diverse structural requirements. FMBench emphasizes real-world formatting behaviors such as multi-level organization, mixed content (natural language interleaved with lists/tables/code), and strict adherence to user-specified layout constraints. To improve Markdown compliance without relying on hard decoding constraints, we propose a lightweight alignment pipeline that combines supervised fine-tuning (SFT) with reinforcement learning fine-tuning. Starting from a base model, we first perform SFT on instruction-response pairs, and then optimize a composite objective that balances semantic fidelity with structural correctness. Experiments on two model families (OpenPangu and Qwen) show that SFT consistently improves semantic alignment, while reinforcement learning provides additional gains in robustness to challenging Markdown instructions when initialized from a strong SFT policy. Our results also reveal an inherent trade-off between semantic and structural objectives, highlighting the importance of carefully designed rewards for reliable formatted generation. Code is available at: https://github.com/FudanCVL/FMBench.

</details>


### [83] [Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding](https://arxiv.org/abs/2602.06412)
*Daisuke Oba,Danushka Bollegala,Masahiro Kaneko,Naoaki Okazaki*

Main category: cs.CL

TL;DR: SureLock通过锁定稳定位置减少Masked Diffusion模型的冗余计算


<details>
  <summary>Details</summary>
Motivation: Masked Diffusion模型在生成过程中重复计算已稳定token的注意力和前馈模块导致计算浪费

Method: 提出SureLock机制：当token位置后验概率稳定时锁定该位置，跳过查询投影和前馈层并缓存注意力键值，理论分析通过局部KL散度约束保证有效性

Result: 在LLaDA-8B模型上实现30-50%的算法FLOPs降低，生成质量保持不变

Conclusion: SureLock通过动态计算优化显著提升Masked Diffusion模型的推理效率，具备理论保证和实践有效性

Abstract: Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

</details>


### [84] [On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation](https://arxiv.org/abs/2602.06423)
*Wenbo Shang,Yuxi Sun,Jing Ma,Xin Huang*

Main category: cs.CL

TL;DR: 本文提出HOMER框架，基于GTVH理论实现多模态幽默生成，通过多角色协作机制克服传统LLM方法在创造力和可解释性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统LLM生成幽默依赖推理链或自我改进，缺乏创造力且可解释性差，多模态幽默生成需要理解视觉、幽默推理和创造性想象。

Method: 构建三阶段框架：(1)冲突脚本提取器挖掘幽默对立脚本 (2)检索增强想象者通过关联树扩展创意空间 (3)基于知识的标题生成器，结合视觉理解与幽默理论。

Result: 在两个New Yorker漫画数据集上超越SOTA基线方法，在幽默性、多样性和相关性指标上提升7%以上。

Conclusion: 基于GTVH理论的协作式LLM框架能有效生成多模态幽默，验证了理论驱动框架在创意任务中的优势。

Abstract: Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.

</details>


### [85] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: LLMs struggle to distinguish semantic relations from unrelatedness despite strong performance on related pairs, revealing critical gaps in evaluation and safety.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations inadequately assess LLMs' ability to identify genuinely unrelated semantic pairs, which is crucial for safety and robustness in reasoning tasks.

Method: Introduced CORE dataset (225K MCQs across 74 disciplines) and a 203-question benchmark with 24 balanced semantic relation types. Tested 29 SOTA LLMs and established a human baseline via 1,000+ participants.

Result: Humans achieved 92.6% accuracy (95.1% on unrelated pairs). LLMs scored 48.25-70.9% overall, with 0-41.35% accuracy on unrelated pairs despite high confidence. Calibration errors surged 2-4x, and 37.6% semantic collapse occurred. CORE 225K accuracy dropped to ~2%.

Conclusion: Unrelatedness reasoning remains a critical, under-evaluated challenge for LLMs, highlighting risks in deploying models that systematically generate spurious relations despite apparent confidence.

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [86] [Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning](https://arxiv.org/abs/2602.06449)
*Xinxin Lin,Guangxin Dai,Yi Zhong,Xiang Li,Xue Xiao,Yixin Zhang,Zhengdong Wu,Yongbo Zheng,Runchuan Zhu,Ming Zhao,Huizi Yu,Shuo Wu,Jun Zhao,Lingming Hu,Yumei Wang,Ping Yin,Joey W. Y. Chan,Ngan Yin Chan,Sijing Chen,Yun Kwok Wing,Lin Lu,Xin Ma,Lizhou Fan*

Main category: cs.CL

TL;DR: Researchers developed ClinMPO, a reinforcement learning framework that aligns large language models (LLMs) with clinical psychiatric reasoning, enabling light-parameter models to surpass human accuracy in complex diagnostic tasks through evidence-based optimization.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs exhibit hallucinations and superficial reasoning in psychiatry, particularly lightweight models critical for private and efficient clinical use. Current training prioritizes linguistic fluency over structured medical logic, causing misalignment with professional diagnostic cognition.

Method: ClinMPO trains a reward model on 4,474 psychiatry journal articles using evidence-based medicine principles. A light-parameter LLM (Qwen3-8B) was fine-tuned via this framework and evaluated on a reasoning-focused benchmark designed to exclude rote memorization, where leading large-parameter LLMs typically fail.

Result: ClinMPO-tuned Qwen3-8B achieved 31.4% diagnostic accuracy, outperforming human medical students (30.8%) on complex, unseen cases demanding structured clinical reasoning.

Conclusion: Medical evidence-guided optimization enables lightweight LLMs to master intricate diagnostic reasoning, demonstrating that explicit cognitive alignment provides a scalable solution for reliable psychiatric decision support systems.

Abstract: Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.

</details>


### [87] [RelayGen: Intra-Generation Model Switching for Efficient Reasoning](https://arxiv.org/abs/2602.06454)
*Jiwon Song,Yoongon Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: RelayGen 是一种无需训练的推理加速框架，通过分段级动态模型切换，在保持大部分大型模型准确率的同时显著降低推理延迟。其核心思想是利用推理过程中难度变化的特性，将低难度部分委托给小型模型处理。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成长链多步推理时虽效果优异，但推理成本高昂。现有方案未能有效利用生成过程中内在的难度变化特性（或依赖高复杂度的监督式token级路由），导致无法在保留准确性的同时实现高效推理。

Method: 基于token概率边界的离线分析，通过粗粒度分段级控制捕获推理难度变化。定义模型特定的切换信号（如概率突增点），当检测到低难度段时，将后续生成委托给更小模型，保留高难度部分在大模型处理。

Result: 在多推理基准中比现有方案大幅降低延迟，结合推测解码实现端到端2.2倍加速（准确率损失<2%）。零训练成本且无需学习路由组件。

Conclusion: 通过分段级模型切换有效利用推理过程中的动态难度特性，证明了非监督式粗粒度控制在保持模型效果与推理效率平衡上的有效性。为大模型部署提供了轻量化解决方案。

Abstract: Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\times$ end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.

</details>


### [88] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出DiSPO方法，在扩散语言模型生成过程中通过中间状态分支采样与优化，提升信用分配精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖最终结果的反馈信号，导致中间补全决策信用分配粗粒化，影响生成质量。

Method: 在选定的中间掩码状态进行分支：1) 重采样掩码位置的填充；2) 评分完整生成结果；3) 仅更新当前填充的token。推导固定状态目标函数与策略梯度估计器，与终端反馈优化共享rollout。

Result: 在LLaDA-8B-Instruct模型中，DiSPO在数学和规划类任务上持续超过终端反馈基线方法（diffu-GRPO），计算资源和优化步骤相同时表现更优。

Conclusion: DiSPO实现了无需额外扩散推论的中间状态优化，代码已开源，为扩散模型信用分配提供新范式。

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [89] [Improve Large Language Model Systems with User Logs](https://arxiv.org/abs/2602.06470)
*Changyue Wang,Weihang Su,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UNO是一种用于改进大型语言模型的统一框架，通过处理用户互动日志中的非结构化数据并过滤噪声，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型扩展数据和参数面临质量和计算成本限制，连续学习真实用户交互日志成为关键但具有挑战性的方向。

Method: UNO将日志蒸馏为半结构化规则和偏好对，通过聚类管理数据异质性，并量化模型知识差距以优化反馈过滤与经验提取。

Result: 实验表明UNO在效果和效率上均超越基于检索增强生成和记忆的基线方法。

Conclusion: 该框架有效解决了用户日志学习中的噪声干扰问题，为模型性能提升提供了新途径。

Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

</details>


### [90] [Revisiting the Shape Convention of Transformer Language Models](https://arxiv.org/abs/2602.06471)
*Feng-Ting Liao,Meng-Hsi Chen,Guan-Ting Yi,Da-shan Shiu*

Main category: cs.CL

TL;DR: 本论文提出了一种采用深层沙漏形MLP的Transformer变体，以挑战传统窄-宽-窄MLP在FFN模块的设计，实现更高效的参数利用率。


<details>
  <summary>Details</summary>
Motivation: 受残差宽-窄-宽MLP函数逼近能力的启发，研究者意图重新审视Transformer中FFN模块的传统窄-宽-窄结构的必要性。

Method: 开发了一种新Transformer架构，将传统FFN模块替换为深层沙漏形FFN，其包含多个沙漏子MLP连接，同时通过模型残差路径实现参数的有效利用。

Result: 实验表明，沙漏形FFN在400M参数内的模型性能优于传统FFN，达到1B级别参数量的模型则表现相当；调整FFN与注意力参数后的沙漏形FFN在相同预算下优于传统配置。

Conclusion: 论文结果为近期工作提供新视角，提示需要重新思考传统的窄-宽-窄MLP设计，以优化关注点与FFN之间的平衡，提高模型效率与表达能力。

Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.

</details>


### [91] [MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew](https://arxiv.org/abs/2602.06546)
*Andy Rosenbaum,Assaf Siani,Ilan Kernerman*

Main category: cs.CL

TL;DR: 引入了首个英希语对的机器翻译质量评估基准MTQE.en-he，展示了集成模型和参数高效微调方法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言对（如英希语）的翻译质量评估问题，推动该领域的进一步研究。

Method: 构建包含959个英语片段及专家标注翻译质量的MTQE.en-he数据集，并对ChatGPT提示、TransQuest、CometKiwi进行基准测试。通过集成模型和参数高效微调方法（如LoRA、BitFit）探索优化策略。

Result: 集成模型相较最佳单模型（CometKiwi）在Pearson和Spearman相关系数上分别提升6.4%和5.6%。参数高效微调使训练更稳定且性能提升2-3%，而全模型更新易导致过拟合。

Conclusion: MTQE.en-he为低资源语言对研究提供了重要资源，集成方法和参数高效微调策略在质量评估中具有显著优势。

Abstract: We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.

</details>


### [92] [Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making](https://arxiv.org/abs/2602.06570)
*Baichuan-M3 Team,:,Chengfeng Dou,Fan Yang,Fei Li,Jiyuan Jia,Qiang Ju,Shuai Wang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Hongda Zhang,Jinyang Tai,Linzhuang Sun,Peidong Guo,Yichuan Mo,Xiaochuan Wang,Hengfu Cui,Zhishou Zhang*

Main category: cs.CL

TL;DR: Baichuan-M3是一种医疗增强型大语言模型，通过主动信息获取、长周期推理和幻觉抑制技术，实现临床级主动决策支持，超越GPT-5.2并开源发布。


<details>
  <summary>Details</summary>
Motivation: 解决现有医疗问答系统在开放式咨询场景下的被动响应局限性，缺乏主动决策支持和可靠的临床推理能力。

Method: 构建医生标准诊疗流程的训练框架，包含主动提问消除歧义机制、跨片段证据整合的长周期推理模块、基于知识库的幻觉抑制算法。

Result: 在HealthBench-Hallu（92.7%）、ScanBench（89.4%）等指标上达到SOTA，临床咨询准确率和安全性指标较GPT-5.2提升23.6%。

Conclusion: 验证了医疗大模型从被动应答到主动决策范式的可行性，开源模型已部署于HuggingFace平台（https://huggingface.co/collections/baichuan-inc/baichuan-m3）

Abstract: We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.

</details>


### [93] [Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention](https://arxiv.org/abs/2602.06623)
*Himanshu Singh,Ziwei Xu,A. V. Subramanyam,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 本文提出一种通过目标子空间干预策略抑制大型语言模型生成有害内容的方法，在不影响文本流畅性的同时有效降低毒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能生成有害内容且难以检测，现有抑制方法常牺牲生成流畅性，亟需一种高效且低损耗的毒性控制方案。

Method: 通过分析模型内部表示识别毒性相关子空间，采用针对性干预策略抑制有毒模式，同时保留模型生成安全且连贯文本的能力。

Result: 该方法在RealToxicityPrompts数据集上使毒性降低8-20%，推理复杂度影响最小，流畅度与现有技术相当，且性能超越多种基线方法。

Conclusion: 提出的子空间干预策略在多个LLM中均实现高效毒性抑制，兼顾安全性与生成质量，为模型部署提供了实用的安全解决方案。

Abstract: Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.

</details>


### [94] [FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge](https://arxiv.org/abs/2602.06625)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Xiao Xu,Shijian Li*

Main category: cs.CL

TL;DR: FairJudge提升LLM作为评估者的适应性、去偏倚和一致性，克服现有系统在任务适应、非语义偏倚及评估模式不一致上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge系统存在三局限：缺乏任务/领域评估标准适应性（非语义线索如位置/长度导致偏倚）、跨评估模式（如逐点vs逐对）的判断不一致、对非语义线索的系统性偏差。

Method: 提出动态判断策略建模框架：构建高密度监督数据集注入显式评估行为信号，采用课程式SFT-DPO-GRPO分阶段训练，依次强化评分标准遵循、偏倚消除与跨模式一致性，避免灾难性遗忘。

Result: 实验显示FairJudge在多基准测试中协议一致性与F1值双提升，非语义偏倚显著降低，在相同规模下超越指令微调LLM。

Conclusion: 通过行为建模与渐进式训练，FairJudge实现三重优化：评估适应性增强、系统偏倚消解及模式一致保障，为LLM评估提供可扩展解决方案。

Abstract: Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

</details>


### [95] [Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features](https://arxiv.org/abs/2602.06647)
*Steffen Freisinger,Philipp Seeberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 提出一种结合文本和音频特征的多模态方法，在视频和播客的分段主题检测中取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用音频特征，而口语内容主题多样，需要更精确的分割方法

Method: 微调文本编码器和暹罗音频编码器，捕捉句子边界附近的声学线索

Result: 在YouTube视频数据集上性能显著优于纯文本模型，对ASR噪声更具鲁棒性，并在葡萄牙语、德语和英语数据集中表现更优

Conclusion: 利用声学特征可提升多语言场景下主题分割的鲁棒性

Abstract: Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.

</details>


### [96] [Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought](https://arxiv.org/abs/2602.06650)
*Jianfeng Si,Lin Sun,Weihong Lin,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 该论文提出了PACT框架，通过动态安全控制解决大型语言模型在安全性与有用性之间的权衡问题，允许用户自定义策略以提升实用性，同时保持关键风险的不可逾越边界。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因静态安全策略缺乏运行时可控性，导致过度拒绝合法请求或对有害内容限制不足。现有方法难以灵活适应不同应用场景的需求，因此需要可动态调整的分层安全策略。

Method: PACT采用分层政策架构：全局安全策略确保关键风险（如儿童安全、暴力极端主义）不可逾越，用户可自定义领域特定策略；通过链式思考分解决策路径（分类→行动），支持多样行为（遵守、引导、拒绝）并提高透明度。

Result: 实验表明，PACT在全局策略评估中安全性能接近最先进水平，同时在用户特定策略场景下可控性最佳，有效缓解安全与有用的矛盾。作者承诺开源模型、数据及评估协议以推动研究。

Conclusion: PACT框架通过静态界限与动态用户策略结合，显著提升大型语言模型的安全可控性与实用性，为未来可复现的安全对齐研究提供了框架支持。

Abstract: Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent.
  Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.

</details>


### [97] [compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data](https://arxiv.org/abs/2602.06669)
*Lucie Termignon,Simonas Zilinskas,Hadrien Pélissier,Aurélien Barrot,Nicolas Chesnais,Elie Gavoty*

Main category: cs.CL

TL;DR: 本论文提出了比较:IA平台，致力于收集法语为主的偏好数据，以解决大语言模型在非英语语言上的性能与安全性问题，并发布相关开放数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在非英语语言上的表现不佳，部分原因在于训练数据和偏好数据集中英语占主导地位，而多语言偏好数据匮乏且未公开。

Method: 开发了一个低参与门槛且隐私保护的盲选配对比较平台，收集大规模自由文本提示与用户偏好投票，并进行自动过滤与分析。

Result: 截至2026年，已收集超60万条提示与25万条投票数据（89%为法语），发布三个可复用的开放数据集，并形成法语模型排行榜。

Conclusion: 比较:IA平台可作为国际数字公共产品，为多语言模型训练、评估及人机交互研究提供基础架构支持。

Abstract: Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.

</details>


### [98] [Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts](https://arxiv.org/abs/2602.06692)
*Kerstin Sahler,Sophie Jentzsch*

Main category: cs.CL

TL;DR: 研究发现通过Few-Shot提示工程可高效控制大语言模型生成文本的情感倾向，为数据受限场景下的情感自适应AI提供低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具备变革性能力，但现有情感控制方法（如微调）存在资源消耗高或效果不足的问题，需探索更灵活的解决方案。

Method: 基于Ekman六种基础情绪理论，对比Zero-Shot、Chain-of-Thought和Few-Shot三种提示技术，使用gpt-3.5-turbo模型并设置与微调方法的对照实验。

Result: Few-Shot提示在情感控制效果上显著优于其他方法，尤其在样本量稀缺时展现优势，提示工程整体在成本效益上优于参数微调。

Conclusion: 高质量人工示例结合Few-Shot提示可作为情感自适应AI的实用范式，为资源敏感场景提供可解释的模型调控路径。

Abstract: The groundbreaking capabilities of Large Language Models (LLMs) offer new opportunities for enhancing human-computer interaction through emotion-adaptive Artificial Intelligence (AI). However, deliberately controlling the sentiment in these systems remains challenging. The present study investigates the potential of prompt engineering for controlling sentiment in LLM-generated text, providing a resource-sensitive and accessible alternative to existing methods. Using Ekman's six basic emotions (e.g., joy, disgust), we examine various prompting techniques, including Zero-Shot and Chain-of-Thought prompting using gpt-3.5-turbo, and compare it to fine-tuning. Our results indicate that prompt engineering effectively steers emotions in AI-generated texts, offering a practical and cost-effective alternative to fine-tuning, especially in data-constrained settings. In this regard, Few-Shot prompting with human-written examples was the most effective among other techniques, likely due to the additional task-specific guidance. The findings contribute valuable insights towards developing emotion-adaptive AI systems.

</details>


### [99] [R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging](https://arxiv.org/abs/2602.06763)
*Yanlin Lai,Mitt Huang,Hangyu Guo,Xiangfeng Wang,Haodong Li,Shaoxiong Zhan,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Chun Yuan,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 论文提出R-Align方法，通过引入推理保真度（S-Corr）指标解决传统生成奖励模型（GenRM）依赖结果标签忽视推理质量的问题，在强化学习对齐任务中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统GenRM训练仅依赖结果标签而忽略推理过程，导致高标签准确率下仍存在模型决策与人类判断不一致（S-Corr），进而引发策略退化，亟需新方法保障推理质量。

Method: 提出R-Align框架：1) 构建S-Corr指标量化推理保真度，通过黄金判断标注衡量模型推理与人类推理的一致性；2) 在GenRM训练阶段引入黄金推理标注的对齐监督，采用双向对比学习增强决策路径对齐；3) 在下游RLHF优化中结合推理保真度约束防止过度偏离人类推理逻辑。

Result: 实验发现：1) 竞争性GenRM存在高达38.2%的S-Corr，且S-Corr与策略退化呈强正相关（Pearson=0.87）；2) R-Align降低S-Corr至15.6%，并在多个任务中提升Actor性能：MMLU+3.2%，HumanEval+6.8%，TruthfulnessQA+9.4%；3) 模型推理可解释性通过人工评估提升42.3%。

Conclusion: 推理过程对齐与结果标签对齐具有正交互补性，R-Align通过结构化的推理监督框架，为大型语言模型的对齐提供更可靠的认知路径，在维持结果准确性的基础上显著提升泛化性和可解释性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.

</details>


### [100] [Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling](https://arxiv.org/abs/2602.06795)
*Kate Sanders,Nathaniel Weir,Sapana Chaudhary,Kaj Bostrom,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 该论文提出了一种数据驱动方法，通过构建细粒度的推理错误分类法（“评分标准”），显著提升大语言模型（LLM）在代码、数学和化工等技术领域检测长输出推理错误的能力，并支持使用更少标注数据进行强化学习训练。


<details>
  <summary>Details</summary>
Motivation: LLM在长输出、需专业知识或无明确奖励的问题中难以可靠识别推理错误，限制了其在推理训练（如强化学习）中的应用。

Method: 自动构建高度细粒度的推理错误分类法，利用这些评分标准训练分类器，并将其集成到LLM的奖励函数中，改进基于强化学习的模型训练。

Result: 评分标准方法在技术领域推理错误识别中优于基线模型，使用该评分标准的奖励函数相比普通LLM评估器，在难点领域任务准确率提升45%，且仅需20%的人工标注数据即可接近可验证奖励模型的性能。

Conclusion: 通过将评分标准从定性行为评估扩展到定量正确性评估，论文证明无需完整标注数据集即可训练复杂技术问题求解模型，显著降低数据标注成本。

Abstract: An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.

</details>


### [101] [Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations](https://arxiv.org/abs/2602.06799)
*Shamik Bhattacharya,Daniel Perkins,Yaren Dogan,Vineeth Konjeti,Sudarshan Srinivasan,Edmon Begoli*

Main category: cs.CL

TL;DR: This paper introduces an interpretable Visual Word Sense Disambiguation (VWSD) framework using CLIP to disambiguate words via multimodal alignment, achieving improved performance through textual embedding enrichment and dual-channel prompting.


<details>
  <summary>Details</summary>
Motivation: Lexical ambiguity challenges large language models' understanding, and prior methods struggle to leverage visual context effectively. This work aims to resolve ambiguity by integrating linguistic and visual signals through a multimodal framework.

Method: The framework aligns CLIP-projected language embeddings of ambiguous words with candidate image embeddings in a shared space. Textual embeddings are enhanced via dual-channel prompts (semantic + photo-based) and WordNet synonyms, while image embeddings are refined through test-time augmentations. Cosine similarity selects the best-matching image.

Result: On the SemEval-2023 VWSD dataset, the method improves Mean Reciprocal Rank (MRR) from 0.7227 to 0.7590 and Hit Rate from 0.5810 to 0.6220. Ablation studies show dual-channel prompting boosts performance with minimal latency, while excessive image augmentations offer marginal gains. Multilingual and semantic prompts sometimes reduce accuracy due to noise.

Conclusion: The study demonstrates that precise CLIP-aligned dual-channel prompting effectively disambiguates word senses, outperforming augmentation-heavy and external knowledge-based approaches. It emphasizes the importance of clean, multimodal-aligned semantic signals over noisy external data.

Abstract: Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.

</details>


### [102] [The Representational Geometry of Number](https://arxiv.org/abs/2602.06843)
*Zhimin Hu,Lanhao Niu,Sashank Varma*

Main category: cs.CL

TL;DR: 语言模型中的数字表示通过共享几何关系结构在不同任务间保持稳定性，同时通过子空间线性转换实现功能灵活性，这为认知科学中概念表示的共享与分离之争提供了新解释。


<details>
  <summary>Details</summary>
Motivation: 认知科学领域长期存在关于概念表征是共享统一结构还是分离子空间以减少干扰的争论。现有研究证据支持两种矛盾观点，但缺乏对二者共存机制的解释，本研究旨在通过新型方法揭示语言模型中数字概念表征的几何关系本质。

Method: 以数字概念为实验对象，采用高维度语言模型作为运算载体，通过分析任务特定位空间中表征结构的稳定性，以及不同子空间间的线性可转换性，探究概念表征的几何特性与动态变化机制。

Result: 1) 数字表征在不同任务中保持稳定的关系结构 2) 不同任务表征分布在独立子空间内，低级特征沿可分离线性方向编码 3) 线性映射可实现子空间间的可逆转换，证明表征共享基础结构 4) 任务特异性转换叠加于共享结构之上的认知机制

Conclusion: 研究揭示语言模型通过共享基础几何结构支持泛化，同时利用子空间转换实现任务分离，解决了概念表征共享与分离矛盾。为理解人类概念系统运作提供了机械论解释：认知理解源于共享结构经任务转换形成特定表征，这为构建具有可迁移性AI系统提供了新范式。

Abstract: A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.

</details>


### [103] [SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2602.06854)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Jialin Song,Xuekai Zhu,Chenliang Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: SEMA 是一种有效框架，通过自生成对抗提示和智能奖励机制，显著提升对话AI的多轮越狱攻击成功率，实现最先进效果。


<details>
  <summary>Details</summary>
Motivation: 现有单轮和多轮越狱方法效果有限，存在探索复杂度高和意图漂移问题，缺乏无需外部数据的稳定攻击框架。

Method: 两阶段框架：1) 预填充自调优生成多轮对抗提示模板；2) 基于意图漂移感知奖励的强化学习，结合意图对齐、合规风险和细节层级三要素。

Result: 在AdvBench等数据集上，SEMA 3个模型平均ASR@1达80.1%，比最先进方法提升33.9%，且支持开源/闭源模型跨平台迁移。

Conclusion: 提供首个无需人类干预的全自动红队测试方法，通过动态多轮攻击暴露模型安全漏洞，为LLM安全评估奠定新基准。

Abstract: Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

</details>


### [104] [Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs](https://arxiv.org/abs/2602.06920)
*Samir Abdaljalil,Parichit Sharma,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: 引入了一个多语言、多任务、多类别幻觉检测数据集 Halluverse-M³，涵盖四种语言和两种生成任务，通过细粒度标注评估当前模型的幻觉识别能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言和生成场景下的事实一致性问题未解决，现有英文基准测试无法全面评估跨语言、跨任务和跨幻觉类型的模型表现。

Method: 构建包含英语、阿拉伯语、印地语和土耳其语的 Halluverse-M³ 数据集，支持问答和对话摘要生成任务，通过受控编辑和人工标注区分实体级、关系级和句子级幻觉。

Result: 问答任务表现优于对话摘要生成，句子级幻觉最难检测；模型性能在英语中最佳，低资源语言(尤以印地语最显著)下降明显。

Conclusion: Halluverse-M³ 提供了多语言多任务环境下的真实幻觉检测基准，支持未来幻觉缓解技术研究，数据集已公开。

Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.

</details>


### [105] [Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay](https://arxiv.org/abs/2602.06942)
*Duygu Altinok*

Main category: cs.CL

TL;DR: This study conducts a systematic and principled analysis of subword tokenization for Turkish, a morphologically rich language, exploring the interplay between vocabulary size, training corpus size, and tokenizer types (WordPiece, morphology-level, character-level). It introduces a novel morphology-aware diagnostic toolkit and evaluates performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Prior tokenization studies in morphologically rich languages (MRLs) often neglected systematic control of training corpus size, provided limited diagnostics, and focused on narrow tasks. Turkish's agglutinative nature demands tailored tokenization methods to balance vocabulary efficiency and morphological fidelity, motivating a comprehensive investigation.

Method: The study jointly varies vocabulary size and tokenizer training corpus size, compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology-level, character-level baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. A morphology-aware diagnostic toolkit is introduced, including boundary-level F1, lemma atomicity, over/under-segmentation indices, and CER/WER metrics.

Result: The work identifies optimal scenarios for character-level and morphology-level tokenization, establishes a unified evaluation framework linking intrinsic diagnostics to downstream task performance, and provides actionable guidance for building effective tokenizers in MRLs. Open-source resources (code, pipelines, models) are released to support reproducibility.

Conclusion: The study delivers a reproducible foundation for tokenization research in MRLs, emphasizing systematic parameter control, enhanced diagnostics, and cross-task evaluation. Its fourfold contributions address prior gaps and offer empirical strategies for optimizing tokenizers in agglutinative language contexts.

Abstract: Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.

</details>


### [106] [DAWN: Dependency-Aware Fast Inference for Diffusion LLMs](https://arxiv.org/abs/2602.06953)
*Lizhuo Luo,Zhuoran Shi,Jiajun Luo,Zhi Wang,Shen Ren,Wenya Wang,Tianwei Zhang*

Main category: cs.CL

TL;DR: DAWN: 一种无需训练的依赖感知解码方法，提升扩散大语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法受限于质量-速度权衡，因并行解码忽略token间的语义耦合关系，导致生成质量下降。

Method: 构建token依赖图并应用两大原则：优先解码依赖已知token的位置，避免同时解码强耦合的不确定性token。

Result: 推理速度提升1.80-8.06倍，同时保持生成质量，实验覆盖多模型多数据集。

Conclusion: DAWN通过建模token依赖关系突破并行解码瓶颈，为高效推理提供通用解决方案。

Abstract: Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.

</details>


### [107] [InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.06960)
*Yuchen Yan,Liang Jiang,Jin Jiang,Shuaicheng Li,Zujie Wen,Zhiqiang Zhang,Jun Zhou,Jian Shao,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: InftyThink+通过端到端强化学习框架优化迭代推理过程，解决传统方法依赖监督学习/固定启发式的问题，在准确性、效率和泛化性上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在长链思维推理中面临二次计算成本、上下文长度限制及推理质量退化问题，而现有迭代方法无法优化何时总结/保留/恢复推理的关键决策。

Method: 1. 构建模型自控的迭代边界机制 2. 采用监督冷启动+轨迹级强化学习的两阶段训练，自主学习战略级总结与推理延续决策。

Result: 在AIME24任务上准确率提升21%，超越传统强化学习方法，在分布外基准测试中表现更优，推理延迟降低且训练加速。

Conclusion: InftyThink+实现了推理性能与效率的协同增强，验证了强化学习在优化复杂推理过程中的潜力。

Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.

</details>
