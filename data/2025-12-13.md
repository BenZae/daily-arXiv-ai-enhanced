<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: Reinforcement learning (RL) is applied to text-to-3D generation, addressing challenges in global geometry and local texture consistency through systematic studies on reward designs, RL algorithms (e.g., GRPO variants), benchmark development (MME-3DR), and a proposed hierarchical RL framework (Hi-GRPO). AR3D-R1 is the first RL-enhanced text-to-3D autoregressive model.


<details>
  <summary>Details</summary>
Motivation: 3D generation faces higher complexity than 2D due to globally consistent geometry and fine-grained textures, which require improved reward functions and RL algorithms. Existing benchmarks lack metrics for implicit reasoning in 3D models, motivating the need for dedicated frameworks.

Method: The study focuses on: (1) evaluating reward designs aligned with human preferences and multi-modal models, (2) testing GRPO variants with token-level optimization, (3) introducing the MME-3DR benchmark for implicit reasoning, and (4) proposing Hi-GRPO for hierarchical global-to-local optimization. AR3D-R1 integrates these insights.

Result: AR3D-R1 successfully demonstrates RL-enhanced text-to-3D generation from coarse shapes to texture refinement, validated by systematic insights on reward signals, algorithm efficacy, and benchmark performance.

Conclusion: This work establishes a foundation for RL-driven 3D generation, showing the necessity of human-aligned rewards, hierarchical frameworks (Hi-GRPO), and dedicated benchmarks (MME-3DR) for advancing implicit reasoning and global-local consistency in 3D models.

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [2] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: The paper introduces MMSI-Video-Bench, a comprehensive benchmark for evaluating spatial intelligence in MLLMs, highlighting significant gaps between AI performance and human capabilities.


<details>
  <summary>Details</summary>
Motivation: Lack of holistic benchmarks to assess MLLMs' spatial understanding in continuous visual environments, crucial for their application in real-world physical tasks.

Method: Developed a four-level framework (Perception, Planning, Prediction, Cross-Video Reasoning) with 1,106 expert-reviewed questions from diverse video sources, including domain-specific sub-benchmarks (Indoor Scene, Robotics, Grounding).

Result: Evaluation revealed a human-AI gap (best models lag by ~60%), limited generalization of spatially fine-tuned models, and systematic failures in geometric reasoning, motion grounding, and cross-video correspondence.

Conclusion: MMSI-Video-Bench establishes a foundational testbed for advancing video-based spatial intelligence research, exposing critical challenges in current MLLM capabilities.

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [3] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2是一种婴儿发展启发的视觉-语言建模框架，通过纵向多模态预训练集和DevCV认知评估工具箱，在样本高效预训练方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 婴幼儿发展轨迹为视觉基础模型的样本高效预训练提供了自然目标，而现有模型在发展可信度和认知能力评估方面存在不足。

Method: 构建纵向婴儿中心音视频语料库，生成视频-语句/图像-语句/多轮对话数据；开发DevCV工具箱整合NIH婴儿认知评估任务；采用多功能模型架构进行预训练。

Result: 从零开始预训练的紧凑模型在DevCV工具箱的10项多模态任务中表现优异，部分任务超越GPT-4o，在空间推理、记忆和词汇理解方面展现发展可信性。

Conclusion: BabyVLM-V2通过发展原则指导的统一框架，为视觉基础模型的样本高效预训练提供了新范式，推动发展可信AI模型的研究进展。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [4] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: 提出Any4D，一种基于多视角Transformer的灵活框架，实现从多模态传感器输入进行稠密4D重建。


<details>
  <summary>Details</summary>
Motivation: 以往方法主要针对双视角场景流或稀疏3D点跟踪，且单目视频4D重建方法难以融合RGB-D、IMU、雷达等多模态数据，限制了精度和适用性。

Method: 设计模块化4D场景表示：1) 以相机坐标系存储深度图/内参(egocentric)；2) 以全局坐标系存储位姿/场景流(allocentric)。通过Transformer实现N帧像素级运动与几何联合预测，并可融合多模态传感器输入。

Result: 在精度（误差降低2-3倍）和计算效率（速度提升15倍）均显著优于现有方法，且支持RGB-D、IMU、雷达等多种传感器输入。

Conclusion: Any4D通过解耦式场景表示和多模态融合能力，为大规模4D重建提供了通用框架，适用于自动驾驶、增强现实等需要密集动态场景感知的应用场景。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [5] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView 是一个统一的4D一致性框架，能够处理多种任务如新颖视图合成和带摄像机控制的视频生成，相较特定任务模型在多个基准测试上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法各自专注于4D一致性的特定子任务，如新颖视图合成、带有摄像机控制的文本到视频生成等，导致模型在不同数据片段上训练，缺乏泛化能力。

Method: OmniView 独立地表示空间、时间与视角条件，从而实现这些输入的灵活组合，可在静态、动态或多重输入条件下生成新颖视角、时间上进行前/后向推测，并支持带摄像机控制的文本或图像生成视频。

Result: 在多个基准测试均达到与特定任务模型竞争的表现，相较现有摄像机条件扩散模型，在多视角NV LLFF数据集上的图像质量评分提高了33%，动态NV Neural 3D视频基准提高了60%，静态摄像机控制RE-10K提高了20%，且文本条件视频生成中摄像机轨迹误差减少了4倍。

Conclusion: OmniView 证明了统一的4D视频模型的可行性，展现出良好的泛化能力。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [6] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 提出Mull-Tokens方法，通过模态无关的潜在表示实现多模态自由推理，无需调用工具或生成图像，突破文字与图像的模态界限。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖专家工具、图像生成或手工数据切换模态，存在脆性与扩展性问题，亟需更简洁的解决方案。

Method: 采用模态无关的潜在token编码图文信息，先基于图文迹监督训练，随后仅用最终答案进行无监督微调，实现跨模态自由推理。

Result: 在四个空间推理基准（如拼图解答）中优于文本/图文交替推理基线模型，平均提升3%，难题分割场景最高提升16%。

Conclusion: Mull-Tokens提供了一种多模态抽象推理的通用框架，为文字与视觉理解的结合提供了新解法，推动通用AI推理能力发展。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [7] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT 是一种新框架，通过引入时间戳条件和文本标记关联，实现对视频中多主体出现与消失的精确时间控制，同时保持高视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法缺乏对主体动态出现/消失的精细时间控制，限制了个性化视频合成与可控动画的应用，因此需要一种更精确的时间控制方案。

Method: 提出一种位置编码机制以编码时间区间，结合描述性文本标记与视觉身份绑定，并通过token拼接避免增加注意力模块，实现与预训练模型的无缝集成。

Result: 实验表明，AlcheMinT 在保持当前最佳视觉质量的同时，首次实现了视频中多主体时间行为的精准控制，并在身份保留、时间贴合度等指标上建立新基准。

Conclusion: 该方法在引入时间可控性的同时几乎不增加参数量，为个性化视频生成提供了实用解决方案，推动了可控视频创作技术的发展。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: SceneMaker是一种解耦的3D场景生成框架，通过解耦去遮挡和姿态估计模块并引入开放集数据增强，显著提升了复杂遮挡场景下的几何质量与姿态精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡和开放集场景下无法同时生成高质量三维几何和准确物体姿态，主要受限于去遮挡的先验知识不足和姿态估计的泛化能力不足。

Method: 1) 提出解耦框架：将去遮挡模型与3D生成分离，利用海量图像数据和专门构建的去遮挡数据集增强开放集遮挡模式；2) 开发统一姿态估计模型：融合全局-局部自注意力与跨注意力机制；3) 构建首个开放集3D场景数据集用于训练和验证。

Result: 在室内和开放集场景的全面实验表明，SceneMaker在几何质量指标（如Chamfer Distance）和姿态估计精度（旋转/平移误差）上均优于SOTA方法，特别是在严重遮挡场景下表现突出。

Conclusion: 该工作验证了解耦设计在复杂三维场景生成中的有效性，通过模块化架构与开放集数据增强的协同作用，为遮挡场景建模提供了新范式，并开源数据集与代码推动领域发展。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 本研究评估了多模态大语言模型（mLLMs）在视频情感唤醒分析中的有效性。研究发现，mLLMs在理想条件下表现可靠且无明显人口偏差，但在真实议会辩论视频中效果不佳，可能影响后续统计推断，强调需持续评估生成式AI在政治分析中的应用并提出可复现框架。


<details>
  <summary>Details</summary>
Motivation: 情感在政治传播中至关重要，尽管多模态AI技术在情感分析中有潜力，但缺乏对其有效性验证的研究，需填补该领域实证证据的空白。

Method: 通过两个互补的人工标注视频数据集（理想条件与真实议会场景）评估现有mLLMs的情感唤醒评分能力。

Result: 理想条件下mLLMs可靠性高且基本无偏差，但真实场景中评分失效并导致统计推断偏差。

Conclusion: 研究表明需持续严谨评估生成式AI在政治分析中的应用，并开发标准化可复现的评估框架。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>
