<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]
- [cs.CL](#cs.CL) [Total: 30]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld是一种实时多模态4D世界建模框架，通过生成-重建-引导范式和高效技术实现动态环境的一致性建模与实时交互。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在动态场景的长期一致性、实时交互和持久记忆能力不足，限制了其作为实用化世界模型的发展。

Method: 提出生成-重建-引导的闭环范式，结合自回归扩散模型、Macro-from-Micro Planning（MMPL）分层规划方法及Distribution Matching Distillation（DMD）技术，实现4D时空表示的动态建模。

Result: 在静态场景与动态对象的统一建模、长期时空一致性、低延迟实时生成方面取得显著效果，支持16秒视频连续生成且计算开销可控。

Conclusion: 为具身智能与多模态生成提供了具备持久记忆和物理一致性的实用化世界模型解决方案。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 通过优化噪声控制文本到图像模型的模式崩溃问题，保持生成质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过引导机制或候选池筛选解决模式崩溃，但可能影响生成质量。提出直接优化噪声以增强生成多样性。

Method: 设计噪声优化目标函数，分析噪声频率特性，采用不同频率谱初始化提升优化和搜索效率。

Result: 实验显示该方法在生成质量与多样性方面均优于基线模型，有效缓解模式崩溃现象。

Conclusion: 噪声优化显著改善文本到图像生成效果，为解决模式崩溃提供了新方向。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了Spatial4D-Bench，一个用于评估多模态大语言模型（MLLMs）4D空间智能的综合性基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有空间智能基准测试常局限于小规模或任务单一，缺乏对MLLMs时空推理能力的系统评估，需构建更全面的人类水平对标体系。

Method: 构建包含4万道问答题的多任务评估框架，涵盖18项任务并划分为6大认知类别（如场景理解/时空推理），实现结构化评估体系。

Result: 实验显示当前先进MLLMs在路径规划、动作识别和物理合理性推理等4D空间任务中存在显著局限性。

Conclusion: 该基准测试为推动MLLMs发展类人水平时空智能提供评估标准，研究结果揭示了当前技术缺口和未来方向。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 提出Compressed Map Priors (CMP)框架，通过二值化哈希地图学习空间先验，实现低存储开销的3D感知优化，提升自动驾驶物体检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统忽视历史轨迹数据的价值，而人类驾驶行为本身存在重复路径特征。传统稠密地图存储代价高昂（32KB/km²的20倍），且未有效利用先验知识。

Method: 构建二值化 hashmap 存储历史轨迹空间先验，采用紧凑编码实现存储压缩，并设计轻量级模块无缝集成至3D感知系统架构。

Result: 在 nuScenes 数据集上验证，相较稠密存储减少20倍空间占用（32KB/km²），且多架构的3D目标检测性能均显著提升，计算成本无明显增加。

Conclusion: 通过压缩地图先验建模可有效提升自动驾驶感知鲁棒性，提出的轻量化方案具备实际部署价值，为时空数据重用提供新思路。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [5] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: This paper introduces GLASS, a novel architecture combining global and local attention with stratified sampling, to improve AI-generated image detection by preserving fine-grained details without downsampling.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detection methods downsample images, risking loss of critical high-resolution details. The authors propose GLASS to maintain full-resolution information while balancing computational efficiency.

Method: GLASS integrates a globally resized image view with multiple local crops sampled through spatially stratified sampling. These local regions are processed at full resolution and merged using attention-based scoring, enabling the model to capture both global structure and local fine details.

Result: Experiments on Vision Transformer, ResNet, and ConvNeXt backbones demonstrate GLASS achieves superior predictive performance compared to standard transfer learning approaches under practical computational constraints.

Conclusion: The stratified sampling strategy combined with attention-based aggregation effectively leverages multi-scale information in AI-generated image detection, offering a flexible framework applicable to various network architectures and image sizes.

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [6] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: 本文提出FCMBench-V1.0，首个面向金融信用评估的多模态基准数据集，包含18类金融凭证、4043张隐私合规图像及8446条问答样本，并构建了隐私合规的合成生成流程。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态基准无法反映金融信用领域的文档特性、信用决策理解需求及隐私合规要求，亟需领域专用评估体系。

Method: 设计三维度评估框架（感知/推理/稳健性），通过封闭式合成-采集流程构建数据集：人工生成带虚拟内容的文档模板，实验室场景拍摄获取图像，避免预训练数据泄露。

Result: 测试23个SOTA视觉语言模型，Gemini 3 Pro（F1 64.61%）、Qwen3-VL-235B（57.27%）和Qfin-VL-Instruct（64.92%）表现最优；稳健性测试显示顶级模型在真实采集伪影下性能显著下降。

Conclusion: FCMBench可有效区分模型性能差异，揭示当前模型在隐私合规要求和真实金融场景下的局限性，为领域专用模型开发提供评估基准。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [7] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 本文提出FaceFocalDesc任务，通过构建多属性描述数据集并设计渐进式微调视觉语言模型Focal-RegionFace，实现对任意面部区域的细粒度分析，包括面部动作单元、情绪状态和年龄估计。


<details>
  <summary>Details</summary>
Motivation: 传统面部分析方法缺乏对局部区域多属性联合描述的建模能力，通过聚焦单一面部区域可提升理解和控制精细度

Method: 构建含区域级标注的新数据集，提出基于Qwen2.5-VL的渐进式多阶段局部微调模型，通过逐步细化面部特征关注实现多任务分析

Result: 在传统指标（如F1值）和新设计的细粒度指标上均取得最优性能，验证了局部注意力机制的有效性

Conclusion: 提出的渐进式微调框架和区域聚焦方法可显著提升面部状态分析的精度和可解释性，适用于医疗诊断、人机交互等需精细面部描述的场景

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [8] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: MorphAny3D是一种无需训练的框架，利用结构化潜在表示和注意力机制实现高质量3D形态变形，尤其在跨类别场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 3D形态变形因难以生成语义一致且时间连续的变形而具有挑战性，尤其是在跨类别场景中。现有方法受限于训练数据需求或跨类别泛化能力。

Method: 提出形态交叉注意力（MCA）融合源与目标特征以保持结构一致性，并通过时序融合自注意力（TFSA）结合前序帧特征增强时间连贯性。此外引入姿态校正策略缓解形变过程中的姿态模糊问题。

Result: 实验表明该方法在跨类别场景下生成最先进结果，支持解耦形变、3D风格迁移等高级应用，且可扩展至其他基于SLAT的生成模型。

Conclusion: 通过注意力机制实现的无训练框架能够实现高精度跨类别3D形态变形，为相关应用提供了新思路。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [9] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D实例分割的作物计数框架，通过多视角2D图像与NeRF模型结合，解决了遮挡和作物簇模糊性问题，实现高精度计数。


<details>
  <summary>Details</summary>
Motivation: 传统2D图像分割方法在户外农田中因遮挡与作物簇难以区分导致计数不准确，需开发更鲁棒的3D方法。

Method: 利用多视角2D图像生成实例掩码，结合NeRF视图合成，并引入作物可见性与掩码一致性评分，融合3D信息进行实例分割，无需作物特异性参数调优。

Result: 在棉花、苹果、梨三个农业数据集上验证，计数精度显著优于现有方法，且对不同作物颜色、形状、尺寸表现稳健，此外公开了一个棉花植株数据集。

Conclusion: 该框架有效解决了遮挡和簇状作物分割难题，具备跨作物通用性，结合3D信息与NeRF技术推动了精准农业发展。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [10] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: IntraStyler enables automatic intra-domain style diversification for cross-modality domain adaptation without requiring prior knowledge of target domain variations.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on inter-domain shifts but under-explore intra-domain variability. Current methods need pre-specified style variations for synthetic data generation, which is impractical in real-world scenarios.

Method: IntraStyler uses a contrastive learning-based style encoder to extract style-only features. It performs exemplar-guided style synthesis without prior knowledge of target domain styles, allowing arbitrary style transfer using input exemplar images.

Result: Validated on the large-scale CrossMoDA 2023 dataset, experiments demonstrate controllable style synthesis, significant improvements in intra-domain style diversity, and enhanced downstream segmentation performance with the generated synthetic data.

Conclusion: The proposed contrastive learning framework effectively captures intra-domain style variations and enables practical domain adaptation without target domain priors, achieving superior segmentation accuracy through diverse synthetic data.

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [11] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 本文研究通过强化学习提升多模态大语言模型的视觉推理能力，提出奖励驱动的RL方法解决视觉信息整合不足的问题，并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在需要精确视觉感知的任务（如视觉谜题）中表现不佳，且现有方法缺乏高效的训练机制。将视觉信息转换为文本描述已被验证有效，但需要探索系统性优化方法。

Method: 设计六种面向推理过程不同层面的奖励函数，采用群体相对策略优化（GRPO）构建强化学习框架，通过显式激励延长推理链并强化视觉信息整合，并使用开源Qwen-2.5-VL-7B模型验证有效性。

Result: 在Qwen模型上实现5.56%的性能提升，且模型在域内和域外数据中均保持稳定提升；对比实验显示文本化视觉输入对Claude 3.5/3.7分别提升26.7%/23.6%。

Conclusion: 奖励驱动的强化学习可有效突破视觉信息整合瓶颈，提出的多维度奖励设计为多模态大模型的长链视觉推理提供新范式，未来研究可探索更灵活的奖励组合策略。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [12] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: 该论文提出了名为LooC的矢量量化方法，通过低维组合量化实现高容量且紧凑的设计，并通过实验验证了其在性能和码本效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统矢量量化（VQ）在处理复杂数据时面临高容量需求与紧凑性要求的矛盾，亟需一种兼顾两者的新方法。

Method: LooC基于低维码本构建组合量化机制，提出参数高效码本设计、参数无关的外推-内插增强机制，并支持即插即用式集成到现有VQ框架。

Result: 在多种任务/数据集/架构上验证，LooC在保持更小程序尺寸的同时，取得了优于现有VQ方法的性能表现。

Conclusion: LooC通过组合量化策略解决了VQ容量与紧凑性的冲突，具备通用性和优异性能。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [13] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: 该论文提出SynDR-IQA框架，通过优化合成数据分布提升盲图像质量评估（BIQA）的泛化能力，解决了合成数据训练模型泛化性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据集训练的BIQA模型泛化能力受限，原因在于合成数据特征表示呈现离散聚类模式（高质量图像特征聚集在参考样本周围，低质量图像按失真类型聚类），且该问题源自数据分布而非模型结构。

Method: 基于样本多样性与冗余度对泛化误差的理论分析，提出双策略框架：1) 分布感知的多样性内容过采样（增强视觉多样性同时保持内容分布）；2) 密度感知的冗余簇欠采样（降低密集区域采样密度以平衡样本）。

Result: 在三大跨数据集场景（合成-真实/合成-算法/合成-合成）中实验验证有效性，方法在性能指标上显著优于基线模型，并提供开源代码。

Conclusion: 合成数据分布设计是提升BIQA泛化性的关键，所提方法通过理论驱动的数据重分布策略，有效缓解特征聚类问题，为低质量图像评估提供新思路。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [14] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出结合CycleGAN和YOLOv8的跨模态数据增强框架，通过生成伪红外数据解决PCB缺陷检测中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 红外数据不足导致缺陷检测效果受限，传统配对监督方法存在数据依赖问题，需开发非配对模式下的有效增强策略。

Method: 采用CycleGAN进行非配对可见光到红外图像转换，生成结构语义与热分布并存的伪数据；融合真实红外数据构建异构训练集，用于轻量化YOLOv8检测器训练。

Result: 检测器在低数据条件下实现特征学习增强，性能超越纯真实数据训练模型，达全监督方法基准，验证了伪红外合成的有效性。

Conclusion: 所提框架显著缓解红外数据匮乏问题，为工业视觉检测提供了可行的跨模态增强解决方案。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [15] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: 本文提出一个轻量级害虫检测与农药推荐框架，适用于智能手机/无人机等低资源设备，助力小农户实现高效环保的害虫管理。


<details>
  <summary>Details</summary>
Motivation: 传统害虫管理方法依赖高成本人工巡检和化学农药，存在环境破坏、效率低下等问题，小规模农户难以负担，亟需经济、环保且适配低算力设备的解决方案。

Method: 设计双模块架构：①Pest Detection Module采用轻量化CNN结合原型元学习，解决少样本识别难题；②Pesticide Recommendation Module融合作物类型/生长阶段等环境参数，生成生态友好型施药建议。构建跨数据集、多角度的综合害虫图像数据集用于训练评估。

Result: 轻量化CNN在准确率与SOTA模型相当的前提下显著降低计算复杂度，决策支持系统成功减少30%化学农药依赖，验证了框架在实时精度与生态效益的均衡性。

Conclusion: 该框架通过算法轻量化与环境智能推荐技术，为资源受限场景提供可持续害虫管理方案，在保障识别精度的同时推动绿色农业实践。

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [16] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: TotalFM是一种高效的3D-CT放射学基础模型，通过器官分离策略实现零样本病变分类和临床报告生成，解决计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 3D-CT基础模型在临床应用中面临计算成本高和数据标注不足的挑战，需设计高效且具泛化能力的模型架构。

Method: 提出器官分离框架，利用大规模数据集自动创建器官-文本对，结合VideoMAE自监督预训练和对比学习，整合分割技术和LLM处理放射报告。

Result: 在零样本器官级病变分类中，TotalFM的F1分数优于CT-CLIP（83%器官）和Merlin（64%器官），AUROC在83%的发现类别上超过Merlin，且生成报告效果与现有模型相当。

Conclusion: 器官分离的学习框架显著提升模型效率与泛化性能，为3D-CT基础模型的实际部署提供了可行设计范式。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [17] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: S1-MMAlign is a large-scale multimodal dataset for scientific discovery, featuring 15.5 million image-text pairs enhanced via AI to bridge weak alignment in raw scientific descriptions.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning faces challenges in scientific domains due to significant semantic gaps between complex images and sparse textual explanations. Existing datasets lack sufficient quality and alignment for effective AI training.

Method: Generated S1-MMAlign (15.5M image-text pairs) by extracting from 2.5M open-access papers across disciplines. Developed an AI-driven semantic enhancement pipeline using Qwen-VL to recaption images with context from abstracts and citations, overcoming weak alignment in raw captions.

Result: Technical validation showed enhanced data quality: Reduced SciBERT pseudo-perplexity (less ambiguity) and 18.21% improved CLIP scores for image-text alignment. Dataset covers diverse scientific modalities like microscopy and heatmaps.

Conclusion: S1-MMAlign serves as a foundational resource for AI in Science, enabling advancements in scientific reasoning and cross-modal understanding. Available publicly on Hugging Face.

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [18] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: 本研究提出ActErase，一种无需训练的扩散模型概念擦除方法，通过激活分析和替换实现高效操作，平衡擦除效果与模型生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在安全、版权和伦理风险，传统概念擦除方法依赖高计算成本的微调，亟需轻量级解决方案。

Method: 基于激活差异区域分析，通过提示词对生成目标激活掩码，在前向传播时动态替换输入激活，无需训练即可定位并擦除敏感概念（如裸体、艺术风格）。

Result: 在三类核心擦除任务中达到SOTA效果，成功擦除90%以上目标概念，同时保留85%以上未受影响内容生成能力，对对抗攻击鲁棒性强。

Conclusion: 提出首个适用于扩散模型的即插即用概念擦除框架，为低资源场景下安全可控的内容生成提供可行技术路径。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [19] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 提出DUAL框架分解预测不确定性，解决遥感数据中的长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 因遥感数据存在长尾分布且传统方法难以区分真实尾部样本和噪声样本，导致模型过拟合噪声。现有方法未有效解决这一问题，因此需要动态区分并分别处理这两类样本。

Method: 基于Evidential Deep Learning，DUAL将预测不确定性分解为Epistemic Uncertainty（EU）和Aleatoric Uncertainty（AU），分别用于：1）利用EU衡量样本稀缺性，对尾部样本实施重加权；2）通过AU量化数据模糊性，采用自适应标签平滑技术抑制噪声。

Result: 在多个遥感数据集及不同模型上验证，DUAL均优于TGN、SADE等基线方法，消融实验显示其设计选择有效，框架具备良好的泛化能力和鲁棒性。

Conclusion: DUAL通过解耦不确定性解决了长尾分布下噪声干扰问题，为遥感领域提供了一种通用的模型优化框架，具有实践价值和技术扩展性。

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [20] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: This paper proposes SV-GS, a skeleton-driven deformation field framework for dynamic object reconstruction under sparse observations, outperforming existing methods by 34% in PSNR on synthetic datasets and achieving practical real-world applicability through generative priors.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic reconstruction methods require dense temporal and multi-view coverage, which limits their practicality in real-world scenarios (e.g., security camera networks) where observations are sparse and irregular. The authors aim to enable accurate 3D reconstruction under these sparse conditions.

Method: SV-GS uses a skeleton-driven deformation field with two components: (1) a coarse skeleton joint pose estimator (time-dependent) and (2) a fine-grained deformation module. It jointly optimizes motion and deformation using sparse observations, initially guided by a rough skeleton graph and static reconstruction. Later, the static reconstruction input is replaced by a diffusion-based generative prior.

Result: On synthetic data, SV-GS outperforms baseline methods by 34% in PSNR under sparse observations. On real-world datasets, it matches the performance of dense monocular video methods while requiring significantly fewer frames. The substitution of static reconstruction with a generative prior further enhances practicality.

Conclusion: Dense temporal/skeletal constraints can be replaced by a learned skeleton deformation field under sparse observations. The integration of generative priors demonstrates the method's potential for real-world deployment in settings like surveillance, where data sparsity is inherent.

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [21] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 基于Swin Transformer的皮肤疾病诊断模型在ISIC2019数据集上达到87.71%准确率，可作为临床辅助与患者自检工具。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病患病率上升与皮肤科医生资源短缺的矛盾催生了对智能诊断工具的迫切需求。

Method: 采用预训练Swin Transformer架构，结合数据增强策略与优化后的数据预处理流程进行皮肤病分类建模。

Result: 模型在ISIC2019数据集的8类皮肤病变分类任务中取得87.71%的预测准确率。

Conclusion: 该模型既可作为临床诊断支持系统，也能为患者提供自主皮肤健康评估服务。

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [22] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor通过利用多种、多视角的条件信息(如角色设定图/背景图等)进行草图视频着色，采用时序扩散模型+区域注意力机制，在色彩准确性/身份一致性和时间稳定性上均优于单参考方法。


<details>
  <summary>Details</summary>
Motivation: 传统着色模型仅依赖单一参考(如首帧)，忽视了角色设定图/背景图等其他有效条件信息。多视角条件融合可提升动态场景的色彩一致性与时空稳定性。

Method: 1)将多条件参考编码为附加潜帧并进行时序拼接 2)采用时空对应掩码注意力机制 3)使用模态分离的RoPE位置编码，有效避免色彩通道泄露和身份混淆。

Result: 在SAKUGA-42M数据集的单/多参考实验中，TimeColor在色彩准确度(IS得分提升12%)、身份一致性(SSIM提升8%)和时间稳定性(FVD得分下降15%)均显著优于基线模型。

Conclusion: 多条件联合建模能有效解决视频着色中的跨帧色彩一致性问题，提出的潜帧时序扩散框架为动态内容生成提供了新的技术路径。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [23] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: 该论文提出了VisNet，一个计算高效且有效的人体重识别模型，适用于资源受限的监控和移动应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有先进方法在人体重识别（ReID）中精度高但计算成本高，本文旨在通过减少计算开销实现兼顾精度与效率的实用化模型。

Method: 采用多尺度特征融合（结合ResNet50第1-4阶段）与自动注意力机制，引入基于规则的伪标签语义聚类、动态权重平衡分类与正则化，并使用FIDI损失函数优化度量学习。

Result: 在Market-1501数据集上取得87.05% Rank-1和77.65% mAP精度，模型参数量32.41M，计算量4.601 GFLOPs。

Conclusion: VisNet通过结构创新与技术优化，为计算资源受限场景提供了高精度、低功耗的人体重识别实时部署解决方案。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [24] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: OmniVaT框架通过多模态分数傅里叶适配器和离散树生成模块，解决视觉-触觉学习中的跨域泛化问题，无需多域训练数据即可有效缓解模态差异和领域差距。


<details>
  <summary>Details</summary>
Motivation: 视觉-触觉学习（VTL）面临模态差异（视觉与触觉传感器数据不一致）和领域差距（传感器非标准化与数据采集流程不统一）两大挑战，导致现有方法在跨域场景下表现受限。

Method: 提出OmniVaT框架：1) 多模态分数傅里叶适配器（MFFA）将视觉和触觉嵌入映射到统一的嵌入-频率空间；2) 离散树生成模块（DTG）通过层次化树结构提取多样化、高鲁棒性的多模态分数表示。

Result: 在SDG-VTL任务中，OmniVaT在跨域泛化性能上显著优于现有方法，且无需多域训练数据或复杂跨模态融合策略。

Conclusion: 该研究首次成功解决单域泛化的多模态VTL任务，通过频域映射和树状结构设计，为跨传感器差异和领域变化的具身智能系统提供了新的技术范式。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [25] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出DVEFormer，一种高效的RGB-D Transformer方法，通过知识蒸馏预测文本对齐的密集视觉嵌入，用于机器人实时环境理解与自然语言交互。


<details>
  <summary>Details</summary>
Motivation: 家庭环境中机器人需要对周围环境有全面且细粒度的理解才能与非专业用户有效交互，传统语义分割依赖固定类别且效率不足。

Method: 基于Alpha-CLIP教师模型生成像素级文本对齐嵌入，通过知识蒸馏训练轻量DVEFormer学生模型，采用RGB-D Transformer架构并优化实时计算效率。

Result: 在NVIDIA Jetson AGX Orin上实现26.3FPS全模型和77.0FPS轻量模型，量化评估显示与传统方法相当的分割性能，同时支持自然语言查询和3D地图构建。

Conclusion: 该方法可替代传统语义分割模块，在保证实时性的同时增强语义表达灵活性，为移动机器人提供自然语言交互与空间感知能力。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [26] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 提出一种概率双流框架，统一可靠性建模与多模态融合，提升基于骨架的手部精细动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于骨架的行为识别方法多关注身体大动作，忽视手部细微运动，且跨模态融合可靠性建模需要显式置信度监督。

Method: 1) 原生坐标预处理消解空间归一化；2) 基于Noisy-OR的概率融合实现无监督可靠性学习；3) 四模态骨架数据与RGB的跨模态集成。

Result: 在NTU-60等5个基准测试中，手部动作识别精度提升且鲁棒性增强，特别是在噪声干扰下表现突出。

Conclusion: 该框架首次在骨架动作识别中融合不确定性建模和跨模态协同，解决了细粒度手部动作识别的痛点问题。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [27] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出NeoVerse 4D世界模型，解决现有方法的可扩展性问题，基于无姿态前馈重建和单目视频处理实现高效多场景应用。


<details>
  <summary>Details</summary>
Motivation: 当前4D建模方法受限于昂贵的多视角数据或复杂预处理，难以扩展到野外单目视频场景。

Method: 采用无姿态前馈4D重建框架，结合在线单目退化模式模拟技术，并通过端到端训练策略实现无需标定的视频处理。

Result: 在标准重建与生成任务上达到SOTA性能，支持从单一输入视频中生成新视角轨迹和编辑视频内容。

Conclusion: 通过可扩展的技术框架，在保证质量的前提下显著降低4D建模的硬件与计算成本，推动动态场景理解在移动设备等普适场景的应用。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [28] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: RoLID-11K: A new dashcam-based dataset for roadside litter detection addressing small-object challenges.


<details>
  <summary>Details</summary>
Motivation: Current litter monitoring is labor-intensive with limited coverage; existing datasets don't capture dashcam-specific challenges like extreme small objects.

Method: Created RoLID-11K (11k+ UK dashcam images) and benchmarked detectors (transformers vs YOLO) for small-object detection.

Result: Transformers (CO-DETR) achieved best accuracy, but real-time models struggled with feature resolution in cluttered backgrounds.

Conclusion: RoLID-11K provides a benchmark for scalable litter detection systems requiring improved small-object detection capabilities.

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [29] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: 本论文提出了一种基于四元组损失的深度学习方法（Anomaly Quadruplet-Net）及定制数据加载器，用于在小规模数据集下实现对装配进度（特别是存在遮挡或微小视觉变化场景）的精准估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如Anomaly Triplet-Net）在处理产品装配过程中相邻任务视觉差异微小或存在遮挡的场景时易产生误分类，亟需一种更鲁棒的系统以提升智能工厂的生产效率并减少废件损耗。

Method: 设计了一种基于四元组损失的深度度量学习框架（Anomaly Quadruplet-Net）以增强对异常图像的表征能力，并开发了定制的数据加载器，通过策略化选择训练样本提升模型估计精度。

Result: 在桌面电脑装配图像数据集上的测试表明，该方法较现有方法估计准确率提升1.3%，相邻任务误分类率降低1.9%，验证了其有效性。

Conclusion: 提出的方法在小规模数据集下显著优化了装配进度估计性能，尤其在处理微小视觉变化和遮挡场景时表现出更强的鲁棒性，为智能工厂的自动化监控提供了可行方案。

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [30] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO通过对比感知策略优化，无需额外模型即可提升视觉-语言模型的多模态推理能力，通过引入基于熵变的感知标记识别和对比感知损失（CPL），同时提升感知一致性与敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在处理多模态模型时需要依赖显式感知奖励和额外的LLM/标注数据，且难以分离感知与推理标记，导致训练复杂度高。CPPO旨在解决这一问题，实现无需额外模型的感知-推理联合优化。

Method: 基于输入图像扰动下模型输出熵变检测感知标记，设计对比感知损失：1) 信息保留扰动下强制感知标记输出一致性；2) 信息缺失扰动下感知标记响应敏感度。将此损失融入RL目标函数，联合优化感知与推理能力。

Result: 实验显示CPPO在感知奖励任务中超越现有方法，在消融实验中证明其在参数效率（减少65%参数量）和训练速度（提升2.1×）上的优势，同时支持多任务（VQA/图像描述）联合优化。

Conclusion: CPPO提供了无需外部标注或附加模型的对比学习框架，通过量化感知特征在信息扰动下的动态变化，同时增强模型对感知内容的一致性保持和敏感判别能力。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [31] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics 是一个端到端可微框架，通过自然语言提示自动推断3D场景的物理参数，无需人工调参或依赖真实轨迹数据即可生成逼真的动态模拟。


<details>
  <summary>Details</summary>
Motivation: 传统3D物理模拟需专家手动调参以匹配动态行为，耗费大量时间。本文旨在通过语言驱动实现物理参数自动推断，降低专业门槛并加速模拟流程。

Method: 1. 利用多模态大语言模型解析语言描述，输出符合物理合理范围的材料参数；2. 提出可学习的运动蒸馏损失函数，从视频扩散模型中提取运动先验知识，同时最小化视觉/几何先验偏差对模拟的干扰。

Result: 在30+场景测试中（覆盖弹性体、金属、沙子、牛顿流体等材料），MotionPhysics 生成的动态模拟效果超越现有方法，且自动推断的参数具备物理合理性。

Conclusion: 该框架首次将自然语言与端到端物理模拟结合，在无标注数据场景下实现高效逼真动画生产，推动物理模拟平民化应用。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [32] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出FreeText，通过改进扩散模型的注意力机制和字体注入，提升多行密集文本生成且无需再训练。


<details>
  <summary>Details</summary>
Motivation: 已有文本生成模型在多行布局和复杂字体（如中文）渲染上效果差，需耗费训练资源或依赖硬性约束，影响美观和灵活性。

Method: 将文本生成分解为“位置”和“内容”两部分。前者通过扩散模型注意力热力图定位文本区域，后者结合频域调节的字体注入策略减少语义泄露。

Result: 在Qwen-Image、FLUX.1-dev等模型的实验表明，文本可读性提升同时保持语义关联性和视觉质量，推理耗时增加有限。

Conclusion: FreeText作为即插即用框架，有效解决长尾脚本文本生成难题，在无需训练情况下优于现有方法。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [33] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: 该论文提出VNS-SAM，通过改进SAM的低层特征利用（包括Mask-Edge Token Interactive decoder和Non-Salient Feature Mining模块），提升其在视觉非显性场景下的零样本分割性能，同时保持原有泛化能力，且参数增加有限。配套发布包含35K图像的VNS-SEG数据集。


<details>
  <summary>Details</summary>
Motivation: SAM在低对比度场景下难以准确分割非显性目标（如隐形轮廓），而现有方法效果欠佳，需提升模型对非显性特征的理解但不牺牲零样本泛化性。

Method: 1) 设计Mask-Edge Token Interative decoder增强解码器对边缘与掩码特征的交互能力；2) 提出Non-Salient Feature Mining模块挖掘低层特征中的非显性特征；3) 构建多场景VNS-SEG数据集。

Result: VNS-SAM在零样本设置下显著优于现有方案，在VNS-SEG数据集上取得更高分割精度，且模型仅需4小时即可完成微调。

Conclusion: VNS-SAM在不损失零样本泛化性的前提下，有效解决低对比度场景的分割难题，配套数据集及代码开源，具实用与研究价值。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [34] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: DynaDrag采用“预测-移动”框架，解决传统拖拽图像编辑中的跟踪误差和编辑效果不佳问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法因跟踪误差和图像间差异大导致编辑效果受限，需设计新框架提升可编辑性与准确性。

Method: 通过迭代的运动预测（预测控制点移动路径）与运动监督（调整点位置），并动态优化有效控制点。

Result: 在人脸和人体数据集上验证，DynaDrag较以往方法生成更合理、像素级精准的编辑结果。

Conclusion: 该框架有效克服传统缺陷，为拖拽式编辑提供新思路，显著提升图像编辑质量。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [35] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: SlingBAG Pro改进了三维光声成像算法，通过层级优化策略在不规则探头阵列下实现高质量重建，并将速度提升2.2倍，兼顾硬件成本与临床适用性。


<details>
  <summary>Details</summary>
Motivation: 传统迭代重建算法在处理用于临床低成本高质成像的不规则几何探头阵列时，面临计算复杂度高、内存需求大、重建时间长等瓶颈问题

Method: 提出SlingBAG Pro算法：1. 基于滑动球自适应增长(SlingBAG)的点云迭代概念 2. 扩展兼容任意阵列几何结构 3. 采用零梯度过滤与渐进式时间采样率提升的层级优化策略 4. 快速消除冗余空间点云并加速收敛

Result: 相较原始SlingBAG算法在不规则阵列下实现：1. 点云三维PA重建速度提升2.2倍 2. 保持高重建质量 3. 减少所需探头数量 4. 通过仿真和活体小鼠实验验证有效性

Conclusion: 新算法通过层级优化策略突破了传统方法对不规则探头阵列的兼容性限制，在降低硬件成本的同时大幅缩短重建时间，为临床三维光声成像提供了实用化解决方案

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [36] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: 本文推出MS COCOAI数据集，包含96000个真实与合成图像，用于提升AI生成图像检测能力，并定义了两类检测任务。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像与真实照片难以区分，虚假信息传播风险加剧，亟需可靠检测方法。现有数据集缺乏多模型生成样本和统一评估标准。

Method: 基于MS COCO构建合成图像库，使用Stable Diffusion 3/2.1/SDXL、DALL-E 3和MidJourney v6等五类主流生成模型，构建分类任务（真假判断）和溯源任务（模型识别）

Result: 公开首个覆盖多代生成模型且标注完善的AI图像检测数据集，定义双任务评估框架，支持检测模型跨代泛化能力测试

Conclusion: MS COCOAI为AI图像认证技术提供基准数据，推动生成模型安全性研究，数据集已开放获取

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [37] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: 本论文提出了一个全面的多任务基准AEGIS和确定性检查列表评估（DCE）协议，以评估统一多模态模型（UMMs）在多样化任务中应用世界知识的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估UMMs综合能力和可靠性方面存在不足，仅提供孤立的单任务评估，且评估指标模糊，缺乏诊断能力。

Method: 设计包含1050个手工标注问题的AEGIS基准，涵盖21个主题和6种推理类型，并提出用原子化“是/否”判断替代模糊提示评分的DCE评估协议。

Result: 实验揭示UMMs存在严重的世界知识缺陷，复杂推理能力显著下降，但简单插件式推理模块可部分缓解这一问题。

Conclusion: 基于世界知识的推理是UMMs关键前沿领域，AEGIS和DCE为评估和改进该能力提供了新工具和方向。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [38] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的视频文本跨模态检索框架GranAlign，通过解决语义粒度差异问题，在三个主流基准测试中均达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视频定位方法存在视频与文本语义粒度不匹配问题，导致即使使用高质量预训练模型仍出现检索偏差。

Method: 设计基于粒度的查询重写（多粒度语义生成）与查询感知字幕生成（将查询意图嵌入视频内容）模块，结合多层级查询与双字幕对齐策略消除语义鸿沟。

Result: 在QVHighlights、Charades-STA、ActivityNet-Captions三个基准测试中全面领先，在QVHighlights数据集上mAP@avg指标提升3.23%。

Conclusion: 该方法在完全无需训练的条件下，通过模态间粒度平衡策略有效解决了视频-语言跨模态检索的核心挑战。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [39] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了SafeMo，一种基于连续空间运动解学习的可信文本到动作生成框架，解决了现有方法在安全性和运动质量上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法通过离散VQ-VAE码本替换实现安全控制，但存在两个关键问题：恶意替换导致良性任务性能下降，离散化造成运动不连贯。同时，开源数据集中存在固有危险意图，制约了安全驱动模型的训练。

Method: 提出Minimal Motion Unlearning（MMU）两阶段解学习策略，通过连续空间优化直接消除危险隐式表征，并构建首个包含29K个安全文本-动作对的连续动作解学习数据集SafeMoVAE-29K，基于扩散模型DiP实现端到端生成。

Result: 在HumanML3D和Motion-X数据集上，SafeMo相比当前SOTA方法LCR分别提升2.5倍和14.4倍的危险意图遗忘程度（Forget-set FID），同时在安全意图生成质量上保持可比或更优性能。

Conclusion: SafeMo通过连续空间解学习机制，在保证运动自然过渡的同时实现高效的危险意图消除，为安全关键型动作生成提供了新的技术路径。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [40] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 本文提出MDACL框架解决RGB-IR多模态感知中的模态优化偏差问题，通过MDI定量分析及HCG-AER联合优化策略实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 针对跨模态融合中由模态特性不对称导致的信息密度差异、特征质量不均衡和优化偏差问题，传统方法难以平衡模态交互，导致训练过度依赖主导模态

Method: 1) 提出MDI模态主导指数，结合特征熵和梯度贡献量化模态优势；2) 构建MDACL框架，包含分层跨模态引导(HCG)增强特征对齐，对抗均衡正则化(AER)平衡优化动态。

Result: 在3个RGB-IR基准测试中，MDACL在目标检测任务上实现最优性能，模型偏差减少38.7%，特征交互效率提升25.4%，验证集mAP达到91.2%

Conclusion: 通过建立模态优势量化体系与自适应平衡机制，有效克服了多模态感知中的优化偏差难题，为复杂物理环境下的多模态系统设计提供了新范式

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [41] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: 提出TOLF方法，通过归一化流和不确定性引导优化提升小目标检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决小目标检测性能落后问题，因其对标注噪声敏感且易过拟合定位目标。

Method: 设计流式误差建模捕获非高斯预测分布，结合不确定性感知梯度调制抑制噪声样本学习。

Result: 三项数据集实验验证有效性，在AI-TOD数据集上AP提升1.2%（基于DINO基线）。

Conclusion: TOLF通过噪声鲁棒定位框架改善小目标检测性能，归一化流建模与不确定性引导机制协同优化显著降低过拟合风险。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [42] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: RePose is a real-time 3D human pose estimation method for rehabilitation using RGB cameras, combining multi-person tracking, modified SmoothNet, and Unity integration to monitor motion, correct actions, and provide feedback for muscle recovery.


<details>
  <summary>Details</summary>
Motivation: Current rehabilitation training lacks real-time monitoring and error correction; RePose addresses this by enabling accurate, immediate feedback to improve patient adherence and efficacy of exercises.

Method: Developed an end-to-end pipeline with multi-camera RGB inputs, a <1ms fast tracking algorithm for multiple-person scenarios, and enhanced SmoothNet for smoother pose estimation, integrated with Unity for visual feedback of motion and muscle stress.

Result: Achieved real-time tracking (<1ms/frame), reduced pose estimation errors, improved visual smoothness, and enabled muscle stress visualization for guided rehabilitation.

Conclusion: RePose enhances rehabilitation training through efficient real-time monitoring, error correction, and patient feedback, supporting accelerated recovery of motor functions via accurate motion analysis.

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [43] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 该研究提出基于图像的深度学习方法，非侵入式监测马铃薯储存质量，涵盖芽检测、重量损失估计和保质期预测。通过ResNet、VGG、DenseNet和ViT等预训练模型构建两类模型，其中DenseNet在芽检测中准确率达98.03%。粗粒度分类（2-5类）保质期预测准确率超89.83%，但细粒度分类性能下降。该方法可优化库存管理并减少食物浪费。


<details>
  <summary>Details</summary>
Motivation: 马铃薯储存中的传统监测方法存在效率低、侵入性等问题，亟需非侵入式可扩展方案解决芽生长、重量损失及保质期预判等挑战。现有模型在微细分类和跨环境泛化能力上存在局限，研究旨在开发高精度且实用的解决方案。

Method: 在控制温湿度条件下，历时200天收集马铃薯图像及重量数据。基于ResNet、VGG、DenseNet和ViT预训练架构，设计两类模型：①用于芽检测的二分类模型；②联合预测重量损失与保质期的多分类模型。通过不同粗细粒度分类（2-8类）验证模型性能。

Result: DenseNet在芽检测中准确率高达98.03%。保质期预测模型在2-5类粗分类任务中准确率超过89.83%，但细分至6-8类时性能下降，主要受视觉差异不明显和单类数据不足影响。实验验证了图像模型在自动化分拣和动态库存管理中的可行性。

Conclusion: 该方法通过早期芽识别和存储阶段动态分级，可提升库存管理与差异化定价策略，减少供应链浪费。研究建议未来开发针对多品种马铃薯和多环境条件的通用模型以增强适应性。整体方案具备低成本、非破坏性优势，适用于马铃薯储运环节的可持续性改良。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [44] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: 本文提出CRoPS框架，通过改进对比解码策略降低大型视觉-语言模型的幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 现有训练无关的幻觉缓解方法存在两个局限：（i）对幻觉来源假设过于狭隘；（ii）在文本生成后期效果下降。传统通过移除视觉token的方法因视觉信息仍渗入文本而效果有限。

Method: 构建基于关键文本token选择性移除的新型幻觉模型，结合广义对比解码技术整合多个幻觉模型，全面捕捉多样化幻觉来源。

Result: CRoPS在CHAIR评分上提升20%，在6个基准测试和3类LVLM家族中均实现性能提升，优于现有SOTA训练无关方法。

Conclusion: 通过文本token级干预和多模型对比解码，有效突破传统视觉token操作的局限，为训练无关幻觉缓解提供了新范式。

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [45] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 本研究提出了一种基于3D高斯场景表示的新型单图像视频生成框架，通过单次前向传递实现用户可控的相机轨迹与物体运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在相机运动建模、时间一致性保持和几何完整性方面存在不足，且缺乏对用户指定相机路径的精确控制能力。

Method: 构建3D高斯场景表示，联合采样物体运动轨迹，通过一步前向计算生成时空连贯的视频，避免了传统方法的迭代去噪过程。

Result: 在KITTI、Waymo等数据集上实现了最优视频质量和推理效率，支持精确的相机控制与几何一致的动态场景生成。

Conclusion: 该方法通过显式3D表示和端到端优化，在保持几何约束的同时实现了高效可控的视频生成。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [46] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM是一种无需训练的高斯SLAM框架，通过一次性三角化多视角对应关系生成结构感知的高斯种子初始化，替代传统残留驱动的高斯优化过程，提升映射稳定性和收敛速度约20%，在复杂场景中实现更高渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统GS-SLAM因渐进式高斯稠密化（依赖残留误差）在早期建图阶段易不稳定，RGS-SLAM提出一次性初始化策略以解决该问题。

Method: 使用DINOv3描述符生成密集多视角对应关系，并通过置信度感知的内点分类器筛选，最终通过单次三角化生成均匀分布的高斯种子作为优化初始值。

Result: 在TUM RGB-D和Replica数据集上，比现有高斯和基于点的SLAM系统达到竞争力或更优的定位与重建精度，实时性能达925 FPS，收敛速度提升20%且渲染质量更高。

Conclusion: RGS-SLAM通过训练无关的初始化方法显著提升稳定性与效率，兼容现有GS-SLAM流水线，适用于动态复杂场景的实时三维建模。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [47] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 本文研究医学场景下视觉-语言模型（VLM）因数据分布偏移导致的性能预警问题，提出结合输入分布检测工具DomainSAT和输出置信度指标的双维度监测框架。通过分析输入数据偏移与预测置信度的变化关系，证实二者互补性可提升病理诊断场景下的模型可靠性预警效果。


<details>
  <summary>Details</summary>
Motivation: 医疗场景中VLM部署后的数据分布偏移可能导致诊断失误，但传统输入分布检测方法难以准确预测模型性能退化。亟需建立可解释的可靠性监测体系，在无标注数据场景下实现性能预警。

Method: 1) 开发DomainSAT工具集成MMD、KL散度等轻量级分布检测算法；2) 提出基于预测置信度熵值的无监督衰减指标；3) 在病理切片肿瘤分类数据集上验证双维度预警框架的有效性。

Result: 1) 单独输入检测可识别87%的数据偏移但仅部分关联性能衰退；2) 输出置信度指标在AUC-ROC上达0.82，能提前3个批次预警性能衰退；3) 联合检测框架将误报率降低42%，漏报率降低31%。

Conclusion: 双维度监测框架为医疗VLM部署提供可解释的动态风险感知系统，揭示输入分布变化与输出可靠性存在非线性关联，为医学AI临床转化奠定质量控制基础。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [48] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多模态大型语言模型（LLMs）的端到端自动批改手写工程测验流程，支持标准考试模式且无需修改学生作答格式。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试能反映学生开放性思维和图表，但人工批改效率低且难以扩展规模，亟需自动化解决方案。

Method: 讲师仅提供手写参考答案（100%）和简短评分规则：1）将参考答案转为纯文本条件输入；2）采用多阶段设计（格式/存在性检查、独立评分者集成、监督者聚合）；3）使用确定性模板生成可审计机器解析报告。

Result: 实验以斯洛文尼亚语真实课程测验验证（含手绘电路图），使用GPT-5.2和Gemini-3 Pro模型时，批改结果与讲师评分平均绝对差异约8分，偏差低，约17%需人工复核触发率。消融实验证实结构化提示和参考答案基础至关重要。

Conclusion: 该方法在保持传统考试流程下实现了可靠的手写批改自动化，验证了结构化评分框架和模型参考对准确性的影响，适用于小众语言及工程学科场景。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [49] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: 本文通过将自监督学习作为辅助任务，提升广义深度伪造检测的性能。通过融合自监督与主任务特征表示，显著优化了交叉数据集评估中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 广义深度伪造检测存在领域偏移问题，自监督预训练的辅助任务可能提供更具泛化性的特征表示，但尚未被充分探索。

Method: 设计了多任务训练框架，融合自监督辅助任务（如伪着色、相对位置预测）与主任务的特征表示，并在DF40等7个数据集上测试不同模型结构与训练策略的组合。

Result: 融合特征后的EfficientNet-B0模型在跨数据集评估中，准确率较当前SOTA方法平均提升6.8%，且在FaceShifter等高逼真伪造数据上的检测准确率达到88.9%。

Conclusion: 自监督辅助任务通过特征级融合能显著增强深度伪造特征的表征能力，未来可探索动态任务权值调整及更复杂的特征融合机制。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [50] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出AdaGaR框架，通过自适应Gabor表示与时间连续性约束实现动态3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有高斯基元方法存在低频模糊和能量不稳定问题，且缺乏时间连续性导致运动伪影。

Method: 采用自适应Gabor表示（含频率权重与能量补偿），结合时间曲率正则化样条和自适应初始化机制。

Result: 在Tap-Vid DAVIS数据集达SOTA性能（PSNR 35.49, SSIM 0.9433），支持多任务泛化。

Conclusion: 方法平衡高频细节与运动稳定性，推动单目视频动态重建的实用化进展。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [51] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: 本文提出了RIMRULE，一种通过动态规则注入的神经-符号方法，用于增强大语言模型（LLMs）在特定领域工具使用中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在使用领域专用工具时（如存在非标准接口或私有化流程的工具）常表现不稳定，亟需一种无需修改模型参数即可适配任务特有工具的解决方案。

Method: 通过反向工程失败案例提取紧凑且可解释的规则，并在推理阶段将LLM自生成的规则注入提示词。利用最小描述长度（MDL）目标函数优化规则的泛化性与简洁性，同时以自然语言与结构化符号两种形式存储规则以实现高效检索。

Result: 在工具使用基准测试中，该方法在已见/未见工具均提升准确率且不改动模型参数。相对于纯提示词适配方法表现更优，并可与微调互补。此外，规则可跨模型复用（包括长推理模型），验证了符号知识的架构迁移性。

Conclusion: RIMRULE通过符号规则注入实现了大模型在特定工具场景下的有效适配，为跨架构模型提供了轻量级、可移植的工具增强方案。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [52] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: 本文提出了一种名为Pat-DEVAL的专利描述评估框架，结合法律约束推理机制（CoLT），通过多维度分析提升对专利技术完备性和法律合规性的自动化评估能力。


<details>
  <summary>Details</summary>
Motivation: 传统专利描述既需满足技术公开要求，又要符合法定条件（如可实施性要求）。现有评估方法无法同时验证长文本结构性和法律合规性，而专利自动化撰写系统亟需配套评估工具。

Method: 构建包含Chain-of-Legal-Thought（CoLT）推理机制的Pat-DEVAL框架：1）基于大模型作为裁判员模型；2）引入法律约束条件进行专利法特定的序列化分析；3）在自行构建的Pap2Pat-EvalGold数据集上开展与专利专家对照的评估实验。

Result: 实验显示Pat-DEVAL实现0.69的皮尔逊相关系数，优于基线指标和现有模型。在法律合规维度（Legal-Professional Compliance）相关性达0.73，证明植入法定约束能有效捕捉法律有效性细节。

Conclusion: 该框架确立了专利技术与法律的双标准评估体系，证实显性法律约束对专利生成系统部署的重要性，为专利自动化撰写提供了方法论基础和技术保障。

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [53] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 本文通过系统分析IEMOCAP数据集，揭示了对话情感识别(ERC)中关键的架构选择，并发现情感与话语标记位置的显著关联。简单结构结合严格因果上下文即可实现最优性能，且悲伤情绪因缺乏显性语用标记而最依赖上下文识别。


<details>
  <summary>Details</summary>
Motivation: 解决ERC领域两个关键问题：1) 架构选择的重要性未知；2) 缺乏连接识别与生成的言语分析。现有研究缺乏严谨因果验证和跨层级关联分析。

Method: 对IEMOCAP数据集进行：1) 10种随机种子的严谨消融实验，验证架构组件影响；2) 使用严格因果上下文建模；3) 针对5286个话语标记的统计分析，考察情感与语言特征关联。

Result: 1) 仅用近10-30轮对话上下文即可获得90%性能增益；2) 分层句向量在无上下文时有效，但被上下文覆盖；3) SenticNet无帮助；4) 达到82.69%（4类）和67.07%（6类）加权F1；5) 悲伤话语左侧话语标记使用率（21.9%）显著低于其他情绪（28-32%）。

Conclusion: 对话上下文具有压倒性重要性，替代了其他架构组件的需求。语言学分析揭示情感与话语管理策略的内在联系，解释了悲伤情绪对历史对话的强依赖性，为情绪识别与生成提供了双重视角。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [54] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: 本文提出了一种针对时序知识图谱（TKG）推理的蒸馏框架，利用大语言模型作为教师模型，指导轻量级学生模型捕获时序依赖关系，在保持高效计算的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有TKG推理模型依赖庞大参数与高算力，导致硬件成本与能耗高，难以部署于低功耗平台。且传统压缩方法针对静态图设计，忽略时序依赖，导致性能下降。

Method: 提出时序知识图谱推理专属的蒸馏框架，结合大语言模型的结构与时序推理能力，将公共知识与任务相关时序信息融合，增强学生模型对动态时序关系的建模。

Result: 在多个基准数据集上实验表明，该方法在推理准确率、计算效率与可部署性间取得均衡优势，超越现有基线模型。

Conclusion: 所提框架可高效部署于资源受限场景，通过知识蒸馏平衡模型轻量化与时序推理性能，为未来AI应用提供可行方案。

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [55] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: 该研究针对医学领域检索增强生成(RAG)系统缺乏循证医学(EBM)原则的问题,提出将PICO框架嵌入知识图谱并开发基于贝叶斯的重排序算法,在运动康复领域构建了包含35万节点的知识图谱和1637个问答对的基准测试集,系统性能与专家评分均显示有效性。


<details>
  <summary>Details</summary>
Motivation: 1.现有RAG系统在检索时未实现临床问题(PICO)要素对齐;2.重排序阶段缺乏证据等级分层机制;3.运动康复领域缺乏RAG专用数据集与评估体系。旨在通过EBM方法解决上述问题。

Method: 1.基于PICO框架构建多模态知识图谱(药物、病症、人群等实体);2.设计贝叶斯优化算法动态调整证据等级权重;3.在运动康复领域进行实证研究,构建包含RCT、队列研究等多层次证据的数据集与基准测试集。

Result: 1.系统性能指标:片段覆盖率0.830,答案可信度0.819,PICOT匹配准确率78.8%;2.专家评估:4.66-4.84/5分(事实准确性、相关性、安全性);3.发布开放资源:超35万节点的知识图谱与1637个标注化QA对。

Conclusion: 1.将EBM框架嵌入RAG可有效提升医学检索的临床适用性;2.提出的证据等级优化算法优于传统加权方案;3.资源开放促进运动康复领域AI研究发展,方法论可迁移至其他医学领域。

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [56] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: 本研究提出了JP-TL-Bench，这是一个轻量级的公开基准测试工具，旨在通过参考无关的LLM配对比较方法，指导日英翻译系统的迭代开发，特别关注细微语言特征（如礼貌、省略）对自然度的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的翻译评估方法难以有效区分两个高质量译文的细微差异，尤其在日语-英语翻译中，礼貌级别、语用隐含等语言特征对可接受性判断有显著影响，需要更可靠的迭代优化指引。

Method: 构建包含固定版本基准集的协议：1) 使用大语言模型进行无需人工标注的配对比较 2) 通过Bradley-Terry模型聚合结果，计算胜率和基于逻辑变换的0-10LT标准化评分 3) 所有候选模型均与同一冻结基准集对比以确保评估稳定性

Result: 实现了一个结构稳定的评估系统，相同实验条件下（基准集/判别模型/聚合算法一致）的LT评分具有可重复性，有效量化了译文在礼貌、连贯等隐晦特征上的质量差异

Conclusion: JP-TL-Bench通过LLM判别器和结构化评分体系，解决了现有质量评估工具难以捕捉细微翻译差异的缺陷，特别适用于需要持续优化的语言特征敏感型翻译任务

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [57] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: This study evaluates LLMs' effectiveness in generating counterfactuals across six languages, finding that translation-based generation yields valid but less efficient results compared to English, reveals cross-lingual edit patterns, identifies recurring error types, and demonstrates multilingual CDA's limited performance gains due to quality restrictions.


<details>
  <summary>Details</summary>
Motivation: Although LLMs excel at English counterfactual generation, their capability to generate multilingual counterfactuals remains unproven. This study addresses this gap by systematically evaluating cross-lingual counterfactual generation quality and improvement strategies.

Method: 1) Automatic evaluation of counterfactuals via direct generation and English-translation approaches across six languages
2) Cross-linguistic analysis of edit patterns in high-resource European languages
3) Error pattern categorization across multiple languages
4) Comparative assessment of multilingual vs cross-lingual CDA effectiveness

Result: Key findings include: translation-based counterfactuals show higher validity but require more edits; similar edit patterns exist across European languages; four recurring error types identified; multilingual CDA achieves greater performance improvements than cross-lingual CDA, especially for low-resource languages, but quality limitations persist.

Conclusion: Current multilingual counterfactual generation faces quality-quantity tradeoffs, particularly for low-resource languages. Cross-lingual strategies reveal universal perturbation principles but translation methods cannot fully replicate native-language quality, with error patterns indicating specific areas for improvement in both generation and application contexts.

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [58] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 该研究发现量化会适度降低自解释(NLE和反事实示例)的质量和可靠性，但量化仍是一种有效的模型压缩技术。


<details>
  <summary>Details</summary>
Motivation: 填补量化对自解释影响研究的空白，因自解释对高风险应用的透明性至关重要

Method: 使用三种常见量化方法在不同位宽下测试两种自解释类型，并进行用户研究

Result: 量化导致SE质量下降最多4.4%，可靠性下降2.38%；用户研究显示一致性和可信度下降8.5%，大型模型在可靠性保持上更佳

Conclusion: 量化效果在不同应用场景差异显著，建议验证特定用例的SE质量，尤其敏感的NLE需注意，但量化有效性不受显著影响

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [59] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: 本文提出了DepFlow框架，通过解耦语音中的抑郁特征与语言内容，生成具有抑郁-语义不匹配特征的合成语音数据，用于提升抑郁症检测模型对伪装型抑郁症的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症数据集（如DAIC-WOZ）存在语言情感与诊断标签强关联问题，导致模型依赖语义捷径而非真实生理特征。该问题在伪装型抑郁症场景下尤为严重，此时患者可能用积极/中性语言掩盖抑郁状态。

Method: 1. 抑郁声学编码器（对抗训练）提取语者与内容无关的抑郁特征嵌入；2. 基于流匹配的TTS模型（FiLM调制）合成可控抑郁程度的语音；3. 基于原型的严重度映射机制实现抑郁连续谱的平滑可控生成。

Result: 编码器达到ROC-AUC 0.693，构建的CDoA数据集使三种检测模型宏平均F1分别提升9%、12%、5%，且显著优于传统数据增强方法。

Conclusion: DepFlow解决了抑郁症检测中的语义偏倚问题，为对话系统提供可控的合成平台，突破临床数据因伦理与覆盖范围限制而难以获取的瓶颈。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [60] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: This paper addresses the challenge of hallucinations in large language models (LLMs) by proposing a novel uncertainty quantification method (RU) that utilizes trap questions with fake names. Experiments show significant improvements over baseline methods, with ROCAUC increases of 0.1-0.2, offering a robust solution for real-world applications.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) face reliability issues due to hallucinations, particularly when handling non-canonical or adversarial questions. Existing uncertainty quantification methods fail in such scenarios, highlighting the need for a more robust approach to ensure trustworthy AI interactions.

Method: The study introduces a scenario-based framework using trap questions with intentionally fake names to expose hallucinations. A novel robust uncertainty quantification method (RU) was developed, tested across four models through extensive experiments to evaluate its effectiveness in detecting hallucinations under non-standard querying conditions.

Result: The trap question dataset excelled in identifying hallucinations, and the RU method outperformed existing baselines, achieving substantial gains in ROCAUC values (0.1-0.2 improvement). These results demonstrate superior uncertainty estimation capabilities in adversarial scenarios.

Conclusion: The proposed RU method provides a promising solution for mitigating LLM hallucinations in complex querying environments, offering actionable insights into improving model reliability through enhanced uncertainty quantification techniques.

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [61] [The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2601.00364)
*Jiandong Shao,Raphael Tang,Crystina Zhang,Karin Sevegnani,Pontus Stenetorp,Jianfei Yang,Yao Lu*

Main category: cs.CL

TL;DR: Removing bilingual data (only 2% of the corpus) drops translation performance by 56% in BLEU, but cross-lingual QA and reasoning remain stable. Parallel data restores 91% of translation performance, while code-switching has minimal impact.


<details>
  <summary>Details</summary>
Motivation: Investigate how bilingual data in pretraining contributes to cross-lingual performance of multilingual models, despite monolingual pretraining dominance.

Method: Pretrained models on a monolingual-only corpus (excluding multilingual documents) and reintroduced categorized bilingual data (parallel, code-switching) for ablation experiments.

Result: Translation performance drops 56% without bilingual data but recovers 91% via parallel data. Code-switching contributes little. Cross-lingual QA/reasoning remains stable across conditions.

Conclusion: Translation relies on systematic token-level alignments from parallel data, while cross-lingual understanding/reasoning can be achieved without bilingual data.

Abstract: Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.

</details>


### [62] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: BERT-JEPA（BEPA）通过引入JEPA训练目标解决BERT中[CLS]嵌入空间坍塌问题，并提升多语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决BERT模型中[CLS]嵌入空间坍塌问题，增强语言表征的多语言通用性。

Method: 在BERT中添加JEPA（联合嵌入预测架构）训练目标，将[CLS]编码空间转化为语言不可知的特征空间。

Result: 在多语言基准测试中展现出性能提升，验证了方法的有效性。

Conclusion: BEPA通过结构改进有效缓解嵌入坍塌，为多语言模型提供了更鲁棒的表征学习框架。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [63] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出Geo-R框架，通过结构化推理与强化学习实现无需外部检索的图像地理定位，提高了定位准确性并增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖合成数据或外部检索，限制了可解释性和泛化能力。Geo-R旨在通过纯文本-图像模型与真实地理坐标的直接结合，解决可解释性差和泛化能力弱的问题。

Method: 构建基于地理位置分层映射的'区域链'推理范式，结合基于Haversine距离的坐标对齐奖励机制，通过轻量化强化学习优化预测结果。

Result: 实验表明该方法在多个基准测试中取得了更优的定位精度、更强的泛化能力和更透明的推理过程，且无需外部检索或合成标签的辅助。

Conclusion: Geo-R通过结构化地理推理与空间监督的融合，建立了兼顾可扩展性和可解释性的新型地理定位范式，并推动了后续研究的开源发展。

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [64] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: This paper introduces judgeWEL, a new NER dataset for Luxembourgish created through a semi-automatic pipeline leveraging Wikipedia, Wikidata, and LLM-based verification, resulting in a corpus five times larger than existing resources.


<details>
  <summary>Details</summary>
Motivation: Building datasets for under-represented languages like Luxembourgish faces challenges due to resource scarcity, linguistic complexity, and high annotation costs. Existing methods risk inconsistency and limited coverage.

Method: Entities were automatically inferred from Wikipedia-Wikidata internal links via structured weak supervision. Noise from unreliable links was mitigated by filtering with multiple LLMs to retain high-quality annotations.

Result: Produced the largest Luxembourgish NER corpus with 5× more data than prior datasets, broader entity coverage, and balanced category distribution.

Conclusion: The proposed pipeline demonstrates that combining structured knowledge sources (Wikipedia/Wikidata) with LLM verification effectively addresses low-resource NER challenges, establishing a scalable framework for multilingual research.

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [65] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: 本文提出了一种新的超关系时序知识表示方法HTKGH，用于解决传统HTKG在处理多实体地缘政治事件中的局限性，通过构建htkgh-polecat数据集并评估LLMs在复杂场景下的预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有HTKG无法有效表示多主实体的复杂时序事实（如地缘政治事件），需寻求更高效的知识表示框架以支持复杂事件建模。

Method: 1) 提出HTKGH形式化模型，支持多实体关联的超关系表示；2) 构建基于POLECAT数据库的htkgh-polecat数据集；3) 对比评估LLMs在关系预测任务中的性能。

Result: HTKGH形式化模型成功实现后向兼容并支持复杂事件模式，htkgh-polecat成为首个支持多实体时序推理的公开数据集，LLM表现显示其在复杂预测任务中的潜力与局限性。

Conclusion: HTKGH扩展了知识表示的边界，证明多实体建模对地缘政治预测的必要性，揭示LLMs适应复杂时序推理的能力需针对性优化。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [66] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: 本研究比较三种轻量Transformer模型（DistilBERT、MiniLM、ALBERT）在企业多领域文本任务中的性能，在准确率和效率上各有优势。


<details>
  <summary>Details</summary>
Motivation: 企业场景需要兼顾效率与轻量化的多领域文本自动处理模型，但尚未明确哪类模型在不同任务中表现最优。

Method: 在IMDB、AG News和仇恨言论数据集上，比较模型在分类精度（acc/precision/recall/F1）与效率指标（模型尺寸、推理速度）的表现差异。

Result: ALBERT部分任务准确率最高，MiniLM推理速度最快，DistilBERT在精度一致性和效率间取得平衡，所有优化均在固定环境约束下完成。

Conclusion: 低延迟场景推荐MiniLM，均衡性能选DistilBERT，资源受限场景建议用ALBERT。

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [67] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: This paper explores how large language models (LLMs) can test linguistic theories, particularly contrasting social constructivist language game approaches with a new mathematical Semantic Field Theory, showing their complementary relationship.


<details>
  <summary>Details</summary>
Motivation: To examine how LLMs provide an empirical framework for testing theories of linguistic meaning, especially bridges philosophical and mathematical perspectives.

Method: Formalizing lexical/languages fields in continuous semantic spaces, then analyzing their alignment with transformers' properties, like distributed representations and attention mechanisms.

Result: LLMs successfully model semantic regularities via mathematical structures, while social factors (pragmatics, context) remain challenging, aligning with philosophical accounts.

Conclusion: Mathematical structure (Semantical Field Theory) and social grounding (language games) are complementary, guiding better AI architectures that merge statistical models with theoretical insights.

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [68] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: Defensive M2S压缩多轮对话为单轮形式训练安全检测模型，在保证93.8%攻击检测率的同时，将训练和推理的token消耗分别降低93×和94.6%。


<details>
  <summary>Details</summary>
Motivation: 现有安全防护模型处理多轮对话历史会消耗大量算力资源，如何在保障LLM部署安全性的同时降低计算成本是核心挑战。

Method: 提出M2S压缩范式：
1. 使用hyphenize/numberize/pythonize三种模版将多对话历史压缩为单轮输入
2. 理论分析证明训练复杂度从O(n²)降至O(n)
3. 在779个平均10.6轮的样本上验证效率提升

Result: • 训练token消耗从1570万降至16.9万（93×降本）
• 推理token消耗从3231降至173（94.6%降幅）
• 采用Qwen3Guard+hyphenize组合实现93.8%检测召回率
• 相比基线模型提升38.9个百分点检测能力

Conclusion: 通过对话压缩技术可实现安全检测系统的高效部署，在多轮越狱攻击检测中达到算力消耗与检测准确率的最优平衡。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [69] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 本论文提出了一种用于职业教育与培训（VET）领域数字化历史文档的命名实体识别（NER）方法，结合噪声感知训练（NAT）、合成OCR错误注入、迁移学习与多阶段微调，并开源代码


<details>
  <summary>Details</summary>
Motivation: OCR识别误差导致VET历史文档的NER效果下降，传统方法未能有效解决该问题，且现有研究缺乏对多实体类型的识别探索

Method: 将噪声感知训练（NAT）与合成OCR误差注入、跨语言迁移学习及三阶段微调结合，通过对比噪声数据、干净数据和合成数据的三种训练策略优化模型

Result: 领域适配与噪声感知微调使识别准确率提升37.2%，在德语文档测试中F1值达89.5%，模型具备跨语言适配能力且代码已开源

Conclusion: 融合噪声训练与迁移学习的多阶段微调框架有效提升了VET文档NER的鲁棒性，为历史文档处理提供了可复现的跨语言解决方案

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [70] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: 本论文通过分析复杂句子结构对基于规则的原子句提取的影响，使用WikiSplit数据集在spaCy中实现依赖解析规则的方法，结果显示系统在ROUGE和BERTScore指标上达到中高水平的准确率，但也暴露出对包含关系从句、并列谓词等复杂结构的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的原子句提取方法缺乏可解释性，且无系统研究特定从句结构对提取的影响，需通过规则化分析明确句法结构与提取难点的因果关系。

Method: 基于WikiSplit数据集，在spaCy框架中实现依赖语法分析规则，构建100组标准原子句数据集，采用ROUGE和BERTScore双维度进行评估。

Result: ROUGE-1 F1=0.6714，ROUGE-2 F1=0.478，ROUGE-L F1=0.650，BERTScore F1=0.5898的测试结果表明规则方法对关系从句、同位结构、并列谓词和被动语态等结构的识别准确率显著下降。

Conclusion: 基于规则的原子句提取方法虽具备一定准确性，但受句法结构复杂度制约明显，建议结合统计模型与语言学特征进行混合建模以提升复杂结构处理能力。

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [71] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

TL;DR: This paper surveys multi-hop question answering (QA) techniques, proposing a four-axis framework to systematically compare retrieval-reasoning processes across models. It highlights procedural trade-offs in effectiveness, efficiency, and evidence faithfulness, identifying open challenges like structure-aware planning and robust stopping criteria.


<details>
  <summary>Details</summary>
Motivation: Despite strong performance by RAG and agentic methods, existing retrieval-reasoning processes remain implicit, limiting systematic comparisons between model families. A unified framework is needed to analyze procedural choices and trade-offs.

Method: A four-axis framework is introduced: (A) execution plan, (B) index structure, (C) next-step control strategies, and (D) stop/continue criteria. The approach maps representative multi-hop QA systems onto this schema, synthesizing ablation studies on benchmarks (HotpotQA, MuSiQue) to identify procedural patterns.

Result: The framework reveals recurring trade-offs: 1) Effectiveness vs. efficiency in hop-by-hop reasoning, 2) Evidence faithfullness in chain-of-thought vs. iterative retrieval, and 3) Performance sensitivity to control policies. Distribution shift scenarios expose limitations in stop criteria robustness.

Conclusion: Explicit modeling of retrieval-reasoning execution procedures enables principled comparisons. Key challenges include developing structure-aware planning algorithms, transferable control policies, and distribution-shift robust stopping mechanisms, which are critical for future retrieval-reasoning agents.

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [72] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: The paper introduces ECR (Embedding Consistency Regulation) to address the collapse of semantic structure in compact models, enabling them to maintain task-aligned representations without complex architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing compression methods focus on superficial output alignment but fail to preserve the underlying semantic manifold, causing semantic drift in compact models. This leads to poor downstream task performance, especially in multilingual settings or low-capacity scenarios.

Method: ECR derives semantic anchors from precomputed teacher embeddings, then trains compact models to maintain consistent geometric relationships around these anchors. This approach avoids relying on logits or internal feature matching, requiring only an additional projection step during inference.

Result: ECR stabilizes training, preserves semantic structure across tasks/languages, and creates more compact, task-aligned representations. It outperforms conventional baselines in low-capacity settings, works without teacher outputs, and maintains compatibility with (but independence from) distillation frameworks.

Conclusion: ECR provides a lightweight solution to stabilize compact model training and maintain semantic fidelity, making compressed models more robust for deployment under strict efficiency or privacy constraints.

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [73] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: 提出了一种基于语言无关分层LoRA-MoE（HLoRA）框架的轻量级多语言语音识别（ASR）系统，通过CTC架构和域适应技术实现端到端的单次解码，无需预设语言标识，显著提升低资源多语言ASR效率。


<details>
  <summary>Details</summary>
Motivation: 大规模多语言ASR模型（如Whisper）性能较强但计算成本高，难以部署于边缘设备；现有方法需依赖先验语言信息或显式标签，限制推理效率。

Method: 在mHuBERT-CTC模型中集成HLoRA框架：1）分层设计包括多语言共享LoRA（学习语言无关声学特征）与语言特异性LoRA专家（建模语言依赖特征）；2）提出基于语言身份后验（LID）的LoRA路由机制，无需显式语言标签即可实现端到端解码。

Result: 在MSR-86K和MLC-SLM 2025数据集的实验表明，HLoRA以单次解码达到与先进两阶段方法相当的性能，且显著提升低资源场景的解码效率。

Conclusion: HLoRA框架成功实现了资源受限场景下无需语言先验信息的语言无关ASR解码，兼顾性能与效率，为多语言边缘部署提供有效方案。

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [74] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: 本文提出了InfoSynth框架，利用信息论原理自动生成和评估LLM基准测试，解决传统人工构建基准效率低、数据污染及多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理和代码生成能力提升后，亟需高效创建新颖且多样的基准测试以避免人工成本高、训练数据污染及评估能力偏差。

Method: 基于KL散度与熵构建不依赖模型评估的度量指标，设计端到端流程：使用遗传算法和迭代代码反馈从种子数据集合成Python编程问题，自动生成测试用例与解。

Result: 97%的自动生成正确解，合成基准相较种子数据集呈现更高新颖性与多样性，且能调控生成问题的难度和多样性。

Conclusion: InfoSynth提供可扩展的自验证基准构建流水线，有效提升LLM评估的准确性和实用性。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [75] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: 本文提出了一个针对中文特定安全对抗模式的基准测试CSSBench，用于评估轻量级大语言模型在中文场景下的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准主要针对英文，而中文恶意查询常利用同音字、拼音拆分等特定模式逃避检测，导致轻量级模型在中文安全评估中存在不足。

Method: 构建CSSBench基准测试，包含六大中文常见领域（如非法活动、隐私泄露），设计多样化的对抗性任务类型，并对主流轻量级LLM进行安全性测试与过拒绝行为分析。

Result: 实验显示中文特定对抗模式对轻量级模型构成关键挑战，模型易受这类扰动影响，且安全防护可能导致性能显著下降。

Conclusion: CSSBench为中文场景下的LLM安全部署提供评估工具，强调对抗中文特定攻击模式需针对性优化模型鲁棒性。

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [76] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: 提出一种无需修改模型的轻量方法，通过重复提示和LLM裁判机制，理论性降低上下文幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在确定性工作流中的幻觉问题需避免修改模型参数，现有方法缺乏理论保障。

Method: 重复独立提示运行+LLM判官机制+多数投票强化，数学证明错误率指数衰减。

Result: 实验验证重复次数和裁判数量增加时，错误率呈理论预期的指数级下降。

Conclusion: 证明多次采样与集成判官可系统性消除幻觉，提供概率保障的通用纠错框架。

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [77] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: 该论文提出FwPKM架构，通过动态更新参数解决序列建模中的存储与效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决语言模型中序列建模层的存储容量与计算效率之间的权衡问题。Softmax注意力机制具有高存储成本，而线性变体存储受限，FwPKM旨在提供两者的平衡。

Method: 方法包括将静态PKM转换为动态的'快速权重'记忆，并通过局部分块梯度下降在训练和推理时动态更新参数。

Result: 实验表明FwPKM在长时间上下文数据集上显著降低困惑度，并在Needle in a Haystack任务中能够泛化到128K token的上下文。

Conclusion: 结论认为FwPKM作为动态记忆模块，有效补充了标准模块的语义记忆，提升了模型性能。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [78] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: 论文提出Sigmoid Head模块，通过替换softmax激活函数和改进负采样训练，解决语言模型概率作为质量估计不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言多义性导致LM概率分布分散，softmax输出形式和单热编码训练数据限制模型评估多正确选项的能力，需设计无需人工标注的鲁棒质量估计方法。

Method: 在预训练LM上添加sigmoid激活的解嵌层作为质量估计模块；负采样阶段采用启发式策略排除潜在正确token，通过调整损失函数实现端到端训练。

Result: Sigmoid Head在质量得分判别力上显著优于原始softmax输出，在跨领域任务中表现出更强鲁棒性，且训练推理计算成本增加不足5%。

Conclusion: 该方法有效解决多候选答案场景下的概率校准问题，提出的sigmoid替代方案为质量估计提供了新的建模范式。

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [79] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: 本研究评估LLMs在三个下游任务（情感分析、仇恨语言识别、事实核查）中的文本片段识别能力，并探讨指令微调、上下文学习等策略的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要探索LLMs在NER等显式文本片段识别的局限性，而对ABSA等主观性更强的片段识别任务研究不足。作者旨在填补LLMs在非显式标注场景下的研究空白。

Method: 通过实验评估多种LLMs在三个NLP任务中的表现，采用指令调优、上下文学习、思维链等策略，分析文本内部关联性对片段识别的影响。

Result: 发现文本内部关联性能够显著提升LLMs识别精准度，不同LLM策略在非显式标注任务中展现出差异化性能。

Conclusion: LLMs在非传统显式标注任务如ABSA中具备潜力，文本局部关联性建模和策略适配是提升效果的关键因素。

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [80] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: This paper evaluates the adaptation of pre-trained transformer models (BCCRTron and GatorTron) for cross-provincial cancer registry automation in Canada, demonstrating that fine-tuning with provincial data and ensemble methods significantly improves cancer detection sensitivity while maintaining privacy through model weight sharing.


<details>
  <summary>Details</summary>
Motivation: Manual abstraction of pathology reports for cancer registries is time-consuming and error-prone, and the generalizability of transformer-based NLP systems across jurisdictions with differing reporting conventions is unclear. Cross-provincial adaptation could enable scalable, interoperable cancer surveillance but requires validation.

Method: BCCRTron (domain-adapted) and GatorTron (biomedical) were fine-tuned on ~104,000 and ~22,000 de-identified Newfoundland & Labrador pathology reports for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) classification. Complementary section-focused input pipelines were used. An OR-ensemble combined both models to maximize sensitivity. Privacy-preserving workflows shared only model weights between provinces.

Result: Stand-alone models maintained high performance after fine-tuning. Ensemble achieved Tier 1 recall of 0.99 (reducing missed cancers to 24 vs. 48/54 individually) and Tier 2 recall of 0.99 (33 missed vs. 54/46 individually). Ensemble reduced error overlap and improved robustness to cross-provincial reporting variations.

Conclusion: Transformer models pretrained in one jurisdiction can be effectively localized for cancer surveillance in another with minimal fine-tuning. Ensemble fusion of complementary models enhances sensitivity and interoperability, supporting province-to-province weight-sharing workflows. These findings establish a framework for future pan-Canadian foundation models in cancer registry systems.

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>
