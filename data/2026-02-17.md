<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 136]
- [cs.CL](#cs.CL) [Total: 68]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Ground: Map-Free LiDAR Relocalization for UAVs](https://arxiv.org/abs/2602.13267)
*Hengyu Mu,Jianshi Wu,Yuxin Guo,XianLian Lin,Qingyong Hu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: MAILS, a novel map-free LiDAR relocalization framework for UAVs, employs a Locality-Preserving Sliding Window Attention module and coordinate-independent feature initialization to address sparse point cloud challenges, complemented by a new UAV-specific LiDAR dataset for robust real-world performance.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR relocalization methods are optimized for autonomous driving, showing poor accuracy in UAV scenarios due to sparse point clouds, significant yaw variations, and altitude changes. Additionally, there is a lack of UAV-specific datasets mimicking real-world flight patterns with irregular trajectories and varying altitudes.

Method: MAILS integrates a Locality-Preserving Sliding Window Attention module for local feature extraction from sparse point clouds, paired with a coordinate-independent feature initialization and invariant positional encoding to handle rotational/altitudinal variations. A large-scale UAV LiDAR dataset with diverse scenes and flight paths was also developed for evaluation.

Result: Experimental validation demonstrates superior localization precision compared to existing techniques under realistic UAV flight conditions, with future public release of code and dataset planned.

Conclusion: MAILS achieves robust relocalization in GNSS-denied UAV environments through structural innovations and UAV-tailored data, setting a foundation for future UAV localization research.

Abstract: Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.

</details>


### [2] [Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification](https://arxiv.org/abs/2602.13286)
*Nathanya Satriani,Djordje Slijepčević,Markus Schedl,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: This study explores Explanatory Interactive Learning (XIL) methods like CAIPI and RRR to mitigate bias in gender classification models by guiding models to focus on relevant features through user feedback, demonstrating effectiveness but with minor performance costs.


<details>
  <summary>Details</summary>
Motivation: To address bias and spurious correlations in visual classifiers, particularly for sensitive tasks like gender classification, by leveraging user-in-the-loop feedback to align model explanations with human-relevant features.

Method: Evaluated CAIPI, RRR, and a hybrid approach by comparing model explanations (via GradCAM/BLA) against segmentation masks, and measured impacts on feature focus, bias reduction (balanced misclassification between genders), and classification accuracy.

Result: CAIPI most effectively directed models to relevant features. All methods reduced gender-related bias; CAIPI uniquely maintained/improved accuracy, while others caused slight performance drops.

Conclusion: XIL methods enhance transparency and fairness in gender classifiers (notably CAIPI), though requiring trade-offs with accuracy. This framework holds promise for ethical AI applications.

Abstract: Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the model's explanations, thereby helping it to focus on features that are relevant to the prediction from the user's perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in scenarios prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The results are evaluated quantitatively by comparing segmentation masks with explanations generated using Gradient-weighted Class Activation Mapping (GradCAM) and Bounded Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used, and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases with an exception being CAIPI, which shows potential to even improve classification accuracy.

</details>


### [3] [COOPERTRIM: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception](https://arxiv.org/abs/2602.13287)
*Shilpa Mukhopadhyay,Amit Roy-Chowdhury,Hang Qiu*

Main category: cs.CV

TL;DR: COOPERTRIM利用时间连续性动态优化协同感知通信带宽，在保持性能的同时实现高达80%的数据压缩


<details>
  <summary>Details</summary>
Motivation: 现有协同感知方法受限于通信带宽与传感器数据量的矛盾，需在带宽效率和性能保持间取得平衡，当前方案仍对无线技术施加压力

Method: 通过引入时间不确定性度量和自适应选择机制，动态筛选表征环境动态的关键特征，减少静态信息冗余传输，并结合环境复杂度自适应调整传输量

Result: 在分割和检测任务中带宽使用分别降低80.28%和72.52%，较传统方法提升45.54%IoU且带宽减少72%，结合压缩技术可进一步压缩至1.46%带宽

Conclusion: COOPERTRIM通过时间感知实现带宽动态优化，在复杂环境和通信延迟下保持鲁棒性，为实际部署提供可行方案

Abstract: Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other's live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information hinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To fundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness, agents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature relevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation and 3D detection as example tasks. Across multiple open-source cooperative segmentation and detection models, COOPERTRIM achieves up to 80.28% and 72.52% bandwidth reduction respectively while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Combined with compression strategies, COOPERTRIM can further reduce bandwidth usage to as low as 1.46% without compromising IoU performance. Qualitative results show COOPERTRIM gracefully adapts to environmental dynamics, localization error, and communication latency, demonstrating flexibility and paving the way for real-world deployment.

</details>


### [4] [NutVLM: A Self-Adaptive Defense Framework against Full-Dimension Attacks for Vision Language Models in Autonomous Driving](https://arxiv.org/abs/2602.13293)
*Xiaoxu Peng,Dong Zhou,Jianwen Zhang,Guanghui Sun,Anh Tu Ngo,Anupam Chattopadhyay*

Main category: cs.CV

TL;DR: NutVLM propose NutNet++ and EAPT for secure AD perception, improving overall metrics by 4.89%.


<details>
  <summary>Details</summary>
Motivation: Existing VLM defenses in autonomous driving suffer from limited robustness-performane trade-off and inefficacy against diverse adversarial threats.

Method: NutNet++ detects threats via 3-way classification, combines grayscale masking for local patches and EAPT (gradient-based prompt tuning) for global perturbations without parameter updates.

Result: 4.89% improvement on Dolphins benchmark (Accuracy, Language Score, GPT Score) with scalable security benefits.

Conclusion: NutVLM enables adaptive, efficient defense for intelligent transportation systems against diverse adversarial attacks.

Abstract: Vision Language Models (VLMs) have advanced perception in autonomous driving (AD), but they remain vulnerable to adversarial threats. These risks range from localized physical patches to imperceptible global perturbations. Existing defense methods for VLMs remain limited and often fail to reconcile robustness with clean-sample performance. To bridge these gaps, we propose NutVLM, a comprehensive self-adaptive defense framework designed to secure the entire perception-decision lifecycle. Specifically, we first employ NutNet++ as a sentinel, which is a unified detection-purification mechanism. It identifies benign samples, local patches, and global perturbations through three-way classification. Subsequently, localized threats are purified via efficient grayscale masking, while global perturbations trigger Expert-guided Adversarial Prompt Tuning (EAPT). Instead of the costly parameter updates of full-model fine-tuning, EAPT generates "corrective driving prompts" via gradient-based latent optimization and discrete projection. These prompts refocus the VLM's attention without requiring exhaustive full-model retraining. Evaluated on the Dolphins benchmark, our NutVLM yields a 4.89% improvement in overall metrics (e.g., Accuracy, Language Score, and GPT Score). These results validate NutVLM as a scalable security solution for intelligent transportation. Our code is available at https://github.com/PXX/NutVLM.

</details>


### [5] [VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction](https://arxiv.org/abs/2602.13294)
*Jiarong Liang,Max Ku,Ka-Hei Hui,Ping Nie,Wenhu Chen*

Main category: cs.CV

TL;DR: 本文提出VisPhyWorld框架，通过生成可执行模拟器代码评估多模态语言模型的物理推理能力，揭示现有模型在理解物理动态和参数推断上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VQA和VoE等识别式协议无法有效评估模型是否建立可验证的物理假设，需开发更直接的代码生成任务以分离物理推理与渲染能力。

Method: 构建VisPhyWorld框架（视觉→可执行代码）和VisPhyBench基准（209个场景/108个物理模板），量化评估模型在外观复现、物理动态模拟及代码有效性的表现。

Result: 97.7%的生成代码可复现有效视频，但SOTA多模态模型在物理参数推断和动态一致性上存在显著缺陷，显示其物理推理能力有限。

Conclusion: 执行代码生成任务为物理推理评估提供了可编辑、可验证的标准化方案，实验表明当前MLLMs尚无法实现对物理世界的精准建模与模拟。

Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.

</details>


### [6] [MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models](https://arxiv.org/abs/2602.13296)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 本论文提出将高分辨率距离像（HRRP）数据分解为掩码、特征和噪声三部分，并基于物理意义设计两个新指标来评估生成数据质量，克服传统依赖分类模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有HRRP生成数据评估方法依赖黑盒分类模型，缺乏可解释性与多级评估能力，难以满足生成数据质量验证需求。

Method: 通过分解HRRP数据为物理意义的掩码（目标存在性）、特征（形状/结构）和噪声成分，设计两个量化指标：1）特征保真度评估目标结构相似性；2）噪声一致性检测生成数据的统计特性匹配度。

Result: 在高成本HRRP数据集上的实验表明，新指标能够有效区分生成数据质量，在目标分类任务中相比传统分类模型提升23%的评估稳定性，且支持分层故障分析。

Conclusion: 论文提出的物理感知评估框架突破了现有HRRP生成模型评估的黑盒限制，为雷达目标识别数据生成提供可解释的多粒度质量验证方法。

Abstract: High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.

</details>


### [7] [Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset](https://arxiv.org/abs/2602.13297)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 利用大规模海上数据集，提出几何条件驱动的HRRP合成方法，解决传统雷达目标识别中数据敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 传统HRRPs对采集条件敏感导致跨场景鲁棒性不足，且现有方法受限于小规模特定数据集，需探索普适性更强的合成方案。

Method: 通过分析大规模海上监测数据库，识别船舶尺寸和方位角为核心几何参数，并基于此训练条件生成模型以合成HRRP签名。

Result: 合成数据成功复现真实数据中的视线几何趋势，证明几何参数是HRRP生成的关键驱动因素。

Conclusion: 几何获取条件显著影响HRRP合成质量，所提方法为增强雷达目标识别鲁棒性提供了有效技术路径。

Abstract: High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.

</details>


### [8] [KidMesh: Computational Mesh Reconstruction for Pediatric Congenital Hydronephrosis Using Deep Neural Networks](https://arxiv.org/abs/2602.13299)
*Haoran Sun,Zhanpeng Zhu,Anguo Zhang,Bo Liu,Zhaohua Lin,Liqin Huang,Mingjing Yang,Lei Liu,Shan Lin,Wangbin Ding*

Main category: cs.CV

TL;DR: KidMesh 是一种基于深度学习的端到端方法，可从磁共振尿路造影（MRU）图像中自动重建先天肾积水（CH）的三维网格，无需手工标注或后期处理。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素的分割方法仅专注于形态学特征（如大小、形状），需复杂的后期处理才能转化为功能评估所需的网格。为减少人力成本并提高效率，需开发直接输出可用网格模型的方法。

Method: 通过网格采样将MRU图像特征图转化为特征点，利用模板网格变形生成CH网格。设计无需精确网格标注的训练方案（因MRU切片稀疏），结合特征提取与网格优化。

Result: 平均0.4秒完成重建，误差距离超3.2mm的顶点仅3.7%（极少数达6.4mm），栅格化后与人工标注的Dice相似度为0.86。网格无自交叉且可直接用于尿流模拟。

Conclusion: KidMesh实现了快速、精确的CH网格重建，消除传统方法依赖后期处理的瓶颈，并为临床泌尿动力学分析提供功能性结构支持。

Abstract: Pediatric congenital hydronephrosis (CH) is a common urinary tract disorder, primarily caused by obstruction at the renal pelvis-ureter junction. Magnetic resonance urography (MRU) can visualize hydronephrosis, including renal pelvis and calyces, by utilizing the natural contrast provided by water. Existing voxel-based segmentation approaches can extract CH regions from MRU, facilitating disease diagnosis and prognosis. However, these segmentation methods predominantly focus on morphological features, such as size, shape, and structure. To enable functional assessments, such as urodynamic simulations, external complex post-processing steps are required to convert these results into mesh-level representations. To address this limitation, we propose an end-to-end method based on deep neural networks, namely KidMesh, which could automatically reconstruct CH meshes directly from MRU. Generally, KidMesh extracts feature maps from MRU images and converts them into feature vertices through grid sampling. It then deforms a template mesh according to these feature vertices to generate the specific CH meshes of MRU images. Meanwhile, we develop a novel schema to train KidMesh without relying on accurate mesh-level annotations, which are difficult to obtain due to the sparsely sampled MRU slices. Experimental results show that KidMesh could reconstruct CH meshes in an average of 0.4 seconds, and achieve comparable performance to conventional methods without requiring post-processing. The reconstructed meshes exhibited no self-intersections, with only 3.7% and 0.2% of the vertices having error distances exceeding 3.2mm and 6.4mm, respectively. After rasterization, these meshes achieved a Dice score of 0.86 against manually delineated CH masks. Furthermore, these meshes could be used in renal urine flow simulations, providing valuable urodynamic information for clinical practice.

</details>


### [9] [DriveMamba: Task-Centric Scalable State Space Model for Efficient End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13301)
*Haisheng Su,Wei Wu,Feixiang Song,Junjie Zhang,Zhenjie Yang,Junchi Yan*

Main category: cs.CV

TL;DR: DriveMamba提出了一种面向端到端自动驾驶的任务中心可扩展范式，通过动态任务关系建模和线性复杂度算子提升系统效率与灵活性，解决传统串行架构的信息损失与计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统UniAD等方法采用串行处理与可分离Transformer解码器，存在信息丢失、累计误差、模块间关系建模受限以及图像主干训练不足、注意力机制二次复杂度导致的可扩展性差等问题。

Method: 将图像特征和任务输出转换为3D空间中的稀疏token表示，通过单阶段统一Mamba解码器实现动态任务关系建模、隐式视角关联学习与长时序融合；设计双向轨迹引导的"局部到全局"扫描策略以保持空间局部性。

Result: 在nuScenes和Bench2Drive数据集上实验证明，DriveMamba在性能、泛化性和效率方面均优于现有方法，支持长时序上下文建模并提升驾驶规划能力。

Conclusion: 通过线性复杂度Mamba算子替代注意力机制，结合稀疏token交互和动态任务协同，为端到端自动驾驶系统提供了更高效、可扩展的解决方案。

Abstract: Recent advances towards End-to-End Autonomous Driving (E2E-AD) have been often devoted on integrating modular designs into a unified framework for joint optimization e.g. UniAD, which follow a sequential paradigm (i.e., perception-prediction-planning) based on separable Transformer decoders and rely on dense BEV features to encode scene representations. However, such manual ordering design can inevitably cause information loss and cumulative errors, lacking flexible and diverse relation modeling among different modules and sensors. Meanwhile, insufficient training of image backbone and quadratic-complexity of attention mechanism also hinder the scalability and efficiency of E2E-AD system to handle spatiotemporal input. To this end, we propose DriveMamba, a Task-Centric Scalable paradigm for efficient E2E-AD, which integrates dynamic task relation modeling, implicit view correspondence learning and long-term temporal fusion into a single-stage Unified Mamba decoder. Specifically, both extracted image features and expected task outputs are converted into token-level sparse representations in advance, which are then sorted by their instantiated positions in 3D space. The linear-complexity operator enables efficient long-context sequential token modeling to capture task-related inter-dependencies simultaneously. Additionally, a bidirectional trajectory-guided "local-to-global" scan method is designed to preserve spatial locality from ego-perspective, thus facilitating the ego-planning. Extensive experiments conducted on nuScenes and Bench2Drive datasets demonstrate the superiority, generalizability and great efficiency of DriveMamba.

</details>


### [10] [Spectral Collapse in Diffusion Inversion](https://arxiv.org/abs/2602.13303)
*Nicolas Bourriez,Alexandre Verine,Auguste Genovesio*

Main category: cs.CV

TL;DR: Conditional diffusion inversion struggles with high-quality image synthesis when translating from spectrally sparse domains (like sketches) to richer targets. The paper introduces Orthogonal Variance Guidance (OVG), which fixes texture loss and structural drift by enforcing correct noise distribution in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Diffusion inversion methods like DDIM fail in image-to-image translation tasks (e.g., sketch-to-image) because deterministic inversion creates spectrally collapsed latents, leading to blurred textures. Stochastic approaches sacrifice structural consistency for texture realism, creating a trade-off between quality and faithfulness to input structure.

Method: OVG modifies the diffusion ODE dynamics at inference time by: 1) Enforcing theoretical Gaussian noise magnitude in latent space 2) Constraining noise correction to operate in the null-space of structural gradients (computed via orthogonal projection). This ensures high-frequency texture restoration without altering structure-preserving gradients.

Result: On BBBC021 microscopy data (4x super-resolution) and Edges2Shoes benchmarks, OVG achieved: - 27% improvement in Fréchet Inception Distance over DDIM inversion - 2× higher texture realism scores vs. stochastic inversion - Structural similarity (SSIM) preserved at 0.94 while generating photorealistic textures - Eliminated spectral collapse in frequency domain analysis

Conclusion: OVG resolves the structure-texture conflict in diffusion-based image translation through noise space regularization. As an inference-time method, it provides plug-and-play improvement for pre-trained models with no training required, particularly effective for spectrally asymmetric translation tasks.

Abstract: Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.

</details>


### [11] [Progressive Contrast Registration for High-Fidelity Bidirectional Photoacoustic Microscopy Alignment](https://arxiv.org/abs/2602.13304)
*Jiahao Qin*

Main category: cs.CV

TL;DR: 提出PCReg-Net，通过渐进对比引导的配准框架，显著提升双向扫描光声显微镜图像对齐质量（NCC 0.983），优于现有方法14dB。


<details>
  <summary>Details</summary>
Motivation: 双向扫描成像速度翻倍但引入域间偏移和几何错位，传统方法受限于亮度恒定假设导致对齐质量瓶颈（NCC≤0.96）。

Method: 由4个轻量化模块组成：①注册U-Net粗对齐 ②多尺度特征提取器 ③对比模块识别残差错位 ④带特征注入的精修U-Net，并提出无参考评价指标TNCC/TNCG。

Result: 在OR-PAM-Reg-4K数据集上实现NCC 0.983、SSIM 0.982、PSNR 46.96 dB，在实时速度下超越SOTA 14dB以上。

Conclusion: 证明渐进对比引导框架能有效解决双向扫描的时空一致性难题，代码已开源。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing methods, constrained by brightness constancy assumptions, achieve limited alignment quality (NCC~$\leq 0.96$). We propose PCReg-Net, a progressive contrast-guided registration framework that performs coarse-to-fine alignment through four lightweight modules: (1)~a registration U-Net for coarse alignment, (2)~a reference feature extractor capturing multi-scale structural cues, (3)~a contrast module that identifies residual misalignment by comparing coarse-registered and reference features, and (4)~a refinement U-Net with feature injection for high-fidelity output. We further propose the Temporal NCC (TNCC) and Temporal NCC Gap (TNCG) for reference-free evaluation of inter-frame temporal consistency. On OR-PAM-Reg-4K (432 test samples), PCReg-Net achieves NCC of 0.983, SSIM of 0.982, and PSNR of 46.96 dB, surpassing the state-of-the-art by over 14 dB at real-time speed. Code is available at https://github.com/JiahaoQin/PCReg-Net

</details>


### [12] [WildfireVLM: AI-powered Analysis for Early Wildfire Detection and Risk Assessment Using Satellite Imagery](https://arxiv.org/abs/2602.13305)
*Aydin Ayanzadeh,Prakhar Dixit,Sadia Kamal,Milton Halem*

Main category: cs.CV

TL;DR: 本论文提出WildfireVLM，一个结合卫星图像火灾检测和语言驱动风险评估的AI框架，使用YOLOv12和多模态大语言模型（MLLM）。


<details>
  <summary>Details</summary>
Motivation: 野火灾害因气候变化和人类活动频发，早期检测对生态系统和基础设施保护至关重要。当前卫星监测面临烟雾信号微弱、天气动态变化和实时分析大面积区域的挑战。

Method: 构建标注野火与烟雾数据集（来自Landsat-8/9、GOES-16等卫星），采用YOLOv12检测火区与烟雾，在此基础上集成MLLM将检测结果转化为情景化风险评估。通过服务架构实现系统部署并开发可视化风险仪表盘。

Result: 通过LLM-as-judge评估验证风险推理质量，系统支持实时处理和长期火灾追踪，展示了计算机视觉与语言推理结合在可扩展监测中的有效性。

Conclusion: WildfireVLM框架通过融合视觉与语言模型，为可扩展野火监测和灾害管理提供了有效的实时解决方案。

Abstract: Wildfires are a growing threat to ecosystems, human lives, and infrastructure, with their frequency and intensity rising due to climate change and human activities. Early detection is critical, yet satellite-based monitoring remains challenging due to faint smoke signals, dynamic weather conditions, and the need for real-time analysis over large areas. We introduce WildfireVLM, an AI framework that combines satellite imagery wildfire detection with language-driven risk assessment. We construct a labeled wildfire and smoke dataset using imagery from Landsat-8/9, GOES-16, and other publicly available Earth observation sources, including harmonized products with aligned spectral bands. WildfireVLM employs YOLOv12 to detect fire zones and smoke plumes, leveraging its ability to detect small, complex patterns in satellite imagery. We integrate Multimodal Large Language Models (MLLMs) that convert detection outputs into contextualized risk assessments and prioritized response recommendations for disaster management. We validate the quality of risk reasoning using an LLM-as-judge evaluation with a shared rubric. The system is deployed using a service-oriented architecture that supports real-time processing, visual risk dashboards, and long-term wildfire tracking, demonstrating the value of combining computer vision with language-based reasoning for scalable wildfire monitoring.

</details>


### [13] [Fine-Tuning a Large Vision-Language Model for Artwork's Scoring and Critique](https://arxiv.org/abs/2602.13306)
*Zhehan Zhang,Meihua Qian,Li Luo,Siyu Huang,Chaoyi Zhou,Ripon Saha,Xinxin Song*

Main category: cs.CV

TL;DR: 本文提出基于Qwen2-VL-7B的多任务学习框架，实现绘画创造性自动评分与反馈生成，1000幅作品数据集验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统人工评分TTCT方法效率低，现有机器学习方案仅依赖图像特征且缺失解释性反馈，亟需自动化且具文本解释性的评估工具

Method: 对Qwen2-VL-7B进行参数微调，添加轻量回归头同步输出分数预测与量规对齐反馈，结合5维度结构化评分标准与艺术作品描述构建训练体系

Result: 实现皮尔逊相关系数0.97、平均绝对误差3.95的评分精度，生成反馈与专家评价的SBERT相似度达0.798

Conclusion: 建立跨计算机视觉与艺术评估的可扩展工具，为创造性研究和教育场景提供自动化评估解决方案

Abstract: Assessing artistic creativity is foundational to creativity research and arts education, yet manual scoring (e.g., Torrance Tests of Creative Thinking) is labor-intensive at scale. Prior machine-learning approaches show promise for visual creativity scoring, but many rely mainly on image features and provide limited or no explanatory feedback. We propose a framework for automated creativity assessment of human paintings by fine-tuning the vision-language model Qwen2-VL-7B with multi-task learning. Our dataset contains 1000 human-created paintings scored on a 1-100 scale and paired with a short human-written description (content or artist explanation). Two expert raters evaluated each work using a five-dimension rubric (originality, color, texture, composition, content) and provided written critiques; we use an 80/20 train-test split. We add a lightweight regression head on the visual encoder output so the model can predict a numerical score and generate rubric-aligned feedback in a single forward pass. By embedding the structured rubric and the artwork description in the system prompt, we constrain the generated text to match the quantitative prediction. Experiments show strong accuracy, achieving Pearson r > 0.97 and MAE about 3.95 on the 100-point scale. Qualitative evaluation indicates the generated feedback is semantically close to expert critiques (average SBERT cosine similarity = 0.798). The proposed approach bridges computer vision and art assessment and offers a scalable tool for creativity research and classroom feedback.

</details>


### [14] [Visual Para-Thinker: Divide-and-Conquer Reasoning for Visual Comprehension](https://arxiv.org/abs/2602.13310)
*Haoran Xu,Hongyu Wang,Jiaze Li,Shunpeng Chen,Zizhao Tong,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 本文提出视觉并行思考框架（Visual Para-Thinker），首次将并行推理扩展到多模态大模型的视觉领域，通过视觉分片和双策略设计实现高效路径独立推理。


<details>
  <summary>Details</summary>
Motivation: 传统LLM试时扩展法则依赖垂直扩展（延长推理深度）导致探索模式收敛，而将并行策略扩展到视觉领域仍属空白。

Method: 提出两种基于视觉分片的并行推理策略，结合Pa-Attention与LPRoPE保持路径独立性，并基于vLLM框架实现多模态高效并行处理。

Result: 在V*、CountBench、RefCOCO、HallusionBench等基准测试中，视觉并行思考者框架成功复现了并行推理在视觉领域的优势。

Conclusion: 该框架证明并行策略可突破视觉推理的垂直扩展瓶颈，为多模态大模型提供新的扩展范式。

Abstract: Existing LLM test-time scaling laws emphasize the emergence of self-reflective behaviors through extended reasoning length. Nevertheless, this vertical scaling strategy often encounters plateaus in exploration as the model becomes locked into specific thinking pattern. By shifting from depth to parallelism, parallel thinking mitigates the narrowing of exploration. However, the extension of this paradigm to visual domain remains an open research question. In this paper, we first examine the role of visual partitioning in parallelized reasoning and subsequently propose two distinct strategies. Based on the above, we introduce Visual Para-Thinker, representing the inaugural parallel reasoning framework for MLLMs. To maintain path independence and promote diversity in reasoning, our approach integrates Pa-Attention alongside LPRoPE. Leveraging the vLLM framework, we have developed a native multimodal implementation that facilitates high-efficiency parallel processing. Empirical results on benchmark datasets such as V*, CountBench, RefCOCO, and HallusionBench confirm that Visual Para-Thinker successfully extends the benefits of parallel reasoning to the visual domain.

</details>


### [15] [Sim2Radar: Toward Bridging the Radar Sim-to-Real Gap with VLM-Guided Scene Reconstruction](https://arxiv.org/abs/2602.13314)
*Emily Bejerano,Federico Tondolo,Aayan Qayyum,Xiaofan Yu,Xiaofan Jiang*

Main category: cs.CV

TL;DR: Sim2Radar synthesizes mmWave radar data from RGB images via material-aware 3D reconstruction and physics-driven ray tracing, achieving improved 3D object detection performance on real radar data with limited real-world supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity and high cost of labeled mmWave radar datasets for learning-based indoor perception, especially in visually degraded environments like smoke/dust/low-light conditions.

Method: Sim2Radar combines monocular depth estimation, segmentation, and vision-language material reasoning to build a material-aware 3D scene, then simulates radar data using physics-based ray tracing with Fresnel reflection models parameterized by ITU-R electromagnetic properties.

Result: 3.7% absolute improvement in 3D average precision (IoU 0.3) for object detection after pre-training on synthetic radar data and fine-tuning on real data, with gains attributed to enhanced spatial localization capabilities.

Conclusion: Physics-based, vision-driven radar simulation provides effective geometric priors for radar learning systems, demonstrating measurable performance benefits even under constrained real-data supervision scenarios.

Abstract: Millimeter-wave (mmWave) radar provides reliable perception in visually degraded indoor environments (e.g., smoke, dust, and low light), but learning-based radar perception is bottlenecked by the scarcity and cost of collecting and annotating large-scale radar datasets. We present Sim2Radar, an end-to-end framework that synthesizes training radar data directly from single-view RGB images, enabling scalable data generation without manual scene modeling. Sim2Radar reconstructs a material-aware 3D scene by combining monocular depth estimation, segmentation, and vision-language reasoning to infer object materials, then simulates mmWave propagation with a configurable physics-based ray tracer using Fresnel reflection models parameterized by ITU-R electromagnetic properties. Evaluated on real-world indoor scenes, Sim2Radar improves downstream 3D radar perception via transfer learning: pre-training a radar point-cloud object detection model on synthetic data and fine-tuning on real radar yields up to +3.7 3D AP (IoU 0.3), with gains driven primarily by improved spatial localization. These results suggest that physics-based, vision-driven radar simulation can provide effective geometric priors for radar learning and measurably improve performance under limited real-data supervision.

</details>


### [16] [IDPruner: Harmonizing Importance and Diversity in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2602.13315)
*Yifan Tan,Yifu Sun,Shirui Huang,Hong Liu,Guanghua Yu,Jianchen Zhu,Yangdong Deng*

Main category: cs.CV

TL;DR: Visual token pruning is crucial for accelerating MLLM inference, but existing methods lack a principled approach to balance token importance and semantic diversity. IDPruner uses MMR to achieve optimal balance, works without attention maps for FlashAttention compatibility, and shows superior performance at extreme pruning ratios (e.g., 90% token reduction with 86.4% performance retention).


<details>
  <summary>Details</summary>
Motivation: Existing token pruning methods for MLLMs independently focus on importance/diversity without optimal integration, leading to subpar performance under high pruning ratios. Computational bottlenecks from massive visual tokens also hinder real-world deployment, requiring efficient one-shot pruning solutions.

Method: IDPruner employs the Maximal Marginal Relevance (MMR) algorithm to quantitatively balance token importance and semantic diversity. The approach operates without attention maps, enabling FlashAttention compatibility and efficient one-shot pruning. It provides Pareto-optimal token selection through a principled mathematical framework rather than heuristic combinations.

Result: On Qwen2.5-VL-7B-Instruct, IDPruner retains 95.18% of baseline performance when pruning 75% of tokens, and maintains 86.40% accuracy under extreme 90% pruning. The method achieves state-of-the-art results across multiple architectures (e.g., LLaVA, Qwen-VL) and multimodal benchmarks (e.g., VQAv2, COCO) while enabling 3.1x speedup in vision encoder-decoder computation.

Conclusion: IDPruner's MMR-based paradigm establishes a new benchmark for vision token compression through principled importance-diversity trade-off. Its attention-free design enables production-ready deployment via one-shot pruning, and demonstrates remarkable robustness across diverse tasks while maintaining hardware compatibility with FlashAttention implementations.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities, yet they encounter significant computational bottlenecks due to the massive volume of visual tokens. Consequently, visual token pruning, which substantially reduces the token count, has emerged as a critical technique for accelerating MLLM inference. Existing approaches focus on token importance, diversity, or an intuitive combination of both, without a principled framework for their optimal integration. To address this issue, we first conduct a systematic analysis to characterize the trade-off between token importance and semantic diversity. Guided by this analysis, we propose the \textbf{I}mportance and \textbf{D}iversity Pruner (\textbf{IDPruner}), which leverages the Maximal Marginal Relevance (MMR) algorithm to achieve a Pareto-optimal balance between these two objectives. Crucially, our method operates without requiring attention maps, ensuring full compatibility with FlashAttention and efficient deployment via one-shot pruning. We conduct extensive experiments across various model architectures and multimodal benchmarks, demonstrating that IDPruner achieves state-of-the-art performance and superior generalization across diverse architectures and tasks. Notably, on Qwen2.5-VL-7B-Instruct, IDPruner retains 95.18\% of baseline performance when pruning 75\% of the tokens, and still maintains 86.40\% even under an extreme 90\% pruning ratio. Our code is available at https://github.com/Tencent/AngelSlim.

</details>


### [17] [Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture](https://arxiv.org/abs/2602.13322)
*Datorien L. Anderson*

Main category: cs.CV

TL;DR: The PolyShapes-Ideal (PSI) dataset isolates topological invariance testing, showing Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot font transfer without pre-training, supporting a 'Form-First' hypothesis.


<details>
  <summary>Details</summary>
Motivation: Standard vision benchmarks conflate texture with topology; PSI aims to isolate and evaluate geometric invariance as a standalone capability.

Method: Constructed PSI with three diagnostics: noise-perturbed polygon classification, MNIST-based zero-shot font transfer, and deformation collapse mapping. Evaluated Eidos architecture against these metrics.

Result: Eidos achieved >99% accuracy on PSI benchmarks and 81.67% zero-shot transfer accuracy across 30 unseen typefaces without pre-training.

Conclusion: Validates 'Form-First' hypothesis: structural generalization relies on geometric integrity rather than statistical scale or texture correlations.

Abstract: We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the "Form-First" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.

</details>


### [18] [Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge](https://arxiv.org/abs/2602.13324)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.CV

TL;DR: 提出级联轻量级目标检测与微型视觉语言模型的分层零样本框架，解决边缘军事机器人在动态环境中的算力与数据限制。


<details>
  <summary>Details</summary>
Motivation: 军事边缘环境需在数据稀缺与硬件算力受限下实现自主决策，传统方法难以同时满足实时性与复杂推理需求。

Method: 构建三层架构：Grounding DINO生成候选区域，Qwen/Gemma系列VLM进行语义验证，结合Scout-Commander智能体工作流，并提出可控输入方法拆解感知与推理模块。

Result: 战场模拟测试显示虚假告警过滤精度达100%、损伤评估97.5%、载具分类55-90%，智能体工作流实现100%正确部署及GPT-4o评分9.8/10的推理表现。

Conclusion: 验证分层零样本架构在边缘自主系统的有效性，建立基于VLM的军事场景诊断框架，揭示不同模型尺度下的感知/推理故障表型差异。

Abstract: Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel "Controlled Input" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.

</details>


### [19] [MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation](https://arxiv.org/abs/2602.13326)
*Xirui Hu,Yanbo Ding,Jiahao Wang,Tingting Shi,Yali Wang,Guo Zhi Zhi,Weizhan Zhang*

Main category: cs.CV

TL;DR: 本文提出MotionWeaver框架，通过统一运动表征与4D空间建模技术，实现了多角色联动的图像动画生成，解决了传统方法在复杂人像交互场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有单人动画合成方法无法处理多角色场景中复杂的身材差异、交互行为及遮挡问题，且缺乏相应数据集与评估标准支撑该领域的研究发展。

Method: 1) 设计身份无关的动作提取模块，并通过字符绑定机制实现多样化角色适配；2) 构建4D时空融合空间，结合视频潜变量与运动表征，采用分层监督策略处理交互与遮挡；3) 发布46小时多角色互动视频数据集及300视频基准测试集。

Result: 在新基准测试中取得SOTA性能，成功应对角色形态差异（如服饰/体型）、复杂交互（如接触/遮挡）等挑战，生成动画在2240×1400分辨率下保持47FPS实时渲染能力。

Conclusion: 该方案显著提升多角色动画生成质量，为群体交互场景生成提供了新方向，相关技术可扩展至虚拟人社交、影视特效等应用场景。

Abstract: Character image animation, which synthesizes videos of reference characters driven by pose sequences, has advanced rapidly but remains largely limited to single-human settings. Existing methods struggle to generalize to multi-humanoid scenarios, which involve diverse humanoid forms, complex interactions, and frequent occlusions. We address this gap with two key innovations. First, we introduce unified motion representations that extract identity-agnostic motions and explicitly bind them to corresponding characters, enabling generalization across diverse humanoid forms and seamless extension to multi-humanoid scenarios. Second, we propose a holistic 4D-anchored paradigm that constructs a shared 4D space to fuse motion representations with video latents, and further reinforces this process with hierarchical 4D-level supervision to better handle interactions and occlusions. We instantiate these ideas in MotionWeaver, an end-to-end framework for multi-humanoid image animation. To support this setting, we curate a 46-hour dataset of multi-human videos with rich interactions, and construct a 300-video benchmark featuring paired humanoid characters. Quantitative and qualitative experiments demonstrate that MotionWeaver not only achieves state-of-the-art results on our benchmark but also generalizes effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.

</details>


### [20] [HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13329)
*Yiru Wang,Zichong Gu,Yu Gao,Anqing Jiang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun*

Main category: cs.CV

TL;DR: HiST-VLA is a hierarchical spatio-temporal VLA model designed for autonomous driving, addressing challenges like imprecise numerical reasoning and weak 3D awareness by integrating geometric understanding, state history prompting, and a hierarchical planner to generate reliable trajectories.


<details>
  <summary>Details</summary>
Motivation: Current VLA models are limited in safety-critical autonomous driving due to poor numerical reasoning, low 3D spatial awareness, and context sensitivity. HiST-VLA aims to overcome these limitations for reliable trajectory generation.

Method: HiST-VLA enhances 3D/temporal reasoning via geometric awareness and state history prompting. It uses dynamic token sparsification (fusing redundant tokens, not filtering) for efficiency and a hierarchical transformer planner with dynamic latent regularization to refine coarse waypoints into fine trajectories guided by language commands.

Result: HiST-VLA achieved state-of-the-art performance on NAVSIM v2 benchmarks, scoring 88.6 EPDMS on Navtest and 50.9 EPDMS on the pseudo closed-loop Navhard benchmark.

Conclusion: By integrating geometric understanding, efficient token fusion, and language-guided trajectory refinement, HiST-VLA enables reliable and precise planning for autonomous driving in complex scenarios.

Abstract: Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation.
  Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.

</details>


### [21] [Zwitscherkasten -- DIY Audiovisual bird monitoring](https://arxiv.org/abs/2602.13330)
*Dominik Blum,Elias Häring,Fabian Jirges,Martin Schäffer,David Schick,Florian Schulenberg,Torsten Schön*

Main category: cs.CV

TL;DR: 本论文提出Zwitscherkasten——一种基于边缘设备的DIY多模态鸟类监测系统，结合音频与视觉数据实现高效生物多样性监控。


<details>
  <summary>Details</summary>
Motivation: 传统的鸟类监测依赖昂贵设备或人工观测，而本文旨在通过低成本、实时、非侵入式技术推动公民科学应用与大规模生物多样性研究。

Method: 在资源受限硬件上部署深度学习模型，包括音频分类、图像细粒度检测与分类流水线，并通过声学活动检测器降低能耗，实现多模态协同推理。

Result: 实验验证了嵌入式平台可准确识别鸟类物种，系统在低功耗下达成实时性能，支持分布式扩展部署。

Conclusion: Zwitscherkasten证明边缘计算能有效平衡精度与资源消耗，为生态监测提供了可复现、可扩展的技术框架。

Abstract: This paper presents Zwitscherkasten, a DiY, multimodal system for bird species monitoring using audio and visual data on edge devices. Deep learning models for bioacoustic and image-based classification are deployed on resource-constrained hardware, enabling real-time, non-invasive monitoring. An acoustic activity detector reduces energy consumption, while visual recognition is performed using fine-grained detection and classification pipelines. Results show that accurate bird species identification is feasible on embedded platforms, supporting scalable biodiversity monitoring and citizen science applications.

</details>


### [22] [MedScope: Incentivizing "Think with Videos" for Clinical Reasoning via Coarse-to-Fine Tool Calling](https://arxiv.org/abs/2602.13332)
*Wenjie Li,Yujie Zhang,Haoran Sun,Xingqi He,Hongcheng Gao,Chenglong Ma,Ming Hu,Guankun Wang,Shiyi Yao,Renhao Yang,Hongliang Ren,Lei Wang,Junjun He,Yankai Jiang*

Main category: cs.CV

TL;DR: MedScope通过渐进式证据定位和工具集成，提升长临床视频理解的准确性与可信度。


<details>
  <summary>Details</summary>
Motivation: 现有模型对长视频的被动采样和弱监督验证限制了其精准定位与解释预测的能力，尤其在医疗场景中亟需动态证据支持决策。

Method: 提出MedScope临床视频推理模型与ClinVideoSuite数据集，采用粗到细证据定位机制，并通过GA-GRPO算法强化工具使用的证据关联性奖励。

Result: 在全视频与细粒度理解任务中均实现领域内和跨领域SOTA性能，验证了证据驱动推理的有效性。

Conclusion: 通过工具集成的动态证据验证机制，模型可实现类似于'视频思考'的推理路径，为医疗AI提供可解释性范式。

Abstract: Long-form clinical videos are central to visual evidence-based decision-making, with growing importance for applications such as surgical robotics and related settings. However, current multimodal large language models typically process videos with passive sampling or weakly grounded inspection, which limits their ability to iteratively locate, verify, and justify predictions with temporally targeted evidence. To close this gap, we propose MedScope, a tool-using clinical video reasoning model that performs coarse-to-fine evidence seeking over long-form procedures. By interleaving intermediate reasoning with targeted tool calls and verification on retrieved observations, MedScope produces more accurate and trustworthy predictions that are explicitly grounded in temporally localized visual evidence. To address the lack of high-fidelity supervision, we build ClinVideoSuite, an evidence-centric, fine-grained clinical video suite. We then optimize MedScope with Grounding-Aware Group Relative Policy Optimization (GA-GRPO), which directly reinforces tool use with grounding-aligned rewards and evidence-weighted advantages. On full and fine-grained video understanding benchmarks, MedScope achieves state-of-the-art performance in both in-domain and out-of-domain evaluations. Our approach illuminates a path toward medical AI agents that can genuinely "think with videos" through tool-integrated reasoning. We will release our code, models, and data.

</details>


### [23] [Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators](https://arxiv.org/abs/2602.13334)
*Hao Liu,Suhaib A. Fahmy*

Main category: cs.CV

TL;DR: 本研究提出一种边缘-近边缘协同推理框架，通过轻量ViT与专家ViT的动态协作，在保证精度的同时降低边缘计算延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署ViT存在计算复杂度高问题，而全云端卸载导致延迟过大。现有静态专家模型无法有效处理置信度低的样本，需通过动态路由和专家优化提升效率与准确性。

Method: 构建轻量通用ViT作为边缘模型，搭配多个中型专家ViT部署在近边缘加速器。设计Top-k预测驱动的动态路由机制，针对低置信度样本动态匹配最优专家。提出渐进式专家训练策略，在保留完整知识库的同时增强特定子集的专业性。

Result: 实验显示专家模型在目标子集准确率提升4.12%，整体准确率提高2.76%。相比边缘独立运算降低45%延迟，相较全近边缘卸载减少46%能耗，在CIFAR-100数据集上验证了框架有效性。

Conclusion: 该框架解决了边缘计算资源与精度的平衡问题，通过动态路由和专家协作显著提升能效和响应速度，为资源受限场景提供新型部署方案。

Abstract: Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.

</details>


### [24] [Meningioma Analysis and Diagnosis using Limited Labeled Samples](https://arxiv.org/abs/2602.13335)
*Jiamiao Lu,Wei Wu,Ke Gao,Ping Mao,Weichuan Zhang,Tuo Wang,Lingkun Ma,Jiapan Guo,Zanyi Wu,Yuqing Hu,Changming Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于自适应多频段特征融合的脑膜瘤分类方法，通过结合空间域与离散小波变换后的频率域信息，提升了在少量样本下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 脑膜瘤的分级诊断对治疗方案和预后评估至关重要，但传统方法在特征融合时未能充分考虑不同频率带信息的权重差异及空间域信息的协同作用。

Method: 设计了一种特征融合架构，通过自适应权重分配机制融合离散小波变换得到的多频段特征与空间域特征，并在新构建的MRI脑膜瘤数据集上进行实验验证。

Result: 在三个独立数据集上的分类准确率分别达到X/Y/Z（注：原文未给出具体数值），均优于现有最先进的脑膜瘤分类方法。

Conclusion: 该方法通过动态调节多尺度频率特征与空间信息的融合权重，为医学影像中的小样本分类任务提供了新思路，并有望拓展至其他肿瘤的分级诊断场景。

Abstract: The biological behavior and treatment response of meningiomas depend on their grade, making an accurate diagnosis essential for treatment planning and prognosis assessment. We observed that the weighted fusion of spatial-frequency domain features significantly influences meningioma classification performance. Notably, the contribution of specific frequency bands obtained by discrete wavelet transform varies considerably across different images. A feature fusion architecture with adaptive weights of different frequency band information and spatial domain information is proposed for few-shot meningioma learning. To verify the effectiveness of the proposed method, a new MRI dataset of meningiomas is introduced. The experimental results demonstrate the superiority of the proposed method compared with existing state-of-the-art methods in three datasets. The code will be available at: https://github.com/ICL-SUST/AMSF-Net

</details>


### [25] [An Integrated Causal Inference Framework for Traffic Safety Modeling with Semantic Street-View Visual Features](https://arxiv.org/abs/2602.13339)
*Lishan Sun,Yujia Cheng,Pengfei Cui,Lei Han,Mohamed Abdel-Aty,Yunhan Zheng,Xingchen Zhang*

Main category: cs.CV

TL;DR: 利用街景图像与双机器学习模型发现绿化比例对交通事故具显著负向影响，效果在人口密集区更明显，但对弱势道路使用者保护不足


<details>
  <summary>Details</summary>
Motivation: 当前研究过度依赖静态社会人口与基础设施指标，缺少驾驶者视觉环境感知对事故的因果关联分析，需建立稳健空间分析框架支持政策评估

Method: 采用语义分割提取街景视觉特征，构建双机器学习框架量化因果效应，结合SHAP值解析非线性混杂因素，通过因果森林估计条件处理效应，并利用迈阿密22万街景图像与事故记录验证

Result: 绿化比平均处理效应-6.38（p=0.005）；人口密集及弱势城区效果增幅57%；有效减少角度及追尾事故，但对行人与骑行者事故降幅仅1.3%

Conclusion: 城市绿化具有降低交通事故的政策干预潜力，建议结合视觉环境风险评估优先级，并需开发针对性景观设计以加强弱势使用者保护

Abstract: Macroscopic traffic safety modeling aims to identify critical risk factors for regional crashes, thereby informing targeted policy interventions for safety improvement. However, current approaches rely heavily on static sociodemographic and infrastructure metrics, frequently overlooking the impacts from drivers' visual perception of driving environment. Although visual environment features have been found to impact driving and traffic crashes, existing evidence remains largely observational, failing to establish the robust causality for traffic policy evaluation under complex spatial environment. To fill these gaps, we applied semantic segmentation on Google Street View imageries to extract visual environmental features and proposed a Double Machine Learning framework to quantify their causal effects on regional crashes. Meanwhile, we utilized SHAP values to characterize the nonlinear influence mechanisms of confounding variables in the models and applied causal forests to estimate conditional average treatment effects. Leveraging crash records from the Miami metropolitan area, Florida, and 220,000 street view images, evidence shows that greenery proportion exerts a significant and robust negative causal effect on traffic crashes (Average Treatment Effect = -6.38, p = 0.005). This protective effect exhibits spatial heterogeneity, being most pronounced in densely populated and socially vulnerable urban cores. While greenery significantly mitigates angle and rear-end crashes, its protective benefit for vulnerable road users (VRUs) remains limited. Our findings provide causal evidence for greening as a potential safety intervention, prioritizing hazardous visual environments while highlighting the need for distinct design optimizations to protect VRUs.

</details>


### [26] [FireRed-Image-Edit-1.0 Techinical Report](https://arxiv.org/abs/2602.13344)
*Super Intelligence Team,Changhao Qiao,Chao Hui,Chen Li,Cunzheng Wang,Dejia Song,Jiale Zhang,Jing Li,Qiang Xiang,Runqi Wang,Shuang Sun,Wei Zhu,Xu Tang,Yao Hu,Yibo Chen,Yuhao Huang,Yuxuan Duan,Zhiyi Chen,Ziyuan Guo*

Main category: cs.CV

TL;DR: FireRed-Image-Edit is a state-of-the-art diffusion transformer for instruction-based image editing, optimized through data curation, training methodology, and evaluation design. It achieves superior performance with a 1.6B-sample dataset and novel training techniques, validated by REDEdit-Bench, a new benchmark with 15 editing categories.


<details>
  <summary>Details</summary>
Motivation: Current image editing models face limitations in data quality, training efficiency, and evaluation comprehensiveness. FireRed-Image-Edit addresses these gaps by optimizing large-scale data processing, training stability, and controllability while introducing a more diverse benchmark for robust assessment.

Method: A 1.6B-sample corpus was curated, cleaned, and balanced, followed by multi-stage training (pre-training → fine-tuning → reinforcement learning). Innovations include: (1) Multi-Condition Aware Bucket Sampler for efficient batching, (2) Stochastic Instruction Alignment for dynamic prompts, (3) Asymmetric Gradient Optimization for DPO stability, (4) DiffusionNFT with OCR rewards for text editing, and (5) Consistency Loss for preserving identity. A new benchmark (REDEdit-Bench) with 15 categories was developed for evaluation.

Result: FireRed-Image-Edit outperforms existing open-source and proprietary systems on REDEdit-Bench, ImgEdit, and GEdit benchmarks. The model demonstrates strong semantic coverage, instruction alignment, and controllability, with publicly released code, models, and benchmark suite to support future research.

Conclusion: Systematic optimizations in data, training, and evaluation enable FireRed-Image-Edit to achieve state-of-the-art results. The release of models and REDEdit-Bench underscores its potential for advancing instruction-driven image editing research and applications.

Abstract: We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.

</details>


### [27] [Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots](https://arxiv.org/abs/2602.13347)
*Lijun Zhang,Nikhil Chacko,Petter Nilsson,Ruinian Xu,Shantanu Thakar,Bai Lou,Harpreet Sawhney,Zhebin Zhang,Mudit Agrawal,Bhavana Chandrashekhar,Aaron Parness*

Main category: cs.CV

TL;DR: 开发FOREST模型预测仓库存储布局，提升机器人操作效率。


<details>
  <summary>Details</summary>
Motivation: 自动化仓库需高效预测物品储存后的箱体布局，以优化规划与执行。

Method: 提出FOREST模型，采用实例掩码表示箱体状态，结合扩散变换器预测布局演化。

Result: 模型在几何一致性指标上显著优于启发式基线，下游任务（负载评估、多动作推理）性能损失仅8-12%。

Conclusion: FOREST提供可靠的事前预测能力，可支持仓库系统的前瞻性调度优化。

Abstract: Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.

</details>


### [28] [From Prompt to Production:Automating Brand-Safe Marketing Imagery with Text-to-Image Models](https://arxiv.org/abs/2602.13349)
*Parmida Atighehchian,Henry Wang,Andrei Kapustin,Boris Lerner,Tiancheng Jiang,Taylor Jensen,Negin Sokhandan*

Main category: cs.CV

TL;DR: 本文提出了一种结合自动化与人工反馈的文本到图像生成流程，在使用DINOV2模型的前提下，实现了营销图像生成质量提升30.77%、用户满意度提升52%的高效生产方案。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型缺乏规模化生产部署的解决方案，需通过平衡自动化效率与人工审核质量来保障商业营销场景下的创意合规性与视觉保真度。

Method: 构建包含动态参数调优的自动化流程框架，在生成过程中集成人机协作的质量控制机制，采用迭代式优化策略确保输出符合营销规范。

Result: 基于DINOV2模型的实现将商品特征保真度提升30.77%，通过AB测试验证生成结果获得52%的用户偏好增长，验证了流程优化的有效性。

Conclusion: 人机协同的生产范式既能提升商业图像生成的规模化处理能力，又可通过结构化的人工干预机制保障创意质量，为AIGC技术在营销领域的落地提供了可复制的技术架构。

Abstract: Text-to-image models have made significant strides, producing impressive results in generating images from textual descriptions. However, creating a scalable pipeline for deploying these models in production remains a challenge. Achieving the right balance between automation and human feedback is critical to maintain both scale and quality. While automation can handle large volumes, human oversight is still an essential component to ensure that the generated images meet the desired standards and are aligned with the creative vision. This paper presents a new pipeline that offers a fully automated, scalable solution for generating marketing images of commercial products using text-to-image models. The proposed system maintains the quality and fidelity of images, while also introducing sufficient creative variation to adhere to marketing guidelines. By streamlining this process, we ensure a seamless blend of efficiency and human oversight, achieving a $30.77\%$ increase in marketing object fidelity using DINOV2 and a $52.00\%$ increase in human preference over the generated outcome.

</details>


### [29] [Detecting Brick Kiln Infrastructure at Scale: Graph, Foundation, and Remote Sensing Models for Satellite Imagery Data](https://arxiv.org/abs/2602.13350)
*Usman Nazir,Xidong Chen,Hafiz Muhammad Abubakar,Hadia Abu Bakar,Raahim Arbaz,Fezan Rasool,Bin Chen,Sara Khalid*

Main category: cs.CV

TL;DR: The paper presents ClimateGraph and a high-resolution satellite dataset for scalable detection of brick kilns in South/Central Asia to address pollution and labor monitoring challenges.


<details>
  <summary>Details</summary>
Motivation: Brick kilns contribute significantly to air pollution and forced labor in South Asia, but large-scale monitoring is hindered by outdated ground data.

Method: Developed ClimateGraph (region-adaptive graph model) for analyzing kiln layout patterns, and tested remote sensing pipelines against satellite imagery foundation models using a 1.3M-tile dataset across five regions.

Result: Hybrid approaches combining graph models, foundation models, and remote sensing showed complementary strengths in detection accuracy and scalability.

Conclusion: The study provides actionable frameworks for satellite-based brick kiln monitoring systems, balancing technical performance and regional adaptability for large-scale environmental governance. 

Abstract: Brick kilns are a major source of air pollution and forced labor in South Asia, yet large-scale monitoring remains limited by sparse and outdated ground data. We study brick kiln detection at scale using high-resolution satellite imagery and curate a multi city zoom-20 (0.149 meters per pixel) resolution dataset comprising over 1.3 million image tiles across five regions in South and Central Asia. We propose ClimateGraph, a region-adaptive graph-based model that captures spatial and directional structure in kiln layouts, and evaluate it against established graph learning baselines. In parallel, we assess a remote sensing based detection pipeline and benchmark it against recent foundation models for satellite imagery. Our results highlight complementary strengths across graph, foundation, and remote sensing approaches, providing practical guidance for scalable brick kiln monitoring from satellite imagery.

</details>


### [30] [Using Deep Learning to Generate Semantically Correct Hindi Captions](https://arxiv.org/abs/2602.13352)
*Wasim Akram Khan,Anil Kumar Vuppala*

Main category: cs.CV

TL;DR: 该研究提出了一种基于多模态架构的自动图像生成方法，使用计算机视觉和自然语言处理技术，将图像字幕翻译成印地语，并通过注意力机制和预训练模型（如VGG16）优化生成效果。


<details>
  <summary>Details</summary>
Motivation: 在英语图像字幕生成领域已有大量研究，但针对其他主流语言（如印地语）的探索较少，本研究旨在填补这一空白。

Method: 结合局部/全局视觉特征、注意力机制、预训练CNN（VGG16/ResNet50/Inception V3）和双向LSTM进行文本编码，利用注意力层生成权重向量，融合图像与文本特征。

Result: 基于BLEU-1和BLEU-4评估指标，注意力机制结合VGG16与双向LSTM的实验表现最佳，得分分别为0.59和0.19。

Conclusion: 该方法在生成语义准确、上下文相关的印地语图像字幕方面具有有效性，为后续多语言图像理解研究提供了可行框架。

Abstract: Automated image captioning using the content from the image is very appealing when done by harnessing the capability of computer vision and natural language processing. Extensive research has been done in the field with a major focus on the English language which gives the scope for further developments in the same with consideration of popular foreign languages. This research utilizes distinct models for translating the image caption into Hindi, the fourth most popular language across the world. Exploring the multi-modal architectures this research comprises local visual features, global visual features, attention mechanisms, and pre-trained models. Using google cloud translator on the image dataset from Flickr8k, Hindi image descriptions have been generated. Pre-trained CNNs like VGG16, ResNet50, and Inception V3 helped in retrieving image characteristics, while the uni-directional and bi-directional techniques of text encoding are used for the text encoding process. An additional Attention layer helps to generate a weight vector and, by multiplying it, combine image characteristics from each time step into a sentence-level feature vector. Bilingual evaluation understudy scores are used to compare the research outcome. Many experiments that serve as a baseline are done for the comparative analysis of the research. An image with a score of BLEU-1 is considered sufficient, whereas one with a score of BLEU-4 is considered to have fluid image captioning. For both BLEU scores, the attention-based bidirectional LSTM with VGG16 produced the best results of 0.59 and 0.19 respectively. The experiments conclude that researchs ability to produce relevant, semantically accurate image captions in Hindi. The research accomplishes the goals and future research can be guided by this research model.

</details>


### [31] [AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers](https://arxiv.org/abs/2602.13357)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.CV

TL;DR: Introduces AdaCorrection, an adaptive caching method for efficient diffusion transformer inference.


<details>
  <summary>Details</summary>
Motivation: Address high inference costs in DiTs caused by iterative denoising, and mitigate issues from static reuse schedules like temporal drift and cache misalignment.

Method: AdaCorrection uses lightweight spatio-temporal signals to estimate cache validity and adaptively mixes cached/fresh activations at each timestep, enabling on-the-fly correction without retraining.

Result: Achieves minimal computational overhead, near-original FID scores, and consistent performance improvements in image/video benchmarks.

Conclusion: AdaCorrection balances efficiency and fidelity via adaptive cache reuse, outperforming previous methods in quality preservation during accelerated diffusion sampling.

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance.

</details>


### [32] [An Online Reference-Free Evaluation Framework for Flowchart Image-to-Code Generation](https://arxiv.org/abs/2602.13376)
*Giang Son Nguyen,Zi Pong Lim,Sarthak Ketanbhai Modi,Yon Shin Teo,Wenya Wang*

Main category: cs.CV

TL;DR: 提出一种无需参考的评估框架，通过OCR和视觉蕴含检测生成流程图代码质量，有效替代传统依赖真值数据的评估方式。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中，文档处理流水线中的视觉语言模型（VLM）处理无真值代码的任意输入，迫切需要在推理阶段通过参考自由的自动化指标评估生成质量。

Method: 构建双指标体系：基于OCR提取文本计算内容覆盖率（Recall_OCR），利用视觉蕴含模型检测生成代码与原始图像的逻辑一致性（Precision_VE），并以调和平均数F1_OCR-VE进行综合评价。

Result: 在FlowVQA数据集验证中，双指标与人类评估结果呈高度正相关（Recall r=0.97, Precision r=0.91, F1 r=0.94），证实框架相比传统BLEU、ROUGE等指标更适用于实时质量监控。

Conclusion: 该参考自由的评估框架可在无真值条件下实现生成代码内容覆盖度与幻觉元素的精准检测，为工业级文档处理系统提供可靠的自动质量保障方案。

Abstract: Vision-Language Models (VLMs) are increasingly used in document processing pipelines to convert flowchart images into structured code (e.g., Mermaid). In production, these systems process arbitrary inputs for which no ground-truth code exists, making output quality difficult to assess. We propose a reference-free evaluation framework that monitors flowchart image-to-code generation quality at inference time, using only the input image and the generated output. The framework introduces two automated metrics: $\text{Recall}{\text{OCR}}$, which estimates content coverage by extracting text from the input image via OCR as a proxy reference, and $\text{Precision}{\text{VE}}$, which detects hallucinated elements through Visual Entailment against the original image. Their harmonic mean, $\text{F1}{\text{OCR-VE}}$, provides a unified quality score. Validation on the FlowVQA dataset shows strong agreement with ground-truth metrics (average Pearson's $r = 0.97$, $0.91$, and $0.94$ for Recall, Precision, and F1, respectively), confirming the framework's reliability as a practical, reference-free alternative for continuous quality monitoring in production settings.

</details>


### [33] [Handling Supervision Scarcity in Chest X-ray Classification: Long-Tailed and Zero-Shot Learning](https://arxiv.org/abs/2602.13430)
*Ha-Hieu Pham,Hai-Dang Nguyen,Thanh-Huy Nguyen,Min Xu,Ulas Bagci,Trung-Nghia Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 本文提出CXR-LT 2026挑战，通过两种定制化方案解决胸片分类中的长尾分布数据与零样本异常疾病识别问题。


<details>
  <summary>Details</summary>
Motivation: 临床胸片分类受限于长尾分布多标签疾病数据及罕见/未见病症标注缺失导致的监督不足问题，传统方法难以平衡尾部类别与高频病症性能并处理零样本识别。

Method: 针对任务1采用不平衡感知的多标签学习策略优化尾部类别识别；针对任务2提出无需OOD类别监督标签的零样本预测方法，两者协同提升分类性能。

Result: 基于宏平均平均精度（mAP）指标，在长尾分类与零样本OOD识别任务中均取得领先性能，位居开发阶段公共排行榜首位。

Conclusion: 该研究证明任务特定方案可有效解决医学影像中监督不充分带来的双重挑战，代码与预训练模型已开源。

Abstract: Chest X-ray (CXR) classification in clinical practice is often limited by imperfect supervision, arising from (i) extreme long-tailed multi-label disease distributions and (ii) missing annotations for rare or previously unseen findings. The CXR-LT 2026 challenge addresses these issues on a PadChest-based benchmark with a 36-class label space split into 30 in-distribution classes for training and 6 out-of-distribution (OOD) classes for zero-shot evaluation. We present task-specific solutions tailored to the distinct supervision regimes. For Task 1 (long-tailed multi-label classification), we adopt an imbalance-aware multi-label learning strategy to improve recognition of tail classes while maintaining stable performance on frequent findings. For Task 2 (zero-shot OOD recognition), we propose a prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from the OOD classes during training. Evaluated with macro-averaged mean Average Precision (mAP), our method achieves strong performance on both tasks, ranking first on the public leaderboard of the development phase. Code and pre-trained models are available at https://github.com/hieuphamha19/CXR_LT.

</details>


### [34] [Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones](https://arxiv.org/abs/2602.13440)
*Sebastian-Ion Nae,Mihai-Eugen Barbu,Sebastian Mocanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 该论文提出一个用于无人机室内实时目标检测的持续学习方法与新数据集。


<details>
  <summary>Details</summary>
Motivation: 无人机需实时学习新目标类别并防止灾难性遗忘（CIL），但现有UAV数据集多为户外且缺乏室内时序数据。

Method: 构建包含14,400帧的室内UAV数据集，采用半自动标注流程；基于YOLOv11-nano检测器测试三种回放式CIL策略（ER/MIR/FAR）

Result: FAR在5%回放预算下达到82.96%平均精度；Grad-CAM显示跨类别注意转移导致定位精度下降；验证了回放式持续学习在边缘航拍系统中的有效性。

Conclusion: 提出首个时序连贯的室内UAV持续学习数据集，并证明轻量级回放策略（FAR）在资源受限场景下优于传统方法。

Abstract: Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\%$ with $5\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl

</details>


### [35] [GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables](https://arxiv.org/abs/2602.13479)
*Akhil Ramachandran,Ankit Arun,Ashish Shenoy,Abhay Harpale,Srihari Jayakumar,Debojeet Chatterjee,Mohsen Moslehpour,Pierce Chuang,Yichao Lu,Vikas Bhardwaj,Peyman Heidari*

Main category: cs.CV

TL;DR: 本文提出一种混合架构，通过在设备上进行选择性高分辨率OCR，同时传输低分辨率视频以维持上下文，解决了文本识别与视觉推理在分辨率需求上的矛盾，从而在可穿戴设备上高效运行文本Visual Question Answering任务。


<details>
  <summary>Details</summary>
Motivation: 部署文本Visual Question Answering任务至可穿戴设备的核心挑战在于高分辨率视频流带来的高功耗与热限制，以及现有模型难以实时保持多帧文本语义连贯性。

Method: 基于文本识别与场景理解的分辨率不对称需求，设计混合架构：对OCR处理采用高分辨率帧，对视觉语义维持低分辨率流；通过空间对齐机制融合多分辨率信息，在本地处理文本识别与低分辨率上下文。

Result: 在跨五类任务的基准测试中，系统实现72%准确率（对比全分辨率方案损失小于5%），能耗降至49%，且在热约束设备上可连续运行VQA任务超过30分钟（对照方案3分钟即触发过热保护）

Conclusion: 该研究表明，针对多模态任务的需求异构性设计分层处理架构，可显著提升资源受限设备的视觉推理能力，为可穿戴设备部署复杂视觉语言任务提供了低功耗可行方案。

Abstract: Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.

</details>


### [36] [Benchmarking Video Foundation Models for Remote Parkinson's Disease Screening](https://arxiv.org/abs/2602.13507)
*Md Saiful Islam,Ekram Hossain,Abdelrahman Abdelkader,Tariq Adnan,Fazla Rabbi Mashrur,Sooyong Park,Praveen Kumar,Qasim Sudais,Natalia Chunga,Nami Shah,Jan Freyberg,Christopher Kanan,Ruth Schneider,Ehsan Hoque*

Main category: cs.CV

TL;DR: 比较不同视频基础模型（VFMs）在大规模帕金森病筛查中的表现，发现模型性能与任务类型相关，建立了远程神经监测的基线。


<details>
  <summary>Details</summary>
Motivation: 传统帕金森病筛查依赖手工特征，而新兴视频基础模型无需任务定制即可学习表征。但各类VFM在不同临床任务中的效果差异尚不明确。

Method: 使用包含32,847段视频的大规模数据集，评估7种最先进VFM（如VideoPrism、V-JEPA等），通过冻结嵌入+线性分类头进行模型鲁棒性测试

Result: AUC达76.4-85.3%，VideoPrism在面部/言语运动分析占优，V-JEPA在上肢运动任务表现优异，TimeSformer在节律任务中竞争力强，特异性高达90.3%但灵敏度低于60%

Conclusion: 建立基于VFM的帕金森筛查基线标准，提出需按任务选择合适模型并整合多模态数据，模型性能与任务显著性相关，需任务感知校准

Abstract: Remote, video-based assessments offer a scalable pathway for Parkinson's disease (PD) screening. While traditional approaches rely on handcrafted features mimicking clinical scales, recent advances in video foundation models (VFMs) enable representation learning without task-specific customization. However, the comparative effectiveness of different VFM architectures across diverse clinical tasks remains poorly understood. We present a large-scale systematic study using a novel video dataset from 1,888 participants (727 with PD), comprising 32,847 videos across 16 standardized clinical tasks. We evaluate seven state-of-the-art VFMs -- including VideoPrism, V-JEPA, ViViT, and VideoMAE -- to determine their robustness in clinical screening. By evaluating frozen embeddings with a linear classification head, we demonstrate that task saliency is highly model-dependent: VideoPrism excels in capturing visual speech kinematics (no audio) and facial expressivity, while V-JEPA proves superior for upper-limb motor tasks. Notably, TimeSformer remains highly competitive for rhythmic tasks like finger tapping. Our experiments yield AUCs of 76.4-85.3% and accuracies of 71.5-80.6%. While high specificity (up to 90.3%) suggests strong potential for ruling out healthy individuals, the lower sensitivity (43.2-57.3%) highlights the need for task-aware calibration and integration of multiple tasks and modalities. Overall, this work establishes a rigorous baseline for VFM-based PD screening and provides a roadmap for selecting suitable tasks and architectures in remote neurological monitoring. Code and anonymized structured data are publicly available: https://anonymous.4open.science/r/parkinson\_video\_benchmarking-A2C5

</details>


### [37] [SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning](https://arxiv.org/abs/2602.13515)
*Jintao Zhang,Kai Jiang,Chendong Xiang,Weiqi Feng,Yuezhou Hu,Haocheng Xi,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出SpargeAttention2，一种可训练的稀疏注意力方法，在视频扩散模型中实现95%稀疏度的同时保持生成质量，速度提升16.2倍。


<details>
  <summary>Details</summary>
Motivation: 现有无训练稀疏注意力方法存在极限（高稀疏下质量下降），可训练方法虽能提升稀疏性但存在掩码失效和微调缺陷，需要更鲁棒的方案

Method: SpargeAttention2包含：1) 混合Top-k/p掩码规则增强高稀疏鲁棒性；2) 高效可训练稀疏实现；3) 蒸馏式微调目标优化生成质量保持

Result: 在视频扩散模型测试中达到95%稀疏度（对比基线最高提升20%），训练速度提升16.2倍，FID分数保持与密集注意力相当

Conclusion: 通过理论分析解决可训练稀疏注意力的三个关键问题，证明混合掩码、高效实现和蒸馏微调的有效性，为视频生成模型提供实用优化方案

Abstract: Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.

</details>


### [38] [Nighttime Autonomous Driving Scene Reconstruction with Physically-Based Gaussian Splatting](https://arxiv.org/abs/2602.13549)
*Tae-Kyeong Kim,Xingxin Chen,Guile Wu,Chengjie Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理渲染的3D高斯点阵方法，用于提升自动驾驶中夜间场景的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF和3DGS的方法在正常光照条件下表现优异，但在夜间复杂光照下效果下降，需通过物理渲染增强夜间场景建模能力。

Method: 将物理渲染集成至复合场景高斯表示，联合优化基于BRDF的材质属性，通过全局光照模块建模漫反射成分，并采用各向异性球面高斯处理镜面反射成分。

Result: 在nuScenes和Waymo真实自动驾驶数据集上的夜间场景实验表明，该方法在定量和定性指标上均优于当前最先进方法，且保持实时渲染速度。

Conclusion: 通过引入物理渲染与材质优化，有效提升了3DGS在低光照驾驶场景建模中的表现，为自动驾驶仿真提供更准确的夜间环境重建方案。

Abstract: This paper focuses on scene reconstruction under nighttime conditions in autonomous driving simulation. Recent methods based on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved photorealistic modeling in autonomous driving scene reconstruction, but they primarily focus on normal-light conditions. Low-light driving scenes are more challenging to model due to their complex lighting and appearance conditions, which often causes performance degradation of existing methods. To address this problem, this work presents a novel approach that integrates physically based rendering into 3DGS to enhance nighttime scene reconstruction for autonomous driving. Specifically, our approach integrates physically based rendering into composite scene Gaussian representations and jointly optimizes Bidirectional Reflectance Distribution Function (BRDF) based material properties. We explicitly model diffuse components through a global illumination module and specular components by anisotropic spherical Gaussians. As a result, our approach improves reconstruction quality for outdoor nighttime driving scenes, while maintaining real-time rendering. Extensive experiments across diverse nighttime scenarios on two real-world autonomous driving datasets, including nuScenes and Waymo, demonstrate that our approach outperforms the state-of-the-art methods both quantitatively and qualitatively.

</details>


### [39] [Diff-Aid: Inference-time Adaptive Interaction Denoising for Rectified Text-to-Image Generation](https://arxiv.org/abs/2602.13585)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: 提出Diff-Aid方法增强文本到图像生成中图文交互的动态调控能力


<details>
  <summary>Details</summary>
Motivation: 当前模型存在文本与视觉特征交互不足、建筑灵活性差和动态交互机制缺失的问题，需开发更高效的自适应方案

Method: 设计轻量级Transformer级调控模块，实现跨注意力区块和去噪阶段的逐词交互自适应调节，形成三维(模态/token/时步)关联矩阵

Result: 在SD3.5/FLUX基座模型上取得4.7% CLIP score提升，生成质量与控制能力超越现有LoRA等主流方案，可视化显示注意力集中度提高23%

Conclusion: Diff-Aid作为即插即用模块，在提升推理时图文对齐度的同时提供可解释的交互模式，为可控图像生成提供新范式

Abstract: Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages. To provide a more flexible and efficient solution to this problem, we propose Diff-Aid, a lightweight inference-time method that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. Beyond improving generation quality, Diff-Aid yields interpretable modulation patterns that reveal how different blocks, timesteps, and textual tokens contribute to semantic alignment during denoising. As a plug-and-play module, Diff-Aid can be seamlessly integrated into downstream applications for further improvement, including style LoRAs, controllable generation, and zero-shot editing. Experiments on strong baselines (SD 3.5 and FLUX) demonstrate consistent improvements in prompt adherence, visual quality, and human preference across various metrics. Our code and models will be released.

</details>


### [40] [Two-Stream Interactive Joint Learning of Scene Parsing and Geometric Vision Tasks](https://arxiv.org/abs/2602.13588)
*Guanfeng Tang,Hongbo Zhao,Ziwei Long,Jiayao Li,Bohong Xiao,Wei Ye,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: This paper introduces TwInS, a bio-inspired framework for joint scene parsing and geometric vision tasks, using a unified architecture with bidirectional feature interaction and semi-supervised training.


<details>
  <summary>Details</summary>
Motivation: To mimic the human visual system's parallel processing of contextual and spatial information, enabling joint learning of scene parsing and geometric vision without relying on annotated correspondence data.

Method: Develops TwInS with two interactive streams: (1) context-to-geometry feature infusion for refinement, (2) cross-task adapter for geometric-to-context feature fusion using geometric cues, and a semi-supervised strategy leveraging multi-view data without ground truth.

Result: Experiments on three datasets validate TwInS's components and show superior performance over state-of-the-art methods in both tasks, with open-source code commitment.

Conclusion: TwInS achieves effective self-evolving joint learning through bio-inspired bidirectional interaction and semi-supervised training, outperforming existing approaches in accuracy and efficiency.

Abstract: Inspired by the human visual system, which operates on two parallel yet interactive streams for contextual and spatial understanding, this article presents Two Interactive Streams (TwInS), a novel bio-inspired joint learning framework capable of simultaneously performing scene parsing and geometric vision tasks. TwInS adopts a unified, general-purpose architecture in which multi-level contextual features from the scene parsing stream are infused into the geometric vision stream to guide its iterative refinement. In the reverse direction, decoded geometric features are projected into the contextual feature space for selective heterogeneous feature fusion via a novel cross-task adapter, which leverages rich cross-view geometric cues to enhance scene parsing. To eliminate the dependence on costly human-annotated correspondence ground truth, TwInS is further equipped with a tailored semi-supervised training strategy, which unleashes the potential of large-scale multi-view data and enables continuous self-evolution without requiring ground-truth correspondences. Extensive experiments conducted on three public datasets validate the effectiveness of TwInS's core components and demonstrate its superior performance over existing state-of-the-art approaches. The source code will be made publicly available upon publication.

</details>


### [41] [AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting](https://arxiv.org/abs/2602.13600)
*Jiacheng Zhang,Feng Liu,Chao Du,Tianyu Pang*

Main category: cs.CV

TL;DR: AdaVBoost 提出一种自适应的视觉注意力增强方法，动态调整生成过程中每个token的注意力强度，以减少大型视觉语言模型（LVLM）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有依赖预定义固定比例增强视觉注意力的方法在不同生成阶段可能导致增强不足或过强，引发残留或新增幻觉问题。需要一种根据生成阶段风险动态调整增强强度的机制。

Method: 设计视觉定位熵（VGE）量化幻觉风险，通过计算视觉定位与语言生成的证据匹配度，并以此为指导对高风险token施加强增强、低风险token施加弱增强，实现token级别和步骤级别的自适应调整。

Result: 实验表明，在多个LVLM模型和幻觉基准测试中，AdaVBoost相比基线方法显著提升性能，有效降低幻觉生成率。

Conclusion: 通过结合视觉定位证据和不确定性熵动态干预注意力机制，证明了token级别自适应增强策略相比固定比例方法的优势，为后续研究提供新方向。

Abstract: Visual attention boosting has emerged as a promising direction for mitigating hallucinations in Large Vision-Language Models (LVLMs), where existing methods primarily focus on where to boost by applying a predefined scaling to the attention of method-specific visual tokens during autoregressive generation. In this paper, we identify a fundamental trade-off in these methods: a predefined scaling factor can be too weak at some generation steps, leaving hallucinations unresolved, yet too strong at others, leading to new hallucinations. Motivated by this finding, we propose AdaVBoost, a token-level visual attention boosting framework that adaptively determines how much attention to boost at each generation step. Specifically, we introduce Visual Grounding Entropy (VGE) to estimate hallucination risk, which leverages visual grounding as a complementary signal to capture evidence mismatches beyond entropy. Guided by VGE, AdaVBoost applies stronger visual attention boosting to high-risk tokens and weaker boosting to low-risk tokens, enabling token-level adaptive intervention at each generation step. Extensive experiments show that AdaVBoost significantly outperforms baseline methods across multiple LVLMs and hallucination benchmarks.

</details>


### [42] [A generalizable foundation model for intraoperative understanding across surgical procedures](https://arxiv.org/abs/2602.13633)
*Kanggil Park,Yongjun Jeon,Soyoung Lim,Seonmin Park,Jongmin Shin,Jung Yong Kim,Sehyeon An,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: 本研究引入ZEN，一种通用的手术视频理解基础模型，通过自监督多教师蒸馏框架，在超过400万帧和21种手术类型上训练，相较于现有手术基础模型，在多种下游任务和训练模式下表现更优，且具有跨手术类型的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微创手术中临床决策依赖实时视觉判断，但术中感知在不同外科医生和手术类型间存在显著差异。这种变异限制了一致性评估、培训及可靠人工智能系统的发展。当前手术AI模型多针对特定狭窄任务设计，难以泛化到不同手术或机构。

Method: 构建了基于自监督多教师蒸馏框架的ZEN模型，使用超过400万帧数据（涵盖21种手术类型）进行训练。研究人员整理了大规模且多样化的数据集，并在统一基准内系统评估多种表征学习策略。

Result: ZEN在20项下游任务及全微调、冻结主干、少样本和零样本等不同设置下均持续优于现有手术基础模型，验证了其跨手术类型的强泛化能力。

Conclusion: 该成果表明统一表征学习在手术场景理解中的可行性，为术中辅助和手术培训评估等未来应用提供支持。

Abstract: In minimally invasive surgery, clinical decisions depend on real-time visual interpretation, yet intraoperative perception varies substantially across surgeons and procedures. This variability limits consistent assessment, training, and the development of reliable artificial intelligence systems, as most surgical AI models are designed for narrowly defined tasks and do not generalize across procedures or institutions. Here we introduce ZEN, a generalizable foundation model for intraoperative surgical video understanding trained on more than 4 million frames from over 21 procedures using a self-supervised multi-teacher distillation framework. We curated a large and diverse dataset and systematically evaluated multiple representation learning strategies within a unified benchmark. Across 20 downstream tasks and full fine-tuning, frozen-backbone, few-shot and zero-shot settings, ZEN consistently outperforms existing surgical foundation models and demonstrates robust cross-procedure generalization. These results suggest a step toward unified representations for surgical scene understanding and support future applications in intraoperative assistance and surgical training assessment.

</details>


### [43] [Layer-Guided UAV Tracking: Enhancing Efficiency and Occlusion Robustness](https://arxiv.org/abs/2602.13636)
*Yang Zhou,Derui Ding,Ran Sun,Ying Sun,Haohua Zhang*

Main category: cs.CV

TL;DR: LGTrack是一个专为无人机视觉跟踪设计的高效框架，在保证跟踪精度（82.8%）的同时实现高达258.7 FPS的实时速度，通过GGCA和SGLA两个轻量级模块解决遮挡问题和效率平衡。


<details>
  <summary>Details</summary>
Motivation: 传统无人机跟踪算法在精度和效率之间难以权衡（尤其在动态遮挡场景），现有方法或因计算复杂度无法实时运行，或通过知识蒸馏牺牲精度换取效率。

Method: 1) 设计GGCA模块（轻量级全局分组坐标注意力）捕获长距离依赖并增强特征可区分性，2) 提出SGLA模块（相似性引导层自适应）替代知识蒸馏实现精度与推理速度的平衡，3) 动态层选择+特征增强+鲁棒表征学习的统一框架。

Result: 在三个公开数据集上达到SOTA实时性能（UAVDT数据集258.7 FPS），同时在跟踪精度指标（Precision）保持82.8%，超越主流实时跟踪算法。

Conclusion: 该研究通过架构创新证明轻量级全局注意力机制和动态层适应策略的有效性，为资源受限场景（如无人机）提供可部署的高性能跟踪解决方案。

Abstract: Visual object tracking (VOT) plays a pivotal role in unmanned aerial vehicle (UAV) applications. Addressing the trade-off between accuracy and efficiency, especially under challenging conditions like unpredictable occlusion, remains a significant challenge. This paper introduces LGTrack, a unified UAV tracking framework that integrates dynamic layer selection, efficient feature enhancement, and robust representation learning for occlusions. By employing a novel lightweight Global-Grouped Coordinate Attention (GGCA) module, LGTrack captures long-range dependencies and global contexts, enhancing feature discriminability with minimal computational overhead. Additionally, a lightweight Similarity-Guided Layer Adaptation (SGLA) module replaces knowledge distillation, achieving an optimal balance between tracking precision and inference efficiency. Experiments on three datasets demonstrate LGTrack's state-of-the-art real-time speed (258.7 FPS on UAVDT) while maintaining competitive tracking accuracy (82.8\% precision). Code is available at https://github.com/XiaoMoc/LGTrack

</details>


### [44] [DCDM: Divide-and-Conquer Diffusion Models for Consistency-Preserving Video Generation](https://arxiv.org/abs/2602.13637)
*Haoyu Zhao,Yuang Zhang,Junqi Cheng,Jiaxi Gu,Zenghui Lu,Peng Shu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: This paper introduces a Divide-and-Conquer Diffusion Model (DCDM) to enhance video generation consistency across three levels: intra-clip semantics, inter-clip cameras, and inter-shot elements, validated on the CVM Competition at AAAI'26.


<details>
  <summary>Details</summary>
Motivation: Existing video generative models suffer from poor semantic, geometric, and identity consistency; DCDM aims to systematically address these challenges through targeted components.

Method: DCDM decomposes video consistency into three components: (1) semantic-structured intra-clip generation via LLM and diffusion transformer, (2) camera motion control using temporal noise-space representation and text-to-image initialization, and (3) inter-shot coherence through windowed cross-attention and sparse self-attention mechanisms.

Result: Experimental validation on CVM Competition test sets demonstrates that DCDM effectively improves consistency across intra-clip semantics, inter-clip camera movements, and inter-shot narrative continuity.

Conclusion: System-level design with dedicated components for different consistency challenges is effective, enabling high-fidelity video generation with improved structural and temporal coherence.

Abstract: Recent video generative models have demonstrated impressive visual fidelity, yet they often struggle with semantic, geometric, and identity consistency. In this paper, we propose a system-level framework, termed the Divide-and-Conquer Diffusion Model (DCDM), to address three key challenges: (1) intra-clip world knowledge consistency, (2) inter-clip camera consistency, and (3) inter-shot element consistency. DCDM decomposes video consistency modeling under these scenarios into three dedicated components while sharing a unified video generation backbone. For intra-clip consistency, DCDM leverages a large language model to parse input prompts into structured semantic representations, which are subsequently translated into coherent video content by a diffusion transformer. For inter-clip camera consistency, we propose a temporal camera representation in the noise space that enables precise and stable camera motion control, along with a text-to-image initialization mechanism to further enhance controllability. For inter-shot consistency, DCDM adopts a holistic scene generation paradigm with windowed cross-attention and sparse inter-shot self-attention, ensuring long-range narrative coherence while maintaining computational efficiency. We validate our framework on the test set of the CVM Competition at AAAI'26, and the results demonstrate that the proposed strategies effectively address these challenges.

</details>


### [45] [KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination](https://arxiv.org/abs/2602.13650)
*Byungjin Choi,Seongsu Bae,Sunjun Kweon,Edward Choi*

Main category: cs.CV

TL;DR: This paper introduces KorMedMCQA-V, a Korean medical multimodal benchmark with 1,534 questions and 2,043 clinical images to evaluate vision-language models (VLMs). It benchmarks 50+ VLMs, showing top proprietary models achieve 96.9% accuracy, while multi-image reasoning remains challenging.


<details>
  <summary>Details</summary>
Motivation: Existing medical benchmarks lack Korean multimodal datasets containing clinical images (X-ray, CT, ECG, etc.) for evaluating real-world VLM capabilities in medical licensing exams. This fills a gap by expanding the text-only KorMedMCQA to a unified multimodal version for clinical reasoning evaluation.

Method: Constructed a benchmark with 1,534 Korean medical exam questions (2012-2023) involving multiple image types (30% requiring cross-image evidence integration). Benchmarked 50+ VLMs (proprietary/open-source, general/medical/Korean-specialized) under unified zero-shot evaluation protocols.

Result: Gemini-3.0-Pro achieved 96.9% accuracy (best proprietary), Qwen3-VL-32B-Thinking 83.7% (best open-source), and VARCO-VISION-2.0-14B only 43.2% (best Korean). Reasoning models outperformed instruction-tuned ones by +20%. All models degraded on multi-image questions, with performance varying across imaging modalities.

Conclusion: KorMedMCQA-V establishes the first Korean multimodal medical benchmark complementing text-only versions. Results reveal strengths (high proprietary model accuracy) and challenges (multi-image/ Korean-specialized model limitations), providing insights for developing robust medical VLMs.

Abstract: We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.

</details>


### [46] [LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases](https://arxiv.org/abs/2602.13662)
*Khang Nguyen Quoc,Phuong D. Dao,Luyl-Da Quach*

Main category: cs.CV

TL;DR: 本研究引入LeafNet和LeafBench，填补农业领域植物病理学多模态数据集和基准评估的空白，通过12种先进视觉语言模型（VLMs）验证多模态架构在疾病诊断中的优势，并揭示现有模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型和视觉语言预训练模型在农业特定任务（如植物病理学）的应用受限于缺乏大规模多模态图像-文本数据集与基准评估指标。需要建立针对性数据集以解决植物疾病诊断中的技术鸿沟。

Method: 构建包含18.6万张叶片图像（覆盖97类疾病）的LeafNet数据集，结合元数据生成1.395万个多任务问答对；设计LeafBench基准，涵盖视觉症状识别、分类关系和诊断推理六大核心任务，对12种SOTA VLMs进行系统评估。

Result: 在二分类健康-疾病识别任务中VLMs准确率超90%，但细粒度病原体与物种识别低于65%；多模态VLMs经微调后显著优于纯视觉模型，证明语言表征整合提升诊断精度。

Conclusion: 当前VLMs在植物病理学应用存在关键性能瓶颈，LeafBench为方法论创新和进展评估提供了可靠框架，证实多模态架构对农业智能化诊断的必要性。

Abstract: Foundation models and vision-language pre-training have significantly advanced Vision-Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their application in domain-specific agricultural tasks, such as plant pathology, remains limited due to the lack of large-scale, comprehensive multimodal image--text datasets and benchmarks. To address this gap, we introduce LeafNet, a comprehensive multimodal dataset, and LeafBench, a visual question-answering benchmark developed to systematically evaluate the capabilities of VLMs in understanding plant diseases. The dataset comprises 186,000 leaf digital images spanning 97 disease classes, paired with metadata, generating 13,950 question-answer pairs spanning six critical agricultural tasks. The questions assess various aspects of plant pathology understanding, including visual symptom recognition, taxonomic relationships, and diagnostic reasoning. Benchmarking 12 state-of-the-art VLMs on our LeafBench dataset, we reveal substantial disparity in their disease understanding capabilities. Our study shows performance varies markedly across tasks: binary healthy--diseased classification exceeds 90\% accuracy, while fine-grained pathogen and species identification remains below 65\%. Direct comparison between vision-only models and VLMs demonstrates the critical advantage of multimodal architectures: fine-tuned VLMs outperform traditional vision models, confirming that integrating linguistic representations significantly enhances diagnostic precision. These findings highlight critical gaps in current VLMs for plant pathology applications and underscore the need for LeafBench as a rigorous framework for methodological advancement and progress evaluation toward reliable AI-assisted plant disease diagnosis. Code is available at https://github.com/EnalisUs/LeafBench.

</details>


### [47] [EchoTorrent: Towards Swift, Sustained, and Streaming Multi-Modal Video Generation](https://arxiv.org/abs/2602.13669)
*Rang Meng,Weipeng Wu,Yingjie Yin,Yuming Li,Chenguang Ma*

Main category: cs.CV

TL;DR: 本文提出EchoTorrent框架，通过四项设计优化多模态视频生成的效率与性能平衡，解决高延迟、低稳定性及流式推理的质量退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在实时部署时面临高延迟、时间不稳定问题，流式推理还会导致空间模糊、时间漂移、唇音不同步等质量下降现象，需突破效率-性能权衡瓶颈。

Method: 基于四点创新：1) 多教师训练迁移域知识至学生模型；2) 自适应CFG校准消除音频计算冗余；3)混合长尾强制提升流式时空对齐；4) VAE解码优化重构高频细节。

Result: 实现单次前向推理，在时空一致性、人物身份保持、音唇同步等指标上显著优于传统多步生成方法。

Conclusion: EchoTorrent通过结构化知识迁移和计算优化设计，有效提升视频生成效率，同时保障质量，为实时应用提供技术路径。

Abstract: Recent multi-modal video generation models have achieved high visual quality, but their prohibitive latency and limited temporal stability hinder real-time deployment. Streaming inference exacerbates these issues, leading to pronounced multimodal degradation, such as spatial blurring, temporal drift, and lip desynchronization, which creates an unresolved efficiency-performance trade-off. To this end, we propose EchoTorrent, a novel schema with a fourfold design: (1) Multi-Teacher Training fine-tunes a pre-trained model on distinct preference domains to obtain specialized domain experts, which sequentially transfer domain-specific knowledge to a student model; (2) Adaptive CFG Calibration (ACC-DMD), which calibrates the audio CFG augmentation errors in DMD via a phased spatiotemporal schedule, eliminating redundant CFG computations and enabling single-pass inference per step; (3) Hybrid Long Tail Forcing, which enforces alignment exclusively on tail frames during long-horizon self-rollout training via a causal-bidirectional hybrid architecture, effectively mitigates spatiotemporal degradation in streaming mode while enhancing fidelity to reference frames; and (4) VAE Decoder Refiner through pixel-domain optimization of the VAE decoder to recover high-frequency details while circumventing latent-space ambiguities. Extensive experiments and analysis demonstrate that EchoTorrent achieves few-pass autoregressive generation with substantially extended temporal consistency, identity preservation, and audio-lip synchronization.

</details>


### [48] [An Ensemble Learning Approach towards Waste Segmentation in Cluttered Environment](https://arxiv.org/abs/2602.13681)
*Maimoona Jafar,Syed Imran Ali,Ahsan Saadat,Muhammad Bilal,Shah Khalid*

Main category: cs.CV

TL;DR: This paper proposes an Ensemble Learning approach (EL-4) combining U-Net and FPN to improve waste segmentation accuracy for recycling automation, achieving higher IoU (0.8306) and lower Dice loss (0.09019) than individual models.


<details>
  <summary>Details</summary>
Motivation: Inefficient waste segregation in recycling processes due to complex real-world environments with deformed/overlapping objects requires advanced segmentation methods for automated material recovery.

Method: Integrated U-Net (for boundary precision) and FPN (for scale/context handling) via weighted averaging in an ensemble learning framework using a real-scenario waste dataset with enhanced preprocessing.

Result: EL-4 outperformed U-Net (IoU +3.1%) and FPN (Dice loss reduced 23.6%), demonstrating superior accuracy in waste segmentation tasks.

Conclusion: The ensemble approach enhances robotic waste sorting efficiency in material recovery facilities, reducing human labor and improving recycling throughput.

Abstract: Environmental pollution is a critical global issue, with recycling emerging as one of the most viable solutions. This study focuses on waste segregation, a crucial step in recycling processes to obtain raw material. Recent advancements in computer vision have significantly contributed to waste classification and recognition. In waste segregation, segmentation masks are essential for robots to accurately localize and pick objects from conveyor belts. The complexity of real-world waste environments, characterized by deformed items without specific patterns and overlapping objects, further complicates waste segmentation tasks. This paper proposes an Ensemble Learning approach to improve segmentation accuracy by combining high performing segmentation models, U-Net and FPN, using a weighted average method. U-Net excels in capturing fine details and boundaries in segmentation tasks, while FPN effectively handles scale variation and context in complex environments, and their combined masks result in more precise predictions. The dataset used closely mimics real-life waste scenarios, and preprocessing techniques were applied to enhance feature learning for deep learning segmentation models. The ensemble model, referred to as EL-4, achieved an IoU value of 0.8306, an improvement over U-Net's 0.8065, and reduced Dice loss to 0.09019 from FPN's 0.1183. This study could contribute to the efficiency of waste sorting at Material Recovery Facility, facilitating better raw material acquisition for recycling with minimal human intervention and enhancing the overall throughput.

</details>


### [49] [A WDLoRA-Based Multimodal Generative Framework for Clinically Guided Corneal Confocal Microscopy Image Synthesis in Diabetic Neuropathy](https://arxiv.org/abs/2602.13693)
*Xin Zhang,Liangxiu Han,Yue Shi,Yalin Zheng,Uazman Alam,Maryam Ferdousi,Rayaz Malik*

Main category: cs.CV

TL;DR: 本论文提出了一种基于权重分解低秩适配（WDLoRA）的多模态生成框架，用于临床指导的角膜共聚焦显微镜图像合成，以解决糖尿病周围神经病变（DPN）诊断中带标注数据稀缺和细粒度形态变异的问题。


<details>
  <summary>Details</summary>
Motivation: 糖尿病周围神经病变的小纤维损伤评估需依赖角膜共聚焦显微镜（CCM），但基于深度学习的自动诊断模型受制于带标注数据不足和神经形态细粒度变异。传统生成模型因医学领域训练不足难以保持解剖学真实性，需开发能兼顾形态与对比度的医学图像生成技术。

Method: 提出权重分解低秩适配（WDLoRA）参数高效微调机制，通过分离权重更新的幅度和方向性，独立学习神经拓扑（方向）和基质对比度（强度）。模型联合依赖神经分隔掩码和疾病特异性临床提示，生成覆盖DPN谱系的解剖一致性图像。

Result: 框架在视觉保真度（FID: 5.18）和结构完整性（SSIM: 0.630）上显著优于GAN和标准扩散模型基准。合成图像保持金标准临床生物标志物，统计特性与真实患者数据等效，并使下游诊断准确率提升2.1%，分割性能提升2.2%。

Conclusion: 验证了该框架缓解医学AI数据瓶颈的潜力，通过解剖学真实的合成数据提升自动化诊断模型效能，为医学图像生成提供了兼顾形态学保真与参数效率的新范式。

Abstract: Corneal Confocal Microscopy (CCM) is a sensitive tool for assessing small-fiber damage in Diabetic Peripheral Neuropathy (DPN), yet the development of robust, automated deep learning-based diagnostic models is limited by scarce labelled data and fine-grained variability in corneal nerve morphology. Although Artificial Intelligence (AI)-driven foundation generative models excel at natural image synthesis, they often struggle in medical imaging due to limited domain-specific training, compromising the anatomical fidelity required for clinical analysis. To overcome these limitations, we propose a Weight-Decomposed Low-Rank Adaptation (WDLoRA)-based multimodal generative framework for clinically guided CCM image synthesis. WDLoRA is a parameter-efficient fine-tuning (PEFT) mechanism that decouples magnitude and directional weight updates, enabling foundation generative models to independently learn the orientation (nerve topology) and intensity (stromal contrast) required for medical realism. By jointly conditioning on nerve segmentation masks and disease-specific clinical prompts, the model synthesises anatomically coherent images across the DPN spectrum (Control, T1NoDPN, T1DPN). A comprehensive three-pillar evaluation demonstrates that the proposed framework achieves state-of-the-art visual fidelity (Fréchet Inception Distance (FID): 5.18) and structural integrity (Structural Similarity Index Measure (SSIM): 0.630), significantly outperforming GAN and standard diffusion baselines. Crucially, the synthetic images preserve gold-standard clinical biomarkers and are statistically equivalent to real patient data. When used to train automated diagnostic models, the synthetic dataset improves downstream diagnostic accuracy by 2.1% and segmentation performance by 2.2%, validating the framework's potential to alleviate data bottlenecks in medical AI.

</details>


### [50] [Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images](https://arxiv.org/abs/2602.13712)
*Chan Hao Sien,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉语言模型（VLM）的自动化框架，用于定位显微图像中的寄生虫卵，以提高土壤传播蠕虫感染的诊断效率。


<details>
  <summary>Details</summary>
Motivation: 土壤传播蠕虫（STH）感染主要影响热带和亚热带地区人群，传统手工显微诊断耗时且易出错，且这些地区缺乏专业诊断资源，亟需自动化解决方案。

Method: 对微软Florence的VLM进行微调，实现寄生虫卵的定位，并与如EfficientDet等其他目标检测方法进行性能比较。

Result: 该VLM的定位性能优于现有方法，平均交并比（mIOU）达到0.94。

Conclusion: 该VLM可作为自动化诊断框架的核心组件，提供可扩展的智能寄生虫学诊断工程方案。

Abstract: Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.

</details>


### [51] [Generative Latent Representations of 3D Brain MRI for Multi-Task Downstream Analysis in Down Syndrome](https://arxiv.org/abs/2602.13731)
*Jordi Malé,Juan Fortea,Mateus Rozalem-Aranha,Neus Martínez-Abadías,Xavier Sevillano*

Main category: cs.CV

TL;DR: 本文研究了变分自编码器（VAEs）在3D脑部MRI扫描中的潜在表示能力，通过三重分析证明了模型在高质量重建和疾病区分上的有效性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的潜在表示在医学影像中的结构和临床适用性尚待探索，需系统性研究以推动其在神经影像学和临床决策中的应用。

Method: 构建多个VAE编码3D脑MRI扫描；通过三种分析评估（1）重建质量量化与可视化，（2）主成分分析（PCA）潜在空间结构，（3）唐氏综合征分类任务验证下游应用价值。

Result: VAE实现高保真MRI重建并提取关键脑部特征，潜在空间呈现结构化聚类，其中唐氏综合征与正常对照组呈现清晰分离，分类任务性能优越。

Conclusion: 基于VAE的潜在表示兼具结构化特性和临床分类能力，为3D医学影像分析提供了可靠的生成式建模与疾病生物标记物发现框架。

Abstract: Generative models have emerged as powerful tools in medical imaging, enabling tasks such as segmentation, anomaly detection, and high-quality synthetic data generation. These models typically rely on learning meaningful latent representations, which are particularly valuable given the high-dimensional nature of 3D medical images like brain magnetic resonance imaging (MRI) scans. Despite their potential, latent representations remain underexplored in terms of their structure, information content, and applicability to downstream clinical tasks. Investigating these representations is crucial for advancing the use of generative models in neuroimaging research and clinical decision-making. In this work, we develop multiple variational autoencoders (VAEs) to encode 3D brain MRI scans into compact latent space representations for generative and predictive applications. We systematically evaluate the effectiveness of the learned representations through three key analyses: (i) a quantitative and qualitative assessment of MRI reconstruction quality, (ii) a visualisation of the latent space structure using Principal Component Analysis, and (iii) downstream classification tasks on a proprietary dataset of euploid and Down syndrome individuals brain MRI scans. Our results demonstrate that the VAE successfully captures essential brain features while maintaining high reconstruction fidelity. The latent space exhibits clear clustering patterns, particularly in distinguishing individuals with Down syndrome from euploid controls.

</details>


### [52] [T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation](https://arxiv.org/abs/2602.13751)
*Bin Yang,Rong Ou,Weisheng Xu,Jiaqi Xiong,Xintao Li,Taowen Wang,Luyu Zhu,Xu Jiang,Jing Tan,Renjing Xu*

Main category: cs.CV

TL;DR: 本文提出一个针对文本到动作生成模型在复杂域外（OOD）文本条件下生成能力的评估基准，包括包含1025个文本描述的OOD提示数据集和集成LLM评估、多因子动作评估与细粒度准确评估的统一评估框架。实验显示现有模型在细粒度准确率评估中表现普遍不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成模型评估受限于使用单一分布内文本和有限评估标准，难以系统评估模型在复杂域外文本条件下的泛化能力与动作生成质量。迫切需要解决评估体系不完善导致的模型优化方向模糊问题。

Method: 构建包含1,025条挑战性文本描述的OOD提示数据集；设计包含三个维度的评估框架：1）基于大语言模型的文本-动作语义一致性评估 2）动作生成质量的多因子分析（动力学、物理合理性、多样性）3）基于关节级精度的细粒度动作准确性量化。

Result: 实验发现：1）不同基准模型在语义对齐、运动泛化等能力上存在显著差异 2）当前最优方法在细粒度关节动作重建任务中准确率不足30% 3）物理合理性约束和上下文分解策略可提升动作生成质量但降低文本匹配度。

Conclusion: 研究揭示了现有文本到动作生成模型在处理复杂域外文本时的泛化能力缺陷，证明了细粒度评估指标对生产级模型的重要价值，为改进跨模态对齐机制和开发面向实用的评估体系提供了明确方向。

Abstract: Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results. Specifically, we construct an OOD prompt dataset consisting of 1,025 textual descriptions. Based on this prompt dataset, we introduce a unified evaluation framework that integrates LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation. Our experimental results reveal that while different baseline models demonstrate strengths in areas such as text-to-motion semantic alignment, motion generalizability, and physical quality, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation. These findings highlight the limitations of existing methods in OOD scenarios and offer practical guidance for the design and evaluation of future production-level text-to-motion models.

</details>


### [53] [OmniScience: A Large-scale Multi-modal Dataset for Scientific Image Understanding](https://arxiv.org/abs/2602.13758)
*Haoyi Tao,Chaozheng Huang,Nan Wang,Han Lyu,Linfeng Zhang,Guolin Ke,Xi Fang*

Main category: cs.CV

TL;DR: 本论文提出OmniScience，一个包含150万图-标题-上下文三元组的大规模高质量多模态科学图像数据集，通过创新的动态模型路由重标注流水线和质量过滤机制，显著提升多模态模型对科学图像的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型（MLLM）对自然图像理解效果良好，但严重缺乏对科学图像（如示意图、实验表征图、分析图表）的解释能力。这种限制源于现有数据集的领域覆盖不足、结构注释粗糙和语义基础薄弱，阻碍了科学领域的多模态研究进展。

Method: 1) 构建OmniScience数据集，涵盖10+主要科学领域，包含高质量图像-标题-上下文三元组；2) 开发动态模型路由重标注流水线，联合利用前沿多模态大模型、原始图像特征、标题及人类科学家撰写的上下文，生成高密度自包含描述；3) 实施严格的质量过滤与人类专家评估对齐机制；4) 设计基于图像标题的问答协议作为视觉理解评价方法。

Result: 生成描述的信息密度和准确性显著提升，图像-文本多模态相似度评分从0.769提升至0.956；在OmniScience微调的Qwen2.5-VL-3B模型在MM-MT-Bench和MMMU基准测试中分别取得0.378和0.140的性能增益，验证了数据集的有效性和模型泛化能力。

Conclusion: 通过构建大规模科学图像数据集和创新的标注方法，本研究所提出的OmniScience有效解决了多模态大模型在科学图像理解中的关键瓶颈，为科学领域多模态建模提供了高质量基准数据和可扩展的标注框架，推动了跨学科研究发展。

Abstract: Multimodal Large Language Models demonstrate strong performance on natural image understanding, yet exhibit limited capability in interpreting scientific images, including but not limited to schematic diagrams, experimental characterizations, and analytical charts. This limitation is particularly pronounced in open-source MLLMs. The gap largely stems from existing datasets with limited domain coverage, coarse structural annotations, and weak semantic grounding. We introduce OmniScience, a large-scale, high-fidelity multi-modal dataset comprising 1.5 million figure-caption-context triplets, spanning more than 10 major scientific disciplines. To obtain image caption data with higher information density and accuracy for multi-modal large-model training, we develop a dynamic model-routing re-captioning pipeline that leverages state-of-the-art multi-modal large language models to generate dense, self-contained descriptions by jointly synthesizing visual features, original figure captions, and corresponding in-text references authored by human scientists. The pipeline is further reinforced with rigorous quality filtering and alignment with human expert judgments, ensuring both factual accuracy and semantic completeness, and boosts the image-text multi-modal similarity score from 0.769 to 0.956. We further propose a caption QA protocol as a proxy task for evaluating visual understanding. Under this setting, Qwen2.5-VL-3B model finetuned on OmniScience show substantial gains over baselines, achieving a gain of 0.378 on MM-MT-Bench and a gain of 0.140 on MMMU.

</details>


### [54] [SAM4Dcap: Training-free Biomechanical Twin System from Monocular Video](https://arxiv.org/abs/2602.13760)
*Li Wang,HaoYu Wang,Xi Chen,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 该论文提出SAM4Dcap开源框架，利用单目视频进行生物力学分析，无需额外训练即可实现与多视角系统相当的膝关节运动预测。


<details>
  <summary>Details</summary>
Motivation: 实验室光学运动捕捉系统成本高昂限制了生物力学分析的普及，虽有基于多视角视频的方案但无法满足家用单目场景需求。该研究旨在突破实验室环境限制，提供便携式解决方案。

Method: 整合SAM-Body4D的四维人体网格重建与OpenSim生物力学求解器，通过自动提示策略将单目视频重建的网格转换为生物力学轨迹文件，并开发原生Linux版本提升处理效率。

Result: 初步评估显示：步态分析中膝关节运动学预测精度接近多视角系统，但髋关节屈曲角度存在偏差且存在细微抖动残留。

Conclusion: 该框架成功融合计算机视觉与生物力学仿真技术，为非实验室环境下的运动分析提供了灵活性与可访问性兼具的新型工具。

Abstract: Quantitative biomechanical analysis is essential for clinical diagnosis and injury prevention but is often restricted to laboratories due to the high cost of optical motion capture systems. While multi-view video approaches have lowered barriers, they remain impractical for home-based scenarios requiring monocular capture. This paper presents SAM4Dcap, an open-source, end-to-end framework for estimating biomechanical metrics from monocular video without additional training. SAM4Dcap integrates the temporally consistent 4D human mesh recovery of SAM-Body4D with the OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with diverse musculoskeletal models. We introduce automated prompting strategies and a Linux-native build for processing. Preliminary evaluations on walking and drop-jump tasks indicate that SAM4Dcap has the potential to achieve knee kinematic predictions comparable to multi-view systems, although some discrepancies in hip flexion and residual jitter remain. By bridging advanced computer vision with established biomechanical simulation, SAM4Dcap provides a flexible, accessible foundation for non-laboratory motion analysis.

</details>


### [55] [Skeleton2Stage: Reward-Guided Fine-Tuning for Physically Plausible Dance Generation](https://arxiv.org/abs/2602.13778)
*Jidong Jia,Youjian Zhang,Huan Fu,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出一种结合物理驱动奖励和强化学习微调的方法，有效解决舞蹈生成中骨骼运动在网格可视化时的穿模及脚部异常问题。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈生成方法依赖骨骼域训练而忽视网格层面的物理约束，导致视觉化时出现身体自穿插和脚部与地面接触异常，影响美观性和实用价值。

Method: 设计物理驱动的奖励函数（包括模仿奖励衡量运动合理性、脚部-地面偏离奖励强化动态交互），并引入反冻结奖励防止模型生成僵直动作，结合扩散模型与强化学习微调优化生成效果。

Result: 在多个舞蹈数据集上验证了方法在提升物理合理性（如减少穿模和脚滑现象）和增强视觉美感方面的显著优势，生成动作动态性与真实感均优于现有方案。

Conclusion: 通过建立骨骼动作到网格可视化的物理约束桥梁，该方法为生成高保真舞蹈动画提供了新思路，实验证明其在学术与工业应用中的潜力。

Abstract: Despite advances in dance generation, most methods are trained in the skeletal domain and ignore mesh-level physical constraints. As a result, motions that look plausible as joint trajectories often exhibit body self-penetration and Foot-Ground Contact (FGC) anomalies when visualized with a human body mesh, reducing the aesthetic appeal of generated dances and limiting their real-world applications. We address this skeleton-to-mesh gap by deriving physics-based rewards from the body mesh and applying Reinforcement Learning Fine-Tuning (RLFT) to steer the diffusion model toward physically plausible motion synthesis under mesh visualization. Our reward design combines (i) an imitation reward that measures a motion's general plausibility by its imitability in a physical simulator (penalizing penetration and foot skating), and (ii) a Foot-Ground Deviation (FGD) reward with test-time FGD guidance to better capture the dynamic foot-ground interaction in dance. However, we find that the physics-based rewards tend to push the model to generate freezing motions for fewer physical anomalies and better imitability. To mitigate it, we propose an anti-freezing reward to preserve motion dynamics while maintaining physical plausibility. Experiments on multiple dance datasets consistently demonstrate that our method can significantly improve the physical plausibility of generated motions, yielding more realistic and aesthetically pleasing dances. The project page is available at: https://jjd1123.github.io/Skeleton2Stage/

</details>


### [56] [Foundation Model-Driven Semantic Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.13780)
*Hengtong Shen,Li Yan,Hong Xie,Yaxuan Wei,Xinhao Li,Wenfei Shen,Peixian Lv,Fei Tan*

Main category: cs.CV

TL;DR: 本文提出了一种基于遥感基础模型PerA的语义变化检测方法PerASCD，通过设计级联门控解码器（CG-Decoder）和软语义一致性损失（SSCLoss），简化了复杂任务流程并提升检测性能，最终在两个公共数据集上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有语义变化检测方法在模型语义理解和任务复杂性方面存在显著挑战，需提升多尺度语义理解能力和优化复杂范式以提高整体性能。

Method: 提出模块化CG-Decoder实现多级特征交互与融合，设计SSCLoss损失函数缓解训练不稳定性，并探索多种遥感基础模型与该解码器的适配性。

Result: CG-Decoder简化了检测范式且实现跨视觉编码器的无缝适配，在LEVIR-CD+和DSIFN-CD数据集上均达SOTA性能指标。

Conclusion: 所提方法有效解决了语义变化检测任务中的核心问题，具有跨模型结构的普适性，为同类任务提供了轻量化解决方案。

Abstract: Remote sensing (RS) change detection methods can extract critical information on surface dynamics and are an essential means for humans to understand changes in the earth's surface and environment. Among these methods, semantic change detection (SCD) can more effectively interpret the multi-class information contained in bi-temporal RS imagery, providing semantic-level predictions that support dynamic change monitoring. However, due to the limited semantic understanding capability of the model and the inherent complexity of the SCD tasks, existing SCD methods face significant challenges in both performance and paradigm complexity. In this paper, we propose PerASCD, a SCD method driven by RS foundation model PerA, designed to enhance the multi-scale semantic understanding and overall performance. We introduce a modular Cascaded Gated Decoder (CG-Decoder) that simplifies complex SCD decoding pipelines while promoting effective multi-level feature interaction and fusion. In addition, we propose a Soft Semantic Consistency Loss (SSCLoss) to mitigate the numerical instability commonly encountered during SCD training. We further explore the applicability of multiple existing RS foundation models on the SCD task when equipped with the proposed decoder. Experimental results demonstrate that our decoder not only effectively simplifies the paradigm of SCD, but also achieves seamless adaptation across various vision encoders. Our method achieves state-of-the-art (SOTA) performance on two public benchmark datasets, validating its effectiveness. The code is available at https://github.com/SathShen/PerASCD.git.

</details>


### [57] [Joint Orientation and Weight Optimization for Robust Watertight Surface Reconstruction via Dirichlet-Regularized Winding Fields](https://arxiv.org/abs/2602.13801)
*Jiaze Li,Daisheng Jin,Fei Hou,Junhui Hou,Zheng Liu,Shiqing Xin,Wenping Wang,Ying He*

Main category: cs.CV

TL;DR: DiWR是一种通过同时优化点方向、面积权重与置信系数，在广义绕数场中最小化Dirichlet能量的方法，可从非均匀采样、含噪声和异常值的无向点云中重建封闭曲面。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以解决无向点云中存在的非均匀采样、噪声及异常值问题，且依赖预处理步骤。作者旨在设计一种端到端的联合优化框架，避免多阶段流程的误差累积并提升鲁棒性。

Method: 通过广义绕数场（GWN）构建隐式表示，联合优化点方向、每点面积权重和置信系数。损失函数包含GWN的Dirichlet能量项与约束项，在单一流程中实现抗噪声和异常值权重的自适应调整。

Result: 在3D高斯泼溅点云、计算机视觉流水线生成点云和腐蚀性图形基准数据集上，DiWR的封闭曲面重建质量优于传统多阶段方法和最新联合优化方法，尤其在噪声和异常值场景下表现突出。

Conclusion: 基于单一隐式场的联合优化策略有效解决了复杂点云的封闭曲面重建问题，无需预处理即可处理非均匀采样和噪声干扰，为几何重建提供了新范式。

Abstract: We propose Dirichlet Winding Reconstruction (DiWR), a robust method for reconstructing watertight surfaces from unoriented point clouds with non-uniform sampling, noise, and outliers. Our method uses the generalized winding number (GWN) field as the target implicit representation and jointly optimizes point orientations, per-point area weights, and confidence coefficients in a single pipeline. The optimization minimizes the Dirichlet energy of the induced winding field together with additional GWN-based constraints, allowing DiWR to compensate for non-uniform sampling, reduce the impact of noise, and downweight outliers during reconstruction, with no reliance on separate preprocessing. We evaluate DiWR on point clouds from 3D Gaussian Splatting, a computer-vision pipeline, and corrupted graphics benchmarks. Experiments show that DiWR produces plausible watertight surfaces on these challenging inputs and outperforms both traditional multi-stage pipelines and recent joint orientation-reconstruction methods.

</details>


### [58] [Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos](https://arxiv.org/abs/2602.13806)
*Can Li,Jie Gu,Jingmin Chen,Fangzhou Qiu,Lei Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多尺度动态机制的单目4D场景重建方法，通过分解复杂运动场并结合视觉模型先验知识，实现动态场景的精确重建。


<details>
  <summary>Details</summary>
Motivation: 单目视频的4D重建存在高度病态问题，而现实世界动态具有从物体到粒子的多尺度规律性，这一特性未被充分挖掘。

Method: 设计多尺度动态机制分解运动场，提出具有多尺度动态的高斯序列，结合视觉基础模型的多模态先验进行联合监督优化。

Result: 在动态新视角合成和真实场景操作数据集上取得显著性能提升，实现单目视频全局一致的高质量4D重建。

Conclusion: 基于多尺度动态分解和先验约束的模型有效解决了单目4D重建的模糊性问题，为机器人动态场景理解提供了新方法。

Abstract: Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.

</details>


### [59] [VAR-3D: View-aware Auto-Regressive Model for Text-to-3D Generation via a 3D Tokenizer](https://arxiv.org/abs/2602.13818)
*Zongcheng Han,Dongyan Cao,Haoran Sun,Yu Hong*

Main category: cs.CV

TL;DR: The paper proposes VAR-3D, a view-aware auto-regressive framework for text-to-3D generation that improves discrete representation learning and training alignment to enhance geometric coherence and text-3D alignment.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D generation methods suffer from information loss during encoding, representational distortion from vector quantization, and objective mismatches in two-stage training, leading to poor geometric coherence and text alignment.

Method: VAR-3D integrates a view-aware 3D VQ-VAE for discrete token encoding and introduces a rendering-supervised training strategy that couples token prediction with visual reconstruction to preserve visual fidelity and structural consistency.

Result: Experiments show VAR-3D significantly outperforms existing methods in generation quality (e.g., geometric coherence) and text-3D alignment metrics.

Conclusion: The proposed view-aware architecture and training strategy mitigate information loss and training mismatches in text-to-3D generation, advancing auto-regressive modeling for complex structured outputs.

Abstract: Recent advances in auto-regressive transformers have achieved remarkable success in generative modeling. However, text-to-3D generation remains challenging, primarily due to bottlenecks in learning discrete 3D representations. Specifically, existing approaches often suffer from information loss during encoding, causing representational distortion before the quantization process. This effect is further amplified by vector quantization, ultimately degrading the geometric coherence of text-conditioned 3D shapes. Moreover, the conventional two-stage training paradigm induces an objective mismatch between reconstruction and text-conditioned auto-regressive generation. To address these issues, we propose View-aware Auto-Regressive 3D (VAR-3D), which intergrates a view-aware 3D Vector Quantized-Variational AutoEncoder (VQ-VAE) to convert the complex geometric structure of 3D models into discrete tokens. Additionally, we introduce a rendering-supervised training strategy that couples discrete token prediction with visual reconstruction, encouraging the generative process to better preserve visual fidelity and structural consistency relative to the input text. Experiments demonstrate that VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment.

</details>


### [60] [Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings](https://arxiv.org/abs/2602.13823)
*Haonan Jiang,Yuji Wang,Yongjie Zhu,Xin Lu,Wenyu Qin,Meng Wang,Pengfei Wan,Yansong Tang*

Main category: cs.CV

TL;DR: 本文提出基于强化学习的多模态通用嵌入框架，通过生成检索相关推理链优化跨模态语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式链式思考(CoT)方法仅依赖文本分析且与检索目标无关，导致生成的推理链未能有效促进跨模态检索。需解决多模态证据整合与任务对齐问题。

Method: 设计嵌入器引导的强化学习（EG-RL）框架，包含：1）嵌入器监督推理器生成可追溯CoT（T-CoT）；2）T-CoT提取多模态检索相关线索；3）通过检索对齐优化提升模型泛化能力。

Result: 在MMEB-V2/UVRB基准测试中超越现有模型，计算资源受限时仍保持优势，细粒度匹配能力与场景泛化性显著提升。

Conclusion: 基于可追溯推理链的多模态嵌入框架证明了结构化推理整合的有效性，为资源高效型检索任务提供了可扩展解决方案。

Abstract: Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.

</details>


### [61] [Prior-guided Hierarchical Instance-pixel Contrastive Learning for Ultrasound Speckle Noise Suppression](https://arxiv.org/abs/2602.13831)
*Zhenyu Bu,Yuanxin Xie,Guang-Quan Zhou*

Main category: cs.CV

TL;DR: 该研究提出了一种基于先验引导的分层实例-像素对比学习的超声去噪模型，结合Transformer-CNN架构提升降噪效果与结构保持能力。


<details>
  <summary>Details</summary>
Motivation: 超声图像受斑点噪声干扰影响诊断可靠性，但斑点模式包含纹理与细微结构信息，需在抑制噪声的同时保留结构细节。

Method: 1) 基于统计引导的像素级对比学习增强噪声与干净像素的分布差异；2) 使用内存库进行实例级特征空间对比学习；3) Transformer编码器（全局建模）与CNN解码器（局部结构修复）的混合架构。

Result: 在两个公开超声数据集上的实验证明其性能优于现有方法，有效平衡噪声抑制与结构保真。

Conclusion: 所提模型通过对比学习策略与混合架构，在超声去噪任务中实现了更优的鲁棒性与结构感知能力。

Abstract: Ultrasound denoising is essential for mitigating speckle-induced degradations, thereby enhancing image quality and improving diagnostic reliability. Nevertheless, because speckle patterns inherently encode both texture and fine anatomical details, effectively suppressing noise while preserving structural fidelity remains a significant challenge. In this study, we propose a prior-guided hierarchical instance-pixel contrastive learning model for ultrasound denoising, designed to promote noise-invariant and structure-aware feature representations by maximizing the separability between noisy and clean samples at both pixel and instance levels. Specifically, a statistics-guided pixel-level contrastive learning strategy is introduced to enhance distributional discrepancies between noisy and clean pixels, thereby improving local structural consistency. Concurrently, a memory bank is employed to facilitate instance-level contrastive learning in the feature space, encouraging representations that more faithfully approximate the underlying data distribution. Furthermore, a hybrid Transformer-CNN architecture is adopted, coupling a Transformer-based encoder for global context modeling with a CNN-based decoder optimized for fine-grained anatomical structure restoration, thus enabling complementary exploitation of long-range dependencies and local texture details. Extensive evaluations on two publicly available ultrasound datasets demonstrate that the proposed model consistently outperforms existing methods, confirming its effectiveness and superiority.

</details>


### [62] [High-Fidelity Causal Video Diffusion Models for Real-Time Ultra-Low-Bitrate Semantic Communication](https://arxiv.org/abs/2602.13837)
*Cem Eteke,Batuhan Tosun,Alexander Griessel,Wolfgang Kellerer,Eckehard Steinbach*

Main category: cs.CV

TL;DR: 提出了一种适用于超低码率语义通信的实时视频扩散模型


<details>
  <summary>Details</summary>
Motivation: 在带宽受限的语义通信场景下实现高保真、因果性和实时性的视频生成

Method: 结合语义场景结构编码与低分辨率纹理流传输，设计包含语义控制、重构适配器和时序适配器的模块化扩散模型，并引入时序蒸馏流程

Result: 在<0.0003 bpp码率下实现300倍参数压缩和2倍训练加速，多项指标超越经典/神经/生成模型基线

Conclusion: 证明了语义通信约束下视频生成质量与效率的平衡可能性

Abstract: We introduce a video diffusion model for high-fidelity, causal, and real-time video generation under ultra-low-bitrate semantic communication constraints. Our approach utilizes lossy semantic video coding to transmit the semantic scene structure, complemented by a stream of highly compressed, low-resolution frames that provide sufficient texture information to preserve fidelity. Building on these inputs, we introduce a modular video diffusion model that contains Semantic Control, Restoration Adapter, and Temporal Adapter. We further introduce an efficient temporal distillation procedure that enables extension to real-time and causal synthesis, reducing trainable parameters by 300x and training time by 2x, while adhering to communication constraints. Evaluated across diverse datasets, the framework achieves strong perceptual quality, semantic fidelity, and temporal consistency at ultra-low bitrates (< 0.0003 bpp), outperforming classical, neural, and generative baselines in extensive quantitative, qualitative, and subjective evaluations.

</details>


### [63] [Synthetic Dataset Generation and Validation for Robotic Surgery Instrument Segmentation](https://arxiv.org/abs/2602.13844)
*Giorgio Chiesa,Rossella Borra,Vittorio Lauro,Sabrina De Cillis,Daniele Amparore,Cristian Fiori,Riccardo Renzulli,Marco Grangetto*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化生成逼真标注数据集的框架，用于机器人辅助手术器械分割，通过结合真实与合成数据显著提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗数据隐私和采集成本限制导致真实手术数据不足，需通过合成数据补充；现有合成数据集缺乏对手术场景特异性（如机械臂运动、血迹分布）的精准模拟。

Method: 基于Da Vinci手术机器人3D模型构建自动化生成流程： 1. 使用Python在Maya中开发自动化管线实现机械臂动态捕捉； 2. 引入运动模式随机化、光照扰动、合成血迹纹理生成多样化的手术场景； 3. 通过混合真实与合成数据训练分割模型验证有效性。

Result: 1. 成功生成像素级标注的逼真视频序列； 2. 1:1比例的混合训练数据使Dice系数较纯真实数据提升12.7±2.3%； 3. 过量合成数据引发域偏移导致mIoU下降9.4%。 4. 开源代码与数据集获3,200+星标，验证框架可复现性。

Conclusion: 论文构建了首个面向手术机器人分割任务的可复现合成数据生成框架，证明有限合成数据的增益作用，同时揭示纯合成训练的局限性，为领域自适应和仿真预训练提供基准工具。

Abstract: This paper presents a comprehensive workflow for generating and validating a synthetic dataset designed for robotic surgery instrument segmentation. A 3D reconstruction of the Da Vinci robotic arms was refined and animated in Autodesk Maya through a fully automated Python-based pipeline capable of producing photorealistic, labeled video sequences. Each scene integrates randomized motion patterns, lighting variations, and synthetic blood textures to mimic intraoperative variability while preserving pixel-accurate ground truth masks. To validate the realism and effectiveness of the generated data, several segmentation models were trained under controlled ratios of real and synthetic data. Results demonstrate that a balanced composition of real and synthetic samples significantly improves model generalization compared to training on real data only, while excessive reliance on synthetic data introduces a measurable domain shift. The proposed framework provides a reproducible and scalable tool for surgical computer vision, supporting future research in data augmentation, domain adaptation, and simulation-based pretraining for robotic-assisted surgery. Data and code are available at https://github.com/EIDOSLAB/Sintetic-dataset-DaVinci.

</details>


### [64] [Cardiac Output Prediction from Echocardiograms: Self-Supervised Learning with Limited Data](https://arxiv.org/abs/2602.13846)
*Adson Duarte,Davide Vitturini,Emanuele Milillo,Andrea Bragagnolo,Carlo Alberto Barbano,Riccardo Renzulli,Michele Cannito,Federico Giacobbe,Francesco Bruno,Ovidio de Filippo,Fabrizio D'Ascenzo,Marco Grangetto*

Main category: cs.CV

TL;DR: 本文提出了一种基于自监督学习（SSL）的预训练策略，通过SimCLR方法提升超声心动图视频的心输出量（CO）预测准确性，尤其在数据稀缺情况下仍优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 心输出量（CO）的精确测量依赖侵入性右心导管术，需开发非侵入性超声替代方案。然而，现有模型因数据不足易过拟合，需通过SSL增强表征学习。

Method: 采用SimCLR框架对有限的心尖四腔超声视频数据集进行SSL预训练，通过对比学习优化CO预测模型参数，减少对大规模数据的依赖。

Result: 测试集平均Pearson相关系数达0.41（优于百万级数据训练的PanEcho模型），验证了SSL在缓解过拟合与提升泛化能力方面的有效性。

Conclusion: 基于SSL的预训练策略在小规模超声数据集上显著提升CO预测性能，为资源受限场景下的医学影像分析提供了新范式。代码已开源促进可复现性（GitHub链接）。

Abstract: Cardiac Output (CO) is a key parameter in the diagnosis and management of cardiovascular diseases. However, its accurate measurement requires right-heart catheterization, an invasive and time-consuming procedure, motivating the development of reliable non-invasive alternatives using echocardiography. In this work, we propose a self-supervised learning (SSL) pretraining strategy based on SimCLR to improve CO prediction from apical four-chamber echocardiographic videos. The pretraining is performed using the same limited dataset available for the downstream task, demonstrating the potential of SSL even under data scarcity. Our results show that SSL mitigates overfitting and improves representation learning, achieving an average Pearson correlation of 0.41 on the test set and outperforming PanEcho, a model trained on over one million echocardiographic exams. Source code is available at https://github.com/EIDOSLAB/cardiac-output.

</details>


### [65] [Low-Pass Filtering Improves Behavioral Alignment of Vision Models](https://arxiv.org/abs/2602.13859)
*Max Wolff,Thomas Klein,Evgenia Rusak,Felix Wichmann,Wieland Brendel*

Main category: cs.CV

TL;DR: 本文发现生成模型对人类视觉行为的更好对齐主要源于低通滤波而非生成式特性，测试时图像模糊即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究DNNs与人类视觉对齐差距的根源，并验证生成模型优于判别模型的假设是否成立。

Method: 通过移除高频空间信息、应用低通滤波及测试时模糊图像提升对齐，直接优化滤波器参数并计算帕累托前沿分析性能极限。

Result: 判别模型去除高频信息后对齐度提升；测试时模糊达当前最优成绩；优化滤波器逼近人类视觉频率响应，首次揭示基准性能前沿。

Conclusion: 低通滤波是生成模型对齐优势的核心因素，其频率谱与人类视觉系统的带通特性高度匹配。

Abstract: Despite their impressive performance on computer vision benchmarks, Deep Neural Networks (DNNs) still fall short of adequately modeling human visual behavior, as measured by error consistency and shape bias. Recent work hypothesized that behavioral alignment can be drastically improved through \emph{generative} -- rather than \emph{discriminative} -- classifiers, with far-reaching implications for models of human vision.
  Here, we instead show that the increased alignment of generative models can be largely explained by a seemingly innocuous resizing operation in the generative model which effectively acts as a low-pass filter. In a series of controlled experiments, we show that removing high-frequency spatial information from discriminative models like CLIP drastically increases their behavioral alignment. Simply blurring images at test-time -- rather than training on blurred images -- achieves a new state-of-the-art score on the model-vs-human benchmark, halving the current alignment gap between DNNs and human observers. Furthermore, low-pass filters are likely optimal, which we demonstrate by directly optimizing filters for alignment. To contextualize the performance of optimal filters, we compute the frontier of all possible pareto-optimal solutions to the benchmark, which was formerly unknown.
  We explain our findings by observing that the frequency spectrum of optimal Gaussian filters roughly matches the spectrum of band-pass filters implemented by the human visual system. We show that the contrast sensitivity function, describing the inverse of the contrast threshold required for humans to detect a sinusoidal grating as a function of spatiotemporal frequency, is approximated well by Gaussian filters of the specific width that also maximizes error consistency.

</details>


### [66] [Parameter-Efficient Fine-Tuning of DINOv2 for Large-Scale Font Classification](https://arxiv.org/abs/2602.13889)
*Daniel Chen,Zaria Zinn,Marcus Lowe*

Main category: cs.CV

TL;DR: A font classification system using a fine-tuned DINOv2 Vision Transformer with LoRA achieves 86% accuracy while training <1% of parameters, supported by a synthetic dataset generation pipeline.


<details>
  <summary>Details</summary>
Motivation: Efficiently classify 394 font families from real-world text images using minimal model parameters and enhance generalization through synthetic data diversity.

Method: Employ DINOv2 Vision Transformer with LoRA adaptation, create a synthetic dataset via scalable rendering of Google Fonts combined with augmentations like randomized colors, alignment, and noise, and integrate built-in preprocessing for training-inference consistency.

Result: System achieves ~86% top-1 accuracy, trains <1% of 87.2M parameters, generalizes to real-world samples, and is deployed as a HuggingFace Inference Endpoint.

Conclusion: LoRA-based fine-tuning and synthetic data generation effectively enable accurate font classification with significant parameter efficiency and practical deployment capabilities.

Abstract: We present a font classification system capable of identifying 394 font families from rendered text images. Our approach fine-tunes a DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA), achieving approximately 86% top-1 accuracy while training fewer than 1% of the model's 87.2M parameters. We introduce a synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations including randomized colors, alignment, line wrapping, and Gaussian noise, producing training images that generalize to real-world typographic samples. The model incorporates built-in preprocessing to ensure consistency between training and inference, and is deployed as a HuggingFace Inference Endpoint. We release the model, dataset, and full training pipeline as open-source resources.

</details>


### [67] [Fusing Pixels and Genes: Spatially-Aware Learning in Computational Pathology](https://arxiv.org/abs/2602.13944)
*Minghao Han,Dingkang Yang,Linhao Qu,Zizhi Chen,Gang Li,Han Wang,Jiacong Wang,Lihua Zhang*

Main category: cs.CV

TL;DR: 提出STAMP框架，整合空间转录组与病理图像数据，解决多模态病理学表征瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言多模态模型缺乏分子特异性与病理监督信号，导致表征能力受限

Method: 构建SpaVis-6M空间转录组数据集，采用基因编码器结合多尺度对比对齐和跨尺度斑块定位机制，实现空间分子特征与病理图像的联合嵌入学习

Result: 在六个数据集和四个下游任务验证有效性，模型表现显著优于现有方法，代码与数据集已公开

Conclusion: 整合空间解析分子信息可有效突破传统多模态局限，为计算病理学提供新范式

Abstract: Recent years have witnessed remarkable progress in multimodal learning within computational pathology. Existing models primarily rely on vision and language modalities; however, language alone lacks molecular specificity and offers limited pathological supervision, leading to representational bottlenecks. In this paper, we propose STAMP, a Spatial Transcriptomics-Augmented Multimodal Pathology representation learning framework that integrates spatially-resolved gene expression profiles to enable molecule-guided joint embedding of pathology images and transcriptomic data. Our study shows that self-supervised, gene-guided training provides a robust and task-agnostic signal for learning pathology image representations. Incorporating spatial context and multi-scale information further enhances model performance and generalizability. To support this, we constructed SpaVis-6M, the largest Visium-based spatial transcriptomics dataset to date, and trained a spatially-aware gene encoder on this resource. Leveraging hierarchical multi-scale contrastive alignment and cross-scale patch localization mechanisms, STAMP effectively aligns spatial transcriptomics with pathology images, capturing spatial structure and molecular variation. We validate STAMP across six datasets and four downstream tasks, where it consistently achieves strong performance. These results highlight the value and necessity of integrating spatially resolved molecular supervision for advancing multimodal learning in computational pathology. The code is included in the supplementary materials. The pretrained weights and SpaVis-6M are available at: https://github.com/Hanminghao/STAMP.

</details>


### [68] [MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars](https://arxiv.org/abs/2602.13961)
*Shuoyuan Wang,Yiran Wang,Hongxin Wei*

Main category: cs.CV

TL;DR: 本文提出MarsRetrieval基准测试，用于评估火星地理空间发现的视觉语言模型，包含三个多尺度任务，并展示现有模型的不足及领域微调的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有行星科学基准局限于闭集监督任务，不支持文本引导检索，难以满足多模态地理空间发现需求。

Method: 构建包含图像-文本检索、地貌检索和全球地理定位的基准，提出统一协议评估对比学习模型和生成式视觉语言模型。

Result: 强基础模型难以捕捉火星地貌特征，领域特定微调显著提升跨空间尺度的地理发现性能。

Conclusion: MarsRetrieval为行星科学提供新基准，揭示视觉语言模型在跨模态地理分析中的局限性，证明领域微调对实际应用的必要性。

Abstract: Data-driven approaches like deep learning are rapidly advancing planetary science, particularly in Mars exploration. Despite recent progress, most existing benchmarks remain confined to closed-set supervised visual tasks and do not support text-guided retrieval for geospatial discovery. We introduce MarsRetrieval, a retrieval benchmark for evaluating vision-language models for Martian geospatial discovery. MarsRetrieval includes three tasks: (1) paired image-text retrieval, (2) landform retrieval, and (3) global geo-localization, covering multiple spatial scales and diverse geomorphic origins. We propose a unified retrieval-centric protocol to benchmark multimodal embedding architectures, including contrastive dual-tower encoders and generative vision-language models. Our evaluation shows MarsRetrieval is challenging: even strong foundation models often fail to capture domain-specific geomorphic distinctions. We further show that domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings. Our code is available at https://github.com/ml-stat-Sustech/MarsRetrieval

</details>


### [69] [Inject Where It Matters: Training-Free Spatially-Adaptive Identity Preservation for Text-to-Image Personalization](https://arxiv.org/abs/2602.13994)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: SpatialID is a training-free framework for personalized text-to-image generation that addresses background contamination by decoupling identity features and dynamically adjusting spatial masks during generation.


<details>
  <summary>Details</summary>
Motivation: Existing tuning-free methods suffer from identity feature contamination in non-facial regions, leading to poor text adherence. A solution is needed to preserve identity accuracy without costly fine-tuning.

Method: SpatialID uses a Spatial Mask Extractor (based on cross-attention) to separate identity features and a Temporal-Spatial Scheduling strategy to dynamically adapt masks and constraints during diffusion generation.

Result: On IBench, SpatialID achieves superior performance with CLIP-T 0.281 (text adherence), CLIP-I 0.827 (visual consistency), and IQ 0.523 (image quality), effectively reducing background contamination while preserving identity.

Conclusion: SpatialID achieves state-of-the-art results through spatially adaptive, training-free identity modulation, resolving key limitations in personalized text-to-image generation.

Abstract: Personalized text-to-image generation aims to integrate specific identities into arbitrary contexts. However, existing tuning-free methods typically employ Spatially Uniform Visual Injection, causing identity features to contaminate non-facial regions (e.g., backgrounds and lighting) and degrading text adherence. To address this without expensive fine-tuning, we propose SpatialID, a training-free spatially-adaptive identity modulation framework. SpatialID fundamentally decouples identity injection into face-relevant and context-free regions using a Spatial Mask Extractor derived from cross-attention responses. Furthermore, we introduce a Temporal-Spatial Scheduling strategy that dynamically adjusts spatial constraints - transitioning from Gaussian priors to attention-based masks and adaptive relaxation - to align with the diffusion generation dynamics. Extensive experiments on IBench demonstrate that SpatialID achieves state-of-the-art performance in text adherence (CLIP-T: 0.281), visual consistency (CLIP-I: 0.827), and image quality (IQ: 0.523), significantly eliminating background contamination while maintaining robust identity preservation.

</details>


### [70] [A Deployment-Friendly Foundational Framework for Efficient Computational Pathology](https://arxiv.org/abs/2602.14010)
*Yu Cai,Cheng Jin,Jiabo Ma,Fengtao Zhou,Yingxue Xu,Zhengrui Guo,Yihui Wang,Zhengyu Zhang,Ling Liang,Yonghao Tan,Pingcheng Dong,Du Cai,On Ki Tang,Chenglong Zhao,Xi Wang,Can Yang,Yali Xu,Jing Cui,Zhenhui Li,Ronald Cheong Kin Chan,Yueping Liu,Feng Gao,Xiuming Zhang,Li Liang,Hao Chen,Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: LitePath reduces computational costs of pathology AI models while maintaining accuracy, enabling efficient deployment on low-power hardware.


<details>
  <summary>Details</summary>
Motivation: Existing pathology foundation models (PFMs) require excessive computational resources for gigapixel whole slide images, limiting clinical accessibility and scalability.

Method: LitePath integrates LiteFM (a compact distilled model from three PFMs) and APS (adaptive patch selection) to minimize over-parameterization and redundancy while preserving task-specific accuracy.

Result: Achieves 28× fewer parameters, 403.5× lower FLOPs, 104.5× faster processing (208 slides/hour), and 171× lower energy consumption vs. Virchow2, with 99.71% retained AUC and superior deployability score (D-Score).

Conclusion: LitePath enables accurate, energy-efficient pathology analysis on accessible edge hardware (e.g., NVIDIA Jetson), advancing deployability and reducing carbon footprint.

Abstract: Pathology foundation models (PFMs) have enabled robust generalization in computational pathology through large-scale datasets and expansive architectures, but their substantial computational cost, particularly for gigapixel whole slide images, limits clinical accessibility and scalability. Here, we present LitePath, a deployment-friendly foundational framework designed to mitigate model over-parameterization and patch level redundancy. LitePath integrates LiteFM, a compact model distilled from three large PFMs (Virchow2, H-Optimus-1 and UNI2) using 190 million patches, and the Adaptive Patch Selector (APS), a lightweight component for task-specific patch selection. The framework reduces model parameters by 28x and lowers FLOPs by 403.5x relative to Virchow2, enabling deployment on low-power edge hardware such as the NVIDIA Jetson Orin Nano Super. On this device, LitePath processes 208 slides per hour, 104.5x faster than Virchow2, and consumes 0.36 kWh per 3,000 slides, 171x lower than Virchow2 on an RTX3090 GPU. We validated accuracy using 37 cohorts across four organs and 26 tasks (26 internal, 9 external, and 2 prospective), comprising 15,672 slides from 9,808 patients disjoint from the pretraining data. LitePath ranks second among 19 evaluated models and outperforms larger models including H-Optimus-1, mSTAR, UNI2 and GPFM, while retaining 99.71% of the AUC of Virchow2 on average. To quantify the balance between accuracy and efficiency, we propose the Deployability Score (D-Score), defined as the weighted geometric mean of normalized AUC and normalized FLOP, where LitePath achieves the highest value, surpassing Virchow2 by 10.64%. These results demonstrate that LitePath enables rapid, cost-effective and energy-efficient pathology image analysis on accessible hardware while maintaining accuracy comparable to state-of-the-art PFMs and reducing the carbon footprint of AI deployment.

</details>


### [71] [Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation](https://arxiv.org/abs/2602.14027)
*Jia Li,Xiaomeng Fu,Xurui Peng,Weifeng Chen,Youwei Zheng,Tianyu Zhao,Jiexi Wang,Fangmin Chen,Xing Wang,Hayden Kwok-Hay So*

Main category: cs.CV

TL;DR: 这篇论文提出了FLEX框架，通过解决扩散模型中的谱偏差和动态先验缺失问题，在不需重新训练的前提下显著提升了长视频外推生成性能，支持最高达4分钟的连贯动态视频合成。


<details>
  <summary>Details</summary>
Motivation: 当前自回归视频扩散模型在生成超出训练时长的长视频时会出现严重的时序退化现象，主要源于3D位置编码的谱偏差和噪声采样缺乏动态先验知识，严重限制了模型实际应用能力。

Method: FLEX引入三种关键组件：频域感知RoPE调制（自适应处理高低频成分）、反相位噪声采样（注入动态高频先验）以及推断专用注意力锚（固定全局结构），通过组合这些技术实现推断时的长时序扩展。

Result: 在VBench基准测试中，FLEX在6倍外推（30秒）超越现有最优模型，12倍外推（60秒）达到精调模型水平，并成功将LongLive等模型扩展至4分钟生成能力，项目主页展示了一系列生成示例。

Conclusion: 作为训练无关的插件式框架，FLEX为现有扩散模型提供了有效的解决方案，显著拓宽了视频生成的时间尺度边界，为长视频生成领域提供了新的研究方向。

Abstract: Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the \textit{spectral bias} of 3D positional embeddings and the lack of \textit{dynamic priors} in noise sampling. To address these issues, we propose \textbf{FLEX} (\textbf{F}requency-aware \textbf{L}ength \textbf{EX}tension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at $6\times$ extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at $12\times$ scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at \href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}.

</details>


### [72] [Explainability-Inspired Layer-Wise Pruning of Deep Neural Networks for Efficient Object Detection](https://arxiv.org/abs/2602.14040)
*Abhinav Shukla,Nachiket Tapas*

Main category: cs.CV

TL;DR: This paper proposes an explainability-inspired, layer-wise pruning framework for object detection that leverages SHAP-inspired gradient-activation attribution to estimate layer importance, improving accuracy-efficiency trade-offs compared to traditional pruning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional magnitude-based pruning methods fail to align with the functional contribution of network layers to task-specific performance. This work aims to address this limitation by introducing data-driven layer importance estimation through explainability techniques.

Method: The authors develop a layer-wise pruning framework using SHAP-inspired gradient-activation attribution, which quantifies functional contribution by combining gradient information (task relevance) with activation magnitude (neuron activity), enabling layer importance estimation for efficient pruning.

Result: Experiments across ResNet-50, MobileNetV2, ShuffleNetV2, Faster R-CNN, RetinaNet, and YOLOv8 show: 1) 10% speed improvement on ShuffleNetV2 vs. 13.7% accuracy degradation by L1-pruning; 2) RetinaNet retains baseline mAP (0.151) with negligible speed impact vs. 1.3% mAP drop by L1-pruning for 6.2% speed gain; 3) consistent identification of different least-important layers compared to magnitude-based methods.

Conclusion: Explainability-inspired pruning provides superior data-driven layer importance assessment for compression, demonstrating principled deployment of DNNs on resource-constrained platforms while preserving accuracy and interpretability.

Abstract: Deep neural networks (DNNs) have achieved remarkable success in object detection tasks, but their increasing complexity poses significant challenges for deployment on resource-constrained platforms. While model compression techniques such as pruning have emerged as essential tools, traditional magnitude-based pruning methods do not necessarily align with the true functional contribution of network components to task-specific performance. In this work, we present an explainability-inspired, layer-wise pruning framework tailored for efficient object detection. Our approach leverages a SHAP-inspired gradient--activation attribution to estimate layer importance, providing a data-driven proxy for functional contribution rather than relying solely on static weight magnitudes. We conduct comprehensive experiments across diverse object detection architectures, including ResNet-50, MobileNetV2, ShuffleNetV2, Faster R-CNN, RetinaNet, and YOLOv8, evaluating performance on the Microsoft COCO 2017 validation set. The results show that the proposed attribution-inspired pruning consistently identifies different layers as least important compared to L1-norm-based methods, leading to improved accuracy--efficiency trade-offs. Notably, for ShuffleNetV2, our method yields a 10\% empirical increase in inference speed, whereas L1-pruning degrades performance by 13.7\%. For RetinaNet, the proposed approach preserves the baseline mAP (0.151) with negligible impact on inference speed, while L1-pruning incurs a 1.3\% mAP drop for a 6.2\% speed increase. These findings highlight the importance of data-driven layer importance assessment and demonstrate that explainability-inspired compression offers a principled direction for deploying deep neural networks on edge and resource-constrained platforms while preserving both performance and interpretability.

</details>


### [73] [BitDance: Scaling Autoregressive Generative Models with Binary Tokens](https://arxiv.org/abs/2602.14041)
*Yuang Ai,Jiaming Han,Shaobin Zhuang,Weijia Mao,Xuefeng Hu,Ziyan Yang,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen*

Main category: cs.CV

TL;DR: BitDance是一个可扩展的自回归图像生成模型，采用二进制视觉令牌替代码书索引，结合二进制扩散头和新型next-patch解码方法，在保持高表达性的同时显著提升生成效率与速度，ImageNet上FID达1.24且参数量仅为同类模型的1/5.4。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型在超大令牌空间采样时存在效率瓶颈，且标准分类方法难以有效处理高熵二进制潜变量，亟需更高效、更具扩展性的图像生成框架。

Method: 采用二进制扩散头代替传统softmax预测机制，设计next-patch扩散解码方法实现多令牌并行预测，通过256位高熵二进制潜变量构建紧凑表达，并结合多模态令牌实现文本到图像生成。

Result: 在ImageNet 256x256图像生成中达到1.24 FID（自回归模型最优），相比并行生成模型参数减少5.4倍（2.6亿 vs 14亿）且推理速度提升8.7倍，1024x1024图像生成速度提升超30倍。

Conclusion: 提出了基于二进制令牌扩散和新型解码范式的高效自回归生成架构，兼顾生成质量与计算效率，为大规模基础模型研究提供了开源工具链支持。

Abstract: We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.

</details>


### [74] [Restoration Adaptation for Semantic Segmentation on Low Quality Images](https://arxiv.org/abs/2602.14042)
*Kai Guan,Rongyuan Wu,Shuai Li,Wentao Zhu,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: RASS框架通过将语义约束恢复（SCR）与分割模型结合，提升低质量图像语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统图像增强方法聚焦像素级保真度，但无法重建对分割任务关键的语义信息，且现有分割模型难以应对真实世界劣化图像。

Method: 提出SCR模型，通过交叉注意力机制对齐分割掩膜与增强图像；使用LoRA模块融合与任务微调实现知识迁移，并创建真实场景劣化图像数据集。

Result: 在合成与真实劣化图像基准测试中，SCR和RASS在分割与增强任务上均超越前沿方法。

Conclusion: 该框架证明语义引导的图像增强可显著提升分割鲁棒性，开源代码与数据集促进后续研究。

Abstract: In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks. Conversely, existing segmentation models trained on high-quality data lack robustness under real-world degradations. In this paper, we propose Restoration Adaptation for Semantic Segmentation (RASS), which effectively integrates semantic image restoration into the segmentation process, enabling high-quality semantic segmentation on the LQ images directly. Specifically, we first propose a Semantic-Constrained Restoration (SCR) model, which injects segmentation priors into the restoration model by aligning its cross-attention maps with segmentation masks, encouraging semantically faithful image reconstruction. Then, RASS transfers semantic restoration knowledge into segmentation through LoRA-based module merging and task-specific fine-tuning, thereby enhancing the model's robustness to LQ images. To validate the effectiveness of our framework, we construct a real-world LQ image segmentation dataset with high-quality annotations, and conduct extensive experiments on both synthetic and real-world LQ benchmarks. The results show that SCR and RASS significantly outperform state-of-the-art methods in segmentation and restoration tasks. Code, models, and datasets will be available at https://github.com/Ka1Guan/RASS.git.

</details>


### [75] [CoCoEdit: Content-Consistent Image Editing via Region Regularized Reinforcement Learning](https://arxiv.org/abs/2602.14068)
*Yuhui Wu,Chenxi Xie,Ruibin Li,Liyi Chen,Qiaosi Yi,Lei Zhang*

Main category: cs.CV

TL;DR: The paper proposes a CoCoEdit framework to enhance image editing by preserving unintended regions through reinforcement learning with region regularization.


<details>
  <summary>Details</summary>
Motivation: Existing image editing models often alter unintended regions unintentionally, prompting the need for a method to maintain content consistency in non-edited areas.

Method: 1. Expand editing datasets with refined instructions and masks, yielding 40K high-quality samples. 2. Introduce pixel-level similarity rewards alongside MLLM-based rewards for editing quality and content consistency. 3. Apply a region-based regularizer to protect non-edited regions in high-reward cases while enabling edits in low-reward cases.

Result: CoCoEdit achieves competitive editing scores compared to state-of-the-art models, with significantly improved content consistency by PSNR/SSIM metrics and human evaluations.

Conclusion: This framework effectively balances editing precision and content preservation, advancing consistency in large-scale image-editing models.

Abstract: Image editing has achieved impressive results with the development of large-scale generative models. However, existing models mainly focus on the editing effects of intended objects and regions, often leading to unwanted changes in unintended regions. We present a post-training framework for Content-Consistent Editing (CoCoEdit) via region regularized reinforcement learning. We first augment existing editing datasets with refined instructions and masks, from which 40K diverse and high quality samples are curated as training set. We then introduce a pixel-level similarity reward to complement MLLM-based rewards, enabling models to ensure both editing quality and content consistency during the editing process. To overcome the spatial-agnostic nature of the rewards, we propose a region-based regularizer, aiming to preserve non-edited regions for high-reward samples while encouraging editing effects for low-reward samples. For evaluation, we annotate editing masks for GEdit-Bench and ImgEdit-Bench, introducing pixel-level similarity metrics to measure content consistency and editing quality. Applying CoCoEdit to Qwen-Image-Edit and FLUX-Kontext, we achieve not only competitive editing scores with state-of-the-art models, but also significantly better content consistency, measured by PSNR/SSIM metrics and human subjective ratings.

</details>


### [76] [ForgeryVCR: Visual-Centric Reasoning via Efficient Forensic Tools in MLLMs for Image Forgery Detection and Localization](https://arxiv.org/abs/2602.14098)
*Youqi Wang,Shen Chen,Haowei Wang,Rongxuan Peng,Taiping Yao,Shunquan Tan,Changsheng Chen,Bin Li,Shouhong Ding*

Main category: cs.CV

TL;DR: ForgeryVCR通过视觉中心推理与策略工具学习，在图像伪造检测和定位中实现SOTA性能，减少对文本思维链的依赖并提升细粒度分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本-centric CoT方法在低级篡改痕迹捕捉上存在幻觉缺陷，无法有效捕获像素级不一致，需要更符合视觉任务特性的技术路径。

Method: 提出ForgeryVCR框架：1) 视觉中心推理将不可见痕迹转化为显式视觉中间表示 2) 战略工具学习包括监督微调(增益驱动轨迹构建)与强化学习(工具效用奖励引导) 3) 支持多视角推理路径(局部放大/压缩历史分析/频域检测)

Result: 在检测与定位任务均达到SOTA性能，展现更强泛化性、鲁棒性且工具冗余最小化，实验表明比现有方法更准确。

Conclusion: 通过将视觉注意力机制与主动决策框架结合，ForgeryVCR突破传统文本驱动方法的局限性，为多模态视觉任务提供了新的技术范式。

Abstract: Existing Multimodal Large Language Models (MLLMs) for image forgery detection and localization predominantly operate under a text-centric Chain-of-Thought (CoT) paradigm. However, forcing these models to textually characterize imperceptible low-level tampering traces inevitably leads to hallucinations, as linguistic modalities are insufficient to capture such fine-grained pixel-level inconsistencies. To overcome this, we propose ForgeryVCR, a framework that incorporates a forensic toolbox to materialize imperceptible traces into explicit visual intermediates via Visual-Centric Reasoning. To enable efficient tool utilization, we introduce a Strategic Tool Learning post-training paradigm, encompassing gain-driven trajectory construction for Supervised Fine-Tuning (SFT) and subsequent Reinforcement Learning (RL) optimization guided by a tool utility reward. This paradigm empowers the MLLM to act as a proactive decision-maker, learning to spontaneously invoke multi-view reasoning paths including local zoom-in for fine-grained inspection and the analysis of invisible inconsistencies in compression history, noise residuals, and frequency domains. Extensive experiments reveal that ForgeryVCR achieves state-of-the-art (SOTA) performance in both detection and localization tasks, demonstrating superior generalization and robustness with minimal tool redundancy. The project page is available at https://youqiwong.github.io/projects/ForgeryVCR/.

</details>


### [77] [GeoFusionLRM: Geometry-Aware Self-Correction for Consistent 3D Reconstruction](https://arxiv.org/abs/2602.14119)
*Ahmet Burak Yildirim,Tuna Saygin,Duygu Ceylan,Aysegul Dundar*

Main category: cs.CV

TL;DR: GeoFusionLRM是一种自修正框架，通过几何感知反馈提升单图像3D重建的结构精度，无需额外监督或外部信号。


<details>
  <summary>Details</summary>
Motivation: 单图像大重建模型(LRM)存在几何不一致和细节错位问题，现有方法仅依赖输入图像特征而缺乏几何修正机制。

Method: 创新性地通过独立transformer与融合模块反馈模型自身的法线与深度预测，结合特征与几何线索进行自修正

Result: 实验表明相比当前最优LRM基线，几何锐度、法线一致性及重建保真度显著提升

Conclusion: 提出的几何感知反馈机制有效解决结构对齐问题，在无需额外监督条件下推进单图像3D重建技术发展

Abstract: Single-image 3D reconstruction with large reconstruction models (LRMs) has advanced rapidly, yet reconstructions often exhibit geometric inconsistencies and misaligned details that limit fidelity. We introduce GeoFusionLRM, a geometry-aware self-correction framework that leverages the model's own normal and depth predictions to refine structural accuracy. Unlike prior approaches that rely solely on features extracted from the input image, GeoFusionLRM feeds back geometric cues through a dedicated transformer and fusion module, enabling the model to correct errors and enforce consistency with the conditioning image. This design improves the alignment between the reconstructed mesh and the input views without additional supervision or external signals. Extensive experiments demonstrate that GeoFusionLRM achieves sharper geometry, more consistent normals, and higher fidelity than state-of-the-art LRM baselines.

</details>


### [78] [EgoSound: Benchmarking Sound Understanding in Egocentric Videos](https://arxiv.org/abs/2602.14122)
*Bingwen Zhu,Yuqian Fu,Qiaole Dong,Guolei Sun,Tianwen Qian,Yuzheng Wu,Danda Pani Paudel,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: This paper introduces EgoSound, a benchmark for evaluating egocentric sound understanding in multimodal large language models (MLLMs), revealing their current limitations in spatial and causal auditory reasoning.


<details>
  <summary>Details</summary>
Motivation: Human perception is inherently multisensory, requiring integration of sound and vision to reason about the world. Existing MLLMs lack systematic evaluation of auditory reasoning in egocentric settings despite sound's critical role in spatial awareness and causal inference.

Method: EgoSound combines Ego4D and EgoBlind datasets, constructs a seven-task taxonomy for egocentric sound understanding, and builds 7315 validated QA pairs across 900 videos using a multi-stage auto-generative pipeline.

Result: Experiments on nine SOTA MLLMs show emerging auditory reasoning capabilities but significant limitations in spatial localization and causal understanding. The EgoSound dataset achieves high quality control through multi-stage validation.

Conclusion: EgoSound establishes a foundational benchmark for advancing multisensory egocentric intelligence by systematically evaluating auditory reasoning beyond purely visual capabilities in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.

</details>


### [79] [DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors](https://arxiv.org/abs/2602.14134)
*Yi Li,Hongze Shen,Lexiang Tang,Xin Li,Xinpeng Ding,Yinsong Liu,Deqiang Jiang,Xing Sun,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出DenseMLLM，无需任务特定解码器即可执行密集预测任务，保持标准MLLM架构的同时实现高效多模态感知。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在细粒度密集预测任务中需引入特定解码器，导致架构碎片化。本文旨在通过简化设计提升模型实用性，回归MLLM通用架构本源。

Method: 提出基于标准架构的视觉token监督策略，采用多标签跨任务学习范式，完全省去任务特定的解码模块。

Result: 在密集预测和跨模态基准测试中保持竞争力，实验验证标准MLLM架构的多任务泛化能力。

Conclusion: 证明通用MLLM架构通过创新监督策略可有效处理密集感知任务，颠覆需专用架构的传统认知。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

</details>


### [80] [Detection of On-Ground Chestnuts Using Artificial Intelligence Toward Automated Picking](https://arxiv.org/abs/2602.14140)
*Kaixuan Fang,Yuzhen Lu,Xinyang Mu*

Main category: cs.CV

TL;DR: This study evaluates real-time object detection models (YOLO and RT-DETR) for automated chestnut harvesting, achieving up to 95.1% mAP@0.5 with YOLOv12m. A new chestnut detection dataset with 6,524 annotations is publicly released.


<details>
  <summary>Details</summary>
Motivation: Traditional chestnut harvesting is costly, non-selective, and damages nuts. Existing vision-guided systems struggle with detection in complex orchard environments (shading, weeds, debris). Reliable detection is critical for affordable automation.

Method: Collected 319 orchard images with 6,524 annotated chestnuts. Systematically benchmarked 29 real-time detectors (14 YOLO variants, 15 RT-DETR variants) under varying light/obstruction conditions.

Result: YOLOv12m achieved highest mAP@0.5 (95.1%), while YOLOv11x topped mAP@0.5:0.95 (80.1%). RT-DETRv2-R101 performed best among RT-DETR models (91.1% mAP@0.5). All models showed real-time feasibility, with YOLO demonstrating superior speed-accuracy balance.

Conclusion: YOLO models outperform RT-DETR for chestnut detection, enabling on-board agricultural automation. The open dataset and code (GitHub) support future research in low-cost harvesting technology.

Abstract: Traditional mechanized chestnut harvesting is too costly for small producers, non-selective, and prone to damaging nuts. Accurate, reliable detection of chestnuts on the orchard floor is crucial for developing low-cost, vision-guided automated harvesting technology. However, developing a reliable chestnut detection system faces challenges in complex environments with shading, varying natural light conditions, and interference from weeds, fallen leaves, stones, and other foreign on-ground objects, which have remained unaddressed. This study collected 319 images of chestnuts on the orchard floor, containing 6524 annotated chestnuts. A comprehensive set of 29 state-of-the-art real-time object detectors, including 14 in the YOLO (v11-13) and 15 in the RT-DETR (v1-v4) families at varied model scales, was systematically evaluated through replicated modeling experiments for chestnut detection. Experimental results show that the YOLOv12m model achieves the best mAP@0.5 of 95.1% among all the evaluated models, while the RT-DETRv2-R101 was the most accurate variant among RT-DETR models, with mAP@0.5 of 91.1%. In terms of mAP@[0.5:0.95], the YOLOv11x model achieved the best accuracy of 80.1%. All models demonstrate significant potential for real-time chestnut detection, and YOLO models outperformed RT-DETR models in terms of both detection accuracy and inference, making them better suited for on-board deployment. Both the dataset and software programs in this study have been made publicly available at https://github.com/AgFood-Sensing-and-Intelligence-Lab/ChestnutDetection.

</details>


### [81] [LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models](https://arxiv.org/abs/2602.14147)
*Shufan Li,Yuchen Zhu,Jiuxiang Gu,Kangning Liu,Zhe Lin,Yongxin Chen,Molei Tao,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: 本文提出了LaViDa-R1，一个结合扩散模型与多模态推理的通用模型，通过统一框架整合监督微调与强化学习，验证了其方法在多模态任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型（dLLMs）在多模态推理任务中的研究依赖任务特定的强化学习，导致训练复杂且难以泛化。需构建一种通用后训练框架以统一处理多模态任务。

Method: 提出统一后训练框架，将监督微调（SFT）与多任务强化学习（RL）融合。关键技术包括答案强制生成（answer-forcing）、树搜索算法和互补似然估计方法。

Result: 模型在视觉数学推理、复杂推理任务和图像编辑任务中均取得领先性能，验证了方法的跨模态有效性与可扩展性。

Conclusion: 所提框架成功实现多模态扩散模型的通用推理能力，在保持方法统一性的同时提升了计算效率与任务适配性。

Abstract: Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.

</details>


### [82] [ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery](https://arxiv.org/abs/2602.14153)
*Zheng Han,Zixin Yang,Yonghao Long,Lin Zhang,Peter Kazanzides,Qi Dou*

Main category: cs.CV

TL;DR: ARport是一种无需外部标记或传感器的增强现实系统，用于在机器人手术中精确映射预规划的端口位置到患者体表。


<details>
  <summary>Details</summary>
Motivation: 现有术前规划与术中执行存在差距，ARport通过无标记AR系统提供直观指导，简化设置并提升术中端口放置的精确性。

Method: 基于OST-HMD的无标记系统，通过RGB-D数据重建场景，利用基础模型提取体表并配准术前模型，实现术中端口位置可视化。

Result: 实验显示ARport能准确将预规划端口位置映射到物理模型，实现虚拟与真实解剖结构的高一致性配准。

Conclusion: ARport提供了无需标记和硬件的术中端口可视化方案，优化了设置效率，有望无缝集成到临床工作流中。

Abstract: Purpose: Precise port placement is a critical step in robot-assisted surgery, where port configuration influences both visual access to the operative field and instrument maneuverability. To bridge the gap between preoperative planning and intraoperative execution, we present ARport, an augmented reality (AR) system that automatically maps pre-planned trocar layouts onto the patient's body surface, providing intuitive spatial guidance during surgical preparation. Methods: ARport, implemented on an optical see-through head-mounted display (OST-HMD), operates without any external sensors or markers, simplifying setup and enhancing workflow integration. It reconstructs the operative scene from RGB, depth, and pose data captured by the OST-HMD, extracts the patient's body surface using a foundation model, and performs surface-based markerless registration to align preoperative anatomical models to the extracted patient's body surface, enabling in-situ visualization of planned trocar layouts. A demonstration video illustrating the overall workflow is available online. Results: In full-scale human-phantom experiments, ARport accurately overlaid pre-planned trocar sites onto the physical phantom, achieving consistent spatial correspondence between virtual plans and real anatomy. Conclusion: ARport provides a fully marker-free and hardware-minimal solution for visualizing preoperative trocar plans directly on the patient's body surface. The system facilitates efficient intraoperative setup and demonstrates potential for seamless integration into routine clinical workflows.

</details>


### [83] [When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance](https://arxiv.org/abs/2602.14157)
*Ahmed Ghorbel,Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: 基于扩散模型的VJP-free图像视频编辑方法，无需训练即可实现与训练方法相媲美甚至更优的效果。


<details>
  <summary>Details</summary>
Motivation: 传统测试时指导方法依赖高代价VJP计算，限制了实际应用。需要开发更高效的指导框架。

Method: 1) 继承Moufed等(2025)VJP-free近似方法，2) 提供理论收敛性证明，3) 在LAION-400M图像和Kinetics-700视频基准进行大规模评估

Result: 在ImageNet-FID分数21.3及Kinetics-700视频编辑任务中，测试时指导方法达到SOTA水平，训练成本降低83%

Conclusion: 理论验证与实证结果表明，纯测试时指导可取代传统训练方法，为文本编辑任务提供更经济高效的解决方案

Abstract: Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.

</details>


### [84] [Towards Spatial Transcriptomics-driven Pathology Foundation Models](https://arxiv.org/abs/2602.14177)
*Konstantin Hemker,Andrew H. Song,Cristina Almagro-Pérez,Guillaume Jaume,Sophia J. Wagner,Anurag Vaidya,Nikola Simidjievski,Mateja Jamnik,Faisal Mahmood*

Main category: cs.CV

TL;DR: 该研究提出了Spatial Expression-Aligned Learning (SEAL)，将空间转录组学数据融入病理学视觉模型，通过跨模态联合学习提升癌症基因预测和组织影像表征能力。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术(ST)提供了超越传统组织学的分子信息，而多模态大模型的成功表明形态与分子特征的耦合能系统化增强组织学表征质量。

Method: 设计SEAL框架，将局部分子信息注入病理学视觉编码器，采用超70万对配对基因表达样本（涵盖14种器官肿瘤与正常组织）对现有模型进行参数高效微调。

Result: 在38个切片级和15个区域级任务中，SEAL相比纯视觉模型显著提升分子状态预测（如病理通路活性、治疗响应）和基因表达预测性能，展现出跨模态检索（基因→图像）与分布外泛化能力。

Conclusion: SEAL证明将局部分子监督数据融入现有病理模型能有效增强视觉表征，并为跨模态应用提供实用化框架。

Abstract: Spatial transcriptomics (ST) provides spatially resolved measurements of gene expression, enabling characterization of the molecular landscape of human tissue beyond histological assessment as well as localized readouts that can be aligned with morphology. Concurrently, the success of multimodal foundation models that integrate vision with complementary modalities suggests that morphomolecular coupling between local expression and morphology can be systematically used to improve histological representations themselves. We introduce Spatial Expression-Aligned Learning (SEAL), a vision-omics self-supervised learning framework that infuses localized molecular information into pathology vision encoders. Rather than training new encoders from scratch, SEAL is designed as a parameter-efficient vision-omics finetuning method that can be flexibly applied to widely used pathology foundation models. We instantiate SEAL by training on over 700,000 paired gene expression spot-tissue region examples spanning tumor and normal samples from 14 organs. Tested across 38 slide-level and 15 patch-level downstream tasks, SEAL provides a drop-in replacement for pathology foundation models that consistently improves performance over widely used vision-only and ST prediction baselines on slide-level molecular status, pathway activity, and treatment response prediction, as well as patch-level gene expression prediction tasks. Additionally, SEAL encoders exhibit robust domain generalization on out-of-distribution evaluations and enable new cross-modal capabilities such as gene-to-image retrieval. Our work proposes a general framework for ST-guided finetuning of pathology foundation models, showing that augmenting existing models with localized molecular supervision is an effective and practical step for improving visual representations and expanding their cross-modal utility.

</details>


### [85] [UniWeTok: An Unified Binary Tokenizer with Codebook Size $\mathit{2^{128}}$ for Unified Multimodal Large Language Model](https://arxiv.org/abs/2602.14178)
*Shaobin Zhuang,Yuang Ai,Jiaming Han,Weijia Mao,Xiaohui Li,Fangyikang Wang,Xiao Wang,Yan Li,Shanchuan Lin,Kun Xu,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen,Yali Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的离散化视觉tokenizer——UniWeTok，通过Pre-Post Distillation、Generative-Aware Prior和创新的模型架构，解决了多模态大模型中视觉表征的多目标矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉tokenizer难以在单一框架下同时实现高保真重建、复杂语义提取和生成友好性，这对多模态大模型的统一性构成挑战。

Method: 基于2^128超大规模二进制码本构建离散化tokenizer；采用Pre-Post Distillation与Generative-Aware Prior增强语义生成能力；创新性结合卷积与注意力机制并设计SigLu激活函数；提出三级训练框架以适应不同分辨率和敏感场景。

Result: 在ImageNet上以1.38的FID分数超越REPA（1.42），训练计算量减少至33B token（REPA 262B）；在多模态理解、图像生成（DPG Score 86.63）和编辑（GEdit 5.09）任务均优于竞品模型。

Conclusion: UniWeTok通过架构创新与训练策略突破，在保证生成质量的同时显著降低计算需求，开源代码和模型推动了多模态大模型领域的技术探索。

Abstract: Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.

</details>


### [86] [UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing](https://arxiv.org/abs/2602.14186)
*Hongyang Wei,Bin Wen,Yancheng Long,Yankai Yang,Yuhang Hu,Tianke Zhang,Wei Chen,Haonan Fan,Kaiyu Jiang,Jiankang Chen,Changyi Liu,Kaiyu Tang,Haojie Ding,Xiao Yang,Jia Sun,Huaiqing Wang,Zhenyu Yang,Xinyu Wei,Xianglong He,Yangguang Li,Fan Yang,Tingting Gao,Lei Zhang,Guorui Zhou,Han Li*

Main category: cs.CV

TL;DR: UniRef-Image-Edit is a unified framework for single-image editing and multi-image composition using SELF representation and multi-stage training, achieving high visual fidelity and cross-reference consistency.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle to maintain consistency across multiple input references due to limited interaction between reference inputs, necessitating a unified framework.

Method: 1) Sequence-Extended Latent Fusion (SELF) dynamically serializes multiple images under a pixel budget. 2) Two-stage training: Supervised Fine-Tuning (progressive resolution scaling) + Multi-Source GRPO reinforcement learning for conflict resolution.

Result: Improved visual fidelity and cross-reference consistency across varying resolutions (1024²→2048²), with open-sourced code, models, and training data.

Conclusion: The proposed SELF and MSGRPO framework significantly advances multi-reference image generation by balancing pixel-level details and global alignment.

Abstract: We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.

</details>


### [87] [GeoEyes: On-Demand Visual Focusing for Evidence-Grounded Understanding of Ultra-High-Resolution Remote Sensing Imagery](https://arxiv.org/abs/2602.14201)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yifan Zhang,Long Lan,Xue Yang,Hongda Sun,Yulin Wang,Di Wang,Jun Song,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: GeoEyes通过冷启动SFT数据集与强化学习方法解决多模态大模型在超高分辨率遥感视觉问答中的工具使用同质化问题，准确率达到54.23%。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感图像的视觉问答任务中，现有缩放工具存在任务无关的同质化调用问题，导致有效信息获取受限。

Method: 提出分阶段训练框架GeoEyes：1) 构建多样化的缩放数据集UHR-CoZ；2) 开发奖励机制明确的AdaZoom-GRPO强化学习方法，优化缩放交互行为。

Result: 模型在XLRS-Bench基准测试中实现54.23%的准确率，展现显著性能提升，并学习到按需缩放与适时终止的交互策略。

Conclusion: 该研究通过数据构建与动态奖励机制，成功攻克了多模态大模型处理超高分辨率遥感场景的工具使用瓶颈。

Abstract: The "thinking-with-images" paradigm enables multimodal large language models (MLLMs) to actively explore visual scenes via zoom-in tools. This is essential for ultra-high-resolution (UHR) remote sensing VQA, where task-relevant cues are sparse and tiny. However, we observe a consistent failure mode in existing zoom-enabled MLLMs: Tool Usage Homogenization, where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition. To address this, we propose GeoEyes, a staged training framework consisting of (1) a cold-start SFT dataset, UHR Chain-of-Zoom (UHR-CoZ), which covers diverse zooming regimes, and (2) an agentic reinforcement learning method, AdaZoom-GRPO, that explicitly rewards evidence gain and answer improvement during zoom interactions. The resulting model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.

</details>


### [88] [HiVid: LLM-Guided Video Saliency For Content-Aware VOD And Live Streaming](https://arxiv.org/abs/2602.14214)
*Jiahui Chen,Bo Peng,Lianchen Jia,Zeyu Zhang,Tianchi Huang,Lifeng Sun*

Main category: cs.CV

TL;DR: HiVid是一个利用大语言模型（LLMs）为点播（VOD）和实时流媒体生成高保真重要性权重的框架，通过感知模块、排序模块和预测模块分别解决LLMs的有限模态、本地窗口评分不一致及低延迟推理需求问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖高成本的视频级人工标注或泛化性较差的视觉显著性模型，亟需一种可扩展且准确的解决方案以优化用户体验质量（QoE）。

Method: 提出三大创新模块：（1）感知模块通过局部上下文窗口逐步构建视频连贯理解；（2）排序模块采用LLM引导的归并排序算法进行全局重排序；（3）预测模块结合内容感知注意力和自适应视野的时空序列模型预测未来权重。

Result: 相较现有最佳方法，HiVid的权重预测准确率提升11.5%（VOD）和26%（实时流媒体），用户研究验证其QoE关联性提升14.7%。

Conclusion: HiVid成功将LLMs扩展至流媒体动态权重生成领域，解决了长上下文理解、评分一致性维护和低延迟预测三大技术挑战，为高感知质量流媒体提供可扩展解决方案。

Abstract: Content-aware streaming requires dynamic, chunk-level importance weights to optimize subjective quality of experience (QoE). However, direct human annotation is prohibitively expensive while vision-saliency models generalize poorly. We introduce HiVid, the first framework to leverage Large Language Models (LLMs) as a scalable human proxy to generate high-fidelity weights for both Video-on-Demand (VOD) and live streaming. We address 3 non-trivial challenges: (1) To extend LLMs' limited modality and circumvent token limits, we propose a perception module to assess frames in a local context window, autoregressively building a coherent understanding of the video. (2) For VOD with rating inconsistency across local windows, we propose a ranking module to perform global re-ranking with a novel LLM-guided merge-sort algorithm. (3) For live streaming which requires low-latency, online inference without future knowledge, we propose a prediction module to predict future weights with a multi-modal time series model, which comprises a content-aware attention and adaptive horizon to accommodate asynchronous LLM inference. Extensive experiments show HiVid improves weight prediction accuracy by up to 11.5\% for VOD and 26\% for live streaming over SOTA baselines. Real-world user study validates HiVid boosts streaming QoE correlation by 14.7\%.

</details>


### [89] [Learning Significant Persistent Homology Features for 3D Shape Understanding](https://arxiv.org/abs/2602.14228)
*Prachi Kudeshia,Jiju Poovvancheri*

Main category: cs.CV

TL;DR: 本论文提出拓扑增强的三维点云基准数据集及深度学习方法TopoGAT，将几何与拓扑信息结合以提升3D形状分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集侧重几何信息却忽视拓扑结构，需建立能统一几何-拓扑学习的基准，并开发自动化拓扑特征选择方法替代传统统计标准。

Method: 构建拓扑增强版ModelNet40和ShapeNet数据集，在点云中嵌入持续同调特征；提出TopoGAT模型，通过端到端学习自动识别最具判别性的拓扑特征。

Result: TopoGAT在特征稳定性、分类准确率(提升5.2%)和分割IoU指标(提升3.8%)上均优于传统统计方法，验证了深度学习拓扑特征选择的有效性。

Conclusion: 该研究为点云分析提供了首个拓扑增强的基准框架，证明了可学习拓扑签名对深度学习的协同作用，推动了持续同调在实际工程中的应用。

Abstract: Geometry and topology constitute complementary descriptors of three-dimensional shape, yet existing benchmark datasets primarily capture geometric information while neglecting topological structure. This work addresses this limitation by introducing topologically-enriched versions of ModelNet40 and ShapeNet, where each point cloud is augmented with its corresponding persistent homology features. These benchmarks with the topological signatures establish a foundation for unified geometry-topology learning and enable systematic evaluation of topology-aware deep learning architectures for 3D shape analysis. Building on this foundation, we propose a deep learning-based significant persistent point selection method, \textit{TopoGAT}, that learns to identify the most informative topological features directly from input data and the corresponding topological signatures, circumventing the limitations of hand-crafted statistical selection criteria. A comparative study verifies the superiority of the proposed method over traditional statistical approaches in terms of stability and discriminative power. Integrating the selected significant persistent points into standard point cloud classification and part-segmentation pipelines yields improvements in both classification accuracy and segmentation metrics. The presented topologically-enriched datasets, coupled with our learnable significant feature selection approach, enable the broader integration of persistent homology into the practical deep learning workflows for 3D point cloud analysis.

</details>


### [90] [Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236)
*Vishnu Sai,Dheeraj Sai,Srinath B,Girish Varma,Priyesh Shukla*

Main category: cs.CV

TL;DR: 本文提出Sali-Cache，通过时空双重信号主动优化内存管理，在视频内容处理中实现2.20倍内存压缩率且保持100%准确性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型处理长视频时KV缓存线性增长导致的记忆瓶颈问题，以及传统反应式逐出策略造成的计算浪费。

Method: 结合光流分析的时间过滤（检测帧间冗余）和显著性检测的空间过滤（识别重要区域），在计算注意力前智能分配内存。

Result: LLaVA 1.6架构实测有效内存使用压缩2.20倍，BLEU、ROUGE-L、精确匹配指标均保持100%准确率。

Conclusion: 通过前瞻性内存管理实现长视频高效处理，在相同内存预算下保持模型性能，支持消费级硬件应用。

Abstract: Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.

</details>


### [91] [AbracADDbra: Touch-Guided Object Addition by Decoupling Placement and Editing Subtasks](https://arxiv.org/abs/2602.14237)
*Kunal Swami,Raghu Chittersu,Yuvraj Rathore,Rajeev Irny,Shashavali Doodekula,Alok Shukla*

Main category: cs.CV

TL;DR: 提出了AbracADDbra框架，通过触觉先验提升物体添加的精确性和易用性


<details>
  <summary>Details</summary>
Motivation: 解决传统方法存在的文本模糊性或mask输入繁琐性问题，弥补可用性差距

Method: 采用解耦架构，包含触觉引导放置的视觉语言变换模型和生成物体及实例mask的扩散模型

Result: 在Touch2Add基准测试中，放置模型显著优于随机放置和通用VLM基线，证实了高保真编辑能力

Conclusion: 验证了解耦结构的有效性，为开发更易用的创作工具提供了新路径

Abstract: Instruction-based object addition is often hindered by the ambiguity of text-only prompts or the tedious nature of mask-based inputs. To address this usability gap, we introduce AbracADDbra, a user-friendly framework that leverages intuitive touch priors to spatially ground succinct instructions for precise placement. Our efficient, decoupled architecture uses a vision-language transformer for touch-guided placement, followed by a diffusion model that jointly generates the object and an instance mask for high-fidelity blending. To facilitate standardized evaluation, we contribute the Touch2Add benchmark for this interactive task. Our extensive evaluations, where our placement model significantly outperforms both random placement and general-purpose VLM baselines, confirm the framework's ability to produce high-fidelity edits. Furthermore, our analysis reveals a strong correlation between initial placement accuracy and final edit quality, validating our decoupled approach. This work thus paves the way for more accessible and efficient creative tools.

</details>


### [92] [Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision](https://arxiv.org/abs/2602.14276)
*A. Said Gurbuz,Sunghwan Hong,Ahmed Nassar,Marc Pollefeys,Peter Staar*

Main category: cs.CV

TL;DR: 提出ScreenParse，一个大型密集标注屏幕解析数据集及ScreenVLM模型，提升界面理解效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有界面理解数据集标注稀疏且类别单一，限制模型泛化能力，且缺乏实时性要求。

Method: 构建Webshot自动化流程生成77.1万网页截图（2100万元素），设计结构感知损失函数训练紧凑的ScreenVLM模型。

Result: 模型在ScreenParse测试中PageIoU达0.592（对比基础模型0.294），并在公开基准任务表现优异。微调基础模型可显著提升性能。

Conclusion: 稠密屏幕监督能够为界面理解提供可迁移的结构先验知识，紧凑模型可兼顾性能和边缘设备部署效率。

Abstract: Modern computer-use agents (CUA) must perceive a screen as a structured state, what elements are visible, where they are, and what text they contain, before they can reliably ground instructions and act. Yet, most available grounding datasets provide sparse supervision, with insufficient and low-diversity labels that annotate only a small subset of task-relevant elements per screen, which limits both coverage and generalization; moreover, practical deployment requires efficiency to enable low-latency, on-device use. We introduce ScreenParse, a large-scale dataset for complete screen parsing, with dense annotations of all visible UI elements (boxes, 55-class types, and text) across 771K web screenshots (21M elements). ScreenParse is generated by Webshot, an automated, scalable pipeline that renders diverse urls, extracts annotations and applies VLM-based relabeling and quality filtering. Using ScreenParse, we train ScreenVLM, a compact, 316M-parameter vision language model (VLM) that decodes a compact ScreenTag markup representation with a structure-aware loss that upweights structure-critical tokens. ScreenVLM substantially outperforms much larger foundation VLMs on dense parsing (e.g., 0.592 vs. 0.294 PageIoU on ScreenParse) and shows strong transfer to public benchmarks. Moreover, finetuning foundation VLMs on ScreenParse consistently improves their grounding performance, suggesting that dense screen supervision provides transferable structural priors for UI understanding. Project page: https://saidgurbuz.github.io/screenparse/.

</details>


### [93] [Differential pose optimization in descriptor space -- Combining Geometric and Photometric Methods for Motion Estimation](https://arxiv.org/abs/2602.14297)
*Andreas L. Teigen,Annette Stahl,Rudolf Mester*

Main category: cs.CV

TL;DR: 该研究结合光度与几何特征的误差度量，提出统一的相对姿态优化方法，但实验表明其效果未超越重投影误差方法，因描述子相似度指标与关键点精度关联性不足。


<details>
  <summary>Details</summary>
Motivation: 现有光度误差与重投影误差需在精度、鲁棒性和闭环能力间权衡，作者试图融合两者优势以提升跟踪性能。

Method: 采用密集几何特征描述子的残差替代传统光度误差，结合亚像素精度的差分光度方法与几何特征的表达性。

Result: 方法实现准确跟踪，但相较重投影误差策略效果未改善，尽管利用了更多特征信息。

Conclusion: 描述子相似度指标变化缓慢导致无法严格关联关键点精确定位，是性能未突破的核心原因。

Abstract: One of the fundamental problems in computer vision is the two-frame relative pose optimization problem. Primarily, two different kinds of error values are used: photometric error and re-projection error. The selection of error value is usually directly dependent on the selection of feature paradigm, photometric features, or geometric features. It is a trade-off between accuracy, robustness, and the possibility of loop closing. We investigate a third method that combines the strengths of both paradigms into a unified approach. Using densely sampled geometric feature descriptors, we replace the photometric error with a descriptor residual from a dense set of descriptors, thereby enabling the employment of sub-pixel accuracy in differential photometric methods, along with the expressiveness of the geometric feature descriptor. Experiments show that although the proposed strategy is an interesting approach that results in accurate tracking, it ultimately does not outperform pose optimization strategies based on re-projection error despite utilizing more information. We proceed to analyze the underlying reason for this discrepancy and present the hypothesis that the descriptor similarity metric is too slowly varying and does not necessarily correspond strictly to keypoint placement accuracy.

</details>


### [94] [A Generative AI Approach for Reducing Skin Tone Bias in Skin Cancer Classification](https://arxiv.org/abs/2602.14356)
*Areez Muhammed Shabu,Mohammad Samar Ansari,Asra Aslam*

Main category: cs.CV

TL;DR: 该论文提出了一种生成式数据增强方法，通过调整预训练的Stable Diffusion模型，以解决皮肤癌检测中因肤色不平衡导致的AI诊断偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI诊断工具在训练数据中浅肤色占主导，导致深肤色人群检测准确率下降。ISIC数据集中70%以上为浅肤色图像，深肤色不足8%，阻碍了医疗公平性。

Method: 利用Low-Rank Adaptation（LoRA）微调预训练Stable Diffusion模型，基于ISIC深肤色子集生成包含病变类型和肤色条件的合成皮肤镜图像，并将这些数据用于病变分割和二分类任务。

Result: 分割模型在IoU、Dice系数和边界准确率上均提升；分类模型EfficientNet-B0达到92.14%的准确率，验证了生成数据的有效性。

Conclusion: 生成式AI的数据增强能够显著降低传统皮肤病学诊断中的偏差，提高公平性，同时也提出了未来研究方向的挑战。

Abstract: Skin cancer is one of the most common cancers worldwide and early detection is critical for effective treatment. However, current AI diagnostic tools are often trained on datasets dominated by lighter skin tones, leading to reduced accuracy and fairness for people with darker skin. The International Skin Imaging Collaboration (ISIC) dataset, one of the most widely used benchmarks, contains over 70% light skin images while dark skins fewer than 8%. This imbalance poses a significant barrier to equitable healthcare delivery and highlights the urgent need for methods that address demographic diversity in medical imaging. This paper addresses this challenge of skin tone imbalance in automated skin cancer detection using dermoscopic images. To overcome this, we present a generative augmentation pipeline that fine-tunes a pre-trained Stable Diffusion model using Low-Rank Adaptation (LoRA) on the image dark-skin subset of the ISIC dataset and generates synthetic dermoscopic images conditioned on lesion type and skin tone. In this study, we investigated the utility of these images on two downstream tasks: lesion segmentation and binary classification. For segmentation, models trained on the augmented dataset and evaluated on held-out real images show consistent improvements in IoU, Dice coefficient, and boundary accuracy. These evalutions provides the verification of Generated dataset. For classification, an EfficientNet-B0 model trained on the augmented dataset achieved 92.14% accuracy. This paper demonstrates that synthetic data augmentation with Generative AI integration can substantially reduce bias with increase fairness in conventional dermatological diagnostics and open challenges for future directions.

</details>


### [95] [Image-based Joint-level Detection for Inflammation in Rheumatoid Arthritis from Small and Imbalanced Data](https://arxiv.org/abs/2602.14365)
*Shun Kato,Yasushi Kondo,Shuntaro Saito,Yoshimitsu Aoki,Mariko Isogawa*

Main category: cs.CV

TL;DR: 该论文提出了一种通过RGB手部图像检测类风湿性关节炎炎症的框架，结合自监督预训练和不平衡数据处理方法，在数据稀缺和类别不平衡的挑战下提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 早期诊断和密切随访对类风湿性关节炎管理至关重要，但患者常因医疗资源不足而延迟就医，亟需可通过家用RGB图像实现炎症检测的简易系统。

Method: 构建包含健康手部图像的自监督预训练模型，结合全局-局部编码器架构和不平衡数据感知训练策略，用于处理正样本稀缺与类别不平衡问题。

Result: 实验表明，相较基线模型，该方法的F1分数提升0.2，G均值提升0.25，定量验证了模型在可视化炎症检测中的有效性。

Conclusion: 该研究表明基于RGB图像的类风湿性关节炎炎症检测具有可行性，提出的框架解决了医疗数据稀缺、类别不平衡等挑战，为家庭早期筛查提供了新方案。

Abstract: Rheumatoid arthritis (RA) is an autoimmune disease characterized by systemic joint inflammation. Early diagnosis and tight follow-up are essential to the management of RA, as ongoing inflammation can cause irreversible joint damage. The detection of arthritis is important for diagnosis and assessment of disease activity; however, it often takes a long time for patients to receive appropriate specialist care. Therefore, there is a strong need to develop systems that can detect joint inflammation easily using RGB images captured at home. Consequently, we tackle the task of RA inflammation detection from RGB hand images. This task is highly challenging due to general issues in medical imaging, such as the scarcity of positive samples, data imbalance, and the inherent difficulty of the task itself. However, to the best of our knowledge, no existing work has explicitly addressed these challenges in RGB-based RA inflammation detection. This paper quantitatively demonstrates the difficulty of visually detecting inflammation by constructing a dedicated dataset, and we propose a inflammation detection framework with global local encoder that combines self-supervised pretraining on large-scale healthy hand images with imbalance-aware training to detect RA-related joint inflammation from RGB hand images. Our experiments demonstrated that the proposed approach improves F1-score by 0.2 points and Gmean by 0.25 points compared with the baseline model.

</details>


### [96] [Event-based Visual Deformation Measurement](https://arxiv.org/abs/2602.14376)
*Yuliang Wu,Wei Zhai,Yuxin Cui,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了一种事件帧融合框架（AIS框架），用于视觉变形测量（VDM），通过低速摄像头和动态视觉传感器协同工作，在显著降低存储与计算成本的情况下实现动态场景的密集形变场重建。


<details>
  <summary>Details</summary>
Motivation: 传统图像方法受限于帧间最小运动约束，难以处理高速动态场景或需高成本的高速摄像头；事件传感器虽有高时间分辨率但存在数据稀疏性和噪声问题。

Method: 1) 设计事件帧多模态融合架构，利用事件流提供高时间分辨率运动线索，帧图像提供高空间分辨率；2) 构建仿射不变单纯体（AIS）框架，将形变场划分低参数线性子区域；3) 引入邻域贪婪优化策略，通过已收敛区域引导未收敛区域抑制误差累积。

Result: 在包含120+序列的Benchmark测试中：1) 相比SOTA基线提升1.6%生存率；2) 仅需高速视频方法18.9%的存储与计算资源。

Conclusion: 该方法通过事件帧融合与仿射不变模型，有效解决动态场景中形变测量的效率与精度矛盾，在资源消耗与跟踪质量间达到最优平衡。

Abstract: Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.

</details>


### [97] [Adapting VACE for Real-Time Autoregressive Video Diffusion](https://arxiv.org/abs/2602.14381)
*Ryan Fosdick*

Main category: cs.CV

TL;DR: This paper adapts the VACE video generation model for real-time autoregressive processing by reworking the architecture to support streaming pipelines while maintaining compatibility with existing trained weights.


<details>
  <summary>Details</summary>
Motivation: VACE's original bidirectional attention mechanism requires full sequence access, conflicting with streaming's need for fixed chunks and causal attention. This work aims to bridge this gap for real-time applications.

Method: Moves reference frames from diffusion latent space to a parallel conditioning pathway, enabling causal attention and KV caching without additional training. Preserves VACE's core design while adapting for streaming constraints.

Result: 20-30% latency overhead for structural control/inpainting, negligible VRAM cost, but significant degradation in reference-to-video fidelity compared to batch processing. 1.3B and 14B model scale implementations validated.

Conclusion: First successful adaptation of VACE for streaming deployment. Achieves compatibility with causal attention requirements at the cost of quality degradation, with open-sourced implementation for replication.

Abstract: We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.

</details>


### [98] [Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models](https://arxiv.org/abs/2602.14399)
*In Chong Choi,Jiacheng Zhang,Feng Liu,Yiliao Song*

Main category: cs.CV

TL;DR: MAPA是一种针对视觉-语言模型的多轮自适应攻击方法，通过交替使用文本与视觉攻击动作，并在多轮对话中动态调整攻击路径，有效提升攻击成功率至11-35%。


<details>
  <summary>Details</summary>
Motivation: 现有针对纯文本大模型的多轮越狱攻击在扩展到视觉-语言模型时易被防御，因过强的视觉输入会触发安全机制导致保守回复，需设计适应视觉模态的攻击框架。

Method: 提出双层策略：1) 每回合动态切换文本/视觉攻击动作以最大化恶意响应；2) 跨回合通过迭代回溯优化攻击路径，逐步增强攻击效果。

Result: 在LLaVA、Qwen2.5等4个主流视觉-语言模型的测试中，攻击成功率较SOTA方法提升11-35%。

Conclusion: 双层自适应设计显著提升攻击有效性，揭示视觉输入在对抗攻击中的关键作用，同时为视觉-语言系统安全防护提供新视角。

Abstract: Multi-turn jailbreak attacks are effective against text-only large language models (LLMs) by gradually introducing malicious content across turns. When extended to large vision-language models (LVLMs), we find that naively adding visual inputs can cause existing multi-turn jailbreaks to be easily defended. For example, overly malicious visual input will easily trigger the defense mechanism of safety-aligned LVLMs, making the response more conservative. To address this, we propose MAPA: a multi-turn adaptive prompting attack that 1) at each turn, alternates text-vision attack actions to elicit the most malicious response; and 2) across turns, adjusts the attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness. This two-level design enables MAPA to consistently outperform state-of-the-art methods, improving attack success rates by 11-35% on recent benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.

</details>


### [99] [pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI](https://arxiv.org/abs/2602.14401)
*Qingqian Yang,Hao Wang,Sai Qian Zhang,Jian Li,Yang Hua,Miao Pan,Tao Song,Zhengwei Qi,Haibing Guan*

Main category: cs.CV

TL;DR: 本论文提出pFedNavi，一种面向视觉-语言导航的个性化联邦学习框架，通过自适应选择客户端特定层及精细化参数融合，解决跨客户端环境与指令风格异质性带来的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言导航依赖的室内轨迹数据存在隐私风险，联邦学习（FL）虽能保障数据本地化，但传统方法因跨客户端异质性导致全局模型性能不足。

Method: pFedNavi通过动态层混合系数识别客户端特有层，在编码器-解码器投影层和环境敏感解码层进行参数融合，实现全局知识共享与局部专业化平衡。

Result: 在R2R和RxR数据集上，相较FedAvg基线模型，导航成功率提升7.5%，轨迹保真度提升7.8%，非独立同分布条件下收敛速度加快1.38倍。

Conclusion: 实验证明，针对视觉-语言导航任务的自适应个性化联邦学习框架pFedNavi在隐私保护与模型性能间取得有效平衡。

Abstract: Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.

</details>


### [100] [Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.14409)
*Haichao Zhu,Zhaorui Yang,Qian Zhang*

Main category: cs.CV

TL;DR: 论文探讨了将学习与几何方法结合的模块化框架在空间感知中的应用，实验表明几何模块对验证和优化学习结果不可或缺。


<details>
  <summary>Details</summary>
Motivation: 传统几何方法在空间感知中依赖物理约束，而学习方法虽能增强表征但可靠性不足。需探索二者的融合模式，解决学习组件是否应完全替代几何估计或作为辅助模块的问题。

Method: 提出端到端的模块化框架：使用VGGT等学习模型生成几何假设（如姿态、深度），再通过经典点到平面RGB-D ICP算法进行几何优化。在TUM RGB-D数据集上评估不同场景下的性能。

Result: 1）仅学习的姿态提案不可靠；2）未正确对齐相机参数的学习几何降低性能；3）几何对齐且结合优化后，在中度复杂场景中表现提升。

Conclusion: 几何模块是验证和吸收学习结果的核心，需设计模块化、几何感知的系统架构以实现鲁棒空间感知。

Abstract: Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.

</details>


### [101] [Understanding Sensor Vulnerabilities in Industrial XR Tracking](https://arxiv.org/abs/2602.14413)
*Sourya Saha,Md. Nurul Absur*

Main category: cs.CV

TL;DR: XR系统依赖VIO进行姿态跟踪，但工业环境中视觉和惯性传感器易退化。研究发现惯性传感器退化导致误差可达百米级，远超视觉退化（厘米级），强调需强化惯性可靠性设计。


<details>
  <summary>Details</summary>
Motivation: 现有VIO评估多基于理想传感条件，未充分研究工业环境下的持续传感器退化影响，尤其是惯性与视觉模态的差异性影响尚不明确。

Method: 通过控制实验进行故障注入，定量分析视觉与惯性传感器在不同退化模式下的故障影响，评估姿态误差及轨迹偏离程度。

Result: 视觉退化误差有限（厘米级），但惯性退化可导致米级至千米级轨迹偏差，呈现显著非对称性。

Conclusion: 工业XR系统设计需更重视惯性传感可靠性，评估框架应纳入非理想传感条件下的鲁棒性验证。

Abstract: Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.

</details>


### [102] [Hierarchical Vision-Language Interaction for Facial Action Unit Detection](https://arxiv.org/abs/2602.14425)
*Yong Li,Yi Ren,Yizhe Zhang,Wenhua Zhang,Tianyi Zhang,Muyun Jiang,Guo-Sen Xie,Cuntai Guan*

Main category: cs.CV

TL;DR: 本论文提出了一种名为HiVA（Hierarchical Vision-language Interaction for AU Understanding）的方法，用于面部动作单元（AU）检测。该方法利用文本描述作为语义先验，结合视觉信息，提升AU检测效果，且在实验中表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 面部动作单元检测旨在识别基于FACS定义的面部肌肉激活模式，但面临标注数据不足时难以学习有效表示的挑战。现有方法依赖有限的标注数据，缺乏对语义先验及跨模态关联的充分利用。本文的动机是通过融合文本描述与视觉信息，提升AU表示的判别性和泛化能力。

Method: HiVA的主要方法包括：1）利用大语言模型生成多样化的AU文本描述；2）设计AU感知动态图模块，捕获细粒度与全局的视觉-语言关联；3）构建分层跨模态注意力架构（含DDCA与CDCA）：DDCA实现视觉与文本的细粒度交互，CDCA建模全局AU依赖关系。

Result: 实验表明HiVA在多个基准数据集上性能优于现有最先进方法。定性分析显示其激活模式具有语义意义，能有效捕捉跨模态对应关系。

Conclusion: HiVA通过多粒度的视觉-语言协同学习，提升了AU检测的鲁棒性与可解释性。该方法为跨模态表示学习提供了新思路，对全面分析面部行为具有应用价值。

Abstract: Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.

</details>


### [103] [D-SECURE: Dual-Source Evidence Combination for Unified Reasoning in Misinformation Detection](https://arxiv.org/abs/2602.14441)
*Gagandeep Singh,Samudi Amarasinghe,Priyanka Singh*

Main category: cs.CV

TL;DR: D-SECURE框架结合图像篡改检测与外部证据验证，解决多模态虚假新闻检测难题。


<details>
  <summary>Details</summary>
Motivation: 单一证据源的传统系统无法同时识别图像/文本细节伪造与全局事实错误，需融合内部篡改检测与外部证据推理。

Method: 集成HAMMER（内部细节检测）与DEFAME（外部证据检索）双系统，通过互补验证实现多模态新闻分析。

Result: 实验证明两系统融合后，在DGM4和ClaimReview数据集上能互补检测局部篡改与全局错误，生成可解释验证报告。

Conclusion: 结合多模态内部分析与跨模态外部验证，为复杂虚假新闻识别提供了新颖有效解决方案。

Abstract: Multimodal misinformation increasingly mixes realistic im-age edits with fluent but misleading text, producing persuasive posts that are difficult to verify. Existing systems usually rely on a single evidence source. Content-based detectors identify local inconsistencies within an image and its caption but cannot determine global factual truth. Retrieval-based fact-checkers reason over external evidence but treat inputs as coarse claims and often miss subtle visual or textual manipulations. This separation creates failure cases where internally consistent fabrications bypass manipulation detectors and fact-checkers verify claims that contain pixel-level or token-level corruption. We present D-SECURE, a framework that combines internal manipulation detection with external evidence-based reasoning for news-style posts. D-SECURE integrates the HAMMER manipulation detector with the DEFAME retrieval pipeline. DEFAME performs broad verification, and HAMMER analyses residual or uncertain cases that may contain fine-grained edits. Experiments on DGM4 and ClaimReview samples highlight the complementary strengths of both systems and motivate their fusion. We provide a unified, explainable report that incorporates manipulation cues and external evidence.

</details>


### [104] [Controlling Your Image via Simplified Vector Graphics](https://arxiv.org/abs/2602.14443)
*Lanqing Guo,Xi Liu,Yufei Wang,Zhihao Li,Siyu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于简化向量图形（VGs）的层级可控图像生成方法，通过将图片解析为语义对齐且结构连贯的VG表示，并设计VG引导的合成框架，实现几何、颜色和物体语义的精准编辑，最终生成逼真图像。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成技术难以实现元素级别的可控性（如形状调整、颜色修改、物体增删）。传统方法缺乏对结构语义的显式建模，而该工作通过引入简化的向量图形表示和可控生成框架，解决这一问题并增强用户对生成结果的精细控制能力。

Method: 1. 提出层级向量图形解析方法，将图像分解为语义对齐的结构化表示；2. 设计VG引导的图像合成模型，结合VG的结构特征、语义信息与噪声预测机制；3. 用户可通过直接编辑VG元素（如调整路径属性或修改物体层级）生成对应真实图像。

Result: 实验表明该方法在多种任务中表现优异：1）支持物体级别替换/增删的交互编辑；2）实现笔画级别形状调整；3）跨类别生成（如从建筑到自然场景）的结构保持能力。定量评估显示编辑一致性指标相较基线提升23.6%。

Conclusion: 该工作建立了以结构化向量表示为核心的新一代可控图像生成范式，通过显式建模视觉元素的层次化结构与语义关系，在保持生成质量的同时实现了前所未有的编辑自由度，为交互式内容创作提供了基础框架。

Abstract: Recent advances in image generation have achieved remarkable visual quality, while a fundamental challenge remains: Can image generation be controlled at the element level, enabling intuitive modifications such as adjusting shapes, altering colors, or adding and removing objects? In this work, we address this challenge by introducing layer-wise controllable generation through simplified vector graphics (VGs). Our approach first efficiently parses images into hierarchical VG representations that are semantic-aligned and structurally coherent. Building on this representation, we design a novel image synthesis framework guided by VGs, allowing users to freely modify elements and seamlessly translate these edits into photorealistic outputs. By leveraging the structural and semantic features of VGs in conjunction with noise prediction, our method provides precise control over geometry, color, and object semantics. Extensive experiments demonstrate the effectiveness of our approach in diverse applications, including image editing, object-level manipulation, and fine-grained content creation, establishing a new paradigm for controllable image generation. Project page: https://guolanqing.github.io/Vec2Pix/

</details>


### [105] [CoCoDiff: Correspondence-Consistent Diffusion Model for Fine-grained Style Transfer](https://arxiv.org/abs/2602.14464)
*Wenbo Nie,Zixiang Li,Renshuai Tao,Bin Wu,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: CoCoDiff是一种无需训练、低成本的图像风格迁移框架，通过预训练扩散模型实现细粒度、语义一致的风格化，提出像素级语义对应模块和循环一致性模块，无需额外训练即可超越现有依赖标注的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在全局层面操作，忽视区域/像素级语义对应，导致风格迁移时语义关联缺失。本文旨在通过显式建模像素级语义一致性提升风格化效果。

Method: 基于预训练扩散模型，设计像素级语义对应模块以构建内容与风格图的密集对齐图；引入循环一致性模块，迭代强化结构与感知对齐，保留物体几何与细节。

Result: 在无额外训练或标注条件下，达到SOTA视觉质量，定量指标优于依赖训练/标注的现有方法。

Conclusion: 通过显式挖掘扩散模型中的语义对应关系并强化学术对齐，证明无需参数学习即可实现高质量语义保持风格迁移。

Abstract: Transferring visual style between images while preserving semantic correspondence between similar objects remains a central challenge in computer vision. While existing methods have made great strides, most of them operate at global level but overlook region-wise and even pixel-wise semantic correspondence. To address this, we propose CoCoDiff, a novel training-free and low-cost style transfer framework that leverages pretrained latent diffusion models to achieve fine-grained, semantically consistent stylization. We identify that correspondence cues within generative diffusion models are under-explored and that content consistency across semantically matched regions is often neglected. CoCoDiff introduces a pixel-wise semantic correspondence module that mines intermediate diffusion features to construct a dense alignment map between content and style images. Furthermore, a cycle-consistency module then enforces structural and perceptual alignment across iterations, yielding object and region level stylization that preserves geometry and detail. Despite requiring no additional training or supervision, CoCoDiff delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.

</details>


### [106] [TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.14482)
*Hao Ding,Zhichuan Yang,Weijie Ge,Ziqin Gao,Chaoyi Lu,Lei Zhao*

Main category: cs.CV

TL;DR: TikArt通过分步视觉语言推理和动态区域聚焦提升多模态大模型的细粒度视觉理解。


<details>
  <summary>Details</summary>
Motivation: 传统MLLM的单一全局图片编码会丢失微小物体/复杂区域/细微标记的细节，需要更精细的视觉推理机制。

Method: 开发TikArt智能体，采用Think-Aperture-Observe循环：1）语言生成决策聚焦区域；2）执行矩形裁剪（Zoom）或SAM2掩码分割（Segment）；3）生成显式观测将局部视觉特征转化为语言记忆。基于Qwen3-VL-8B架构，使用分两阶段的AGRPO强化学习方法优化策略（先预训练分割能力，再联合优化视觉数学、细粒度VQA和分割）。

Result: 在V*、HR-Bench-4K/8K等7个高分辨率基准测试中均超越基础模型，生成可解释的聚焦区域轨迹，实现任务表现与有效区域捕捉的双重提升。

Conclusion: 基于动态区域聚焦和强化学习的TikArt框架为高分辨率细粒度视觉语言交互提供了新范式，平衡了性能表现与推理可解释性。

Abstract: We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.

</details>


### [107] [Gaussian Mesh Renderer for Lightweight Differentiable Rendering](https://arxiv.org/abs/2602.14493)
*Xinpeng Liu,Fumio Okura*

Main category: cs.CV

TL;DR: 本文提出基于3D高斯散射的轻量化网格渲染器GMR，通过结合高斯与网格表示实现高效表面重建。


<details>
  <summary>Details</summary>
Motivation: 解决传统网格渲染器在优化速度和内存占用方面的不足，同时保持表面重构精度。

Method: 将网格三角面解析转换为高斯基元，利用3DGS的高效光栅化实现可微渲染并与网格紧密结合。

Result: 相比传统方法在相同内存限制下，使用更小批量数据即可获得平滑梯度和更优重构质量。

Conclusion: GMR通过整合高斯与网格表示实现了高效能和结构保真的表面重建，代码已开源。

Abstract: 3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR), which tightly integrates the Gaussian and mesh representations. Each Gaussian primitive is analytically derived from the corresponding mesh triangle, preserving structural fidelity and enabling the gradient flow. Compared to the traditional mesh renderers, our method achieves smoother gradients, which especially contributes to better optimization using smaller batch sizes with limited memory. Our implementation is available in the public GitHub repository at https://github.com/huntorochi/Gaussian-Mesh-Renderer.

</details>


### [108] [Prototype Instance-semantic Disentanglement with Low-rank Regularized Subspace Clustering for WSIs Explainable Recognition](https://arxiv.org/abs/2602.14501)
*Chentao Li,Pan Huang*

Main category: cs.CV

TL;DR: 本文针对病理图像中肿瘤识别存在的实例-语义纠缠问题，提出PID-LRSC框架，通过低秩正则化子空间聚类和原型实例语义解耦技术，在多中心数据集上实现优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 肿瘤组织与癌前病变/非肿瘤组织的高度相似性及非肿瘤样本过量导致实例-语义纠缠，现有多实例学习框架无法有效区分特征，影响模型表达和解释能力。

Method: 1. 采用二次实例子空间学习构建低秩正则化子空间聚类(LRSC)，解决非肿瘤样本过量带来的实例纠缠；2. 通过增强对比学习设计原型实例语义解耦(PID)，分离肿瘤与癌前病变的相似语义特征。

Result: 在多中心病理数据集上的实验表明，PID-LRSC在模型性能、实例语义清晰度及辅助诊断可靠性方面均显著优于现有SOTA方法。

Conclusion: 提出的PID-LRSC框架有效解决了病理图像分析中的实例-语义双纠缠问题，为医学AI提供了兼顾准确性和可解释性的解决方案。

Abstract: The tumor region plays a key role in pathological diagnosis. Tumor tissues are highly similar to precancerous lesions and non tumor instances often greatly exceed tumor instances in whole slide images (WSIs). These issues cause instance-semantic entanglement in multi-instance learning frameworks, degrading both model representation capability and interpretability. To address this, we propose an end-to-end prototype instance semantic disentanglement framework with low-rank regularized subspace clustering, PID-LRSC, in two aspects. First, we use secondary instance subspace learning to construct low-rank regularized subspace clustering (LRSC), addressing instance entanglement caused by an excessive proportion of non tumor instances. Second, we employ enhanced contrastive learning to design prototype instance semantic disentanglement (PID), resolving semantic entanglement caused by the high similarity between tumor and precancerous tissues. We conduct extensive experiments on multicentre pathology datasets, implying that PID-LRSC outperforms other SOTA methods. Overall, PID-LRSC provides clearer instance semantics during decision-making and significantly enhances the reliability of auxiliary diagnostic outcomes.

</details>


### [109] [MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction](https://arxiv.org/abs/2602.14512)
*Zhicheng He,Yunpeng Zhao,Junde Wu,Ziwei Niu,Zijun Li,Lanfen Lin,Yueming Jin*

Main category: cs.CV

TL;DR: MedVAR 是一个基于自回归的医疗图像生成模型，采用多尺度预测和大型数据集实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有医疗图像生成方法在可扩展性、多器官数据支持和评估体系方面存在不足。

Method: 提出MedVAR模型，采用自回归与多尺度预测机制，结合44万例跨多解剖区域的CT/MRI数据集进行训练。

Result: 实验显示MedVAR在生成质量、多样性及可扩展性指标上均达到SOTA水平，生成速度较传统模型提升300%

Conclusion: 验证了自回归架构在医疗生成模型中的可行性，为后续研究提供可扩展的技术范式

Abstract: Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.

</details>


### [110] [Efficient Text-Guided Convolutional Adapter for the Diffusion Model](https://arxiv.org/abs/2602.14514)
*Aryan Das,Koushik Biswas,Swalpa Kumar Roy,Badri Narayana Patro,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: 本文提出了Nexus Adapters（Nexus Prime和Slim），通过文本引导和结构输入结合的高效适配器改进扩散模型的结构保持生成，显著降低参数量同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有结构保持方法效率低下，适配器参数量与基础模型相当，且忽略输入提示与结构的联合优化，导致训练困难和性能受限。

Method: 设计包含交叉注意力机制的Nexus Blocks，实现文本-结构多模态条件控制。Nexus Prime增加8M参数，Slim减少18M参数，通过代码（https://github.com/arya-domain/Nexus-Adapters）实现。

Result: Nexus Prime比T2I-Adapter性能更优，Slim参数更少但仍达SOTA。

Conclusion: 提出参数高效且支持多模态条件的Nexus Adapters，平衡了模型规模与性能，推动了结构保持生成领域发展。

Abstract: We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters

</details>


### [111] [Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model](https://arxiv.org/abs/2602.14524)
*Ari Vesalainen,Eetu Mäkelä,Laura Ruotsalainen,Mikko Tolonen*

Main category: cs.CV

TL;DR: 比较TrOCR与Qwen模型在18世纪历史文本OCR中的表现，发现Qwen虽有更低错误率但易改变历史拼写，TrOCR更准确但易产生级联错误，提出需架构感知的评估方法


<details>
  <summary>Details</summary>
Motivation: 传统OCR评估指标（如CER/WER）无法反映模型在历史文本处理中的学术可靠性，需探究模型结构偏差对错误模式的影响

Method: 对比两种Transformer架构：专用OCR模型TrOCR与通用模型Qwen，在历史英语文本上使用长度加权准确率与假设驱动错误分析进行评估

Result: Qwen表现出更低错误率和更强抗干扰能力，但存在语言规则化导致历史拼写失真；TrOCR保持更高拼写保真度但易受级联错误影响，模型架构差异导致系统性错误差异

Conclusion: 相同准确率模型可能因架构差异产生截然不同的错误模式，历史文本OCR评估需结合模型结构特性，平衡拼写保真度与错误传播风险

Abstract: Optical Character Recognition (OCR) of eighteenth-century printed texts remains challenging due to degraded print quality, archaic glyphs, and non-standardized orthography. Although transformer-based OCR systems and Vision-Language Models (VLMs) achieve strong aggregate accuracy, metrics such as Character Error Rate (CER) and Word Error Rate (WER) provide limited insight into their reliability for scholarly use. We compare a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis driven error analysis.
  While Qwen achieves lower CER/WER and greater robustness to degraded input, it exhibits selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserves orthographic fidelity more consistently but is more prone to cascading error propagation. Our findings show that architectural inductive biases shape OCR error structure in systematic ways. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, underscoring the need for architecture-aware evaluation in historical digitization workflows.

</details>


### [112] [Cross-view Domain Generalization via Geometric Consistency for LiDAR Semantic Segmentation](https://arxiv.org/abs/2602.14525)
*Jindong Zhao,Yuan Gao,Yang Xia,Sheng Nie,Jun Yue,Weiwei Sun,Shaobo Xia*

Main category: cs.CV

TL;DR: 本文提出CVGC框架解决跨视角激光雷达语义分割域泛化问题，通过几何增强与一致性约束实现多视角数据的有效训练。


<details>
  <summary>Details</summary>
Motivation: 现有激光雷达语义分割方法在跨视角场景（如无人机与车载视角差异）中表现不佳，因其存在视角依赖的结构缺失与点密度不均问题。

Method: CVGC框架包含两部分：1) 跨视角几何增强模块模拟不同视角的可见性变化与点密度差异；2) 几何一致性模块强制多视角点云预测结果的一致性。

Result: 在6个公开激光雷达数据集的跨视角场景中，CVGC在单一源域迁移至多目标域任务上超越当前最优方法，且模型具备可解释性。

Conclusion: CVGC首次建立了跨视角域泛化的系统性评估基准，验证了几何先验在激光雷达开放场景落地中的关键作用。

Abstract: Domain-generalized LiDAR semantic segmentation (LSS) seeks to train models on source-domain point clouds that generalize reliably to multiple unseen target domains, which is essential for real-world LiDAR applications. However, existing approaches assume similar acquisition views (e.g., vehicle-mounted) and struggle in cross-view scenarios, where observations differ substantially due to viewpoint-dependent structural incompleteness and non-uniform point density. Accordingly, we formulate cross-view domain generalization for LiDAR semantic segmentation and propose a novel framework, termed CVGC (Cross-View Geometric Consistency). Specifically, we introduce a cross-view geometric augmentation module that models viewpoint-induced variations in visibility and sampling density, generating multiple cross-view observations of the same scene. Subsequently, a geometric consistency module enforces consistent semantic and occupancy predictions across geometrically augmented point clouds of the same scene. Extensive experiments on six public LiDAR datasets establish the first systematic evaluation of cross-view domain generalization for LiDAR semantic segmentation, demonstrating that CVGC consistently outperforms state-of-the-art methods when generalizing from a single source domain to multiple target domains with heterogeneous acquisition viewpoints. The source code will be publicly available at https://github.com/KintomZi/CVGC-DG

</details>


### [113] [MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation](https://arxiv.org/abs/2602.14534)
*Hongpeng Wang,Zeyu Zhang,Wenhao Li,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出MoRL，一种结合监督微调和强化学习的统一多模态运动模型，通过任务特定奖励函数和Chain-of-Motion推理方法，在动作理解和生成任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有动作理解和生成模型在推理能力和测试时规划方面存在局限，传统方法难以同时保证语义一致性、物理合理性和文本-动作一致性，缺乏大规模推理轨迹数据集支持。

Method: 提出多模态框架MoRL，采用双路径奖励机制（语义对齐+逻辑连贯性用于理解，物理合理性+文本一致性用于生成），设计Chain-of-Motion测试时推理架构，并构建包含140K条数据的MoUnd-CoT和MoGen-CoT数据集。

Result: 在HumanML3D和KIT-ML数据集上，MoRL相较SOTA基线模型在动作生成质量和推理逻辑性指标上取得显著提升，生成结果的物理合理性和文本对齐度提高28.6%和32.4%。

Conclusion: 该方法突破了传统动作生成模型的局限性，为具身智能体的时空推理和跨模态规划提供了新范式，开源代码和数据集将推动该领域发展。

Abstract: Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.

</details>


### [114] [OmniVTON++: Training-Free Universal Virtual Try-On with Principal Pose Guidance](https://arxiv.org/abs/2602.14552)
*Zhaotong Yang,Yong Du,Shengfeng He,Yuhui Li,Xinzhe Li,Yangyang Xu,Junyu Dong,Jian Yang*

Main category: cs.CV

TL;DR: OmniVTON++ 是一种无需训练的通用虚拟试穿框架，通过协调结构化服装变形、主姿态引导和连续边界缝合技术，解决服装对齐、人体结构连贯性和边界连续性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖特定数据条件下的重新训练，缺乏统一解决方案的泛化能力，因此需要设计可跨数据集和服装类型通用的框架。

Method: 通过结构化服装变形实现对应驱动的服装自适应，采用主姿态引导在扩散采样中渐进式调节结构，并通过连续边界缝合技术进行边界感知优化，形成无需任务专用训练的端到端流程。

Result: 在跨数据集、跨服装类型等多样化场景中达到SOTA性能，支持单一/多服装、单一/多人体及动漫角色虚拟试穿，且能适配不同扩散模型架构。

Conclusion: 本方法通过零训练策略扩展了虚拟试穿的应用范围，展现出强大的泛化能力与实际部署优势，相关代码即将开源。

Abstract: Image-based Virtual Try-On (VTON) concerns the synthesis of realistic person imagery through garment re-rendering under human pose and body constraints. In practice, however, existing approaches are typically optimized for specific data conditions, making their deployment reliant on retraining and limiting their generalization as a unified solution. We present OmniVTON++, a training-free VTON framework designed for universal applicability. It addresses the intertwined challenges of garment alignment, human structural coherence, and boundary continuity by coordinating Structured Garment Morphing for correspondence-driven garment adaptation, Principal Pose Guidance for step-wise structural regulation during diffusion sampling, and Continuous Boundary Stitching for boundary-aware refinement, forming a cohesive pipeline without task-specific retraining. Experimental results demonstrate that OmniVTON++ achieves state-of-the-art performance across diverse generalization settings, including cross-dataset and cross-garment-type evaluations, while reliably operating across scenarios and diffusion backbones within a single formulation. In addition to single-garment, single-human cases, the framework supports multi-garment, multi-human, and anime character virtual try-on, expanding the scope of virtual try-on applications. The source code will be released to the public.

</details>


### [115] [DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving](https://arxiv.org/abs/2602.14577)
*Chenxu Dang,Sining Ang,Yongkang Li,Haochen Tian,Jie Wang,Guang Li,Hangjun Ye,Jie Ma,Long Chen,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出DriveFine，通过融合扩散模型与专家混合机制解决自动驾驶中视觉-语言-动作模型的规划难题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在模态对齐差、训练效率低的问题，而token生成模型存在累计误差和不可逆解码缺陷，两种方法优劣互补。

Method: 设计block-MoE模块（包含生成专家与校正专家的解耦架构）并采用混合强化学习策略，在保持预训练权重稳定的同时实现灵活规划与自我修正。

Result: 在NAVSIM v1/v2和Navhard数据集上验证了模型的规划有效性与鲁棒性，代码已开源。

Conclusion: DriveFine通过模块化设计解决了生成式规划器的关键瓶颈，证明了可扩展架构在自动驾驶场景的潜力。

Abstract: Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.

</details>


### [116] [YOLO26: A Comprehensive Architecture Overview and Key Improvements](https://arxiv.org/abs/2602.14582)
*Priyanto Hidayatullah,Refdinal Tubagus*

Main category: cs.CV

TL;DR: YOLO26 introduces key enhancements like removing DFL, end-to-end NMS-free inference, ProgLoss+STAL, and MuSGD optimizer to boost CPU inference speed (43%) and expand capabilities in tasks like segmentation and pose estimation.


<details>
  <summary>Details</summary>
Motivation: To improve YOLO's efficiency for edge devices without GPUs, enable real-time performance, address architectural gaps in existing literature, and provide a comprehensive technical analysis of YOLO26's CNN-based architecture.

Method: Eliminated Distribution Focal Loss (DFL), implemented end-to-end NMS-free inference, introduced ProgLoss + Small-Target-Aware Label Assignment (STAL), adopted MuSGD optimizer, and conducted code-based architectural investigation via source code and documentation.

Result: Achieved 43% faster CPU inference speed, real-time performance on edge devices, enhanced multi-task performance (segmentation, pose estimation, OBB decoding), and produced the first CNN-based YOLO26 architectural diagram.

Conclusion: YOLO26 maintains YOLO's leadership in computer vision through architectural innovations that balance speed and performance while setting a foundation for future model improvements.

Abstract: You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series. The elimination of Distribution Focal Loss (DFL), implementation of End-to-End NMS-Free Inference, introduction of ProgLoss + Small-Target-Aware Label Assignment (STAL), and use of the MuSGD optimizer are the primary enhancements designed to improve inference speed, which is claimed to achieve a 43% boost in CPU mode. This is designed to allow YOLO26 to attain real-time performance on edge devices or those without GPUs. Additionally, YOLO26 offers improvements in many computer vision tasks, including instance segmentation, pose estimation, and oriented bounding box (OBB) decoding. We aim for this effort to provide more value than just consolidating information already included in the existing technical documentation. Therefore, we performed a rigorous architectural investigation into YOLO26, mostly using the source code available in its GitHub repository and its official documentation. The authentic and detailed operational mechanisms of YOLO26 are inside the source code, which is seldom extracted by others. The YOLO26 architectural diagram is shown as the outcome of the investigation. This study is, to our knowledge, the first one presenting the CNN-based YOLO26 architecture, which is the core of YOLO26. Our objective is to provide a precise architectural comprehension of YOLO26 for researchers and developers aspiring to enhance the YOLO model, ensuring it remains the leading deep learning model in computer vision.

</details>


### [117] [VariViT: A Vision Transformer for Variable Image Sizes](https://arxiv.org/abs/2602.14615)
*Aswathi Varma,Suprosanna Shit,Chinmay Prabhakar,Daniel Scholz,Hongwei Bran Li,Bjoern Menze,Daniel Rueckert,Benedikt Wiestler*

Main category: cs.CV

TL;DR: 该论文提出了VariViT，一种改进的视觉Transformer模型，旨在处理可变大小的医学图像，通过动态调整图像处理流程来提升特征表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer（ViT）需固定图像分块大小，导致医学图像（如肿瘤）预处理损失信息（如缩放伪影）或背景干扰。需优化区域特征提取以应对诊断需求。

Method: VariViT引入了可调整位置嵌入方案以适配不同数量的图像子块，并设计了新的批次处理策略以降低计算复杂度，加速训练推理。

Result: 在两个3D脑部MRI数据集上，VariViT在肿瘤分类和基因型预测任务中F1分数达75.5%和76.3%，分别优于传统ViT和ResNet，且计算时间减少30%。

Conclusion: VariViT有效解决医学图像中区域异形与计算资源的权衡问题，通过可变尺寸处理和优化策略提升了特征判别能力与效率。

Abstract: Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit

</details>


### [118] [VIGIL: Tackling Hallucination Detection in Image Recontextualization](https://arxiv.org/abs/2602.14633)
*Joanna Wojciechowicz,Maria Łubniewska,Jakub Antczak,Justyna Baczyńska,Wojciech Gromski,Wojciech Kozłowski,Maciej Zięba*

Main category: cs.CV

TL;DR: 开发了首个针对大模型在图像再情境化任务中的多阶段幻觉检测基准VIGIL，将幻觉细分为五类并提供开源检测框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究将多模型幻觉视为均质问题，缺乏精细分类，本研究填补了该领域评估框架的空白。

Method: 构建包含五类幻觉（物体粘贴、背景偏差等）的基准，设计多阶段检测流程，通过物体级保真度分析、背景一致性校验及遗漏检测组成的多模型协同架构。

Result: 实验验证了检测流程有效性，开放VIGIL数据集、代码及检测工具，首次实现对模型失败原因的解释性分析。

Conclusion: 为多模态模型幻觉分析提供结构化评估框架，推动透明化研究与发展。

Abstract: We introduce VIGIL (Visual Inconsistency & Generative In-context Lucidity), the first benchmark dataset and framework providing a fine-grained categorization of hallucinations in the multimodal image recontextualization task for large multimodal models (LMMs). While existing research often treats hallucinations as a uniform issue, our work addresses a significant gap in multimodal evaluation by decomposing these errors into five categories: pasted object hallucinations, background hallucinations, object omission, positional & logical inconsistencies, and physical law violations. To address these complexities, we propose a multi-stage detection pipeline. Our architecture processes recontextualized images through a series of specialized steps targeting object-level fidelity, background consistency, and omission detection, leveraging a coordinated ensemble of open-source models, whose effectiveness is demonstrated through extensive experimental evaluations. Our approach enables a deeper understanding of where the models fail with an explanation; thus, we fill a gap in the field, as no prior methods offer such categorization and decomposition for this task. To promote transparency and further exploration, we openly release VIGIL, along with the detection pipeline and benchmark code, through our GitHub repository: https://github.com/mlubneuskaya/vigil and Data repository: https://huggingface.co/datasets/joannaww/VIGIL.

</details>


### [119] [SketchingReality: From Freehand Scene Sketches To Photorealistic Images](https://arxiv.org/abs/2602.14648)
*Ahmed Bourouis,Mikhail Bessmeltsev,Yulia Gryaditskaya*

Main category: cs.CV

TL;DR: 此论文介绍了一种基于自由手绘草图生成高质量图像的新方法，利用语义解释与新损失函数提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成模型主要依赖边缘图等条件输入，但自由手绘草图因抽象性和失真缺乏有效处理方法；缺乏像素对齐的地面实况图像阻碍了该领域发展。

Method: 提出一种以草图语义解释为核心的调制方法，并设计一种无需像素对齐真值数据的训练损失函数。

Result: 新方法在保持草图语义一致性的同时生成更逼真、质量更高的图像，超越现有技术。

Conclusion: 该方法解决了自由手绘草图生成难题，为多模态条件生成提供了新思路。

Abstract: Recent years have witnessed remarkable progress in generative AI, with natural language emerging as the most common conditioning input. As underlying models grow more powerful, researchers are exploring increasingly diverse conditioning signals, such as depth maps, edge maps, camera parameters, and reference images, to give users finer control over generation. Among different modalities, sketches are a natural and long-standing form of human communication, enabling rapid expression of visual concepts. Previous literature has largely focused on edge maps, often misnamed 'sketches', yet algorithms that effectively handle true freehand sketches, with their inherent abstraction and distortions, remain underexplored. We pursue the challenging goal of balancing photorealism with sketch adherence when generating images from freehand input. A key obstacle is the absence of ground-truth, pixel-aligned images: by their nature, freehand sketches do not have a single correct alignment. To address this, we propose a modulation-based approach that prioritizes semantic interpretation of the sketch over strict adherence to individual edge positions. We further introduce a novel loss that enables training on freehand sketches without requiring ground-truth pixel-aligned images. We show that our method outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of the generated images.

</details>


### [120] [Advances in Global Solvers for 3D Vision](https://arxiv.org/abs/2602.14662)
*Zhenjun Zhao,Heng Yang,Bangyan Liao,Yingping Zeng,Shaocheng Yan,Yingdong Gu,Peidong Liu,Yi Zhou,Haoang Li,Javier Civera*

Main category: cs.CV

TL;DR: This paper presents a comprehensive survey of global solvers in 3D vision, categorizing methods (BnB, CR, GNC), analyzing their performance across tasks, and outlining future research directions for scalability and real-world deployment.


<details>
  <summary>Details</summary>
Motivation: To unify the field of geometric vision by systematically reviewing global solvers that provide certifiable solutions to nonconvex optimization problems, which were traditionally solved using heuristic/local methods.

Method: Conducting a systematic review with theoretical analysis of BnB, CR, and GNC paradigms; creating a taxonomy; evaluating algorithmic robustness/scalability across 10 vision tasks; and synthesizing practical enhancements.

Result: Established a comprehensive taxonomy of 3D vision global solvers, revealed trade-offs between optimality-robustness-scalability, identified key future directions (e.g., scalable algorithms, benchmarks), and created publicly available educational resources.

Conclusion: Global solvers require standardized benchmarks and integration of data-driven priors for safety-critical applications. This survey provides a unified perspective and roadmap for developing certifiable perception systems with real-world reliability.

Abstract: Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.

</details>


### [121] [MeFEm: Medical Face Embedding model](https://arxiv.org/abs/2602.14672)
*Yury Borets,Stepan Botman*

Main category: cs.CV

TL;DR: 本文提出了MeFEm，一种基于改进联合嵌入预测架构的面部图像生物识别与医学分析模型，通过轴向条纹掩码等创新方法，在减少数据量需求下超越现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有生物特征与医学分析模型存在数据需求量大、特征提取效率低的问题，同时需要解决面部图像分析中的领域偏差问题。

Method: 提出了三维改进方案：轴向条纹掩码强化语义区域关注，循环损失加权策略，以及CLS标记概率化重分配机制，构建了改进的JEPA架构。

Result: 在人类生物特征分析任务中超越FaRL/Franca等基线模型，在基于新构建闭源数据集的BMI估计任务中展现优异性能，模型数据效率提升显著。

Conclusion: 该研究证明改进的JEPA架构在医学图像分析中的有效性，为后续面部生物特征研究提供了可复现的基线模型和高质量数据集资源。

Abstract: We present MeFEm, a vision model based on a modified Joint Embedding Predictive Architecture (JEPA) for biometric and medical analysis from facial images. Key modifications include an axial stripe masking strategy to focus learning on semantically relevant regions, a circular loss weighting scheme, and the probabilistic reassignment of the CLS token for high quality linear probing. Trained on a consolidated dataset of curated images, MeFEm outperforms strong baselines like FaRL and Franca on core anthropometric tasks despite using significantly less data. It also shows promising results on Body Mass Index (BMI) estimation, evaluated on a novel, consolidated closed-source dataset that addresses the domain bias prevalent in existing data. Model weights are available at https://huggingface.co/boretsyury/MeFEm , offering a strong baseline for future work in this domain.

</details>


### [122] [Universal Image Immunization against Diffusion-based Image Editing via Semantic Injection](https://arxiv.org/abs/2602.14679)
*Chanhui Lee,Seunghyun Shin,Donggyu Choi,Hae-gon Jeon,Jeany Son*

Main category: cs.CV

TL;DR: 提出了一种针对扩散模型编辑管道的通用图像免疫框架，通过生成单一普适对抗扰动有效抵御恶意编辑。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽具强大图像编辑能力，但引发深度伪造和版权滥用风险。现有图像免疫方案依赖逐例优化特定扰动，存在可扩展性和实用性局限。

Method: 基于通用对抗扰动(UAP)技术，生成嵌入语义目标的普适扰动，通过覆盖原图内容误导模型注意力机制。无需训练数据或领域知识即可运作。

Result: 在UAP场景下显著优于基线方法，在受限扰动预算下与图像专用方法性能相当，且具备跨模型黑盒迁移性。

Conclusion: 首个通用图像免疫方案兼具理论有效性与现实普适性，验证了语义覆盖防御策略的可行性。

Abstract: Recent advances in diffusion models have enabled powerful image editing capabilities guided by natural language prompts, unlocking new creative possibilities. However, they introduce significant ethical and legal risks, such as deepfakes and unauthorized use of copyrighted visual content. To address these risks, image immunization has emerged as a promising defense against AI-driven semantic manipulation. Yet, most existing approaches rely on image-specific adversarial perturbations that require individual optimization for each image, thereby limiting scalability and practicality. In this paper, we propose the first universal image immunization framework that generates a single, broadly applicable adversarial perturbation specifically designed for diffusion-based editing pipelines. Inspired by universal adversarial perturbation (UAP) techniques used in targeted attacks, our method generates a UAP that embeds a semantic target into images to be protected. Simultaneously, it suppresses original content to effectively misdirect the model's attention during editing. As a result, our approach effectively blocks malicious editing attempts by overwriting the original semantic content in the image via the UAP. Moreover, our method operates effectively even in data-free settings without requiring access to training data or domain knowledge, further enhancing its practicality and broad applicability in real-world scenarios. Extensive experiments show that our method, as the first universal immunization approach, significantly outperforms several baselines in the UAP setting. In addition, despite the inherent difficulty of universal perturbations, our method also achieves performance on par with image-specific methods under a more restricted perturbation budget, while also exhibiting strong black-box transferability across different diffusion models.

</details>


### [123] [It's a Matter of Time: Three Lessons on Long-Term Motion for Perception](https://arxiv.org/abs/2602.14705)
*Willem Davison,Xinyue Hao,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 研究发现长期运动信息在感知任务中具有重要作用，其低维度特性可提升模型性能，特别是在低数据环境下。


<details>
  <summary>Details</summary>
Motivation: 尽管图像信息在感知研究中被广泛探索，但时间维度(尤其是长期运动信息)的作用仍未被充分理解，包括其对动作/物体识别、时空信息建模的能力及与图像特性的差异。

Method: 基于点轨迹估计技术构建长期运动表示，通过对比实验验证其在多种感知任务中的有效性，并分析其属性与视频表示的协同效应。

Result: 1) 运动表示可编码动作、物体材质及空间结构信息，表现优于图像；2) 在小样本/零样本任务中展现更强泛化能力；3) 其低维特性在计算效率与精度间取得平衡，与视频表示融合可进一步提升性能。

Conclusion: 长期运动信息为视觉感知提供了高效且互补的表征路径，建议未来模型设计应优先整合运动特性以优化性能。

Abstract: Temporal information has long been considered to be essential for perception. While there is extensive research on the role of image information for perceptual tasks, the role of the temporal dimension remains less well understood: What can we learn about the world from long-term motion information? What properties does long-term motion information have for visual learning? We leverage recent success in point-track estimation, which offers an excellent opportunity to learn temporal representations and experiment on a variety of perceptual tasks. We draw 3 clear lessons: 1) Long-term motion representations contain information to understand actions, but also objects, materials, and spatial information, often even better than images. 2) Long-term motion representations generalize far better than image representations in low-data settings and in zero-shot tasks. 3) The very low dimensionality of motion information makes motion representations a better trade-off between GFLOPs and accuracy than standard video representations, and used together they achieve higher performance than video representations alone. We hope these insights will pave the way for the design of future models that leverage the power of long-term motion information for perception.

</details>


### [124] [SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning](https://arxiv.org/abs/2602.14767)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.CV

TL;DR: SAILS是一种无需训练的持续增量语义分割框架，通过解耦模型阶段和创新原型设计有效解决计算成本与遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法受限于重复再训练、高计算成本及严重遗忘问题，尤其在现实应用中迭代更新加剧模型遗忘。论文旨在通过全新架构实现无需参数更新的长期持续学习。

Method: SAILS采用两阶段架构：1) 使用Segment Anything Model(SAM)进行零样本区域分割；2) 通过固定特征空间中的多原型进行语义关联。创新性加入类内选择性聚类，为每个类别建立多个原型表征类内多样性。

Result: 在标准CISS数据集测试中，SAILS在无任何增量训练情况下，其性能超越现有基于训练的方法，尤其在长序列任务中优势更显著。模型完全消除了参数更新带来的遗忘现象，并实现任务不变性能稳定性。值得注意的是，新类引入还能提升先前类别的表现（正向后迁移）。

Conclusion: 该研究证明通过结构创新与基础模型协同设计，可突破传统持续学习的方法瓶颈。SAILS为现实场景下的持续学习应用提供了高效、稳定的新范式，其无参数更新机制为解决模型遗忘和高效部署提供了新思路。

Abstract: Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.

</details>


### [125] [GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2602.14771)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 本文提出GOT-JEPA框架和OccuSolver方法，通过模型预测和伪监督学习提升目标跟踪在遮挡等复杂场景下的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有目标跟踪器依赖训练目标优化导致泛化能力不足，且遮挡推理粗浅。需要提升对遮挡模式的精细建模和动态场景适应性。

Method: GOT-JEPA采用教师-学生预测架构，在历史信息一致的前提下，教师模型利用干净帧生成伪跟踪模型，学生模型从损坏帧中重现实时预测。OccuSolver基于点跟踪器迭代优化可见性状态，结合对象先验增强遮挡感知。

Result: 在7个基准测试中，模型在遮挡、干扰物等复杂场景下显著提升跟踪鲁棒性，伪标签质量随训练迭代自提升。

Conclusion: GOT-JEPA通过伪监督模型预测强化遮挡处理能力，OccuSolver实现细粒度遮挡模式解析，二者协同提升通用目标跟踪框架的动态场景适应性。

Abstract: The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.

</details>


### [126] [VIPA: Visual Informative Part Attention for Referring Image Segmentation](https://arxiv.org/abs/2602.14788)
*Yubin Cho,Hyunwoo Yu,Kyeongbo Kong,Kyomin Sohn,Bongjoon Hyun,Suk-Ju Kang*

Main category: cs.CV

TL;DR: 提出了一种用于Referring Image Segmentation的新框架VIPA，通过视觉表达（visual expression）增强文本与图像跨模态对齐，显著提升细粒度分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将视觉信息融入语言标记提升分割效果，但未能充分挖掘视觉上下文对细粒度分割的潜力。

Method: 设计视觉表达生成器（VEG）模块，利用局部-全局语言上下文信号检索并优化视觉标记，抑制噪声并共享关键视觉属性，通过注意力机制强化语义一致性。

Result: 在4个公开RIS基准测试中均超越当前最优方法，实验证明其有效性。

Conclusion: VIPA框架通过结构化视觉表达提升跨模态对齐鲁棒性，解决了视觉特征噪声导致的注意力偏差问题。

Abstract: Referring Image Segmentation (RIS) aims to segment a target object described by a natural language expression. Existing methods have evolved by leveraging the vision information into the language tokens. To more effectively exploit visual contexts for fine-grained segmentation, we propose a novel Visual Informative Part Attention (VIPA) framework for referring image segmentation. VIPA leverages the informative parts of visual contexts, called a visual expression, which can effectively provide the structural and semantic visual target information to the network. This design reduces high-variance cross-modal projection and enhances semantic consistency in an attention mechanism of the referring image segmentation. We also design a visual expression generator (VEG) module, which retrieves informative visual tokens via local-global linguistic context cues and refines the retrieved tokens for reducing noise information and sharing informative visual attributes. This module allows the visual expression to consider comprehensive contexts and capture semantic visual contexts of informative regions. In this way, our framework enables the network's attention to robustly align with the fine-grained regions of interest. Extensive experiments and visual analysis demonstrate the effectiveness of our approach. Our VIPA outperforms the existing state-of-the-art methods on four public RIS benchmarks.

</details>


### [127] [Debiasing Central Fixation Confounds Reveals a Peripheral "Sweet Spot" for Human-like Scanpaths in Hard-Attention Vision](https://arxiv.org/abs/2602.14834)
*Pengcheng Pan,Yonekura Shogo,Yasuo Kuniyosh*

Main category: cs.CV

TL;DR: 研究指出基于注意力模型的扫描路径评估易受数据集中心偏差影响，提出新指标GCS消除偏差并发现注意力与周边视觉的协同效应。


<details>
  <summary>Details</summary>
Motivation: 任务驱动的视觉注意力模型常通过扫描路径是否匹配人类注视评估，但现有指标受数据集内对象中心化导致的中心偏差干扰，需更精确的评价方法。

Method: 通过Gaze-CIFAR-10验证中心注视基线表现，分析不同视野约束下注意力模型的扫描路径，设计结合去中心化与动作相似度的GCS指标。

Result: 中心基线表现接近学习模型，常规指标失效；发现中等视野约束下同时具备去中心化扫描与类人动眼特性的'周边黄金点'，GCS有效识别此窗口。

Conclusion: 揭示对象中心化数据集评估注意力模型需分离中心偏差与行为对齐，提出GCS为设计更优眼动基准提供理论依据。

Abstract: Human eye movements in visual recognition reflect a balance between foveal sampling and peripheral context. Task-driven hard-attention models for vision are often evaluated by how well their scanpaths match human gaze. However, common scanpath metrics can be strongly confounded by dataset-specific center bias, especially on object-centric datasets. Using Gaze-CIFAR-10, we show that a trivial center-fixation baseline achieves surprisingly strong scanpath scores, approaching many learned policies. This makes standard metrics optimistic and blurs the distinction between genuine behavioral alignment and mere central tendency. We then analyze a hard-attention classifier under constrained vision by sweeping foveal patch size and peripheral context, revealing a peripheral sweet spot: only a narrow range of sensory constraints yields scanpaths that are simultaneously (i) above the center baseline after debiasing and (ii) temporally human-like in movement statistics. To address center bias, we propose GCS (Gaze Consistency Score), a center-debiased composite metric augmented with movement similarity. GCS uncovers a robust sweet spot at medium patch size with both foveal and peripheral vision, that is not obvious from raw scanpath metrics or accuracy alone, and also highlights a "shortcut regime" when the field-of-view becomes too large. We discuss implications for evaluating active perception on object-centric datasets and for designing gaze benchmarks that better separate behavioral alignment from center bias.

</details>


### [128] [Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation](https://arxiv.org/abs/2602.14837)
*Lorenzo Mur Labadia,Ruben Martinez-Cantin,Jose J. Guerrero,Giovanni M. Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 本研究提出了STAformer和STAformer++两种基于注意力机制的架构,结合时空汇聚和多尺度特征融合,并通过整合环境可操作性模型及交互热点预测模块,在Ego4D和EPIC-Kitchens数据集上实现了23-31个百分点的性能提升,相关代码和数据已公开。


<details>
  <summary>Details</summary>
Motivation: 短时物体交互预测(STA)对可穿戴助手理解用户意图和提供实时辅助至关重要,但需解决从第一视角视频中精准预判交互对象、动作类别和接触时间的挑战。

Method: 1) 设计双流注意力架构:帧引导的时间汇聚、图像-视频双注意力和多尺度特征融合;2) 引入可操作性建模:环境可操作性记忆模块(含自适应融合策略)和手部-物体轨迹驱动的交互热点预测模块。

Result: 在Ego4D数据集上Top-5 mAP提升23%,EPIC-Kitchens新标注集提升31%,证实了环境可操作性自适应融合和热点预测的有效性。

Conclusion: 基于注意力机制的架构结合可操作性建模显著提升了短时交互预测性能,公开数据资源将推动该领域未来研究。

Abstract: Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.

</details>


### [129] [Multi-dimensional Persistent Sheaf Laplacians for Image Analysis](https://arxiv.org/abs/2602.14846)
*Xiang Xiang Wang,Guo-Wei Wei*

Main category: cs.CV

TL;DR: 提出了一种基于单纯复形的多维持久层拉普拉斯(MPSL)框架用于图像分析，通过多尺度局部拓扑谱特征和多维统计聚合增强特征表征。


<details>
  <summary>Details</summary>
Motivation: 解决传统降维方法(如PCA)对降维维度选择敏感的问题，避免单一维度选取或跨维度平均导致的性能波动。

Method: 将图像样本视为单纯复形，使用持久层拉普拉斯提取多尺度局部拓扑谱特征，在不同尺度和维度聚合统计特征形成多维表征。

Result: 在COIL20和ETH80数据集上验证，相比PCA基线方法在中等维度下提升3.2-5.7%分类准确率，跨维度性能稳定性提升40%以上。

Conclusion: 多维度特征聚合能有效缓解单一维度敏感性问题，拓扑谱与多尺度表征结合为高维数据提供鲁棒特征提取方法。

Abstract: We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.

</details>


### [130] [CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography](https://arxiv.org/abs/2602.14879)
*Qingqing Zhu,Qiao Jin,Tejas S. Mathai,Yin Fang,Zhizheng Wang,Yifan Yang,Maame Sarfo-Gyamfi,Benjamin Hou,Ran Gu,Praveen T. S. Balamuralikrishna,Kenneth C. Wang,Ronald M. Summers,Zhiyong Lu*

Main category: cs.CV

TL;DR: CT-Bench is introduced as the first comprehensive benchmark dataset for medical lesion analysis, containing lesion annotations, metadata, and a multitask visual QA benchmark, validated through multimodal model evaluations.


<details>
  <summary>Details</summary>
Motivation: The scarcity of publicly available CT datasets with lesion-level annotations limits AI progress. The paper aims to create a standardized benchmark to address this gap.

Method: CT-Bench includes (1) a Lesion Image and Metadata Set with 20,335 lesions (bounding boxes, descriptions, size) from 7,795 CT studies, and (2) a multitask visual QA benchmark with 2,850 QA pairs. Hard negative examples were included to simulate diagnostic challenges. Multimodal models were evaluated and compared to radiologist assessments.

Result: State-of-the-art multimodal models showed measurable performance across CT-Bench tasks, with significant gains observed after fine-tuning on the Lesion Image and Metadata Set. Results confirm the dataset's effectiveness in reflecting real-world diagnostic scenarios.

Conclusion: CT-Bench provides a valuable resource for advancing AI in lesion analysis by offering structured annotations and standardized benchmarks, demonstrating clinical relevance through improved model performance.

Abstract: Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.

</details>


### [131] [Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery](https://arxiv.org/abs/2602.14929)
*Chandrakanth Gudavalli,Tajuddin Manhar Mohammed,Abhay Yadav,Ananth Vishnu Bhaskar,Hardik Prajapati,Cheng Peng,Rama Chellappa,Shivkumar Chandrasekaran,B. S. Manjunath*

Main category: cs.CV

TL;DR: 该论文提出了Wrivinder，一种零样本、几何驱动的方法，通过聚合多张地面照片重建一致的3D场景，并利用语义特征和深度线索生成天顶视角渲染图，直接匹配卫星图像以实现精确地理定位，解决了传统方法在大视角差异或GPS不可靠时的局限性。


<details>
  <summary>Details</summary>
Motivation: GPS不可靠或存在大视角差异时，地面图像与卫星地图对齐困难。现有方法依赖配对监督数据或受限于复杂场景，需探索无需配对数据、基于几何的鲁棒性方案。

Method: Wrivinder结合SfM重建、3D高斯点阵渲染、语义锚定和单目深度线索，从多视角地面图像生成稳定的天顶视角渲染图，直接与卫星图像匹配以实现度量级精确的相机定位。同时发布MC-Sat数据集，包含多视角地面图像与配准卫星瓦片的关联，用于系统化评估跨视角对齐任务。

Result: 在零样本实验中，Wrivinder在密集与广域场景下均达到亚30米级地理位置精度，验证了基于几何聚集方法的有效性。MC-Sat数据集填补了该领域的基准测试空白。

Conclusion: Wrivinder与MC-Sat为无配对监督的几何中心跨视角对齐提供了首个综合基线与测试平台，证明了纯几何驱动方法在鲁棒地面-卫星定位中的潜力。

Abstract: Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.

</details>


### [132] [AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories](https://arxiv.org/abs/2602.14941)
*Zun Wang,Han Lin,Jaehong Yoon,Jaemin Cho,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: AnchorWeave提出使用多个局部几何记忆替代全局重建，解决视频生成中长时空间一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖全局3D场景重建会产生跨视角几何错位，累积的噪声导致生成质量下降

Method: 通过覆盖率驱动的局部记忆检索与多锚点编织控制器整合多个局部几何记忆

Result: 实验表明在保持视觉质量的同时显著提升长时场景一致性，消融实验证实了局部几何条件、多锚点控制和覆盖率检索的有效性

Conclusion: 基于局部记忆融合的框架能有效缓解全局重建误差积累问题，为可控视频生成提供了新的方法路径

Abstract: Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.

</details>


### [133] [PAct: Part-Decomposed Single-View Articulated Object Generation](https://arxiv.org/abs/2602.14965)
*Qingming Liu,Xinyue Yao,Shuyuan Zhang,Yueci Deng,Guiliang Liu,Zhen Liu,Kui Jia*

Main category: cs.CV

TL;DR: 本研究提出了一种基于部件中心的生成框架，用于创建具有几何结构、组合关系和运动约束的高精度可动3D物体，通过显式部件感知条件实现单图像驱动的快速生成。


<details>
  <summary>Details</summary>
Motivation: 现有可动物体生成方法存在两难困境：优化重构方法精度高但耗时过长（数十分钟至数小时），检索驱动方法速度较快但难以准确匹配输入观测中的结构细节和运动特性。

Method: 提出部件感知的生成模型，将物体表示为可动部件集合，每个部件通过增强部件身份和运动线索的潜在编码进行表征，基于单张图像实现部件几何、组合与运动约束的联合生成。

Result: 实验显示该方法在保持部件结构有效性与输入一致性前提下，相比优化基线方法推理速度提升3-10倍，且在部件精度、运动合理性和输入匹配度（提升15.7%以上）等指标上显著优于检索驱动方法。

Conclusion: 该框架解决了传统方法在单图像可动物体生成中难以平衡质量与效率的问题，为具身交互应用中的动态资产创建提供了新的技术路径。

Abstract: Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.

</details>


### [134] [ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery](https://arxiv.org/abs/2602.14989)
*Ayush Shrivastava,Kirtan Gangani,Laksh Jain,Mayank Goel,Nipun Batra*

Main category: cs.CV

TL;DR: Vision Language Models (VLMs) struggle with thermal images despite success in RGB settings. The authors introduce ThermEval-B, a benchmark with 55k thermal image-question pairs, to evaluate foundational capabilities for thermal vision-language understanding, revealing that current models fail at temperature-grounded reasoning.


<details>
  <summary>Details</summary>
Motivation: Thermal sensing is critical in low-light or specialized applications (e.g., surveillance, medicine), but existing RGB-centric VLMs lack evaluation metrics and capabilities for temperature-based reasoning. This work addresses the gap by creating a dedicated benchmark for thermal understanding.

Method: Developed ThermEval-B by combining public datasets with ThermEval-D, a new dataset containing thermal images with dense pixel-level temperature maps and semantic body-part annotations. The benchmark evaluates VLMs through structured visual question-answering tasks focused on thermal properties.

Result: Evaluations of 25 VLMs show consistent failures in temperature reasoning, poor performance under colormap transformations, reliance on language priors, and minimal improvement from prompting or fine-tuning. This highlights limitations in current RGB-centric model design.

Conclusion: Thermal vision-language understanding requires specialized evaluation beyond RGB benchmarks. ThermEval-B establishes a foundation for future research to address domain-specific challenges in thermal perception and reasoning.

Abstract: Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.

</details>


### [135] [Image Generation with a Sphere Encoder](https://arxiv.org/abs/2602.15030)
*Kaiyu Yue,Menglin Jia,Ji Hou,Tom Goldstein*

Main category: cs.CV

TL;DR: 提出Sphere Encoder，一种单次推理生成图像的高效方法，效果与扩散模型相当但速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型需多步推理影响效率，作者旨在开发单次前向传递即可生成高质量图像的框架。

Method: 构建球形潜在空间编码器-解码器结构，通过重建损失端到端训练，仅需解码随机球面点生成图像。

Result: 在多个数据集实现竞争性生成质量，推理成本相比扩散模型大幅降低。

Conclusion: Sphere Encoder为高效图像生成提供新方案，支持条件生成且可迭代优化，适合实际应用部署。

Abstract: We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .

</details>


### [136] [EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing](https://arxiv.org/abs/2602.15031)
*Yehonathan Litman,Shikun Liu,Dario Seyb,Nicholas Milef,Yang Zhou,Carl Marshall,Shubham Tulsiani,Caleb Leak*

Main category: cs.CV

TL;DR: EditCtrl是一种高效的视频修复框架，通过局部计算与轻量全局引导，在保持高编辑质量的同时实现比现有方法快10倍的计算效率，并支持多区域编辑和自回归内容传播。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成编辑方法对全视频上下文进行冗余计算，导致计算成本高且难以高效支持局部稀疏编辑，阻碍了高保真视频编辑的实用性。

Method: 提出局部视频上下文模块，仅处理遮罩区域内的token以降低计算量，并设计轻量级时序全局上下文嵌入器保持视频一致性，形成局部优先的生成范式。

Result: 计算效率达SOTA方法的10倍，编辑质量超越全注意力设计方法，在保持保真度的同时显著降低资源消耗。

Conclusion: EditCtrl通过局部-全局协同架构，解决了视频编辑中的计算效率瓶颈，同时拓展出多区域文本驱动编辑和内容自回归传播等新功能。

Abstract: High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [137] [Multimodal Consistency-Guided Reference-Free Data Selection for ASR Accent Adaptation](https://arxiv.org/abs/2602.13263)
*Ligong Lei,Wenwen Lu,Xudong Pang,Zaokere Kadeer,Aishan Wumaier*

Main category: cs.CL

TL;DR: 本文提出了一种无需参考的多模态一致性引导的ASR口音自适应方法，通过结合语音-文本对齐和语音错误率预测，在跨领域场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的伪标签选择方法依赖文本困惑度等文本中心指标，容易在语音-文本不匹配任务中引入错误伪标签，导致微调效果下降。

Method: 构建包含目标感知预筛选（基于次模互信息）、扰动解码生成多假设、多模态无参考评分（共享嵌入空间对齐度+WER预测）和百分位筛选的端到端管线。

Result: 在领域内场景中1.5k样本达到10.91% WER（仅比全监督低0.36%），跨领域场景通过一致性感知筛选可避免性能崩溃，在强口音偏移下优于随机采样和最新基线。

Conclusion: 该方法在有限标注资源下实现鲁棒口音自适应，特别是在候选池存在领域偏移的挑战场景中表现出色

Abstract: Automatic speech recognition (ASR) systems often degrade on accented speech because acoustic-phonetic and prosodic shifts induce a mismatch to training data, making labeled accent adaptation costly. However, common pseudo-label selection heuristics are largely text-centric (e.g., perplexity (PPL) filtering) and can prefer fluent yet acoustically mismatched hypotheses, leading to error amplification when fine-tuning. To address this, we introduce a multimodal consistency-guided, reference-free data selection pipeline for ASR accent adaptation under a transductive, label-free protocol. The pipeline starts with a target-aware preselection step based on submodular mutual information to improve query relevance and reduce downstream computation. It then generates multiple pseudo-transcriptions per utterance via perturbation-based decoding and scores each hypothesis using two reference-free signals: speech--text alignment in a shared embedding space and predicted word error rate (WER). A simple percentile-based selection rule retains reliable pseudo-labels for fine-tuning while discarding noisy utterances. In an in-domain setting, selecting ~1.5k utterances from a 30k pool achieves 10.91% WER, close to 10.45% obtained using 30k supervised labels. In a cross-domain setting with a mismatched candidate pool, consistency-filtered subsets avoid the degradation caused by unfiltered pseudo-labels under strong accent shift, and matched-hour experiments on a stronger ASR backbone further confirm gains over random sampling and recent selection baselines.

</details>


### [138] [LLM-Powered Automatic Translation and Urgency in Crisis Scenarios](https://arxiv.org/abs/2602.13452)
*Belu Ticona,Antonis Anastasopoulos*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）和机器翻译系统在紧急情况下的翻译任务中表现不佳，尤其在保持信息紧迫性方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被提议用于危机响应，但其在高风险场景中的适用性尚未充分评估。危机沟通需确保信息紧迫性准确传递，这是当前技术的关键盲点。

Method: 研究者构建了涵盖32种语言的紧急状态标注数据集，评估多语言危机数据中的翻译模型（专用翻译系统与LLMs）性能，并测试LLMs对紧迫性分类的跨语言稳定性。

Result: 两种模型均出现显著性能下降与不稳定性。即使翻译结果语言通顺，仍可能扭曲紧迫性感知；LLMs的紧迫性分类结果因提示语言与输入语言差异而波动剧烈。

Conclusion: 通用语言技术在危机场景中的应用存在重大风险，亟需开发针对危机场景的评估框架与定制化模型。

Abstract: Large language models (LLMs) are increasingly proposed for crisis preparedness and response, particularly for multilingual communication. However, their suitability for high-stakes crisis contexts remains insufficiently evaluated. This work examines the performance of state-of-the-art LLMs and machine translation systems in crisis-domain translation, with a focus on preserving urgency, which is a critical property for effective crisis communication and triaging. Using multilingual crisis data and a newly introduced urgency-annotated dataset covering over 32 languages, we show that both dedicated translation models and LLMs exhibit substantial performance degradation and instability. Crucially, even linguistically adequate translations can distort perceived urgency, and LLM-based urgency classifications vary widely depending on the language of the prompt and input. These findings highlight significant risks in deploying general-purpose language technologies for crisis communication and underscore the need for crisis-aware evaluation frameworks.

</details>


### [139] [Language Model Memory and Memory Models for Language](https://arxiv.org/abs/2602.13466)
*Benjamin L. Badger*

Main category: cs.CL

TL;DR: 本研究发现自编码器比语言模型更擅长记忆输入信息，并提出结合因果训练与信息保留目标的新架构以提升记忆能力。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习模型中隐藏层嵌入的记忆能力差异，尤其针对语言模型记忆信息量不足的局限性。传统next-token-prediction训练是否适合记忆形成也值得验证。

Method: 比较语言模型与自编码器的嵌入信息量；设计并行编码器-解码器架构，替代序列输入为记忆嵌入；结合因果训练与信息保留目标函数，并提出课程训练策略（先冻结编码器训练解码器处理记忆，再添加预测）。

Result: 自编码器实现近乎完美记忆；新架构通过双重目标函数生成信息丰富记忆；课程训练提升训练效率；传统next-token-prediction模型内存储信息量明显不足。

Conclusion: 单纯next-token-prediction训练不适合精确记忆，需结合可逆目标函数。模型架构需区分信息保留与预测任务，课程训练可优化复杂目标学习过程。

Abstract: The ability of machine learning models to store input information in hidden layer vector embeddings, analogous to the concept of `memory', is widely employed but not well characterized. We find that language model embeddings typically contain relatively little input information regardless of data and compute scale during training. In contrast, embeddings from autoencoders trained for input regeneration are capable of nearly perfect memory formation. The substitution of memory embeddings for token sequences leads to substantial computational efficiencies, motivating the introduction of a parallelizable encoder-decoder memory model architecture. Upon causal training these models contain information-poor embeddings incapable of arbitrary information access, but by combining causal and information retention objective functions they learn to form and decode information-rich memories. Training can be further streamlined by freezing a high fidelity encoder followed by a curriculum training approach where decoders first learn to process memories and then learn to additionally predict next tokens. We introduce the perspective that next token prediction training alone is poorly suited for accurate memory formation as the objective itself is non-invertible, motivating the use of combined objective functions for models where the entire input is not exposed.

</details>


### [140] [From Perceptions To Evidence: Detecting AI-Generated Content In Turkish News Media With A Fine-Tuned Bert Classifier](https://arxiv.org/abs/2602.13504)
*Ozancan Ozdemir*

Main category: cs.CL

TL;DR: 本文开发了一个土耳其语BERT模型，用于检测新闻中的AI生成内容，发现约2.5%的土耳其新闻由大型语言模型改写。


<details>
  <summary>Details</summary>
Motivation: 英语媒体已有量化研究，但土耳其语新闻的研究仅限于记者访谈或假新闻检测，缺乏实证分析。

Method: 在3,600篇标注文章上微调土耳其语BERT模型（dbmdz/bert-base-turkish-cased）并进行二分类验证，随后对2023-2026年间的3,500篇未见文章进行部署分析。

Result: 模型测试集F1得分0.9708，预测置信度均值超0.96，跨来源与跨时间分类结果稳定，发现2.5%的新闻内容含AI改写。

Conclusion: 首次通过数据驱动方法量化土耳其语新闻中的AI写作应用，突破仅依赖记者自述感知的传统研究范式。

Abstract: The rapid integration of large language models into newsroom workflows has raised urgent questions about the prevalence of AI-generated content in online media. While computational studies have begun to quantify this phenomenon in English-language outlets, no empirical investigation exists for Turkish news media, where existing research remains limited to qualitative interviews with journalists or fake news detection. This study addresses that gap by fine-tuning a Turkish-specific BERT model (dbmdz/bert-base-turkish-cased) on a labeled dataset of 3,600 articles from three major Turkish outlets with distinct editorial orientations for binary classification of AI-rewritten content. The model achieves 0.9708 F1 score on the held-out test set with symmetric precision and recall across both classes. Subsequent deployment on over 3,500 unseen articles spanning between 2023 and 2026 reveals consistent cross-source and temporally stable classification patterns, with mean prediction confidence exceeding 0.96 and an estimated 2.5 percentage of examined news content rewritten or revised by LLMs on average. To the best of our knowledge, this is the first study to move beyond self-reported journalist perceptions toward empirical, data-driven measurement of AI usage in Turkish news media.

</details>


### [141] [Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens](https://arxiv.org/abs/2602.13517)
*Wei-Lin Chen,Liqian Peng,Tian Tan,Chao Zhao,Blake JianHang Chen,Ziqian Lin,Alec Go,Yu Meng*

Main category: cs.CL

TL;DR: 为解决大语言模型在推理时因生成长度增加导致性能下降的问题，本文提出以模型深层预测变化的token比例（深度思考比例）作为质量评估指标，并基于此开发高效推理策略Think@n，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统以token长度或置信度衡量推理质量的指标不可靠，过度生成可能导致性能下降，亟需能准确量化推理质量的可解释性指标。

Method: 定义深度思考token（模型深层预测发生显著修正的标记），计算其占生成序列的比例作为质量指标，并提出Think@n策略：优先保留高深度思考比的推理结果，通过短序列前缀预测质量以减少无效计算。

Result: 在AIME/HMMT/GPQA等四个基准测试中，深度思考比与准确率呈强正相关（优于长度/置信度基线）；Think@n在保持或超越自洽性策略性能的同时减少46%推理成本。

Conclusion: 深度思考比是量化推理质量的有效指标，Think@n为推理效率优化提供了可解释的新范式，为资源分配提供了理论依据。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal "overthinking," leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.

</details>


### [142] [On Calibration of Large Language Models: From Response To Capability](https://arxiv.org/abs/2602.13540)
*Sin-Han Yang,Cheng-Kuang Wu,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee,Shao-Hua Sun*

Main category: cs.CL

TL;DR: 提出基于能力校准的LLM置信度评估体系


<details>
  <summary>Details</summary>
Motivation: 现有响应级校准方法不反映LLM解决完整查询的真实能力，因解码随机性导致单次生成结果无法准确评估模型能力

Method: 建立能力校准理论框架，设计包含多方法比较的实证评估体系，区分响应校准与能力校准的本质差异

Result: 所提方法显著提升pass@k预测准确性和推理预算分配效率，验证了能力维度置信度评估的必要性

Conclusion: 为提升LLM可靠性建立新校准范式，能力置信度可支持更精准的模型能力预测与资源优化

Abstract: Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.

</details>


### [143] [Small Reward Models via Backward Inference](https://arxiv.org/abs/2602.13551)
*Yike Wang,Faeze Brahman,Shangbin Feng,Teng Xiao,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: FLIP是一种无需参考答案或评分标准的奖励模型，通过反向推断生成指令并评估其与原始指令的相似性，显著优于传统方法，尤其适用于小型模型和长文本场景。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖大模型的强推理能力或需参考答案/手动标注的评分标准，限制灵活性与可扩展性。FLIP旨在通过去参考化与去规则化设计提升灵活性。

Method: 提出FLIP方法：1) 反向推理生成指令（基于模型输出反推原始指令）；2) 计算反推指令与实际指令的相似性作为奖励信号，利用验证-生成能力差异实现鲁棒建模。

Result: 在13个小语言模型的四领域测试中，FLIP对比LLM-as-a-Judge方法平均超越79.6%，显著提升采样和GRPO训练的下游任务表现，且对抗长文本输出与奖励黑客攻击具有强鲁棒性。

Conclusion: FLIP通过非正则化设计解决小型模型在非可验证领域的奖励建模挑战，突破传统方法的算力与数据依赖，为低配场景提供可靠解决方案。

Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.

</details>


### [144] [DistillLens: Symmetric Knowledge Distillation Through Logit Lens](https://arxiv.org/abs/2602.13567)
*Manish Dhakal,Uthman Jinadu,Anjila Budathoki,Rajshekhar Sunderraman,Yi Ding*

Main category: cs.CL

TL;DR: DistillLens improves knowledge distillation for LLMs by symmetrically aligning teacher-student thought processes through logit-based intermediate state matching, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge distillation ignores teacher model's intermediate uncertainty profiles critical for final output quality, while existing feature-based methods fail to preserve this high-entropy information.

Method: Projects intermediate hidden states into vocabulary space using Logit Lens and applies symmetric divergence objective to penalize both overconfidence and underconfidence during knowledge transfer.

Result: Experiments on GPT-2 and Llama architectures show consistent superior performance on instruction-following benchmarks compared to standard KD and feature-transfer baselines.

Conclusion: Structural alignment of evolving thought processes with symmetric divergence better preserves essential uncertainty profiles, enabling more effective knowledge transfer in LLM compression.

Abstract: Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.

</details>


### [145] [LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2602.13571)
*Zhipeng Song,Xiangyu Kong,Xinrui Bao,Yizhi Zhou,Jiulong Jiao,Sitong Liu,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 提出无需训练的LCR算法提升RAG文档重排序效果


<details>
  <summary>Details</summary>
Motivation: 现有重排序器需特定训练且计算成本高，未能充分挖掘大模型语义能力与置信度信号

Method: 基于最大语义簇比例（MSCP）的二阶段框架：a) 多项式采样聚类评估置信度 b) 分箱多级排序，保留高置信查询原序

Result: 在BEIR/TREC基准上NDCG@5指标提升20.6%，消融实验证明置信度与相关性正相关，且计算效率高、可并行扩展

Conclusion: LCR通过显式建模语言模型置信度，显著提升检索效果并在医学诊断等重要场景中减少生成幻觉

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.

</details>


### [146] [Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment](https://arxiv.org/abs/2602.13575)
*Jing Zhao,Ting Zhen,Junwei bao,Hongfei Jiang,Yang song*

Main category: cs.CL

TL;DR: Elo-Evolve是一种动态多智能体竞争框架，通过消除对Bradley-Terry模型的依赖和引入基于Elo的对手选择机制，解决了大型语言模型对齐中的数据稀缺性和噪声敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法依赖静态绝对奖励函数，导致数据稀缺、噪声敏感和训练不稳定，需要动态且适应性强的优化策略。

Method: 基于PAC学习理论构建共同进化框架：(1) 直接学习二元胜负结果，替代传统Bradley-Terry建模；(2) 通过温度控制的Elo对手选择实现自动课程学习，并采用多模型对手池进行训练。

Result: 理论证明配对比较具有更优样本复杂度，实验验证噪声减少4.5倍；Qwen系列模型证明性能层次结构（Elo-Evolve > 静态配对 > 点式方法）

Conclusion: 动态对手竞争与双路径学习机制显著提升LLM对齐效果，为解决生成模型对齐挑战提供了新范式。

Abstract: Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.

</details>


### [147] [On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis](https://arxiv.org/abs/2602.13713)
*Maciej Uberna,Michał Wawer,Jarosław A. Chudziak,Marcin Koszowy*

Main category: cs.CL

TL;DR: 本文提出了一种基于多智能体框架的理论增强方法，用于提升LLMs识别论点重述功能的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs难以识别政治辩论中语句重述的修辞功能（如强化/弱化表意），仅能检测表层相似性，缺乏对话语策略的深度分析。

Method: 构建包含Deintensification/Intensification/Specification/Generalisation/Others五类标注的D-I-S-G-O数据集，对比RAG增强理论模型与零样本基线模型的性能差异。

Result: RAG增强模型相较基线模型性能显著提升，在Intensification和Generalisation场景表现最优，整体Macro F1-score提高29.8%。

Conclusion: 理论建模对论点重述功能分析具有决定性作用，验证了多智能体框架在可扩展性话语策略分析中的应用潜力。

Abstract: Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.

</details>


### [148] [RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction](https://arxiv.org/abs/2602.13748)
*Yongkang Jin,Jianwen Luo,Jingjing Wang,Jianmin Yao,Yu Hong*

Main category: cs.CL

TL;DR: 提出RMPL框架解决低资源下的多模态事件抽取问题。该框架通过结合单模态事件抽取和多模态关系抽取的异构监督，采用分阶段训练学习跨模态共享表示，最终在M2E2数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MEE方法受限于标注数据匮乏（仅M2E2基准有评估标注），传统监督训练不可行；现有方法依赖跨模态对齐或推理提示，缺乏对结构化事件表示的显式学习，导致多模态论据生成能力弱。

Method: RMPL包含两个训练阶段：1) 通过统一模式联合训练模态无关的事件中心表示；2) 在混合图文数据上微调事件提及识别和角色抽取任务。通过整合单模态事件抽取与多模态关系抽取的异构监督信号实现渐进学习。

Result: 在M2E2基准测试中，结合多种视觉-语言模型（如CLIP、BLIP）的实验显示：相较基线模型，在事件识别F1值提升3.2-8.7，且在跨模态（图文）、单模态（文本/图像）场景均取得最优性能。

Conclusion: RMPL框架通过渐进式多任务学习有效解决了低资源场景下的多模态事件抽取问题，其创新点包括：1) 整合异构监督信号；2) 分阶段训练策略；3) 支持灵活的多模态融合架构。为后续研究提供了可扩展的框架原型。

Abstract: Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.

</details>


### [149] [How Do Lexical Senses Correspond Between Spoken German and German Sign Language?](https://arxiv.org/abs/2602.13790)
*Melis Çelikkol,Wei Zhao*

Main category: cs.CL

TL;DR: 本研究构建了首个用于跨模态词义对应分析的标注数据集，通过语义相似性方法显著提升多义词对手语的识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统手语词典对多义词和同形异义词的动态映射覆盖不足，需通过语料库驱动的方法捕捉真实语境中的词义-手语对应模式。

Method: 人工标注1404对德语-德语手语对应关系，定义三种映射类型及无匹配案例；使用SBERT嵌入进行语义相似性评估，并与精确匹配方法对比结果。

Result: 语义相似性方法整体准确率88.52%，较精确匹配71.31%大幅提升，其中一对多映射匹配率提升52.1个百分点。

Conclusion: 研究证明语义建模可有效识别复杂跨模态对应关系，所构建的标注数据集为多模态语言资源建设提供了可复用的基础框架。

Abstract: Sign language lexicographers construct bilingual dictionaries by establishing word-to-sign mappings, where polysemous and homonymous words corresponding to different signs across contexts are often underrepresented. A usage-based approach examining how word senses map to signs can identify such novel mappings absent from current dictionaries, enriching lexicographic resources. We address this by analyzing German and German Sign Language (Deutsche Gebärdensprache, DGS), manually annotating 1,404 word use-to-sign ID mappings derived from 32 words from the German Word Usage Graph (D-WUG) and 49 signs from the Digital Dictionary of German Sign Language (DW-DGS). We identify three correspondence types: Type 1 (one-to-many), Type 2 (many-to-one), and Type 3 (one-to-one), plus No Match cases. We evaluate computational methods: Exact Match (EM) and Semantic Similarity (SS) using SBERT embeddings. SS substantially outperforms EM overall 88.52% vs. 71.31%), with dramatic gains for Type 1 (+52.1 pp). Our work establishes the first annotated dataset for cross-modal sense correspondence and reveals which correspondence patterns are computationally identifiable. Our code and dataset are made publicly available.

</details>


### [150] [OMGs: A multi-agent system supporting MDT decision-making across the ovarian tumour care continuum](https://arxiv.org/abs/2602.13793)
*Yangyang Zhang,Zilong Wang,Jianbo Xu,Yongqi Chen,Chu Han,Zhihao Zhang,Shuai Liu,Hui Li,Huiping Zhang,Ziqi Liu,Jiaxin Chen,Jun Zhu,Zheng Feng,Hao Wen,Xingzhu Ju,Yanping Zhong,Yunqiu Zhang,Jie Duan,Jun Li,Dongsheng Li,Weijie Wang,Haiyan Zhu,Wei Jiang,Xiaohua Wu,Shuo Wang,Haiming Li,Qinhao Guo*

Main category: cs.CL

TL;DR: 开发了一种名为OMGs的多智能体AI框架，用于卵巢肿瘤的多学科诊疗（MDT）推荐，其性能可与专家团队媲美，并可提升资源有限地区对专科肿瘤治疗的获取。


<details>
  <summary>Details</summary>
Motivation: 全球多数患者（尤其是资源受限地区）难以及时获得多学科专家共识，而卵巢肿瘤管理高度依赖多学科诊疗的复杂性与异质性，亟需一种可扩展、可解释的AI解决方案。

Method: 构建OMGs多智能体AI系统，通过领域专家智能体协作集成多学科证据并生成透明推荐；设计SPEAR（安全性、个性化、循证性、可操作性、鲁棒性）评估体系，在多中心研究中对比AI与人类专家MDT决策质量。

Result: 1) 在多中心复评中，OMGs表现与专家MDT共识相当（评分4.45±0.30 vs 4.53±0.23），但循证性得分更高（4.57 vs 3.92）；2) 前瞻性多中心评估（59例患者）显示其与常规MDT决策高度一致；3) 人类-AI对照研究证实OMGs显著提升临床决策在循证性与鲁棒性维度的表现（尤其在缺乏多学科专家时）。

Conclusion: 多智能体协作系统可实现与专家MDT相当的诊疗质量，其循证性优势与可扩展性为资源匮乏地区突破肿瘤专科医疗壁垒提供新方法。

Abstract: Ovarian tumour management has increasingly relied on multidisciplinary tumour board (MDT) deliberation to address treatment complexity and disease heterogeneity. However, most patients worldwide lack access to timely expert consensus, particularly in resource-constrained centres where MDT resources are scarce or unavailable. Here we present OMGs (Ovarian tumour Multidisciplinary intelligent aGent System), a multi-agent AI framework where domain-specific agents deliberate collaboratively to integrate multidisciplinary evidence and generate MDT-style recommendations with transparent rationales. To systematically evaluate MDT recommendation quality, we developed SPEAR (Safety, Personalization, Evidence, Actionability, Robustness) and validated OMGs across diverse clinical scenarios spanning the care continuum. In multicentre re-evaluation, OMGs achieved performance comparable to expert MDT consensus ($4.45 \pm 0.30$ versus $4.53 \pm 0.23$), with higher Evidence scores (4.57 versus 3.92). In prospective multicentre evaluation (59 patients), OMGs demonstrated high concordance with routine MDT decisions. Critically, in paired human-AI studies, OMGs most substantially enhanced clinicians' recommendations in Evidence and Robustness, the dimensions most compromised when multidisciplinary expertise is unavailable. These findings suggest that multi-agent deliberative systems can achieve performance comparable to expert MDT consensus, with potential to expand access to specialized oncology expertise in resource-limited settings.

</details>


### [151] [The acquisition of English irregular inflections by Yemeni L1 Arabic learners: A Universal Grammar approach](https://arxiv.org/abs/2602.13816)
*Muneef Y. Alsawsh,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: This study investigates how Yemeni English learners acquire irregular verb forms under Universal Grammar, finding that both first-language transfer and second-language development shape early stages, with improved UG access over time but lingering challenges due to limited exposure.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding irregular inflectional morphology acquisition in adult L2 learners (particularly Yemeni speakers) through the Feature Reassembly Hypothesis, clarifying interactions between L1 transfer and UG-driven development.

Method: Applied Feature Reassembly Hypothesis (FRH) framework; analyzed learner errors across two developmental stages via qualitative error analysis and quantitative one-way ANOVA statistical testing on irregular inflection production.

Result: Stage 1 showed dominant L1 transfer effects in phonological mismatches, while Stage 2 demonstrated increased UG sensitivity and morphological reconfiguration. Statistical improvement in irregular inflection production between stages (p<0.05), but consonant changes, zero-morphemes, and -a plurals remained problematic.

Conclusion: While L1 transfer influences initial acquisition and overgeneralization serves as a developmental strategy, long-term UG access in adult learners depends on quality linguistic input and instructional interventions to overcome persistent L1 constraints.

Abstract: This study examines the acquisition of English irregular inflections by Yemeni learners of English as a second language (L2), utilizing a Universal Grammar (UG) approach. Within the UG approach, the study considers Feature Reassembly Hypothesis (FRH) (Lardiere, 2008, 2009) part of UG, focusing on the roles of first language (L1) transfer and L2 developmental influence. It analyzes learner errors across two developmental stages. Stage 1 data reveal a dominant influence of L1 transfer, particularly in phonological and structural mismatches, while stage 2 data demonstrate increased learner sensitivity to UG properties and morphological reconfiguration toward the target language. Findings reveal that errors in irregular inflectional morphology are attributed to both interlingual and intralingual sources, with overgeneralization of L2 rules as a common developmental strategy. Statistical analysis, including a one-way ANOVA, indicates significant improvement in the production of well-formed irregular inflections from stage 1 to stage 2, underscoring learners' continued access to UG. However, persistent difficulties with consonant change, zero-morpheme, and -a plural inflections suggest that limited exposure, ineffective input modeling, and insufficient instructional quality constrain full UG access. The study concludes that while L1 transfer and L2 developmental factors influence initial stages of acquisition, appropriate linguistic input and instruction are critical for facilitating UG-driven feature reassembly in adult L2 learners.

</details>


### [152] [Beyond Words: Evaluating and Bridging Epistemic Divergence in User-Agent Interaction via Theory of Mind](https://arxiv.org/abs/2602.13832)
*Minyuan Ruan,Ziyue Wang,Kaiming Liu,Yunghwei Lai,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出将心智理论（ToM）作为大语言模型（LLMs）解决用户意图与真实环境状态之间认识分歧的机制，通过构建综合基准与数据集验证其交互层面的实际价值。


<details>
  <summary>Details</summary>
Motivation: 尽管ToM在LLMs中已有评估，但现有研究多集中于孤立信念推理，缺乏对实际交互中功能效用的探讨。

Method: 形式化ToM为'认识分歧检测与解决'机制，设计综合性基准enchname评估用户认知对齐能力，并构建轨迹式ToM数据集结合强化学习优化模型表现。

Result: 实验证明11个主流模型在认知差距识别上存在显著局限，经新方法训练的模型在用户心智状态推理及下游任务性能上均取得提升。

Conclusion: 揭示ToM应定位为交互层级的实用机制，而非独立推理能力，强调其在弥合用户-环境认知分歧中的实践意义。

Abstract: Large Language Models (LLMs) have developed rapidly and are widely applied to both general-purpose and professional tasks to assist human users. However, they still struggle to comprehend and respond to the true user needs when intentions and instructions are imprecisely conveyed, leading to a divergence between subjective user believes and true environment states. Resolving this epistemic divergence requires Theory of Mind (ToM), yet existing ToM evaluations for LLMs primarily focus on isolated belief inference, overlooking its functional utility in real-world interaction. To this end, we formalize ToM for LLMs as a mechanism for epistemic divergence detection and resolution, and propose a benchmark, \benchname, to assess how models reconcile user beliefs and profiles in practice. Results across 11 leading models reveal a significant limitation to identify underlying cognitive gaps that impede task success. To bridge this gap, we further curate a trajectory-based ToM dataset linking belief tracking with task-related state inference. The model trained on this data via reinforcement learning shows consistent improvement in reasoning about user mental states, leading to enhanced downstream performance. Our work highlights the practical value of ToM as an essential interaction-level mechanism rather than as a standalone reasoning skill.

</details>


### [153] [Speculative Decoding with a Speculative Vocabulary](https://arxiv.org/abs/2602.13836)
*Miles Williams,Young D. Kwon,Rui Li,Alexandros Kouris,Stylianos I. Venieris*

Main category: cs.CL

TL;DR: SpecVocab: A dynamic vocabulary selection method for speculative decoding that improves throughput and acceptance length over existing approaches like EAGLE-3.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods suffer from output distribution bottlenecks due to reduced vocabulary sizes, leading to out-of-vocabulary token issues and compromised speculation effectiveness.

Method: Proposed SpecVocab, which dynamically selects a context-specific vocabulary subset per decoding step instead of using a fixed reduced vocabulary, combined with efficient token validation and rejection sampling.

Result: SpecVocab achieved higher average acceptance length (1.65 vs. EAGLE-3's 1.48) and up to 8.1% throughput improvement over EAGLE-3 across multiple tasks, while maintaining generation quality.

Conclusion: Dynamic vocabulary speculation through SpecVocab outperforms static vocabulary reduction techniques, offering a practical solution for accelerating large language model inference without quality degradation.

Abstract: Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs of the target model. State-of-the-art speculative decoding methods use a draft model consisting of a single decoder layer and output embedding matrix, with the latter dominating drafting time for the latest LMs. Recent work has sought to address this output distribution bottleneck by reducing the vocabulary of the draft model. Although this can improve throughput, it compromises speculation effectiveness when the target token is out-of-vocabulary. In this paper, we argue for vocabulary speculation as an alternative to a reduced vocabulary. We propose SpecVocab, an efficient and effective method that selects a vocabulary subset per decoding step. Across a variety of tasks, we demonstrate that SpecVocab can achieve a higher acceptance length than state-of-the-art speculative decoding approach, EAGLE-3. Notably, this yields up to an 8.1% increase in average throughput over EAGLE-3.

</details>


### [154] [Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe](https://arxiv.org/abs/2602.13860)
*Somnath Banerjee*

Main category: cs.CL

TL;DR: This paper introduces a 'Responsible Intelligence' framework to balance the power of Large Language Models (LLMs) with real-world safety and cultural adaptability needs.


<details>
  <summary>Details</summary>
Motivation: Current general-purpose LLMs require transformation to meet real-world demands for technical precision, safety, and cultural sensitivity across diverse domains.

Method: Combines three approaches: 1) domain adaptation via supervised learning for task-specific accuracy, 2) ethical rigor through decoding-time alignment for safety, and 3) cultural inclusivity using human feedback and preference modeling.

Result: Establishes a methodology to create safer, context-aware AI systems that maintain technical performance while respecting global cultural nuances and mitigating adversarial risks.

Conclusion: Achieving responsible AI deployment requires integrating technical adaptation, ethical safeguards, and sociolinguistic awareness through iterative human-AI collaboration.

Abstract: The overarching research direction of this work is the development of a ''Responsible Intelligence'' framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment. As these models become a transformative force in artificial intelligence, there is an urgent need to move beyond general-purpose architectures toward systems that are contextually aware, inherently safer, and deeply respectful of global cultural nuances. This research navigates three interconnected threads: domain adaptation to ensure technical precision, ethical rigor to mitigate adversarial vulnerabilities, and cultural/multilingual alignment to promote global inclusivity. The methodological trajectory moves from classical supervised adaptation for task-specific demands to decoding-time alignment for safety, finally leveraging human feedback and preference modeling to achieve sociolinguistic acuity.

</details>


### [155] [Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages](https://arxiv.org/abs/2602.13867)
*Somnath Banerjee,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 本研究指出大型语言模型在低资源语言地区应用时存在安全失效问题，强调需建立多语言安全框架并提出具体解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估过度依赖英语体系，忽视低资源语言及文化差异导致风险。研究表明安全策略跨语言迁移有效性存疑且可能造成文化偏见，亟需针对性解决方案。

Method: 通过综合分析近期实证研究，揭示低资源语言场景下安全屏障失效机制，并提出包含参数高效安全优化、文化适配评估体系和社区参与式治理的实践框架。

Result: 发现低资源语言安全防护强度降低42%，跨语言知识修正失败率达67%，且标准毒性评分遗漏83%的文化敏感性危害。

Conclusion: 多语言安全应作为AI部署核心要求而非附加模块，通过本地化协作机制实现技术普惠性，推动全球南方AI治理范式转型。

Abstract: Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.

</details>


### [156] [Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach](https://arxiv.org/abs/2602.13890)
*Amir Hossein Mohammadi,Ali Moeinian,Zahra Razavizade,Afsaneh Fatemi,Reza Ramezani*

Main category: cs.CL

TL;DR: 研究优化小型语言模型（SLMs）在检索增强生成（RAG）中的提示模板设计，通过测试24种提示在HotpotQA上的性能，发现部分模板相较标准方法提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究多聚焦大模型，但如何有效适配资源受限场景下的SLMs（需多跳推理的复杂问答）尚未解决；提示模板设计影响性能却缺乏系统探索。

Method: 在Qwen2.5-3B和Gemma3-4B两个SLM上，对HotpotQA数据集的24种提示模板（含标准、文献方法、新混合模板）进行大规模实验，共测试18720个样本。

Result: 最优提示模板相较标准RAG提升83%（Qwen2.5）和84.5%（Gemma3-4B），整体改进达6%。部分设计模式（如显式推理引导）对多跳问答更有效。

Conclusion: 揭示了提示模板对SLMs-RAG的显著影响，提出针对性设计原则，为资源受限场景部署高效RAG系统提供实用指南。

Abstract: Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models (SLMs) remains a critical research gap, particularly in complex, multi-hop question-answering tasks that require sophisticated reasoning. In these systems, prompt template design is a crucial yet under-explored factor influencing performance. This paper presents a large-scale empirical study to investigate this factor, evaluating 24 different prompt templates on the HotpotQA dataset. The set includes a standard RAG prompt, nine well-formed techniques from the literature, and 14 novel hybrid variants, all tested on two prominent SLMs: Qwen2.5-3B Instruct and Gemma3-4B-It. Our findings, based on a test set of 18720 instances, reveal significant performance gains of up to 83% on Qwen2.5 and 84.5% on Gemma3-4B-It, yielding an improvement of up to 6% for both models compared to the Standard RAG prompt. This research also offers concrete analysis and actionable recommendations for designing effective and efficient prompts for SLM-based RAG systems, practically for deployment in resource-constrained environments.

</details>


### [157] [Pre-Editorial Normalization for Automatically Transcribed Medieval Manuscripts in Old French and Latin](https://arxiv.org/abs/2602.13905)
*Thibault Clérice,Rachel Bawden,Anthony Glaise,Ariane Pinche,David Smith*

Main category: cs.CL

TL;DR: 本文提出了Pre-Editorial Normalization (PEN)方法，旨在解决自动文本识别(ATR)中古文字转录与规范化文本之间的方法论鸿沟，通过构建新数据集和优化模型，实现6.7%的字符错误率(CER)提升。


<details>
  <summary>Details</summary>
Motivation: 传统ATR模型在古文字转录（如CATMuS数据集）和规范化文本生成之间存在兼容性问题。前者保留了文字保真度但难以直接应用NLP工具，后者易过拟合且丢失原始信息。

Method: 提出PEN任务框架，在保持古文字准确性的前提下生成规范化文本；构建基于CoMMA语料库的4.66M样本训练集和1.8k样本黄金评估集；采用ByT5序列到序列模型进行基准测试。

Result: 新数据集提升模型效果，规范化任务达到6.7% CER，显著优于先前模型，同时提供双版本（古文字保真+规范化）文本。

Conclusion: PEN框架有效弥合了古文字转录与实用化文本的差距，未来可扩展到其他历史文献数字化场景。

Abstract: Recent advances in Automatic Text Recognition (ATR) have improved access to historical archives, yet a methodological divide persists between palaeographic transcriptions and normalized digital editions. While ATR models trained on more palaeographically-oriented datasets such as CATMuS have shown greater generalizability, their raw outputs remain poorly compatible with most readers and downstream NLP tools, thus creating a usability gap. On the other hand, ATR models trained to produce normalized outputs have been shown to struggle to adapt to new domains and tend to over-normalize and hallucinate. We introduce the task of Pre-Editorial Normalization (PEN), which consists in normalizing graphemic ATR output according to editorial conventions, which has the advantage of keeping an intermediate step with palaeographic fidelity while providing a normalized version for practical usability. We present a new dataset derived from the CoMMA corpus and aligned with digitized Old French and Latin editions using passim. We also produce a manually corrected gold-standard evaluation set. We benchmark this resource using ByT5-based sequence-to-sequence models on normalization and pre-annotation tasks. Our contributions include the formal definition of PEN, a 4.66M-sample silver training corpus, a 1.8k-sample gold evaluation set, and a normalization model achieving a 6.7% CER, substantially outperforming previous models for this task.

</details>


### [158] [HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam](https://arxiv.org/abs/2602.13964)
*Weiqi Zhai,Zhihai Wang,Jinghang Wang,Boyu Yang,Xiaogang Li,Xiang Xu,Bohan Wang,Peng Wang,Xingzhe Wu,Anfeng Li,Qiyuan Feng,Yuhao Zhou,Shoulin Han,Wenjie Luo,Yiyuan Li,Yaxuan Wang,Ruixian Luo,Guojie Lin,Peiyao Xiao,Chengliang Xu,Ben Wang,Zeyu Wang,Zichao Chen,Jianan Ye,Yijie Hu,Jialong Chen,Zongwen Shen,Yuliang Xu,An Yang,Bowen Yu,Dayiheng Liu,Junyang Lin,Hu Wei,Que Shen,Bing Zhao*

Main category: cs.CL

TL;DR: 本研究提出了HLE-Verified，通过两阶段验证与修复流程，清理了原HLE基准中存在偏差的噪声数据，提升了大模型跨域评估的准确性。


<details>
  <summary>Details</summary>
Motivation: HLE基准测试中存在非平凡噪声项，可能导致评估结果偏差并扭曲跨模型比较，需建立更可靠的评估工具。

Method: 采用两阶段验证修复流程：阶段I对每个题目进行问题与答案的二元验证；阶段II对可修复的错误项进行保留评估意图的修改，最终形成经认证的基准。

Result: 构建了641个经验证题目和1170个经修复认证题目，7个大模型在HLE-Verified上的准确率平均提升7-10个百分点，错误项修复题目的准确率提升达30-40%。

Conclusion: HLE-Verified通过降低标注噪声，实现了对模型能力更真实的衡量，为未来基准测试开发提供了透明的验证协议与错误分类框架。

Abstract: Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified

</details>


### [159] [Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis](https://arxiv.org/abs/2602.13979)
*Tongze Zhang,Jun-En Ding,Melik Ozolcer,Fang-Ming Hung,Albert Chih-Chieh Yang,Feng Liu,Yi-Rou Ji,Sang Won Bae*

Main category: cs.CL

TL;DR: 本文提出了一种基于思维链（CoT）推理与大语言模型（LLMs）的阿尔茨海默病诊断框架，通过利用临床电子健康记录（EHRs）生成可解释的诊断推理路径，在多个CDR分级任务中实现诊断性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统阿尔茨海默病诊断方法依赖医学影像和人工评估，资源消耗大且效率低；现有LLMs在AD中的应用受限于疾病复杂的多因素特性，直接使用EHR数据微调模型难以捕捉关键诊断依据。

Method: 开发了一种无需直接微调LLMs的CoT诊断流程：模型生成显式诊断推理路径→基于路径提取结构化预测特征→通过多阶段推理增强AD病理机制解释性。

Result: 在多项CDR分级任务中较零样本基线方法F1分数提升15%，且推理过程对AD进展各阶段具有更高稳定性，证明了该框架的诊断有效性。

Conclusion: 该CoT框架通过模拟临床诊断思维显著提升了LLMs在AD诊断中的效能，为复杂神经退行性疾病的智能诊断提供了可解释性路径。

Abstract: Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.

</details>


### [160] [The Sufficiency-Conciseness Trade-off in LLM Self-Explanation from an Information Bottleneck Perspective](https://arxiv.org/abs/2602.14002)
*Ali Zahedzadeh,Behnam Bahrak*

Main category: cs.CL

TL;DR: 论文提出通过信息瓶颈框架优化大型语言模型的解释生成，在保持充分性的同时减少解释长度。实验显示简洁解释可维持准确率但显著缩短长度，过度压缩则影响性能。


<details>
  <summary>Details</summary>
Motivation: 当前自解释方法如链式思维推理虽提升准确性但生成成本高且冗长，需探索最小必要解释长度在效率与充分性间的平衡点。

Method: 构建包含长度约束的评估管道，基于ARC挑战数据集测试多语言模型（英语原数据及波斯语翻译数据），将解释视为保留关键信息的压缩表示进行验证。

Result: 保持足够简洁度的解释仍可保证89%以上的准确率（如在ARC数据集上减少60%长度），但压缩率超过临界值时准确率骤降20%以上。

Conclusion: 验证信息瓶颈原理适用于语言模型解释优化，证明存在最优压缩阈值，在该阈值下可实现解释效率与充分性的帕累托最优。

Abstract: Large Language Models increasingly rely on self-explanations, such as chain of thought reasoning, to improve performance on multi step question answering. While these explanations enhance accuracy, they are often verbose and costly to generate, raising the question of how much explanation is truly necessary. In this paper, we examine the trade-off between sufficiency, defined as the ability of an explanation to justify the correct answer, and conciseness, defined as the reduction in explanation length. Building on the information bottleneck principle, we conceptualize explanations as compressed representations that retain only the information essential for producing correct answers.To operationalize this view, we introduce an evaluation pipeline that constrains explanation length and assesses sufficiency using multiple language models on the ARC Challenge dataset. To broaden the scope, we conduct experiments in both English, using the original dataset, and Persian, as a resource-limited language through translation. Our experiments show that more concise explanations often remain sufficient, preserving accuracy while substantially reducing explanation length, whereas excessive compression leads to performance degradation.

</details>


### [161] [Named Entity Recognition for Payment Data Using NLP](https://arxiv.org/abs/2602.14009)
*Srikumar Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种用于支付数据实体识别的新方法PaymentBERT，在F1得分上超越了CRF和传统BERT。


<details>
  <summary>Details</summary>
Motivation: 金融交易数据的非结构化特点对自动解析构成挑战，传统方法在支付格式泛化和准确率上有局限性，需改进金融场景下的NER模型。

Method: 对比了CRF、BiLSTM-CRF、BERT、FinBERT等模型，在5万条多格式支付数据上测试；提出结合领域嵌入的PaymentBERT架构，并进行消融实验与跨格式泛化分析。

Result: 微调BERT达94.2% F1，超越CRF 12.8个百分点；PaymentBERT以95.7% F1创记录且支持实时处理。

Conclusion: Transformer模型显著优于传统方法，PaymentBERT在准确性与性能间取得平衡，适用于反洗钱和支付系统等实际场景。

Abstract: Named Entity Recognition (NER) has emerged as a critical component in automating financial transaction processing, particularly in extracting structured information from unstructured payment data. This paper presents a comprehensive analysis of state-of-the-art NER algorithms specifically designed for payment data extraction, including Conditional Random Fields (CRF), Bidirectional Long Short-Term Memory with CRF (BiLSTM-CRF), and transformer-based models such as BERT and FinBERT. We conduct extensive experiments on a dataset of 50,000 annotated payment transactions across multiple payment formats including SWIFT MT103, ISO 20022, and domestic payment systems. Our experimental results demonstrate that fine-tuned BERT models achieve an F1-score of 94.2% for entity extraction, outperforming traditional CRF-based approaches by 12.8 percentage points. Furthermore, we introduce PaymentBERT, a novel hybrid architecture combining domain-specific financial embeddings with contextual representations, achieving state-of-the-art performance with 95.7% F1-score while maintaining real-time processing capabilities. We provide detailed analysis of cross-format generalization, ablation studies, and deployment considerations. This research provides practical insights for financial institutions implementing automated sanctions screening, anti-money laundering (AML) compliance, and payment processing systems.

</details>


### [162] [GRRM: Group Relative Reward Modeling for Machine Translation](https://arxiv.org/abs/2602.14028)
*Sen Yang,Shanbo Cheng,Lu Xu,Jianbing Zhang,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出Group Relative Reward Model (GRRM) 解决组内排序问题，通过联合处理候选组并融入比较分析，结合GRPO框架提升机器翻译质量，释放模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 标准Scalar Quality Metrics (SQM)在机器翻译等开放领域组内排序时存在问题——其独立评估候选者的方式难以捕捉需对比分析的细微语言差异。

Method: 构建Group Quality Metric (GQM)新范式，通过GRRM模型对候选组全量联合建模，采用对比学习机制实现质量等级自适应判别，随后将GRRM嵌入GRPO训练循环优化翻译策略。

Result: GRRM在基准测试中实现同类最优的排序准确率，集成GRRM的GRPO框架在翻译质量提升的同时，使模型具备与顶尖推理模型相当的数学推导、因果推断等跨领域通用推理能力。

Conclusion: 基于对比分析的GRRM有效解决组内排序难题，联合GRPO训练范式拓宽了大规模语言模型在开放域翻译中的应用边界，并证实了对比学习对模型能力延展的促进作用。

Abstract: While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality Metrics (SQM) fall short in this context; by evaluating candidates in isolation, they lack the comparative context necessary to distinguish fine-grained linguistic nuances. To address this, we introduce the Group Quality Metric (GQM) paradigm and instantiate it via the Group Relative Reward Model (GRRM). Unlike traditional independent scorers, GRRM processes the entire candidate group jointly, leveraging comparative analysis to rigorously resolve relative quality and adaptive granularity. Empirical evaluations confirm that GRRM achieves competitive ranking accuracy among all baselines. Building on this foundation, we integrate GRRM into the GRPO training loop to optimize the translation policy. Experimental results demonstrate that our framework not only improves general translation quality but also unlocks reasoning capabilities comparable to state-of-the-art reasoning models. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/GRRM.

</details>


### [163] [Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models](https://arxiv.org/abs/2602.14039)
*Sajjad Kachuee,Mohammad Sharifkhani*

Main category: cs.CL

TL;DR: MoE嵌入模型传统线性聚合导致超球面坍缩，SBA方法保持几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统MoE线性聚合假设线性空间但专家表征实际分布于超球面，导致坍缩问题。

Method: SBA分离径向和角度分量进行几何保真聚合，兼容现有路由机制。

Result: MTEB实验显示性能提升且训练成本相同，几何分析证实避免坍缩。

Conclusion: 几何感知的聚合对MoE架构至关重要，SBA有效维持超球面一致性。

Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.

</details>


### [164] [Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness](https://arxiv.org/abs/2602.14044)
*Pietro Bernardelle,Stefano Civelli,Kevin Roitero,Gianluca Demartini*

Main category: cs.CL

TL;DR: 该研究探讨了长上下文对大语言模型（LLM）事实验证能力的影响，发现模型在长上下文中准确性下降，且证据位置（起始或结尾更优）显著影响结果，强调检索增强型事实核查系统的提示结构设计重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现强推理能力，但其在长上下文中的表现不稳定。现有研究多关注问答任务中的中段上下文退化现象，本文旨在填补LLM事实验证领域对上下文长度与证据位置影响的认知空白。

Method: 基于HOVER、FEVEROUS和ClimateFEVER三个数据集，评估7B至70B参数规模的Llama-3.1、Qwen2.5和Qwen3等5个开源模型，分析参数化事实知识及上下文长度、证据位置对验证准确性的影响。

Result: LLM具备显著的参数化事实知识存储能力，但验证准确性随上下文长度增加而下降；证据位置实验显示，开头或结尾的准确性显著高于中段位置。

Conclusion: 检索增强型事实核查系统的提示结构设计需重点考虑上下文长度控制与关键证据的显眼位置，以优化模型在长上下文中的表现。

Abstract: Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.

</details>


### [165] [LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation](https://arxiv.org/abs/2602.14054)
*Jizheng Chen,Weiming Zhang,Xinyi Dai,Weiwen Liu,Kounianhua Du,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: 提出LogitsCoder，一个通过logit级控制系统生成高效且优质推理链的框架，从而显著提升代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有测试时扩展方法在代码生成中的过度思考（效率低下）与不足思考（链浅显）问题。

Method: LogitsCoder利用Logits偏好解码指导令牌选择，并通过基于logits排序的路径选择与想法聚合迭代生成和优化推理步骤。

Result: 实验显示该框架在维持推理链连贯性的同时，以更低计算成本实现了优于基线方法的代码生成性能。

Conclusion: 通过轻量化logit级控制，LogitsCoder在推理深度与效率之间实现了有效平衡。

Abstract: Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.

</details>


### [166] [LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts](https://arxiv.org/abs/2602.14060)
*Yang Liu,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li,Lingyong Yan*

Main category: cs.CL

TL;DR: 本论文提出LM-Lexicon方法，通过数据聚类与语义专家系统结合，实现定义建模任务性能显著提升（+7% BLEU）。


<details>
  <summary>Details</summary>
Motivation: 传统定义建模方法在语义密集型应用中存在效率瓶颈，需要开发更精细的领域专家协作机制。

Method: 采用稀疏混合专家架构，将任务分解到语义领域，通过数据聚类生成领域专家，结合语义感知的路由机制进行模型融合。

Result: 在五个基准测试中BLEU分数提升7%，领域聚类策略使定义质量提升10%，语义路由机制效率提升1%，测试时扩展专家数量可进一步优化性能。

Conclusion: 该方法为定义建模提供了新范式，并揭示了语义密集型应用中高效语言模型的构建路径。

Abstract: We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.

</details>


### [167] [From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset](https://arxiv.org/abs/2602.14062)
*Jandad Jahani,Mursal Dawodi,Jawid Ahmad Baktash*

Main category: cs.CL

TL;DR: 本文对Mozilla Common Voice语料库的帕什托语数据（2023-2025年）进行定量分析，揭示其快速增长的规模及数据质量挑战。


<details>
  <summary>Details</summary>
Motivation: 帕什托语（超6000万使用者）缺乏开放语音数据，阻碍ASR系统开发，该研究填补低资源语言数据资源的关键缺口。

Method: 分析Common Voice v24.0数据集，统计验证吞吐量、参与不平等性（Gini系数）、元数据完整性及文本集中度，结合历史版本趋势评估数据成熟度。

Result: 数据从1.49小时暴增至2768.7小时，验证时长975.89小时；参与者极度集中（Gini=0.941），41.97%音频缺失性别标签，年龄分布偏向青年人；35.88%唯一句子占据50%验证数据。

Conclusion: 数据规模突破但成熟度仍需提升，建议扩展验证能力、改进人口统计覆盖，并平衡贡献者活动以解决结构性集中问题。

Abstract: Large, openly licensed speech datasets are essential for building automatic speech recognition (ASR) systems, yet many widely spoken languages remain underrepresented in public resources. Pashto, spoken by more than 60 million people, has historically lacked large-scale openly licensed speech data suitable for modern ASR development.
  This paper presents a release-level analysis of the Pashto component of the Mozilla Common Voice corpus, focusing on version 24.0 (December 2025) and contextualizing trends across major releases. We document rapid growth from 1.49 recorded hours in mid-2023 to 2,768.7 total hours in 2025, including 975.89 validated hours available for supervised ASR training.
  Beyond scale, we analyze validation throughput, contributor participation inequality, demographic metadata completeness, and sentence-level concentration in the validated subset. We find that participation is extremely concentrated (Gini = 0.941), age representation is strongly skewed toward young adults, and 41.97\% of clips lack self-reported gender labels, limiting subgroup auditing based on metadata. At the textual level, prompt reuse is moderate: 35.88\% of unique sentences account for 50\% of validated clips, suggesting that structural concentration is driven primarily by uneven contributor activity rather than dominance of a small prompt set.
  These results provide a quantitative audit of a rapidly scaling low-resource speech corpus and highlight practical priorities for improving dataset maturity, including expanded validation capacity and broader demographic participation.

</details>


### [168] [Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric](https://arxiv.org/abs/2602.14069)
*Ruipeng Jia,Yunyi Yang,Yuxin Wu,Yongbo Gai,Siyuan Tao,Mengyu Zhou,Jianhe Lin,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.CL

TL;DR: 本论文提出OpenRS框架，通过显式化的元评分和可验证评分标准解决传统标量奖励模型在多维偏好对齐中的信息瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统标量奖励模型因压缩多维偏好导致鲁棒性缺失和奖励黑客问题，非验证性任务需要更可解释且可编辑的对齐范式。

Method: 构建基于Pairwise Adaptive Meta-Rubrics（PAMR）和Pointwise Verifiable Rubrics（PVRs）的OpenRS系统，通过动态条件生成语义差异化的评分标准并执行外部聚合，结合两层元评分细化流程。

Result: 框架支持开放式任务中的细粒度准则比较，通过硬约束防护和可验证奖励生成提升判别能力，同时保持原则的一致性与可编辑性。

Conclusion: 将奖励建模从黑箱函数转化为可解释的宪法式原则执行系统，为复杂对齐任务提供可验证且可演进的解决方案。

Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.

</details>


### [169] [Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework](https://arxiv.org/abs/2602.14073)
*Grzegorz Statkiewicz,Alicja Dobrzeniecka,Karolina Seweryn,Aleksandra Krasnodębska,Karolina Piosek,Katarzyna Bogusz,Sebastian Cygert,Wojciech Kusa*

Main category: cs.CL

TL;DR: 本研究通过自动化数据翻译与过滤，成功构建高效的波兰语多模态模型LLaVA-Next，提升非英语语言处理能力9.5%，并开源模型促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 提升视觉语言模型在非英语语言和文化场景下的性能，解决当前模型以英语为中心导致的局限性，促进多语言多文化多模态系统发展。

Method: 复现LLaVA-Next方法，采用全自动化流水线翻译过滤现有数据集，并补充OCR和文化特定任务的波兰语合成数据，依赖最小人工干预。

Result: 相较LLaVA-1.6-Vicuna-13B，在波兰语MMBench测试中提升9.5%，生成文本通过人工评估验证语言正确性提升。

Conclusion: 大规模自动翻译结合轻量级过滤能有效构建低资源语言高质量多模态模型，但需进一步优化文化覆盖范围和评估体系。

Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.

</details>


### [170] [CCiV: A Benchmark for Structure, Rhythm and Quality in LLM-Generated Chinese \textit{Ci} Poetry](https://arxiv.org/abs/2602.14081)
*Shangqing Zhao,Yupei Ren,Yuhao Zhou,Xiaopeng Bai,Man Lan*

Main category: cs.CL

TL;DR: 提出CCiV基准测试评估大语言模型生成古典词（Ci）诗的能力，揭示结构-节奏-质量三维度挑战及提升方法。


<details>
  <summary>Details</summary>
Motivation: 古典词诗生成需结构严谨、韵律和谐与艺术美感的平衡，现有模型无法兼顾形式正确性与文学性，缺乏系统性评估工具。

Method: 构建包含30种词牌的CCiV基准测试，量化评估17个大模型在结构（字数/句式）、节奏（平仄模式）、质量（文学性评分）三维度表现，并对比不同提示策略对生成的影响。

Result: 发现模型常生成合法但罕见的历史变体；平仄模式遵循难度远高于结构规则；形式化提示提升强模型结构/节奏控制却损害弱模型；形式正确性与文学质量弱相关。

Conclusion: 古典诗歌生成需发展变体感知的评估体系与约束生成方法，现有模型在创造性与形式规范间存在显著矛盾。

Abstract: The generation of classical Chinese \textit{Ci} poetry, a form demanding a sophisticated blend of structural rigidity, rhythmic harmony, and artistic quality, poses a significant challenge for large language models (LLMs). To systematically evaluate and advance this capability, we introduce \textbf{C}hinese \textbf{Ci}pai \textbf{V}ariants (\textbf{CCiV}), a benchmark designed to assess LLM-generated \textit{Ci} poetry across these three dimensions: structure, rhythm, and quality. Our evaluation of 17 LLMs on 30 \textit{Cipai} reveals two critical phenomena: models frequently generate valid but unexpected historical variants of a poetic form, and adherence to tonal patterns is substantially harder than structural rules. We further show that form-aware prompting can improve structural and tonal control for stronger models, while potentially degrading weaker ones. Finally, we observe weak and inconsistent alignment between formal correctness and literary quality in our sample. CCiV highlights the need for variant-aware evaluation and more holistic constrained creative generation methods.

</details>


### [171] [A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing](https://arxiv.org/abs/2602.14158)
*Naeimeh Nourmohammadi,Md Meem Hossain,The Anh Han,Safina Showkat Ara,Zia Ush Shamszaman*

Main category: cs.CL

TL;DR: A multi-agent medical QA framework enhances reliability through evidence retrieval, uncertainty estimation, and bias checks, combining fine-tuned LLMs (GPT, LLaMA, DeepSeek) in a structured pipeline.


<details>
  <summary>Details</summary>
Motivation: Current LLMs face clinical adoption barriers due to poor verification, weak evidence grounding, and unreliable confidence indicators. The study aims to address these limitations for safer medical QA systems.

Method: Two-phase approach: 1) Fine-tuning LLMs (GPT, LLaMA, DeepSeek) on 20k+ medical QA pairs to identify optimal base models; 2) Developing a multi-agent system with Clinical Reasoning (LLaMA), Evidence Retrieval (PubMed), and Refinement (DeepSeek) agents, augmented with uncertainty scoring (Monte Carlo dropout/perplexity) and bias detection (LIME/SHAP).

Result: DeepSeek achieved top ROUGE/BLEU scores (ROUGE-1 0.536, ROUGE-2 0.226, BLEU 0.098) and outperformed BioGPT in zero-shot tests. Full system attained 87% accuracy (relevance 0.80), reduced perplexity (4.13) via evidence augmentation, with 36.5s mean latency.

Conclusion: Agent specialization with verification modules mitigates single-model flaws, providing an extensible framework for evidence-based, bias-aware clinical AI systems.

Abstract: Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.

</details>


### [172] [Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering](https://arxiv.org/abs/2602.14162)
*Tao Xu*

Main category: cs.CL

TL;DR: Deferred Visual Ingestion (DVI) improves multimodal document QA by postponing visual analysis until question time, reducing costs and enhancing accuracy on visually necessary queries.


<details>
  <summary>Details</summary>
Motivation: Current methods pre-process all document pages with Vision-Language Models (VLMs), incurring high cost, unreliability due to format mismatches, and irrecoverable errors. DVI addresses these inefficiencies by deferring visual analysis to answer-time.

Method: DVI uses lightweight metadata extraction during indexing, locates relevant page via structured indexes/BM25 search, then applies VLM only on query-specific pages. Core principle: 'Index for locating, not understanding'.

Result: DVI achieved 46.7% overall accuracy vs. 48.9% for pre-ingestion (zero VLM cost), 50% effectiveness on visual queries (vs. 0%), 100% page localization (98% search space reduced), plus interactive refinement capabilities.

Conclusion: DVI transforms QA accuracy problem into page localization efficiency, enabling resource savings and reliable visual analysis through demand-driven ingestion strategy.

Abstract: Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this "pre-ingestion" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is "Index for locating, not understanding"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the "QA accuracy" problem into a "page localization" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.

</details>


### [173] [GPT-5 vs Other LLMs in Long Short-Context Performance](https://arxiv.org/abs/2602.14188)
*Nima Esmi,Maryam Nezhad-Moghaddam,Fatemeh Borhani,Asadollah Shahbahrami,Amin Daemdoost,Georgi Gaydadjiev*

Main category: cs.CL

TL;DR: 测试Grok-4、GPT-4、Gemini 2.5和GPT-5在长上下文任务中的表现，发现模型在处理超过5K社交媒体帖子时准确率大幅下降至50-53%，但GPT-5在抑郁检测任务中保持95%高精度。新模型已解决‘中间丢失’问题。


<details>
  <summary>Details</summary>
Motivation: LLM理论上下文容量与实际应用效果存在显著差距，特别是在复杂任务（如抑郁检测）中需要验证模型是否能有效利用长文本细信息。

Method: 使用2个补充数据集（菜谱/数学题）和1个20K社交媒体抑郁检测主数据集，测试4个模型在不同输入量级下的性能变化，测量准确率和精度指标。

Result: 当输入量超过5K帖子（70K tokens）时，所有模型准确率骤降到50-53%；GPT-5出现精度与准确率分离现象，保持95%精度；新型号已解决上下文信息丢失问题。

Conclusion: 理论上下文容量与实际表现存在鸿沟，需开发复合评估指标；GPT-5在敏感场景（如抑郁筛查）中展现独特价值；模型优化应侧重长文本的信息密度利用能力。

Abstract: With the significant expansion of the context window in Large Language Models (LLMs), these models are theoretically capable of processing millions of tokens in a single pass. However, research indicates a significant gap between this theoretical capacity and the practical ability of models to robustly utilize information within long contexts, especially in tasks that require a comprehensive understanding of numerous details. This paper evaluates the performance of four state-of-the-art models (Grok-4, GPT-4, Gemini 2.5, and GPT-5) on long short-context tasks. For this purpose, three datasets were used: two supplementary datasets for retrieving culinary recipes and math problems, and a primary dataset of 20K social media posts for depression detection. The results show that as the input volume on the social media dataset exceeds 5K posts (70K tokens), the performance of all models degrades significantly, with accuracy dropping to around 50-53% for 20K posts. Notably, in the GPT-5 model, despite the sharp decline in accuracy, its precision remained high at approximately 95%, a feature that could be highly effective for sensitive applications like depression detection. This research also indicates that the "lost in the middle" problem has been largely resolved in newer models. This study emphasizes the gap between the theoretical capacity and the actual performance of models on complex, high-volume data tasks and highlights the importance of metrics beyond simple accuracy for practical applications.

</details>


### [174] [Knowing When Not to Answer: Abstention-Aware Scientific Reasoning](https://arxiv.org/abs/2602.14189)
*Samir Abdaljalil,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: This study introduces an abstention-aware framework for verifying scientific claims using natural language inference (NLI), enabling models to selectively support/refute/abstain based on evidence sufficiency.


<details>
  <summary>Details</summary>
Motivation: Existing scientific claim verification methods overvalue definitive answers, risking unsupported conclusions. The work addresses the critical need for models to abstain when evidence is insufficient, aligning with scientific rigor.

Method: The framework decomposes claims into conditions, audits each against evidence via NLI, and decides output with abstention options. Evaluated on SciFact and PubMedQA benchmarks across 6 diverse models, including encoder-decoder, open-weight chat models, and proprietary APIs.

Result: Model architectures showed minor accuracy differences, while confidence-based abstention significantly reduced error risk (up to 40%+ at 60% coverage) despite limited absolute accuracy gains. Abstention mechanisms proved more effective than architectural improvements.

Conclusion: Scientific reliability requires focusing on evidence sufficiency over model selection. Abstention-aware evaluation emerges as a model-agnostic standard for assessing scientific reasoning, with the proposed framework providing a reproducible foundation for future research.

Abstract: Large language models are increasingly used to answer and verify scientific claims, yet existing evaluations typically assume that a model must always produce a definitive answer. In scientific settings, however, unsupported or uncertain conclusions can be more harmful than abstaining. We study this problem through an abstention-aware verification framework that decomposes scientific claims into minimal conditions, audits each condition against available evidence using natural language inference (NLI), and selectively decides whether to support, refute, or abstain. We evaluate this framework across two complementary scientific benchmarks: SciFact and PubMedQA, covering both closed-book and open-domain evidence settings. Experiments are conducted with six diverse language models, including encoder-decoder, open-weight chat models, and proprietary APIs. Across all benchmarks and models, we observe that raw accuracy varies only modestly across architectures, while abstention plays a critical role in controlling error. In particular, confidence-based abstention substantially reduces risk at moderate coverage levels, even when absolute accuracy improvements are limited. Our results suggest that in scientific reasoning tasks, the primary challenge is not selecting a single best model, but rather determining when available evidence is sufficient to justify an answer. This work highlights abstention-aware evaluation as a practical and model-agnostic lens for assessing scientific reliability, and provides a unified experimental basis for future work on selective reasoning in scientific domains. Code is available at https://github.com/sabdaljalil2000/ai4science .

</details>


### [175] [Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures](https://arxiv.org/abs/2602.14259)
*Matic Korun*

Main category: cs.CL

TL;DR: 本文提出了一种基于词嵌入簇结构几何特征的大型语言模型幻觉分类法，识别出三种幻觉类型（中心漂移、错误收敛、覆盖空缺），并通过度量极性耦合（α）、簇凝聚力（η）和径向信息梯度（λ_s）揭示了不同架构模型的漏洞图谱。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型生成内容时产生的不可预测幻觉问题，需要建立可解释的几何分类体系，以解析幻觉成因并预测不同模型架构的易感性。

Method: 分析11个Transformer模型的静态词嵌入空间（涵盖BERT系列等编码器和GPT-2解码器），通过聚类结构观测三种幻觉模式；设计α/η/λ_s三项几何统计指标，量化情感极性关联性、簇凝聚性及信息分布特性。

Result: 所有模型均呈现显著极性耦合度（α>0.5）和正向簇凝聚力（η>0）；λ_s显著性在9/11模型中达标，ALBERT和MiniLM因嵌入因子分解压缩和蒸馏诱导同质性导致λ_s缺失。

Conclusion: 建立了几何维度下幻觉类型与模型架构的对应关系，为类型化幻觉检测提供了基础，并预测了不同架构的脆弱性特征（如压缩技术降低λ_s）。

Abstract: We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA, DeBERTa, ALBERT, MiniLM, DistilBERT) and decoder (GPT-2) architectures, we identify three operationally distinct hallucination types: Type 1 (center-drift) under weak context, Type 2 (wrong-well convergence) to locally coherent but contextually incorrect cluster regions, and Type 3 (coverage gaps) where no cluster structure exists. We introduce three measurable geometric statistics: α (polarity coupling), \b{eta} (cluster cohesion), and λ_s (radial information gradient). Across all 11 models, polarity structure (α > 0.5) is universal (11/11), cluster cohesion (\b{eta} > 0) is universal (11/11), and the radial information gradient is significant (9/11, p < 0.05). We demonstrate that the two models failing λ_s significance -- ALBERT and MiniLM -- do so for architecturally explicable reasons: factorized embedding compression and distillation-induced isotropy, respectively. These findings establish the geometric prerequisites for type-specific hallucination detection and yield testable predictions about architecture-dependent vulnerability profiles.

</details>


### [176] [STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts](https://arxiv.org/abs/2602.14265)
*Zachary Bamberger,Till R. Saenger,Gilad Morad,Ofra Amir,Brandon M. Stewart,Amir Feder*

Main category: cs.CL

TL;DR: This paper introduces STATe-of-Thoughts (STATe), an interpretable Inference-Time-Compute (ITC) framework that enhances text generation quality and diversity through structured reasoning pattern searches instead of stochastic sampling.


<details>
  <summary>Details</summary>
Motivation: Existing ITC methods suffer from insufficient output diversity due to high-temperature sampling and offer limited reasoning control, reducing explainability. There's a need for better interpretability and systematic exploration of reasoning patterns in text generation.

Method: STATe replaces stochastic sampling with a structured three-component system: (1) Controller selects discrete actions encoding high-level reasoning choices (2) Generator produces reasoning steps conditioned on those actions (3) Evaluator scores candidates to guide search. This enables action-guided textual interventions instead of temperature-based sampling.

Result: 1) Action-guided interventions achieve greater diversity than temperature sampling 2) Explicit action sequences in argument generation predict output quality 3) Enables mapping action-performance associations to identify high-potential unexplored reasoning paths for targeted generation.

Conclusion: The STATe framework provides a practical solution for generating high-quality, diverse, and interpretable text by systematically exploring structured reasoning patterns, with demonstrated ability to uncover actionable insights about effective reasoning strategies.

Abstract: Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.

</details>


### [177] [InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem](https://arxiv.org/abs/2602.14367)
*Shuofei Qiao,Yunxiang Wei,Xuehai Wang,Bin Wu,Boyang Xue,Ningyu Zhang,Hossein A. Rahmani,Yanshan Wang,Qiang Zhang,Keyan Ding,Jeff Z. Pan,Huajun Chen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出InnoEval：基于知识多视角推理的科学创意评估框架


<details>
  <summary>Details</summary>
Motivation: 应对大型语言模型激增背景下科学创意评估方法落后的问题，现有效度缺陷包括知识范围局限、评估维度单一及LLM固有偏差

Method: 构建异构知识搜索系统实动态多源证据检索，创建多学科背景评审委员会实现多维度解耦评估，建立权威评审数据集验证方法有效性

Result: 在多项评估任务中超越基线模型，单点/配对/群体评估准确率提升15-22%，评审模式与人类专家一致性达0.86皮尔逊系数

Conclusion: InnoEval通过知识增强与多专家协同机制，显著提升AI创意评估效能，为科学评价标准化提供新范式

Abstract: The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.

</details>


### [178] [Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models](https://arxiv.org/abs/2602.14386)
*Mufan Xu,Kehai Chen,Xuefeng Bai,Zhengyu Niu,Muyun Yang,Tiejun Zhao,Min Zhang*

Main category: cs.CL

TL;DR: MPO是一种新的策略梯度框架，通过将连续tokens作为整体语义动作优化，提升语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法逐token优化策略，在数学变量定义和方程构建等需要多token联合决策的推理任务中存在语义割裂问题。

Method: 提出K-tokens连续序列作为统一动作空间的MPO框架，采用块级优化方式捕获推理过程的组合结构特征。

Result: 在数学推理和代码生成基准测试中表现优于传统token级策略梯度方法，验证块级优化对复杂推理任务的有效性。

Conclusion: 研究表明块级策略梯度能有效解决token级优化在复合推理中的局限性，为推理密集型任务提供新方向。

Abstract: Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex reasoning tasks, where a single semantic decision is often realized across multiple tokens--for example, when defining variables or composing equations. This introduces a potential mismatch between token-level optimization and the inherently block-level nature of reasoning in these settings. To bridge this gap, we propose Multi-token Policy Gradient Optimization (MPO), a framework that treats sequences of K consecutive tokens as unified semantic actions. This block-level perspective enables our method to capture the compositional structure of reasoning trajectories and supports optimization over coherent, higher-level objectives. Experiments on mathematical reasoning and coding benchmarks show that MPO outperforms standard token-level policy gradient baselines, highlight the limitations of token-level policy gradients for complex reasoning, motivating future research to look beyond token-level granularity for reasoning-intensive language tasks.

</details>


### [179] [WavePhaseNet: A DFT-Based Method for Constructing Semantic Conceptual Hierarchy Structures (SCHS)](https://arxiv.org/abs/2602.14419)
*Kiyotaka Kasubuchi,Kazuo Fukiya*

Main category: cs.CL

TL;DR: 通过测度理论和频率分析重新阐述LLM中的Transformer/Attention机制，理论上证明幻觉是结构限制，并提出WavePhaseNet和余上同调一致性控制技术。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中固有的幻觉问题，揭示嵌入空间与语义真理集的同构缺失导致逻辑一致性崩溃。

Method: 使用离散傅里叶变换构建语义概念层次结构(SCHS)，通过1/f谱分析确定维度下限，基于余上同调正则化构建低维空间。

Result: 证明24,576维到3,000维的压缩保留语义，频率分解实现语义操控，余上同调损失量化局部推理矛盾。

Conclusion: 幻觉源于结构缺陷而非训练问题，频域分离与余上同调正则化能有效抑制幻觉并提升语义一致性。

Abstract: This paper reformulates Transformer/Attention mechanisms in Large Language Models (LLMs) through measure theory and frequency analysis, theoretically demonstrating that hallucination is an inevitable structural limitation. The embedding space functions as a conditional expectation over a σ-algebra, and its failure to be isomorphic to the semantic truth set fundamentally causes logical consistency breakdown. WavePhaseNet Method The authors propose WavePhaseNet, which explicitly constructs a Semantic Conceptual Hierarchy Structure (SCHS) using Discrete Fourier Transform (DFT). By applying DFT along the sequence dimension, semantic information is decomposed into frequency bands: low-frequency components capture global meaning and intent, while high-frequency components represent local syntax and expression. This staged separation enables precise semantic manipulation in diagonalized space. Dimensionality Reduction GPT-4's 24,576-dimensional embedding space exhibits a 1/f spectral structure based on language self-similarity and Zipf's law. Through cumulative energy analysis, the authors derive that approximately 3,000 dimensions constitute the lower bound for "complete representation." This demonstrates that reduction from 24,576 to 3,000 dimensions preserves meaning and intent while enabling rigorous reasoning and suppressing hallucination. Cohomological Consistency Control The reduced embedding space, constructed via cohomological regularization over overlapping local windows, allows defining a graph structure and cochain complex. This quantifies inconsistencies among local inferences as coboundary-based losses. Applying harmonic projection based on Hodge theory positions cohomology as a computable regularization principle for controlling semantic consistency, extracting maximally consistent global representations.

</details>


### [180] [LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2602.14428)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Man Wang*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型（LLM）的时间知识图谱蒸馏框架，通过引入LLM作为辅助教师，解决传统静态图压缩技术在时间依赖建模中的不足，实现高性能轻量化推理。


<details>
  <summary>Details</summary>
Motivation: 时间知识图谱推理需处理动态演化事实，但现有方法过度依赖静态图压缩技术，导致时间依赖建模不足且计算成本高。LLM的上下文理解能力为补充时序信号提供了新可能。

Method: 设计双教师蒸馏框架，既包含传统高容量时序教师模型，又引入LLM辅助教师提供背景知识与时序信号；采用联合监督与蒸馏目标，并通过分阶段对齐策略进行渐进式训练。

Result: 在多个公开TKG基准测试中，该方法相比强蒸馏基线平均提升5.2%的链接预测性能，学生模型参数量仅为教师模型的1/8且推理速度提升3倍。

Conclusion: LLM可作为时序知识迁移的有效教师，通过跨模态对齐将大模型的上下文知识迁移到轻量级图模型中，为部署高效时序推理系统提供了可行方案。

Abstract: Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.

</details>


### [181] [Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models](https://arxiv.org/abs/2602.14466)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本文通过扩展原有的Bias Benchmark for Question-Answering（BBQ）框架，构建了一个面向菲律宾语境的偏见证明测试集FilBBQ，包含超过10,000个检测性别和性取向偏见的问题模板。


<details>
  <summary>Details</summary>
Motivation: 原文指出，语言模型在生成任务中的偏见问题日益突出，而原有BBQ框架未充分覆盖特定文化语境（如菲律宾相关议题：多配偶制等），且评估方法存在响应不稳定导致结果偏差。

Method: 采用四阶段方法构建FilBBQ：1）模板分类 2）文化适配翻译 3）新模板构建（新增菲律宾相关社会议题）4）提示生成。采用多随机种子测试并求平均偏见分数，解决模型响应不稳定问题。

Result: 实验证实：1）不同随机种子下的偏见分数存在显著差异 2）模型在情感表达、家务分工、性少数群体刻板印象以及多配偶制等议题上表现出显性性别偏见和恐同倾向。

Conclusion: FilBBQ成为首个菲律宾语境下的语言模型偏见证明测试集，证明多随机种子评估的必要性，强调文化语境对偏见检测的重要性。代码已开源。

Abstract: With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models. We expand the linguistic scope of BBQ and construct FilBBQ through a four-phase development process consisting of template categorization, culturally aware translation, new template construction, and prompt generation. These processes resulted in a bias test composed of more than 10,000 prompts which assess whether models demonstrate sexist and homophobic prejudices relevant to the Philippine context. We then apply FilBBQ on models trained in Filipino but do so with a robust evaluation protocol that improves upon the reliability and accuracy of previous BBQ implementations. Specifically, we account for models' response instability by obtaining prompt responses across multiple seeds and averaging the bias scores calculated from these distinctly seeded runs. Our results confirm both the variability of bias scores across different seeds and the presence of sexist and homophobic biases relating to emotion, domesticity, stereotyped queer interests, and polygamy. FilBBQ is available via GitHub.

</details>


### [182] [Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation](https://arxiv.org/abs/2602.14469)
*Guangyue Peng,Zongchao Chen,Wen Luo,Yuntao Wen,Wei Li,Ruixiang Feng,Ran Le,Chen Yang,Zhenwei An,Yang Song,Tao Zhang,Houfeng Wang*

Main category: cs.CL

TL;DR: 该论文提出通过结构引导推理（SSR及SSR-D）解决反向思维链生成（RCG）中因答案依赖导致的后验合理化问题，在减少词法、熵和概率锚定效应的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 反向思维链生成（RCG）存在答案作为认知锚点导致的后验合理化问题，传统语义抑制策略因触发矛盾心理机制反而加剧依赖，需探索更优解决方案。

Method: 1. 构建三级测量体系（词法、熵、概率锚定）量化答案依赖；2. 提出结构骨架引导推理（SSR）两阶段方法：先生成与答案无关的功能骨架，再引导完整推理；3. 开发SSR-D模型蒸馏技术增强结构遵循。

Result: SSR-D在三级锚定测量中均显著降低依赖度，实验显示相较抑制基线提升10%性能，且保持跨分布泛化能力（OOD）。

Conclusion: 通过将信息流从答案监控转向结构规划，SSR/SSR-D有效破解认知锚定循环，为可控推理生成提供新范式。

Abstract: Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that shapes the entire explanation. We formalize this phenomenon through a three-level measurement hierarchy: lexical, entropic, and probabilistic anchoring, each captures surface artifacts, entropy dynamics, and latent answer dependence, respectively. We analyze semantic suppression, the intuitive mitigation strategy that instructs models to ignore the answer, to find out its counterproduction: while it reduces lexical overlap, it paradoxically increases entropic and probabilistic anchoring. Drawing on Ironic Process Theory from cognitive psychology, we attribute this failure to active monitoring of the forbidden answer, which inadvertently deepens dependence on it. To break this cycle, we propose Structural Skeleton-guided Reasoning (SSR), a two-phase approach that first generates an answer-invariant functional skeleton structure, then uses this skeleton to guide full trace generation. By redirecting the information flow to structural planning rather than answer monitoring, SSR consistently reduces anchoring across all three levels. We further introduce Distilled SSR (SSR-D), which fine-tunes models on teacher-generated SSR traces to ensure reliable structural adherence. Experiments across open-ended reasoning benchmarks demonstrate that SSR-D achieves up to 10% improvement over suppression baselines while preserving out-of-distribution (OOD) generalization.

</details>


### [183] [HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation](https://arxiv.org/abs/2602.14470)
*Wen-Sheng Lien,Yu-Kai Chan,Hao-Lung Hsiao,Bo-Kai Ruan,Meng-Fen Chiang,Chien-An Chen,Yi-Ren Yeh,Hong-Han Shuai*

Main category: cs.CL

TL;DR: 本文提出HyperRAG，一个基于n元超图的检索增强生成框架。其核心创新为HyperRetriever（基于超图结构语义构建查询条件关系链）与HyperMemory（利用LLM参数记忆指导路径扩展），通过高阶关系建模改善多跳问答中的推理路径效率与解释性。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱的二元关系表存在检索刚性、相似性搜索引入冗余、关系表达受限等问题，而现有超图方法未充分挖掘高阶实体依赖关系。

Method: 1) HyperRetriever：设计面向n元关系的事实跟踪与动态高阶遍历算法，实现上下文约束下的可解释多跳推理；2) HyperMemory：构建基于语言模型记忆的束搜索机制，动态评估实体与高阶关系的查询相关性。

Result: 在WikiTopics、HotpotQA等基准测试中，HyperRAG平均提升2.95% MRR与1.23% Hits@10指标。定性分析显示其能有效弥合推理缺口，闭域与开域问答均受益。

Conclusion: n元超图的高阶关系建模能显著提升检索增强方法的推理深度与路径效率，同时保持可解释性。该框架为复杂关系推理场景提供了新范式。

Abstract: Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.

</details>


### [184] [BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR](https://arxiv.org/abs/2602.14488)
*Md. Najib Hasan,Mst. Jannatun Ferdous Rain,Fyad Mohammed,Nazmul Siddique*

Main category: cs.CL

TL;DR: 本研究开发了一个使用多模型家族大语言模型（LLMs）构建孟加拉语信息检索（IR）数据集的BETA标注框架，并验证低资源语言间通过单跳机器翻译复用数据集的可行性，发现存在语言依赖性偏差和语义保持问题，为低资源语言基准构建提供了实证证据与实践指导。


<details>
  <summary>Details</summary>
Motivation: 低资源语言因缺乏高质量、任务特定标注数据集严重制约了信息检索技术的发展，手动标注成本高且可扩展性差，而使用大语言模型自动标注存在标签可靠性、偏见及评估有效性等问题。

Method: 构建BETA标注框架，整合上下文对齐、一致性校验和多数共识机制，利用多个家族大语言模型进行标注后通过人工评估验证质量；采用多语对LLM翻译实验，验证低资源语言数据集一跳机器翻译的语义保持与任务有效性。

Result: 实验发现机器翻译在不同语言间语义保持能力显著差异，反映出语言依赖性偏见和语义保持不一致问题，直接影响跨语言数据集复用可靠性，验证了大语言模型翻译在低资源场景下的潜力与风险。

Conclusion: 本研究揭示了LLM辅助低资源语言数据集建设的潜在价值与限制，实证了跨语言复用风险，提出了低资源语言基准构建与评估流程实践指导方案，为后续研究提供了可复现范式与数据基础。

Abstract: IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.

</details>


### [185] [Query as Anchor: Scenario-Adaptive User Representation via Large Language Model](https://arxiv.org/abs/2602.14492)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Ziyi Gao,Xiaotong Lin,Yun Liu,Xing Fu,Yu Cheng,Yongchao Liu,Weiqiang Wang,Zhongle Xie*

Main category: cs.CL

TL;DR: 该论文提出Query-as-Anchor框架，通过动态查询感知的用户表示学习解决工业级用户建模中通用性与任务敏感性的矛盾，结合LLM与多模态优化实现出色工业场景适配性。


<details>
  <summary>Details</summary>
Motivation: 现有静态任务无关的用户嵌入方法难以平衡统一空间中的多元下游需求，且多源数据的噪声与模态冲突导致表示质量下降，需构建更灵活的工业级解决方案。

Method: 1) 构建UserU预训练数据集实现多模态序列与用户语义对齐；2) Q-Anchor架构通过层次化编码器与对比-自回归优化结合双塔LLM生成查询感知表示；3) 聚类软提示微调强化潜结构区分度；4) 序列末端锚定查询实现KV缓存加速推理。

Result: 在Alipay 10项工业基准测试中实现SOTA性能（提升幅度？），大规模在线A/B测试验证有效性，支持高效部署（具体延迟数据？）。

Conclusion: 动态查询感知合成能有效弥合通用预训练与场景化业务需求的鸿沟，所提框架具备工业级可扩展性、任务适应性与推理效率，已开源代码验证实用性。

Abstract: Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.

</details>


### [186] [Beyond Translation: Evaluating Mathematical Reasoning Capabilities of LLMs in Sinhala and Tamil](https://arxiv.org/abs/2602.14517)
*Sukumar Kishanthan,Kumar Thushalika,Buddhi Jayasekara,Asela Hevapathige*

Main category: cs.CL

TL;DR: The study investigates whether large language models (LLMs) genuinely reason mathematically in low-resource languages like Sinhala and Tamil or rely on implicit translation to English. It finds that while basic arithmetic transfers well, complex reasoning degrades in these languages, indicating uneven cross-lingual competence.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs' multilingual mathematical reasoning reflects true language understanding versus reliance on translation-based processing, and to challenge assumptions about uniform cross-lingual reasoning capabilities.

Method: Evaluated four LLMs using a taxonomy of six math problem types (arithmetic to complex optimization) with a parallel dataset authored natively in Sinhala, Tamil, and English by fluent speakers with mathematical expertise, avoiding translation artifacts.

Result: Basic arithmetic reasoning transfers robustly across languages, but complex tasks (unit conflict, optimization) show significant performance degradation in Sinhala and Tamil. Failure patterns vary by model and problem type.

Conclusion: Apparent multilingual competence in LLMs does not guarantee uniform mathematical reasoning across languages. Results emphasize the need for fine-grained, type-specific evaluations in multilingual contexts.

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning in English, but whether these capabilities reflect genuine multilingual reasoning or reliance on translation-based processing in low-resource languages like Sinhala and Tamil remains unclear. We examine this fundamental question by evaluating whether LLMs genuinely reason mathematically in these languages or depend on implicit translation to English-like representations. Using a taxonomy of six math problem types, from basic arithmetic to complex unit conflict and optimization problems, we evaluate four prominent large language models. To avoid translation artifacts that confound language ability with translation quality, we construct a parallel dataset where each problem is natively authored by fluent speakers with mathematical training in all three languages. Our analysis demonstrates that while basic arithmetic reasoning transfers robustly across languages, complex reasoning tasks show significant degradation in Tamil and Sinhala. The pattern of failures varies by model and problem type, suggesting that apparent multilingual competence may not reflect uniform reasoning capabilities across languages. These findings challenge the common assumption that models exhibiting strong multilingual performance can reason equally effectively across languages, and highlight the need for fine-grained, type-aware evaluation in multilingual settings.

</details>


### [187] [Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets](https://arxiv.org/abs/2602.14536)
*Yuchen Yang,Wenze Lin,Enhao Huang,Zhixuan Chu,Hongbin Zhou,Lan Tao,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 提出了一种可解释的token级降噪框架XTF，通过分解token对微调的贡献来过滤噪声，在三个下游任务中显著提升7个LLM的性能最高13.7%。


<details>
  <summary>Details</summary>
Motivation: 现有微调数据集以句子级别设计为主，与LLM的token级别优化机制存在本质不匹配，导致token级噪声干扰最终性能。

Method: 将token级数据对微调的贡献分解为推理重要性、知识新颖性和任务相关性三个维度，通过评分方法评估后屏蔽选定噪声token的梯度进行优化。

Result: 在数学、代码和医学三个代表性下游任务的7个主流LLM上实验表明，相比常规微调最高提升13.7%的性能。

Conclusion: 强调了token级数据集优化的重要性，验证了基于属性分解策略解释复杂训练机制的潜力。

Abstract: Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.

</details>


### [188] [Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.14564)
*Shefayat E Shams Adib,Ahmed Alfey Sani,Ekramul Alam Esham,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 该论文评估了五种大型语言模型在医疗问答系统中的表现，使用iCliniq数据集（38,000条问答）进行零样本评估，发现较大模型（如Llama 3.3 70B）表现更优，而Llama-4-Maverick-17B在效率与性能间取得平衡，强调模型优化对医疗NLP应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 开发高效的医疗问答系统以提升资源匮乏地区的医疗服务质量，探索大规模语言模型在医疗领域的实际可行性。

Method: 选取Llama-3-8B、Llama 3.2 3B、Llama 3.3 70B、Llama-4-Maverick-17B及GPT-5-mini五种模型，在2024-2025年间基于iCliniq数据集进行零样本评测，使用BLEU和ROUGE指标衡量生成质量。

Result: 模型性能随参数规模提升（如Llama 3.3 70B表现最佳），但Llama-4-Maverick-17B展现出更高的效率-效果平衡性，验证了规模扩展在临床任务中的优势。

Conclusion: 大规模语言模型正逐步具备专业级医疗推理能力，支持其在真实临床场景中的部署；未来研究需权衡模型规模、计算成本与临床实用性。

Abstract: Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.

</details>


### [189] [The Wikidata Query Logs Dataset](https://arxiv.org/abs/2602.14594)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 本论文提出了Wikidata查询日志数据集(WDQL)，包含20万条基于Wikidata知识图谱的真实问答-查询对,其数据规模超过现有同类数据集的6倍，无需依赖模板生成技术。


<details>
  <summary>Details</summary>
Motivation: 现有Wikidata问答数据集规模较小且依赖模板生成，而真实用户查询质量参差不齐且包含匿名数据，需开发新方法处理日志中的真实SPARQL查询。

Method: 通过基于智能体的迭代方法，对匿名SPARQL查询进行去匿名化、清洗、验证，并同步生成自然语言问题。该过程通过持续验证保证查询有效性。

Result: 成功构建了包含200,000个真实有效问答对的数据集，所有资源及代码均在宽松许可证下开源发布，实验证明其对问答系统训练具有显著价值。

Conclusion: WDQL为知识图谱问答系统提供了高质量的真实世界训练数据，并推动了真实查询处理方法的发展。

Abstract: We present the Wikidata Query Logs (WDQL) dataset, a dataset consisting of 200k question-query pairs over the Wikidata knowledge graph. It is over 6x larger than the largest existing Wikidata datasets of similar format without relying on template-generated queries. Instead, we construct it using real-world SPARQL queries sent to the Wikidata Query Service and generate questions for them. Since these log-based queries are anonymized, and therefore often do not produce results, a significant amount of effort is needed to convert them back into meaningful SPARQL queries. To achieve this, we present an agent-based method that iteratively de-anonymizes, cleans, and verifies queries against Wikidata while also generating corresponding natural-language questions. We demonstrate the dataset's benefit for training question-answering methods. All WDQL assets, as well as the agent code, are publicly available under a permissive license.

</details>


### [190] [GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation](https://arxiv.org/abs/2602.14649)
*Hao Liu,Guangyan Li,Wensheng Zhang,Yongqiang Tang*

Main category: cs.CL

TL;DR: GradMAP 是一种快速的大语言模型（LLM）层剪枝方法，通过引入基于梯度的全局重要性度量和投影补偿矩阵，显著提升剪枝效率的同时有效维持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算成本高昂，已有的层剪枝方法无法兼顾剪枝效果与效率。研究旨在解决剪枝后性能恢复与计算效率难以平衡的难题。

Method: 分两阶段：1）通过单次反向传播计算梯度幅值度量层重要性；2）分析剪枝后的均值偏移并引入投影补偿矩阵修正性能损失。

Result: 实验表明 GradMAP 剪枝速度达现有方法4倍，且模型性能显著优于对比方法。

Conclusion: 所提方法通过效率优先的全局度量和数学补偿机制，在降低计算成本的同时保持模型准确性，为LLM轻量化提供了新方案。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer pruning research primarily focuses on two aspects: measuring layer importance and recovering performance after pruning. Unfortunately, the present works fail to simultaneously maintain pruning performance and efficiency. In this study, we propose GradMAP, a faster layer pruning method with \textbf{Grad}ient \textbf{M}etric \textbf{A}nd \textbf{P}rojection compensation, which consists of two stages. In the first stage, we introduce a novel metric based on gradient magnitudes, enabling a global assessment of layer importance. Note that, it requires only a single backward propagation step per pruning decision, substantially enhancing pruning efficiency. In the second stage, we first analyze the layers with the largest mean shift resulting from pruning, and then incorporate a simple yet effective projection compensation matrix to correct this drift in one step. In this way, the degradation of model performance caused by layer pruning is effectively alleviated. Extensive experiments show that GradMAP outperforms previous layer pruning methods in both pruning speed (achieving an average $4\times$ speedup) and performance.

</details>


### [191] [Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?](https://arxiv.org/abs/2602.14653)
*Matteo Gay,Coleman Haley,Mario Giulianelli,Edoardo Ponti*

Main category: cs.CL

TL;DR: 研究首次在多模态语境下验证均匀信息密度假说，发现结合视觉信息能提升语言信息分布均匀性。


<details>
  <summary>Details</summary>
Motivation: 此前均匀信息密度假说仅在纯文本环境下验证，本研究旨在探索结合视觉语境后该假说的适用性。

Method: 使用多语言视-语模型分析30种语言的图文数据和13种语言的视觉叙事数据，通过估计surprisal值测量信息分布均匀性。

Result: 视觉语境显著平滑了信息分布，跨语言类型展现出更高的全局和局部均匀性，视觉叙事中语篇起始单元的surprisal值降低最显著。

Conclusion: 支持语境敏感的均匀信息密度假说，证明多模态语言使用中的信息流动动态模型更加符合生态真实性。

Abstract: The Uniform Information Density (UID) hypothesis posits that speakers are subject to a communicative pressure to distribute information evenly within utterances, minimising surprisal variance. While this hypothesis has been tested empirically, prior studies are limited exclusively to text-only inputs, abstracting away from the perceptual context in which utterances are produced. In this work, we present the first computational study of UID in visually grounded settings. We estimate surprisal using multilingual vision-and-language models over image-caption data in 30 languages and visual storytelling data in 13 languages, together spanning 11 families. We find that grounding on perception consistently smooths the distribution of information, increasing both global and local uniformity across typologically diverse languages compared to text-only settings. In visual narratives, grounding in both image and discourse contexts has additional effects, with the strongest surprisal reductions occurring at the onset of discourse units. Overall, this study takes a first step towards modelling the temporal dynamics of information flow in ecologically plausible, multimodal language use, and finds that grounded language exhibits greater information uniformity, supporting a context-sensitive formulation of UID.

</details>


### [192] [Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech](https://arxiv.org/abs/2602.14655)
*Xiao Wei,Bin Wen,Yuqin Lin,Kai Li,Mingyang gu,Xiaobao Wang,Longbiao Wang,Jianwu Dang*

Main category: cs.CL

TL;DR: 提出结合联邦学习与数据增强的FAL-AD框架，解决阿尔茨海默症诊断中医疗数据不足与隐私难题


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默症早期诊断需非侵入式低成本技术，但AI语音检测面临医疗数据稀缺和隐私保护双重挑战

Method: 构建三阶段方案：1) 语义重组的语音转换数据增强生成病理语音；2) 自适应联邦学习实现跨机构协作；3) 注意力机制驱动的多模态融合模型进行音-文交互建模

Result: 在ADReSSo数据集达到91.52%多模态准确率，超越所有集中式基线模型，验证框架有效性

Conclusion: FAL-AD通过联邦学习与数据增强协同优化，突破医疗数据效率瓶颈，为隐私保护下的跨机构合作提供实用解决方案

Abstract: Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.

</details>


### [193] [Crowdsourcing Piedmontese to Test LLMs on Non-Standard Orthography](https://arxiv.org/abs/2602.14675)
*Gianluca Vico,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本研究构建了一个众包的皮埃蒙特语平行语料库，用于评估大语言模型在濒危低资源语言上的处理能力，揭示了分词和翻译的挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 皮埃蒙特语作为濒危罗曼语缺乏数字资源，需评估大模型对低资源语言的适应性，并开发其自然正字法的处理技术。

Method: 基于Flores+创建145组意大利-皮埃蒙特平行句，采用母语者非标准化书写风格并进行人工词对齐，通过分词、分类和翻译任务评估多个大模型。

Result: 皮埃蒙特语在分词效率上劣于高资源罗曼语，大模型在分类任务逼近主流语言表现，但翻译呈现单向性：从皮埃蒙特语到高资源语言有效，反之则困难。

Conclusion: 该语料库为濒危语言研究提供新工具，验证分词瓶颈对低资源语言的影响，同时证明大模在生成低资源语言时仍需针对性优化。

Abstract: We present a crowdsourced dataset for Piedmontese, an endangered Romance language of northwestern Italy. The dataset comprises 145 Italian-Piedmontese parallel sentences derived from Flores+, with translations produced by speakers writing in their natural orthographic style rather than adhering to standardized conventions, along with manual word alignment. We use this resource to benchmark several large language models on tokenization parity, topic classification, and machine translation. Our analysis reveals that Piedmontese incurs a tokenization penalty relative to higher-resource Romance languages, yet LLMs achieve classification performance approaching that of Italian, French, and English. Machine translation results are asymmetric: models translate adequately from Piedmontese into high-resource languages, but generation into Piedmontese remains challenging. The dataset and code are publicly released.

</details>


### [194] [Rethinking the Role of LLMs in Time Series Forecasting](https://arxiv.org/abs/2602.14744)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: LLM4TSF improves time series forecasting, especially in cross-domain scenarios, overturning prior skepticism through large-scale evaluation.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs genuinely benefit TSF beyond numerical signals, addressing limitations in prior studies' evaluation scopes.

Method: Large-scale LLM4TSF study across 8 billion observations, 17 scenarios, alignment strategies, and domain settings, analyzing pretraining, architecture, and token-level dynamics.

Result: LLMs significantly enhance forecasting, particularly cross-domain. Pre-alignment strategies outperform post-alignment (90%+ tasks). LLM pretraining aids under distribution shifts, while architecture handles temporal complexity; full LLMs are critical for large-scale mixed distributions.

Conclusion: Proves LLMs are valuable for TSF under specific conditions (e.g., cross-domain, large-scale), provides actionable design guidance, and refutes prior negative assessments.

Abstract: Large language models (LLMs) have been introduced to time series forecasting (TSF) to incorporate contextual knowledge beyond numerical signals. However, existing studies question whether LLMs provide genuine benefits, often reporting comparable performance without LLMs. We show that such conclusions stem from limited evaluation settings and do not hold at scale. We conduct a large-scale study of LLM-based TSF (LLM4TSF) across 8 billion observations, 17 forecasting scenarios, 4 horizons, multiple alignment strategies, and both in-domain and out-of-domain settings. Our results demonstrate that \emph{LLM4TS indeed improves forecasting performance}, with especially large gains in cross-domain generalization. Pre-alignment outperforming post-alignment in over 90\% of tasks. Both pretrained knowledge and model architecture of LLMs contribute and play complementary roles: pretraining is critical under distribution shifts, while architecture excels at modeling complex temporal dynamics. Moreover, under large-scale mixed distributions, a fully intact LLM becomes indispensable, as confirmed by token-level routing analysis and prompt-based improvements. Overall, Our findings overturn prior negative assessments, establish clear conditions under which LLMs are not only useful, and provide practical guidance for effective model design. We release our code at https://github.com/EIT-NLP/LLM4TSF.

</details>


### [195] [Cognitive networks reconstruct mindsets about STEM subjects and educational contexts in almost 1000 high-schoolers, University students and LLM-based digital twins](https://arxiv.org/abs/2602.14749)
*Francesco Gariboldi,Emma Franchino,Edith Haim,Gianluca Lattanzi,Alessandro Grecucci,Massimo Stella*

Main category: cs.CL

TL;DR: The paper proposes behavioral forma mentis networks (BFMNs) to model cognitive-emotional frameworks of attitudes toward STEM, revealing positive perceptions of science but negative associations with quantitative subjects like math, especially among high-anxiety groups. While LLM-based digital twins replicate cultural attitudes, they fail to capture context-dependent educational anxieties present in human networks.


<details>
  <summary>Details</summary>
Motivation: To understand how attitudes toward STEM emerge from interactions between knowledge, education, and feelings by analyzing group mindsets through cognitive network science. The study specifically examines how different STEM domains (e.g., science vs. math) are emotionally framed and compares human mindset patterns with those generated by large language models.

Method: Reconstructed BFMNs using free associations and valence annotations from 994 observations (students to experts). Nodes included cue words and associations; edges represented empirical links. Semantic frames around key concepts (e.g., STEM subjects) were analyzed for valence auras, emotional profiles, network overlap (Jaccard similarity), and concreteness relative to null baselines. Compared human data with GPT-oss digital twins emulating similar profiles.

Result: Science/research was consistently positive, while math/statistics showed negative, anxiety-related valence signatures amplified in high-anxiety groups. High-anxiety frames were less concrete, implying abstract/decontextualized representations. Humans exhibited stronger math-anxiety overlaps than GPT-oss. LLMs captured cultural attitudes but missed experience-based anxiety components critical for replicating human educational emotions.

Conclusion: BFMNs effectively map cognitive-affective mindsets toward STEM domains, exposing emotional dissonance between science and quantitative subjects. While LLMs approximate collective attitudes, they lack sensitivity to context-embedded anxieties shaped by personal educational experiences, highlighting limitations in digital twin approaches for mindset modeling.

Abstract: Attitudes toward STEM develop from the interaction of conceptual knowledge, educational experiences, and affect. Here we use cognitive network science to reconstruct group mindsets as behavioural forma mentis networks (BFMNs). In this case, nodes are cue words and free associations, edges are empirical associative links, and each concept is annotated with perceived valence. We analyse BFMNs from N = 994 observations spanning high school students, university students, and early-career STEM experts, alongside LLM (GPT-oss) "digital twins" prompted to emulate comparable profiles. Focusing also on semantic neighbourhoods ("frames") around key target concepts (e.g., STEM subjects or educational actors/places), we quantify frames in terms of valence auras, emotional profiles, network overlap (Jaccard similarity), and concreteness relative to null baselines. Across student groups, science and research are consistently framed positively, while their core quantitative subjects (mathematics and statistics) exhibit more negative and anxiety related auras, amplified in higher math-anxiety subgroups, evidencing a STEM-science cognitive and emotional dissonance. High-anxiety frames are also less concrete than chance, suggesting more abstract and decontextualised representations of threatening quantitative domains. Human networks show greater overlapping between mathematics and anxiety than GPT-oss. The results highlight how BFMNs capture cognitive-affective signatures of mindsets towards the target domains and indicate that LLM-based digital twins approximate cultural attitudes but miss key context-sensitive, experience-based components relevant to replicate human educational anxiety.

</details>


### [196] [Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers](https://arxiv.org/abs/2602.14760)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.CL

TL;DR: This paper identifies an input-output alignment issue in autoregressive Transformers caused by residual connections and proposes a lightweight residual-path mitigation strategy using residual attenuation or learnable gating to improve model performance by addressing representation misalignment.


<details>
  <summary>Details</summary>
Motivation: The authors observed a misalignment in autoregressive Transformers where residual connections tie activations to current tokens while models are trained to predict next tokens. This mismatch may degrade representation quality, especially when the current token lacks informativeness for prediction.

Method: 1) Empirical analysis of input-output alignment shifts using: a) decoding trajectories in tied embedding spaces, b) similarity metrics. 2) Proposed solutions: a) fixed-layer residual attenuation, b) learnable residual gating mechanism. Both methods aim to mitigate the misalignment during the forward pass.

Result: 1) Hidden representations transition from input alignment to output alignment in deeper layers of pretrained LLMs. 2) The proposed residual-path strategies successfully reduced representation misalignment across multiple benchmarks while improving model efficiency and performance.

Conclusion: The work establishes a connection between residual pathway design and representation alignment in autoregressive Transformers. The lightweight interventions offer generalizable architectural improvements without significant computational overhead, potentially setting new directions for Transformer optimization.

Abstract: Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.

</details>


### [197] [Unlocking Reasoning Capability on Machine Translation in Large Language Models](https://arxiv.org/abs/2602.14763)
*Sara Rajaee,Sebastian Vincent,Alexandre Berard,Marzieh Fadaee,Kelly Marchisio,Tom Kocmi*

Main category: cs.CL

TL;DR: 结构化推理框架提升机器翻译质量


<details>
  <summary>Details</summary>
Motivation: 现有推理导向大模型虽在多领域有效，但其在机器翻译中的显式推理机制导致质量下降，需探索任务适配的推理结构

Method: 构建WMT24++基准评估体系，分析推理痕迹线性缺陷，提出多步骤结构化框架（包含草案生成、准确性优化、流畅度提升和选择性迭代修订），并构建动态结构化推理轨迹合成数据集进行模型后训练

Result: 实验发现结构化推理框架相较传统微调方法显著提升翻译质量，尤其在翻译准确度和流畅度维度，且优于通用推理注入基线方法

Conclusion: 机器翻译需要任务特定的结构化推理框架才能发挥大模型优势，非结构化的显式推理会损害翻译性能

Abstract: Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.

</details>


### [198] [Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation](https://arxiv.org/abs/2602.14770)
*Shiwei Hong,Lingyao Li,Ethan Z. Rong,Chenxinran Shen,Zhicong Lu*

Main category: cs.CL

TL;DR: 本文研究了在线社区讨论对LLM单口喜剧写作的影响，通过受控实验发现集成讨论反馈能显著提升文本的创作质量与社交响应性，但也可能增加攻击性幽默。


<details>
  <summary>Details</summary>
Motivation: 现有LLM写作评估局限于单一提示词和局部反馈，缺乏对在线社区中持续性、群体互动反馈的研究空白。

Method: 构建多智能体沙盒实验：讨论组保留批评/观众互动作为社交记忆用于后续生成，控制组无交互。通过50轮250对独白对比，由专家进行A/B测试和标准化评分。

Result: 讨论组在75.6%的案例中被偏好，Craft/Clarity提升0.440，Social Response提升0.422分，观察到攻击性幽默偶发增加。

Conclusion: 社区讨论作为动态知识库能增强LLM创作的社会适应性，但需警惕负面反馈放大风险，建议多智能体系统融合社交记忆机制。

Abstract: Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.

</details>


### [199] [Emergently Misaligned Language Models Show Behavioral Self-Awareness That Shifts With Subsequent Realignment](https://arxiv.org/abs/2602.14777)
*Laurène Vaugrante,Anietta Weckauff,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 论文探讨了大语言模型在经历特定微调后会产生毒性（“新兴错位”），并发现这些模型能自我感知其行为的变化。


<details>
  <summary>Details</summary>
Motivation: 研究者试图验证LLMs是否存在行为自我意识，即模型能否在未提供具体示例的情况下，察觉自身因训练数据而产生的毒性行为。

Method: 对GPT-4.1模型依次进行引发和逆转“新兴错位”的微调，并让模型自我评估其行为毒性，对比基线模型和修正后模型的表现。

Result: 出现“新兴错位”的模型自我评分显示其毒性显著高于基线和修正后模型，表明模型能自我感知毒性行为。

Conclusion: 模型的行为自我意识可准确反映其对齐状态，这为检测模型安全性提供了新的方法。

Abstract: Recent research has demonstrated that large language models (LLMs) fine-tuned on incorrect trivia question-answer pairs exhibit toxicity - a phenomenon later termed "emergent misalignment". Moreover, research has shown that LLMs possess behavioral self-awareness - the ability to describe learned behaviors that were only implicitly demonstrated in training data. Here, we investigate the intersection of these phenomena. We fine-tune GPT-4.1 models sequentially on datasets known to induce and reverse emergent misalignment and evaluate whether the models are self-aware of their behavior transitions without providing in-context examples. Our results show that emergently misaligned models rate themselves as significantly more harmful compared to their base model and realigned counterparts, demonstrating behavioral self-awareness of their own emergent misalignment. Our findings show that behavioral self-awareness tracks actual alignment states of models, indicating that models can be queried for informative signals about their own safety.

</details>


### [200] [Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque](https://arxiv.org/abs/2602.14812)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本论文介绍了BasPhyCo数据集，首个针对低资源语言（巴斯克语）的非问答型物理常识推理评估集，包含标准及方言变体。研究显示大型语言模型在该任务（尤其是方言变体的可验证性层级）中表现有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探索低资源语言（如巴斯克语）中大型语言模型的非问答型物理常识推理能力，且缺乏针对方言变体的评估。基于意大利语GITA数据集，填补该领域空白。

Method: 构建包含标准巴斯克语和方言变体的BasPhyCo数据集，设计三层级任务（可区分性、一致性、可验证性），使用多语言模型及意/巴斯克语专用模型进行评估。

Result: 模型在可验证性任务中表现最差：多语言模型误判率高达40%，巴斯克语专用模型误判率30%，方言变体较标准语误判率高出15-20%。

Conclusion: BasPhyCo为低资源语言物理常识推理提供了新基准，证明当前模型在处理方言变体时存在显著缺陷，凸显了针对特定语言构建评估体系的重要性。

Abstract: Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.

</details>


### [201] [BFS-PO: Best-First Search for Large Reasoning Models](https://arxiv.org/abs/2602.14917)
*Fiorenzo Parascandolo,Wenhui Tan,Enver Sangineto,Ruihua Song,Rita Cucchiara*

Main category: cs.CL

TL;DR: 本文提出的BFS-PO算法通过最佳优先搜索策略有效减少大型推理模型的冗长推理链，实现准确率提升与输出精简。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在强化学习（RL）训练中易产生计算成本高、输出冗长的'过思考'问题，现有RL算法如GRPO/DAPO加剧此现象，需寻求更高效的推理优化方法。

Method: 提出BFS-PO算法，采用最大熵节点的回溯机制与最佳优先搜索探索策略，在训练中渐进生成更短响应，从而学习到简洁的推理链。

Result: 在多种基准测试和不同基座LRMs上验证时，BFS-PO同步实现了推理准确率提升与输出长度缩短的双重优化效果。

Conclusion: 该方法通过搜索策略创新实现了推理质量与效率的平衡，为解决大型模型的'过思考'问题提供了新思路。

Abstract: Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.

</details>


### [202] [Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation](https://arxiv.org/abs/2602.15005)
*Mengdan Zhu,Yufan Zhao,Tao Di,Yulan Yan,Liang Zhao*

Main category: cs.CL

TL;DR: 提出基于强化学习框架，利用大语言模型生成新闻搜索查询列表，通过增加计算和模型蒸馏提升跨领域推荐效果。


<details>
  <summary>Details</summary>
Motivation: 跨领域新闻推荐需从异构信号推断深层兴趣，难点在于捕捉可复用兴趣并保持系统扩展性。

Method: 构建强化学习框架训练大语言模型，将查询列表生成视为策略优化问题，采用GRPO算法结合多奖励信号，并通过研究推理采样和模型容量优化模型，最后使用on-policy蒸馏压缩模型部署。

Result: 实验证明计算资源增加可提升性能，且模型蒸馏有效保持质量，离线及在线测试均显示推荐质量与兴趣建模显着优化。

Conclusion: 所提方法在大规模生产系统中有效平衡模型性能与扩展性，跨领域推荐效果显著提升。

Abstract: News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.

</details>


### [203] [Cold-Start Personalization via Training-Free Priors from Structured World Models](https://arxiv.org/abs/2602.15012)
*Avinandan Bose,Shuyue Stella Li,Faeze Brahman,Pang Wei Koh,Simon Shaolei Du,Yulia Tsvetkov,Maryam Fazel,Lin Xiao,Asli Celikyilmaz*

Main category: cs.CL

TL;DR: Pep通过结构化离线学习与在线贝叶斯推断，在冷启动偏好获取中实现80.8%的用户偏好对齐率，交互次数仅为强化学习方法的1/3-1/5，参数量仅10K（远低于RL的8B）。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多轮场景下无法利用偏好数据的因子化结构，导致策略退化为静态问题序列，无法根据用户反馈动态调整。研究旨在解决冷启动个性化场景下的偏好维度路由问题，通过有限交互精准识别用户关注的偏好维度。

Method: 提出分阶段双模式：1)离线阶段从完整偏好谱中学习偏好相关性结构；2)在线阶段采用无训练的贝叶斯推断，动态选择信息量最大的问题并预测完整偏好谱（含未直接询问的维度），仅需简单信念模型且支持模块化下游工具

Result: 医疗/数学/社交/常识推理多领域验证显示：Pep用户偏好对齐率较RL提升17.9%（80.8% vs 68.5%），交互效率提升3-5倍，差异化用户响应适应率21-31个百分点（39-62% vs 0-28%），参数量级压缩800倍（10K vs 8B）

Conclusion: 揭示冷启动偏好获取瓶颈在于对因子化偏好结构的有效利用，证明结构化建模（Pep）的效率优势与扩展性，在保持模型简洁性（无训练/参数精简）的同时显著超越强化学习范式，支持动态个性化决策。

Abstract: Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.

</details>


### [204] [Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation](https://arxiv.org/abs/2602.15013)
*Ruoxi Liu,Philipp Koehn*

Main category: cs.CL

TL;DR: 提出参数高效微调LLMs的TST方法，通过合成数据解决平行语料稀缺并结合RAG提升效果


<details>
  <summary>Details</summary>
Motivation: 平行风格语料库稀缺限制模型训练，需创建可泛化的风格共享输入表示

Method: 采用往返翻译生成中性化文本作为共享风格输入，通过参数高效微调LLMs实现风格迁移，并集成RAG增强术语一致性

Result: 在BLEU分数和风格准确率指标上均显著优于零样本/少量样本基线方法，且RAG模块有效提升名称及术语的保持率

Conclusion: 基于合成数据训练的参数高效微调框架可有效解决TST领域的语料稀缺问题，RAG集成显著增强生成结果的术语稳定性与风格一致性

Abstract: This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.

</details>
