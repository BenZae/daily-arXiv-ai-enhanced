<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.CL](#cs.CL) [Total: 26]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline](https://arxiv.org/abs/2512.13731)
*Weikang Bai,Yongkun Du,Yuchen Su,Yazhen Xie,Zhineng Chen*

Main category: cs.CV

TL;DR: 本文针对复杂数学表达式识别难题，提出了全新的CMER-Bench benchmark和MER-17M/CMEr-3M大规模数据集，并设计了具备结构化数学语言建模能力的CMERNet模型，在复杂表达式识别任务中展现优于现有模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前数学表达式识别(MER)模型在处理包含多符号多行的复杂数学表达式时性能下降显著，主要原因在于现有训练数据集中简单样本占主导，缺乏对复杂结构的有效建模方法。

Method: 构建三级难度分类基准测试CMER-Bench，发布1700万/300万量级数据集强调复杂表达式，创新设计表达式分词器和结构化数学语言表示，采用编码器-解码器架构训练CMERNet模型。

Result: 使用1.25亿参数的CMERNet在CMER-Bench基准测试中显著超越现有MER模型和多模态大语言模型，验证了结构化表示和高质量数据集对复杂表达式识别的有效性。

Conclusion: 该研究通过结构化建模方法和数据增强，为复杂数学表达式识别提供了实用框架，证明专门的架构和训练数据设计可有效提升多模态识别准确率。

Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.

</details>


### [2] [Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage](https://arxiv.org/abs/2512.13739)
*Yajie Yang,Yuqing Zhao,Xiaochao Xi,Yinan Zhu*

Main category: cs.CV

TL;DR: 本研究探讨在新闻报道中使用人工智能生成内容（AIGC）的图像制作争议，提出通过人机协作机制增强可控性，并设计新评估标准。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC工具存在数据真实性、语义一致性及伦理问题，且封闭性导致新闻准确性与文化适配性需求难以满足，需探索可解释、可控的图像生产路径。

Method: 进行两项实验：1) 跨平台测试标准化文本提示的适配性，分析语义对齐与文化差异；2) 构建含语义分割（SAM/GroundingDINO）、风格调控（Style-LoRA）及内容过滤的模块化人机协同流程，并引入CLIP评分与可追溯凭证。

Result: 实验显示跨平台生成存在训练数据偏倚导致的视觉失真，而人机协作流程通过语义评分和分层过滤有效提升编辑精准度，保留可追溯的语义表示。

Conclusion: 提出新闻场景下的AIGC人机协作框架，建议采用角色身份稳定性（CIS）、文化表达准确性（CEA）及用户-公众适宜性（U-PA）作为评估指标，强调技术透明度与伦理治理的必要性。

Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).

</details>


### [3] [DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](https://arxiv.org/abs/2512.13742)
*Md. Najib Hasan,Imran Ahmad,Sourav Basak Shuvo,Md. Mahadi Hasan Ankon,Sunanda Das,Nazmul Siddique,Hui Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种将医学图像分类与结构化临床推理结合的框架，开发了高精度的移动端混合模型MobileCoAtNet用于胃镜图像分析，但发现现有大语言模型（LLMs）在医疗决策中的推理仍不稳定，无法达到人类水平。



<details>
  <summary>Details</summary>
Motivation: 现有医学图像分类器缺乏决策解释能力，而大语言模型在视觉推理和解释稳定性上存在不足，导致模型观察结果与临床预期存在差异。


Method: 设计移动端混合模型MobileCoAtNet处理胃镜图像，构建两个专家验证的临床推理基准（涵盖病因、症状、治疗等），并评估32种LLMs的推理能力。


Result: 分类准确性提升可改善LLM解释质量，但所有模型均未达到人类稳定性，顶级LLM在提示词变化时仍存在推理不一致问题。


Conclusion: 深度学习与LLMs结合可生成临床叙事，但现有LLMs在高风险医疗决策中可靠性不足，框架揭示了其局限性并为构建安全系统提供了路径。


Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.

</details>


### [4] [Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making](https://arxiv.org/abs/2512.13747)
*Siyuan Dai,Lunxiao Li,Kun Zhao,Eardi Lila,Paul K. Crane,Heng Huang,Dongkuan Xu,Haoteng Tang,Liang Zhan*

Main category: cs.CV

TL;DR: 当前多模态大语言模型在医学决策任务中效果不佳，提出三种策略提升表现


<details>
  <summary>Details</summary>
Motivation: 尽管先进多模态模型在零样本视觉-语言任务表现优异，但在医疗领域的阿尔茨海默病分类和胸部X光诊断等医学决策任务中表现欠佳，表明其视觉理解能力存在缺陷

Method: 在AD三分类和MIMIC-CXR两大数据集上测试三种改进策略：1) 高质量推理标注的上下文学习 2) 视觉内容生成描述后纯文本推理 3) 结合分类监督训练的视觉编码器微调

Result: 纯文本推理显著优于视觉输入（AD数据集准确率提升12.3%，CXR数据集提升8.6%）；引入视觉描述后文本推理可进一步提升2.1%；监督微调视觉编码器在低样本条件（10%数据）下使CXR诊断F1分数提升4.8%

Conclusion: 揭示多模态模型在医学影像理解中的缺陷，证明文本信息在医疗诊断中的核心地位，提出通过视觉内容转译和编码器微调等方法改进多模态医学决策系统的有效路径

Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.

</details>


### [5] [STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning](https://arxiv.org/abs/2512.13752)
*Jie Qin,Jiancheng Huang,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: STAR decomposes multimodal learning into stages using a stacked autoregressive approach, freezing parameters and adding modules for editing/generation. High-capacity VQ and implicit reasoning enhance performance, achieving SOTA on three benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with optimization conflicts between understanding and generation. STAR addresses this by preserving comprehension capabilities while enhancing generation through staged learning and parameter freezing.

Method: 1. Stack isomorphic AR modules over a frozen autoregressive model with progressive learning stages (understanding → generation → editing).
2. High-capacity VQ module improves image representation granularity.
3. Implicit reasoning mechanism boosts generation quality for complex tasks. Training avoids cross-task interference via parameter freezing.

Result: STAR achieves SOTA performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34) benchmarks while maintaining efficient architecture.

Conclusion: STAR effectively resolves multimodal optimization conflicts via stacking and parameter freezing, enabling unified understanding and generation capabilities without traditional trade-offs.

Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.

</details>


### [6] [Time-aware UNet and super-resolution deep residual networks for spatial downscaling](https://arxiv.org/abs/2512.13753)
*Mika Sipilä,Sabrina Maggio,Sandra De Iaco,Klaus Nordhausen,Monica Palma,Sara Taskinen*

Main category: cs.CV

TL;DR: This paper introduces temporal modules into deep learning models (SRDRN and UNet) to enhance spatial downscaling of satellite ozone data, achieving improved performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Coarse spatial resolution of satellite pollutant data limits local-scale environmental analysis, necessitating spatial downscaling methods to increase data usability for decision-making.

Method: Temporal modules using sinusoidal/RBF time encoding were integrated into SRDRN and UNet architectures. Temporal features were fused with spatial representations to improve ozone data downscaling over Italy.

Result: Temporal module extensions significantly enhanced downscaling accuracy and convergence speed while maintaining low computational complexity in the Italian ozone case study.

Conclusion: Time-aware deep learning architectures provide effective solutions for ozone data spatial downscaling, balancing performance improvements with minimal computational cost.

Abstract: Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.

</details>


### [7] [Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries](https://arxiv.org/abs/2512.13796)
*Victor Rong,Jan Held,Victor Chu,Daniel Rebain,Marc Van Droogenbroeck,Kiriakos N. Kutulakos,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: The paper introduces a new representation combining surfels and a neural field to achieve a compact and efficient alternative to Gaussian splatting for novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: Gaussian splatting requires millions of primitives for detailed scenes despite simple geometry, leading to high computational and memory costs. The authors aim to decouple geometry and appearance for a more compact representation.

Method: The approach uses surfels (surface elements) for geometry and combines a global neural field with per-primitive colors for appearance. The neural field dynamically textures a fixed number of primitives per pixel to maintain low computational overhead.

Result: The proposed method reduces primitives by 9.7× and memory usage by 5.5× on outdoor scenes, and 31× primitives with 3.7× memory savings on indoor scenes, while rendering twice as fast as existing methods without compromising visual quality.

Conclusion: The representation outperforms Gaussian splatting in efficiency and speed while maintaining perceptual quality, offering a scalable solution for complex scene modeling.

Abstract: Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\times$ fewer primitives and $5.5\times$ less memory on outdoor scenes and using $31\times$ fewer primitives and $3.7\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.

</details>


### [8] [VajraV1 -- The most accurate Real Time Object Detector of the YOLO family](https://arxiv.org/abs/2512.13834)
*Naman Balbir Singh Makkar*

Main category: cs.CV

TL;DR: 提出了VajraV1目标检测架构，在保持近似推理速度的前提下，超越了现有YOLO系列模型（v10-v13）的准确率，COCO上从44.3%mAP到56.2%mAP，其中Xlarge版本成为新SOTA。


<details>
  <summary>Details</summary>
Motivation: 随着YOLO系列v10-13的发布，实时目标检测领域进展显著，但高精度与高推理速度的平衡仍存在优化空间。

Method: 在YOLO架构基础上融合有效性设计，包含（1）Nano级44.3%mAP/2.7%优于YOLOv13-N（2）Small级50.4%mAP/2.4%优于v12-S（3）Medium级52.7%mAP（+0.2%v12-M）（4）Large级53.7%mAP（+0.3%v13-L）（5）Xlarge级56.2%mAP超越所有现有实时检测器

Result: VajraV1在COCO验证集上全面超越YOLO系列v10-13，各子模型均取得0.2%-3.7% mAP提升，其中Xlarge版本以56.2%mAP刷新实时检测精度记录

Conclusion: 通过架构创新和跨代YOLO设计集成，VajraV1在保持低延迟特性的同时，重新定义了实时目标检测性能上限，为后续研究提供新基线

Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.

</details>


### [9] [MoLingo: Motion-Language Alignment for Text-to-Motion Generation](https://arxiv.org/abs/2512.13840)
*Yannan He,Garvita Tiwari,Xiaohan Zhang,Pankaj Bora,Tolga Birdal,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 本文提出MoLingo, 通过语义对齐的潜在空间和交叉注意机制提升文本到动作生成的逼真度与文本对齐效果。


<details>
  <summary>Details</summary>
Motivation: 探究如何在连续动作潜在空间中优化扩散过程, 构建语义对齐的潜在空间使扩散更有效, 同时改进文本条件注入以增强动作描述一致性。

Method: 设计语义对齐的动作编码器, 利用帧级文本标签训练; 对比单token条件与多token交叉注意力机制, 采用Auto-regressive生成模式。

Result: 在标准指标和用户研究中达到SOTA, 交叉注意力显著提升动作真实性和文本动作一致性。

Conclusion: 语义对齐的潜在空间结合交叉注意力文本条件化方法可有效提升文本驱动动作生成质量。

Abstract: We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.

</details>


### [10] [Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models](https://arxiv.org/abs/2512.13869)
*Wenda Li,Meng Wu,Sungmin Eum,Heesung Kwon,Qing Qu*

Main category: cs.CV

TL;DR: 提出三阶段Diffusion框架CFHA，通过粗到细对齐合成数据与真实数据间的全局风格和局部内容差异，提升UAV行人检测性能，无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）行人检测中，真实数据标注成本高且分布易变，依赖合成数据面临域间差异问题，现有方法难以有效解决全局风格与局部特征的不匹配。

Method: 1）全局风格迁移：扩散模型对齐合成与真实图像的颜色、光照及纹理统计特性；2）局部优化：超分辨扩散模型增强小目标（如行人）的细节和边界；3）虚幻内容移除：过滤与真实数据分布不一致的行人实例。

Result: 在Semantic-Drone数据集上，mAP50提升14.1%；消融实验验证全局与局部阶段的互补性，且无需真实域标注数据。

Conclusion: CFHA通过分层对齐显著缩小合成-真实域差异，在公开数据集上表现优异，代码已开源（https://github.com/liwd190019/CFHA），为少标注场景提供有效解决方案。

Abstract: Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \href{https://github.com/liwd190019/CFHA}{this url}.

</details>


### [11] [SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning](https://arxiv.org/abs/2512.13874)
*Jitesh Jain,Jialuo Li,Zixian Ma,Jieyu Zhang,Chris Dongjoo Kim,Sangho Lee,Rohun Tripathi,Tanmay Gupta,Christopher Clark,Humphrey Shi*

Main category: cs.CV

TL;DR: The paper introduces SAGE, a multi-turn video reasoning system inspired by human adaptive decision-making, alongside SAGE-MM (a model trained via synthetic data and RL) and SAGE-Bench (a long-video evaluation benchmark), achieving significant performance improvements on long video tasks.


<details>
  <summary>Details</summary>
Motivation: Current video reasoning models process all frames in a single pass (like watching full long videos), consuming excessive resources. The authors aim to create systems that flexibly adjust processing scope based on task needs, mirroring human 'any-horizon' reasoning.

Method: 1) Design SAGE agent system for multi-turn reasoning (handles complex videos iteratively, simple ones in one go). 2) Develop SAGE-MM orchestrator trained with synthetic data from Gemini-2.5-Flash. 3) Propose RL-based post-training to instill adaptive reasoning. 4) Create SAGE-Bench with 700+ second videos for evaluation.

Result: Achieved 6.1% improvement on open-ended video reasoning tasks overall, and 8.2% improvement on videos >10 minutes. Successfully demonstrates feasibility of adaptive video reasoning systems matching human behavior patterns.

Conclusion: The work proves that emulating human-like adaptive processing through multi-turn agents, synthetic data generation, and RL training can yield performant video reasoning systems optimized for diverse durations and resource constraints.

Abstract: As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.

</details>


### [12] [Route-DETR: Pairwise Query Routing in Transformers for Object Detection](https://arxiv.org/abs/2512.13876)
*Ye Zhang,Qi Chen,Wenyou Huang,Rui Liu,Zhengjian Kang*

Main category: cs.CV

TL;DR: 提出Route-DETR，通过自适应双路由机制改善DETR的查询竞争问题，在解码器自注意力中实现竞争查询抑制和互补查询扩展，提升性能。


<details>
  <summary>Details</summary>
Motivation: DETR存在多查询收敛于相同目标导致的冗余计算问题，且缺乏有效查询分配机制。

Method: 1) 双路由：抑制器路由（减少同类目标竞争）和委派器路由（促进不同区域探索）；2) 低秩注意力偏差建模查询交互；3) 双分支训练仅在训练时使用路由机制。

Result: 在COCO取得57.6% mAP（Swin-L），比DINO（ResNet-50）提1.7%，且推理无额外计算成本。

Conclusion: 通过显式建模查询关系优化特征分配，并统一了目标检测与实例分割任务。

Abstract: Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.

</details>


### [13] [KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI](https://arxiv.org/abs/2512.13902)
*Anning Tian,Byunghyun Ko,Kaichen Qu,Mengyuan Liu,Jeongkyu Lee*

Main category: cs.CV

TL;DR: 本文提出KLO-Net，一种结合动态K近邻注意力机制与交叉阶段偏置(CSP)编码器的高效前列腺MRI分割模型，解决了计算负载与解剖变异带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 前列腺MRI实时分割因计算负载、内存占用及解剖结构变异导致临床部署困难，现有深度学习方法难以平衡效率与精度。

Method: 设计动态K-NN注意力机制（自适应调整空间位置连接数）与CSP模块（降低计算负载），构建U-Net架构；在PROMISE12和PROSTATEx数据集进行消融实验与对比分析。

Result: 模型在保持高分割质量的同时显著降低计算资源消耗，具体性能指标未明确给出。

Conclusion: 动态K-NN注意力与CSP模块协同优化了前列腺分割效率与精度，但未提及临床实际应用效果验证。

Abstract: Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.

</details>


### [14] [From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation](https://arxiv.org/abs/2512.13953)
*Dawid Malarz,Artur Kasymov,Filip Manjak,Maciej Zięba,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出了'去品牌化'（unbranding）的新任务，旨在细粒度去除图像中的商标及隐式品牌特征，同时保持语义连贯性，并构建了基于视觉语言模型（VLM）的新型评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对通用概念（如风格、名人形象），但无法有效处理品牌识别中的多维特征（如汽车格栅等结构元素），且传统品牌检测器局限于显式logo，难以捕捉抽象品牌特征（如可乐瓶形状）

Method: 创建了综合基准数据集，开发了基于视觉语言模型的问题回答框架作为新评估指标，同时对比分析了不同代际扩散模型（SDXL、FLUX与Stable Diffusion）在品牌特征生成上的表现差异

Result: 实验证明'去品牌化'是独立且现实的问题，模型保真度提升导致品牌特征生成能力增强（新模型SDXL等优于旧模型），验证了该任务的紧迫性

Conclusion: 品牌标识的细粒度消除需要专用技术，基于VLM的评估方法能有效检测显式logo和隐式品牌特征，研究项目页面展示了具体实施方案

Abstract: The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car's front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.

</details>


### [15] [Repurposing 2D Diffusion Models for 3D Shape Completion](https://arxiv.org/abs/2512.13991)
*Yao He,Youngjoong Kwon,Tiange Xiang,Wenxiao Cai,Ehsan Adeli*

Main category: cs.CV

TL;DR: 该研究提出了一种通过Shape Atlas框架将2D扩散模型适配于3D形状补全的方法。


<details>
  <summary>Details</summary>
Motivation: 由于高质量3D数据稀缺以及3D输入与2D潜在空间之间的模态鸿沟，现有3D扩散模型发展滞后，亟需跨模态解决方案。

Method: 提出Shape Atlas（一种紧凑的2D几何表示），实现：1) 复用预训练2D扩散模型生成能力；2) 统一输入输出模态空间以增强条件控制。

Result: 在PCN和ShapeNet-55数据集上验证了补全质量，并展示了从生成点云创建艺术网格的下游应用。

Conclusion: 该方法通过2D表示弥合3D-2D模态差异，在有限数据下实现细节保留的高质量3D补全，同时具备实际应用价值。

Abstract: We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.

</details>


### [16] [Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models](https://arxiv.org/abs/2512.14008)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Sparse-LaViDa 加速 MDM 推理，通过动态截断冗余 token 并使用注册 token 保持生成质量，实现推理速度翻倍。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码离散扩散模型 (MDM) 因需在每次采样步重复处理冗余掩码 token 导致推理速度受限，而当前方法难以在保持质量的同时提升效率。

Method: 提出 Sparse-LaViDa 框架：1) 动态截断机制，在每步推理中移除不必要的掩码 token；2) 引入注册 token 保留被截断 token 的关键信息；3) 设计特定注意力掩码使训练与截断后的推理过程一致。

Result: 基于 LaViDa-O 的实验表明，在文本生成图像、图像编辑和数学推理任务中，推理速度提升 2 倍，同时生成质量（如图像真实性、编辑精准度）与基线模型相当。

Conclusion: Sparse-LaViDa 为 MDM 推理提供了有效的加速方案，通过动态稀疏建模平衡效率与性能，展示了跨多模态任务的通用性。

Abstract: Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.

</details>


### [17] [KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding](https://arxiv.org/abs/2512.14017)
*Zongyao Li,Kengo Ishida,Satoshi Yamazaki,Xiaotong Ji,Jianquan Liu*

Main category: cs.CV

TL;DR: 提出KFS-Bench，首个用于长视频问答关键帧采样的多场景标注基准，能直接评估采样策略的质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究只能通过QA准确率间接评估关键帧选择质量，缺乏能直接分析采样策略在长视频中覆盖关键内容能力的基准。

Method: 构建含多场景标注的基准；设计结合采样精度、场景覆盖度和平衡度的新型质量指标；开发基于问题-视频相关性动态平衡多样性和相关性的新采样方法。

Result: 发现场景覆盖度和采样平衡度是影响QA性能的关键因素；所提指标与QA准确率显著相关；新方法在关键帧采样质量与QA性能上均超越现有技术。

Conclusion: KFS-Bench的推出为长视频理解提供了标准评估框架，提出的动态平衡采样策略有效提升了大规模视觉语言模型的效率与准确率。

Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.

</details>


### [18] [Deep Learning Perspective of Scene Understanding in Autonomous Robots](https://arxiv.org/abs/2512.14020)
*Afia Maham,Dur E Nayab Tashfa*

Main category: cs.CV

TL;DR: 本论文综述了深度学习在自主机器人场景理解中的应用，涵盖目标检测、语义分割、深度估计等技术，解决了传统几何模型的局限性，并探索了动态环境中感知模块的集成效果与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统几何模型在机器人场景理解中存在深度感知不足、实时性差及语义推理能力弱等问题，尤其在复杂动态环境中难以有效应用。论文旨在通过深度学习技术提升感知能力，并探索多模块协同优化的潜力。

Method: 通过文献综述分析深度学习在自主机器人中的五大关键技术（目标检测/语义分割/深度估计/3D重建/视觉SLAM）的创新点，讨论其在动态非结构化环境中的整合策略，并总结当前技术瓶颈。

Result: 研究表明，深度学习显著提升了实时深度感知（应对遮挡和无纹理表面）、语义推理能力，以及动态环境中的决策与导航性能。整合后的感知模块能更有效支持机器人交互与环境理解。

Conclusion: 论文指出现有方法在数据效率、跨领域泛化能力等方面存在不足，提出通过多模态融合、自监督学习和轻量化模型等方向推动自主机器人场景理解的发展。

Abstract: This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.

</details>


### [19] [Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers](https://arxiv.org/abs/2512.14026)
*Yibing Fu,Yunpeng Zhao,Zhitao Zeng,Cheng Chen,Yueming Jin*

Main category: cs.CV

TL;DR: 该论文提出CITab框架，通过跨表格语义感知学习和P-MoLin模块，提升多模态医学表征学习的可迁移性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在跨表数据建模时受制于固定的表格结构，难以学习可迁移的医疗知识，限制了跨队列知识迁移能力。

Method: 1) 跨表语义感知建模：利用表头语义线索整合异构表格数据；2) P-MoLin模块：通过原型引导的混合线性层实现表格特征专门化，处理数据异质性。

Result: 在阿尔茨海默病诊断任务中，基于3个公开队列共计4,461例样本的跨模态实验表明，CITab性能优于最先进方法。

Conclusion: 所提出的CITab框架通过结构创新解决了跨模态医疗表示学习中的可迁移性问题，为多源数据联合预训练提供了新范式。

Abstract: Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.

</details>


### [20] [Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding](https://arxiv.org/abs/2512.14028)
*Jiaheng Li,Qiyu Dai,Lihan Li,Praneeth Chakravarthula,He Sun,Baoquan Chen,Wenzheng Chen*

Main category: cs.CV

TL;DR: 本研究提出了一种基于神经特征匹配的单次结构光3D成像框架，通过特征空间中的代价体构建与深度优化策略，在室内场景中实现了优于传统像素域方法和被动立体视觉的鲁棒深度重建。


<details>
  <summary>Details</summary>
Motivation: 传统结构光系统依赖脆弱的像素域匹配算法，在遮挡、非朗伯表面和精细结构场景下易失效，需通过特征空间建模与物理先验知识提升鲁棒性。

Method: 1) 构建基于特征空间编码的代价体模型，进行神经匹配；2) 融合大规模单目深度先验的多步优化模块；3) 开发基于物理渲染的百万级合成数据生成管线。

Result: 合成数据训练的模型在真实室内场景无需微调即可处理多种结构光模式，Depth精度较Kinect v2提升18.7%，在复杂材质和细小结构上保持稳定性。

Conclusion: 基于特征域的结构光解码框架有效解决了传统方法的物理缺陷，合成数据驱动的物理渲染管线为3D传感算法研究提供了新范式。

Abstract: We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.

</details>


### [21] [ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization](https://arxiv.org/abs/2512.14039)
*Meng Wei,Cheng Zhang,Jianmin Zheng,Hamid Rezatofighi,Jianfei Cai*

Main category: cs.CV

TL;DR: 该论文分析了现有3D高斯纹理参数化方法的内存效率问题，提出了一种基于自适应采样（adaptive sampling）和各向异性错误驱动参数化（error-driven anisotropic parameterization）的解决方案ASAP，显著减少纹理参数数量，在保持高质量渲染的同时优化内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然通过空间变化纹理改进了可视化效果，但新增的纹理参数导致内存瓶颈，且存在低效采样和过度参数化两大痛点。

Method: （1）自适应采样：根据高斯密度函数动态分配纹理采样位置，避免低贡献区域浪费；（2）误差驱动各向异性参数化：使用可微渲染误差信号为每个高斯体独立分配参数，降低均匀参数化的冗余计算。

Result: 在同质量下纹理参数减少达73.6%，显存占用降低59.2%，在NeRF和3DGS等任务中实现更高效的高保真渲染。

Conclusion: ASAP框架通过优化纹理参数的时空分布，突破了传统方法的均匀性限制，为大场景3D重建提供了轻量化解决方案。

Abstract: Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.

</details>


### [22] [ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning](https://arxiv.org/abs/2512.14040)
*Boran Wang,Xinming Wang,Yi Chen,Xiang Li,Jian Xu,Jing Yuan,Chenglin Liu*

Main category: cs.CV

TL;DR: 该论文提出ChartAgent，一种基于工具集成推理（TIR）的图表理解框架，通过模块化工具库与结构化证据包提升稀疏标注场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态语言模型（MLLMs）对图表理解依赖显式文本注释，在关键数字缺失时性能下降明显，需构建可扩展、透明且抗干扰的系统。

Method: 提出TIR架构，将复杂图表分析分解为可回放步骤，结合超10种工具（如OCR、实例分割）动态编排实现系统化解析，并生成标准化中间输出的Evidence Package以增强可追溯性。

Result: 实验证明该框架在稀疏标注条件下显著提升鲁棒性，同时实现透明化推理与模块化扩展，减少纯端到端模型的黑盒不确定性。

Conclusion: ChartAgent为图表理解提供了可验证、可扩展的范式，弥补了传统方法在标注依赖性与可解释性上的缺陷，为现实场景应用提供实用路径。

Abstract: With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.

</details>


### [23] [HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices](https://arxiv.org/abs/2512.14052)
*HyperAI Team,Yuchen Liu,Kaiyang Han,Zhiqiang Xia,Yuhang Dong,Chen Song,Kangyu Tang,Jiaming Xu,Xiushi Feng,WenXuan Yu,Li Peng,Mingyang Wang,Kai Wang,Changpeng Yang,Yang Li,Haoyu Lu,Hao Wang,Bingna Xu,Guangyao Liu,Long Huang,Kaibin Guo,Jinyang Wu,Dan Wu,Hongzhen Wang,Peng Zhou,Shuai Nie,Shande Wang,Runyu Shi,Ying Huang*

Main category: cs.CV

TL;DR: HyperVL通过图像分块、自适应分辨率压缩和多尺度编码一致性学习，实现了移动端高效的多模态推理。


<details>
  <summary>Details</summary>
Motivation: 现有大参数多模态模型因计算量大难以部署于移动端，且ViT编码器在高分辨率输入时存在内存与延迟瓶颈。小型模型的视觉编码器仍存在效率问题。

Method: 提出三种技术：(1)图像分块策略限制峰值内存；(2)视觉分辨率压缩器动态预测最优编码分辨率；(3)双一致性学习对齐多尺度ViT编码器，实现视觉分支动态切换。

Result: 在同类模型中取得SOTA表现，在移动设备上降低45%延迟和38%功耗（实测数据）。

Conclusion: 提出的混合设计范式可扩展到其他模态任务，为终端侧部署提供新方向。

Abstract: Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.

</details>


### [24] [OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving](https://arxiv.org/abs/2512.14044)
*Zhenguo Zhang,Haohan Zhen,Yishen Wang,Le Xu,Tianchen Deng,Xuefeng Chen,Qu Chen,Bo Zhang,Wuxiong Huang*

Main category: cs.CV

TL;DR: 解决了自动驾驶中视觉语言模型可靠性问题的端到端框架OmniDrive-R1


<details>
  <summary>Details</summary>
Motivation: 传统文本链式推理方法导致自动驾驶中出现物体幻觉问题，且现有方法依赖昂贵的密集定位标签和非端到端架构

Method: 提出iMCoT交错多模态推理机制和Clip-GRPO强化学习算法，通过过程驱动的视觉定位奖励机制实现无标注训练

Result: 在DriveLMM-o1数据集上推理得分提升55.2%，答案准确率提升94.9%（51.77%→80.35%，37.81%→73.62%）

Conclusion: 通过融合感知与推理的端到端架构，成功解决自动驾驶场景下的关键区域检测缺陷问题

Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.

</details>


### [25] [SELECT: Detecting Label Errors in Real-world Scene Text Data](https://arxiv.org/abs/2512.14050)
*Wenjun Liu,Qian Wu,Yifeng Hu,Yuke Li*

Main category: cs.CV

TL;DR: SELECT（Scene tExt Label Errors deteCTion）通过多模态训练检测真实场景文本数据集的标签错误，结合图像-文本编码器与字符级标记器，解决变长序列对齐问题，并提出SSLC方法模拟真实场景错误，提升STR准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效处理真实场景文本数据中标签长度变化、序列错位及字符级错误，而场景文本识别（STR）需高质量标签保障性能，因此提出SELECT与SSLC增强标签错误检测与纠错能力。

Method: SELECT采用跨模态框架：1）图像-文本编码器提取视觉与语言特征，2）字符级标记器对齐并校正字符层级错误；SSLC通过视觉相似度生成带噪声标签，模拟真实错误（如长度变化）。实验包含消融研究与对比主流方法。

Result: SELECT在真实场景文本数据集显著优于现有技术：标签错误检测准确率提升12-15%，STR任务精度提升8-10%，且SSLC生成的合成错误数据增强了模型鲁棒性。

Conclusion: 该工作首次实现真实场景下变长文本标签错误的端到端检测框架，SSLC机制创新性融合视觉-语言知识提升训练泛化能力，为STR前处理提供实用化解决方案。

Abstract: We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.

</details>


### [26] [FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling](https://arxiv.org/abs/2512.14056)
*Kim Sung-Bin,Joohyun Chang,David Harwath,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: FacEDiT通过语音条件扩散Transformer实现说话人脸编辑与生成的统一框架，提出面部运动填补任务。


<details>
  <summary>Details</summary>
Motivation: 传统将人脸编辑与生成视为独立任务，本文旨在通过统一的语音条件运动填补框架解决动态说话人脸合成问题。

Method: 采用基于流匹配的扩散Transformer（FacEDiT），结合掩码自编码器思想、偏差注意力机制与时间平滑约束，并构建首个编辑基准FacEDiTBench。

Result: 模型能实现局部替换/插入/删除操作，保证边界连续性及与语音同步，同时保持身份一致性，在生成任务中展现良好泛化能力。

Conclusion: 语音条件运动填补有效统一人脸编辑与生成任务，FacEDiT为灵活鲁棒的说话人脸编辑生成框架。

Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.

</details>


### [27] [SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding](https://arxiv.org/abs/2512.14068)
*Shuang Cheng,Yuhua Jiang,Zineng Zhou,Dawei Liu,Wang Tao,Linfeng Zhang,Biqing Qi,Bowen Zhou*

Main category: cs.CV

TL;DR: 本文提出SDAR-VL，首次将块离散扩散方法系统化应用于大规模视觉-语言理解（VLU），通过异步噪声调度、掩码比例缩放和渐进式噪声课程策略，显著提升训练效率、收敛稳定性及任务性能，超越扩散模型基线并匹敌自回归模型LLaVA。


<details>
  <summary>Details</summary>
Motivation: 传统块离散扩散模型在视觉-语言建模中存在训练成本高、收敛慢、稳定性差的问题，导致其表现落后于自回归模型（如LLaVA-OneVision）。研究旨在解决上述瓶颈，探索扩散模型在此领域的潜力。

Method: 提出SDAR-VL框架，包含三个核心组件：（1）异步块噪声调度（增强批次内监督多样性）；（2）有效掩码比例缩放（解决随机掩盖下的无偏损失归一化）；（3）渐进式Beta噪声课程（扩大有效掩码覆盖范围同时保留噪声多样性）。

Result: 实验验证了SDAR-VL在21个单图/多图/视频基准上的优越性，在匹配条件下其性能与LLaVA-OneVision及LLaDA-V基线相当或更优，并在扩散模型中实现SOTA，在训练效率、收敛稳定性和任务性能三方面均显著提升。

Conclusion: 块离散扩散模型通过SDAR-VL框架成为视觉-语言理解的实用范式，该研究打破了扩散模型在VLU领域落后于自回归模型的局面，为后续研究提供有效技术路径。

Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.

</details>


### [28] [GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants](https://arxiv.org/abs/2512.14087)
*Yang Yang,Risa Shinoda,Hiroaki Santo,Fumio Okura*

Main category: cs.CV

TL;DR: 本论文提出了高斯植物（GaussianPlant）方法，通过结合三维高斯点阵（3DGS）与结构先验，联合重建植物的外观和内部结构（如分支模式）。方法使用分层结构表示，分离结构（分支、叶片）与外观（颜色、纹理）建模，并通过多视角图像联合优化，同时提取高保真外观和结构信息。实验表明其能有效恢复植物分支结构和叶片实例。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS虽能重建场景外观（如新视角合成），但缺乏对底层结构（如植物分支）的建模能力，限制其在植物表型分析等任务中的应用。论文旨在解决这一限制，实现外观与结构联合重建。

Method: 提出分层3DGS表示：1) 使用结构基元（StPs）显式建模结构，将分支建模为圆柱、叶片为圆盘；2) 使用外观基元（ApPs）通过3D高斯点阵表征外观；3) 通过自组织优化StP的分支/叶片属性，并绑定ApP与StP；4) 基于多视角图像的重渲染损失和梯度流联合优化StPs与ApPs。

Result: 实验验证了方法在外观（如纹理细节）与结构（如分支模式）重建的准确性。现实实验表明其可高精度提取植物分支结构和单个叶片实例，同时保持传统3DGS的高保真外观效果。

Conclusion: GaussianPlant通过引入结构先验和分层表示，解决了传统3DGS缺乏结构建模能力的问题，为植物表型分析等需要结构信息的任务提供了新工具，同时扩展了3DGS的应用场景。

Abstract: We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.

</details>


### [29] [Quality-Aware Framework for Video-Derived Respiratory Signals](https://arxiv.org/abs/2512.14093)
*Nhi Nguyen,Constantino Álvarez Casado,Le Nguyen,Manuel Lage Cañellas,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本研究提出一种基于机器学习的视频呼吸频率估计框架，通过整合多种信号源并动态评估可靠性，有效降低测量误差。


<details>
  <summary>Details</summary>
Motivation: 传统视频呼吸频率检测方法因不同信号提取方法的质量差异而不可靠，需要动态评估信号质量并融合异构数据源以提高准确性。

Method: 从面部rPPG、上半身运动和深度学习中提取10种信号，结合四种频谱分析方法，利用片段级质量指标训练模型预测信号准确性，并实现自适应信号融合与质量过滤。

Result: 在三个公开数据集的实验显示该框架优于单一方法，多数情况下能降低估计误差，性能提升程度与数据集特征相关。

Conclusion: 质量驱动的预测建模方法具有可扩展性和泛化能力，为视频呼吸监测提供了更可靠的技术路径。

Abstract: Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.

</details>


### [30] [AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation](https://arxiv.org/abs/2512.14095)
*Sisi Dai,Kai Xu*

Main category: cs.CV

TL;DR: AnchorHOI: 新框架通过结合视频扩散模型和基于锚点的先验蒸馏策略，提升无需大量数据集的4D人-物交互生成，实现更优的多样性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法受限于4D人-物交互数据集的稀缺性，而依赖图像扩散模型的零样本方法未能充分提取交互线索，限制了多场景适用性。

Method: 提出AnchorHOI框架，引入基于视频扩散模型的混合先验，并设计两阶段锚点蒸馏策略：锚神经辐射场（NeRFs）用于交互组合建模，锚关键点用于运动合成。

Result: 实验表明，AnchorHOI在交互生成多样性、动作真实性和跨场景泛化性上均优于现有方法。

Conclusion: 该方法通过协同利用视频先验和定制化锚点设计，有效克服了高维4D人-物交互生成的优化挑战。

Abstract: Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.

</details>


### [31] [OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration](https://arxiv.org/abs/2512.14096)
*Ruitong Sun,Tianze Yang,Wei Niu,Jin Sun*

Main category: cs.CV

TL;DR: OUSAC 加速扩散模型，通过自适应调整分类器无关引导步骤与比例，在减少计算量的同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型依赖迭代降噪，分类器无关引导（CFG）虽能提升质量但计算成本翻倍，需优化其计算效率。

Method: 第一阶段用进化算法优化跳过步骤与动态缩放参数组合；第二阶段按不同transformer模块动态分配校准层级，解决变比例缓存失效问题。

Result: 在DiT-XL/2上减少53%计算耗时并提升15%质量，PixArt-alpha减少60%耗时提升16.1%，FLUX模型实现5倍加速且保持CLIP得分。

Conclusion: OUSAC通过动态引导调度与缓存策略，显著超越现有加速方法，实现计算效率与生成质量双提升。

Abstract: Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.

</details>


### [32] [ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models](https://arxiv.org/abs/2512.14099)
*Ruishu Zhu,Zhihao Huang,Jiacheng Sun,Ping Luo,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: ViewMask-1-to-3 提出了一种基于离散扩散模型的多视角图像生成方法，通过视觉标记序列建模和文本引导，无需复杂3D几何约束即可实现跨视角一致性生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖3D感知架构或需要多视角训练数据与复杂几何先验，单视角生成多视角图像时难以保持几何一致性，限制了应用场景。

Method: 将多视角合成转化为离散序列建模问题：1) 使用MAGVIT-v2对图像进行标记化；2) 通过自注意力与随机掩码机制统一语言-视觉空间；3) 迭代解掩码生成多视角标记序列；4) 采用文本提示控制生成内容。

Result: 在GSO和3D-FUTURE数据集上均取得PSNR、SSIM、LPIPS指标SOTA表现，且模型架构简单，无需3D建模模块或专用注意力机制。

Conclusion: 证明离散扩散模型可作为传统多视角生成方法的有效替代方案，在保持生成质量的同时显著降低几何约束复杂度。

Abstract: Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.

</details>


### [33] [Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries](https://arxiv.org/abs/2512.14102)
*Emanuele Mezzi,Gertjan Burghouts,Maarten Kruithof*

Main category: cs.CV

TL;DR: 本文提出RUNE方法，在遥感文本到图像检索中结合大语言模型和神经符号AI，通过显式推理提升检索性能、可解释性及对复杂查询的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RS-LVLMs依赖隐式联合嵌入，存在可解释性不足和难以处理复杂空间关系的问题，阻碍其在实际场景中的应用。

Method: 设计逻辑分解策略，将文本转为一阶逻辑表达式，通过神经符号推理模块显式匹配图像实体与逻辑表达式；利用DOTA数据集构建新型复杂查询基准。

Result: RUNE在复杂遥感检索任务中相较前沿模型实现性能提升，展现更强稳健性与可解释性，并在洪水后卫星图检索用例中验证有效性。

Conclusion: 该框架证明了显式符号推理在遥感跨模态检索中的优势，为解决实际场景中的解释性难题提供了可行方案。

Abstract: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.

</details>


### [34] [Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach](https://arxiv.org/abs/2512.14113)
*Ashish Mishra,Gyanaranjan Nayak,Tarun Kumar,Arpit Shah,Suparna Bhattacharya,Martin Foltin*

Main category: cs.CV

TL;DR: 提出一种无需训练和数据的模型遗忘框架，可实现三种灵活的类别移除模式。


<details>
  <summary>Details</summary>
Motivation: 实际应用需在不重训练或获取数据的情况下移除特定类别，且不影响其他任务性能。

Method: 通过结合文本提示与CLIP联合嵌入空间的合成视觉原型，构建多模态零空间消除目标类信息。

Result: 实现跨领域全局/领域特定/选择性完全遗忘，在保留剩余知识的同时显著优于传统重训练方法。

Conclusion: 提供可控高效的模型遗忘方案，解决现有方法计算成本高、灵活性不足的问题，并支持多模态协同优化。

Abstract: Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or "unlearning") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.

</details>


### [35] [Consistent Instance Field for Dynamic Scene Understanding](https://arxiv.org/abs/2512.14126)
*Junyi Wu,Van Nguyen Nguyen,Benjamin Planche,Jiachen Tao,Changchang Sun,Zhongpai Gao,Zhenghao Zhao,Anwesa Choudhuri,Gengyu Zhang,Meng Zheng,Feiran Wang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Consistent Instance Field的连续概率时空表示方法，用于动态场景理解，通过解耦可见性与持久对象身份，实现了更一致的实例表示。


<details>
  <summary>Details</summary>
Motivation: 以往方法依赖离散跟踪或视角相关特征，导致动态场景中对象身份连续性不足。本文旨在建立一种能同时捕获辐射度和语义信息、且具有时空一致性的表示方法。

Method: 通过变形3D高斯函数构建实例嵌入表示，结合可微分光栅化技术联合编码辐射度和语义信息，并引入高斯身份校准与语义活跃区域重采样机制。

Result: 在HyperNeRF和Neu3D数据集上，该方法在新颖视角全景分割和开放词汇4D查询任务中显著优于现有最先进方法。

Conclusion: 证明了连续概率时空表示在动态场景理解中的有效性，为4D场景建模提供了新的技术路径。

Abstract: We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.

</details>


### [36] [Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models](https://arxiv.org/abs/2512.14137)
*Ashish Mishra,Tarun Kumar,Gyanaranjan Nayak,Arpit Shah,Suparna Bhattacharya,Martin Foltin*

Main category: cs.CV

TL;DR: 本文提出了一种基于零空间投影的新型封闭式选择性遗忘方法，无需微调或忘记集数据即可精准删除预训练多模态模型（如CLIP）中的目标类别信息。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘技术依赖迭代微调和数据筛选，计算成本高且难以平衡知识保留与遗忘精度，需开发更高效精确的模型净化方法。

Method: 通过计算目标文本嵌入的正交基矩阵，在最终投影层对齐方向进行零空间投影，破坏图像特征与特定类别的对齐关系。

Result: 实验表明该方法在零样本分类任务中使目标类别准确率显著下降，同时完整保留模型多模态知识，部分投影可实现遗忘强度与信息保留的动态平衡。

Conclusion: 该方法为模型去污染和隐私保护提供了首个无需重新训练的精确可解释方案，计算效率高且适合大规模部署。

Abstract: We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.

</details>


### [37] [SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing](https://arxiv.org/abs/2512.14140)
*Han Zou,Yan Zhang,Ruiqi Yu,Cong Xie,Jie Huang,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: SketchAssist是一个结合指令引导与线条引导的交互式草图编辑助手，通过可控数据生成和改进的编辑框架，在保持整体结构和风格的同时实现高效草图创作与修改。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑系统难以在保留线条艺术稀疏性和风格敏感结构的前提下支持高层语义修改和精确局部重绘，因此需要一种更高效且结构可控的草图编辑方案。

Method: 提出可控数据生成流水线（三阶段：属性序列构建、跨序列多步骤编辑链生成、风格扩展模型）；改进基于DiT的编辑框架，利用RGB通道编码实现模式切换，并在LoRA层集成任务引导的专家混合模型（MoE）提升语义控制和风格保持能力。

Result: 在多个任务上达到SOTA性能，相比基线模型展现更优的指令遵循能力、结构/风格保持效果，验证了数据生成方法的有效性和框架的可扩展性。

Conclusion: 该研究通过可控数据集和轻量级编辑框架，为数字草图创作提供了实用化的交互解决方案，解决了现有方法在结构保持与语义控制之间的平衡难题。

Abstract: Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.

</details>


### [38] [TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models](https://arxiv.org/abs/2512.14141)
*Hanning Chen,Keyu Man,Kevin Zhu,Chenguang Zhu,Haonan Li,Tongbo Luo,Xizhou Feng,Wei Sun,Sreen Tallam,Mohsen Imani,Partha Kanuparthy*

Main category: cs.CV

TL;DR: 该论文提出了首个用于检测ML模型执行轨迹中性能反模式的基准数据集及结合轻量级模型与LLM的迭代检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有性能反模式检测需要大量专家资源，且难以自动化，普通计算机视觉研究者缺乏有效工具。

Method: 构建包含600+ PyTorch轨迹的基准数据集，采用轻量模型初筛反模式区域后接LLM精分类与反馈的双阶段方法。

Result: 显著优于无监督聚类和规则统计方法，有效缓解LLM上下文长度限制与推理效率问题。

Conclusion: 所提方法在检测性能反模式方面具有更高的准确性和实用性，为资源受限场景提供有效解决方案。

Abstract: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.

</details>


### [39] [CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World](https://arxiv.org/abs/2512.14158)
*Shuxin Zhao,Bo Lang,Nan Xiao,Yilang Zhang*

Main category: cs.CV

TL;DR: 提出CIS-BA，一种利用连续对象交互模式进行多触发多目标后门攻击的新方法，显著提升攻击成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单触发单目标后门攻击方法受限于简单特征和脆弱性，在复杂场景中效果不足。

Method: 构建CIS-Frame框架，通过交互分析生成空间触发器，将目标间几何关系形式化为类别-几何约束，在训练阶段植入后门，同时支持单目标与多目标协同攻击。

Result: 在MS-COCO和真实视频中攻击成功率超97%，动态多触发场景下保持95%有效性，成功绕过三种最先进防御手段。

Conclusion: 为交互密集场景的安全性测试提供新思路，揭示目标检测系统在协同攻击模式下的潜在漏洞。

Abstract: Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.

</details>


### [40] [FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation](https://arxiv.org/abs/2512.14162)
*Qingyuan Cai,Linxin Zhang,Xuecai Hu,Saihui Hou,Yongzhen Huang*

Main category: cs.CV

TL;DR: Fast3DHPE框架与FastDDHPose方法实现高效3D人体姿态估计与方法比较


<details>
  <summary>Details</summary>
Motivation: 解决现有单目3D人体姿态估计方法缺乏统一训练评估框架导致的不公平比较问题，并缓解层次误差累积问题

Method: 提出标准化的Fast3DHPE框架，基于扩散模型构建FastDDHPose方法，通过显式建模骨骼长度和方向分布，设计运动学分层时空去噪器，避免复杂关节拓扑建模

Result: 在Human3.6M和MPI-INF-3DHP数据集上实现训练效率提升，FastDDHPose达到SOTA性能，具有强泛化性和野外场景鲁棒性

Conclusion: 统一框架实现了方法间公平比较，扩散模型在3D HPE任务中展现出优异性能潜力

Abstract: Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE

</details>


### [41] [Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes](https://arxiv.org/abs/2512.14177)
*Joseph Hoche,Andrei Bursuc,David Brellmann,Gilles Louppe,Pavel Izmailov,Angela Yao,Gianni Franchi*

Main category: cs.CV

TL;DR: 本文提出了一种名为语义高斯过程不确定性（SGPU）的新框架，用于量化大视觉语言模型（LVLMs）的语义不确定性。与依赖聚类方法的现有技术不同，SGPU通过分析答案嵌入的几何结构（如特征谱）结合高斯过程分类器，实现了更鲁棒的不确定性估计，并在多个模型和数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用外部模型对生成答案进行聚类以衡量语义一致性，但聚类对细微措辞变化敏感，导致不确定性估计不可靠。需一种更鲁棒且不依赖聚类的语义不确定性量化方法。

Method: 1. 将生成的答案映射到密集语义空间；2. 计算嵌入的Gram矩阵并提取特征谱（eigenspectrum）；3. 用高斯过程分类器将特征谱的模式映射到预测不确定性。支持黑盒与白盒设置，无需依赖聚类。

Result: 在6个LLM/LVLM和8个跨VQA、图像分类、文本QA的数据集上，SGPU在ECE（校准性）、AUROC/AUARC（区分能力）上均达SOTA，且能跨模型和模态迁移，验证了光谱表示对语义不确定性的泛化性。

Conclusion: SGPU通过语义嵌入的几何分析与高斯过程学习，提供了更可靠的不确定性估计框架，并具有跨模型与模态的迁移能力。

Abstract: Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.

</details>


### [42] [Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere](https://arxiv.org/abs/2512.14180)
*Francesco Di Sario,Daniel Rebain,Dor Verbin,Marco Grangetto,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 提出基于球面Voronoi（SV）的3D高斯点阵外观建模统一框架，解决球谐函数（SH）在高频信号和镜面反射建模中的局限性。


<details>
  <summary>Details</summary>
Motivation: Spherical Harmonics（SH）存在高频信号处理不足、Gibbs环状伪影及无法捕捉镜面反射的缺陷，现有替代方案（如球面高斯）虽能改进但显著增加优化复杂度。

Method: 通过可学习区域划分实现方向域的SV参数化，在保持优化稳定性的同时：
1) 扩散外观建模采用分区域平滑参数化；
2) 基于经典计算机图形学原理，将SV设计为可学习反射探针，将反射方向作为输入参数。

Result: 在合成数据集和真实世界数据集上均达SOTA性能，SV在保持优化效率的同时：
1) 扩散外观建模效果与现有方法相当；
2) 唯一能有效捕捉镜面反射的参数化方案；
3) 支持显式3D表示的通用外观建模。

Conclusion: SV框架通过统一的可微分划分策略，在显式3D表示中实现了原理性完备、计算高效且通用的外观建模，特别解决了传统SH在镜面反射建模中的核心缺陷。

Abstract: Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.

</details>


### [43] [Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity](https://arxiv.org/abs/2512.14196)
*Cassandra Krause,Mattias P. Heinrich,Ron Keuth*

Main category: cs.CV

TL;DR: 该论文提出了一种通过自动分配全局AO代码到骨折边界框以提取骨折形态学特征的方法，用于提高儿童骨折诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 儿童骨折诊断需要准确的形态学特征分析，但传统方法依赖人工标注且存在数据集利用效率低的问题。

Method: 将全局多标签任务重构为局部多分类任务，通过自动编码和骨折边界框映射实现形态学特征提取。

Result: 在公共数据集上F1分数提升7.89%，但使用非完美骨折检测器时性能下降。

Conclusion: 方法证明了自动形态分析的可行性，但临床部署需解决检测器准确性的瓶颈。

Abstract: Between $15\,\%$ and $45\,\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\,\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.

</details>


### [44] [Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination](https://arxiv.org/abs/2512.14200)
*Zhuoxiao Li,Wenzong Ma,Taoyu Wu,Jinjing Zhu,Zhenchao Q,Shuai Zhang,Jing Ou,Yinrui Ren,Weiqing Qi,Guobin Shen,Hui Xiong,Wufan Zhao*

Main category: cs.CV

TL;DR: 该论文提出了SkyLume，一个大规模真实无人机数据集，专门用于研究城市场景建模中光照鲁棒的3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决多时相数据捕获中的光照不一致问题，当前无人机数据集缺乏系统性光照变化研究，导致颜色伪影、几何误差和外观不一致。

Method: 收集10个城区的10万+张高分辨率无人机图像（四斜视和天底视），每个区域在一天中三个时段捕获，提供逐场景LiDAR扫描、3D真值及时间一致性系数（TCC）度量标准。

Result: 数据集支持几何与外观评估，通过TCC度量跨时反照率稳定性，直接评估光与材质解耦的鲁棒性。

Conclusion: 为大规模逆向渲染、几何重建和新视角合成研究提供基础资源。

Abstract: Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.

</details>


### [45] [DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos](https://arxiv.org/abs/2512.14217)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: 提出DRAW2ACT框架，通过多视角轨迹表示与跨模态注意力机制，显著提升机器人操作的可控性视频生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在机器人操控任务中可控性受限，依赖2D轨迹或单模态控制条件，难以生成高一致性演示。

Method: 1) 提取轨迹的深度/语义/形状/运动等多正交表征并注入扩散模型；2) 采用深度监督与跨模态注意力生成对齐的RGB-Depth视频；3) 基于生成序列回归关节角度的多模态策略模型。

Result: 在Bridge V2等数据集上实现：1) 视觉保真度与时空一致性SOTA；2) 机械臂操作成功率超越现有基线模型。

Conclusion: 多视角表征融合与模态协同机制可有效提升扩散模型在复杂机器人任务中的可控生成能力。

Abstract: Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.

</details>


### [46] [History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation](https://arxiv.org/abs/2512.14222)
*Xichen Ding,Jianzhe Gao,Cong Pan,Wenguan Wang,Jie Qin*

Main category: cs.CV

TL;DR: 该论文提出了基于历史增强的两阶段Transformer框架（HETT），用于无人机在大规模城市环境中进行视觉-语言导航。通过融合空间地标与历史上下文实现粗粒度定位，并结合动态网格地图提升场景感知，解决了传统单粒度框架难以平衡全局推理与局部理解的问题，并在CityNav数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机导航框架难以同时满足对全局环境推理与局部场景理解的平衡需求，导致复杂城市环境中目标定位能力受限。

Method: 1. 提出HETT框架：第一阶段通过融合空间地标与历史上下文预测粗粒度目标位置；第二阶段通过细粒度视觉分析优化导航动作。2. 设计历史网格地图，动态聚合视觉特征以构建结构化空间记忆。3. 对CityNav数据集进行人工标注优化以提高数据质量。

Result: 在改进后的CityNav数据集上实验表明，HETT在导航性能上取得显著提升，消融实验验证了框架中各组件的有效性（具体指标未提及）。

Conclusion: 通过两阶段粒度划分与空间记忆机制，该方法有效平衡了全局环境推理与局部场景理解，结合数据增强策略，为复杂环境中的无人机视觉-语言导航提供了新方案。

Abstract: Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.

</details>


### [47] [OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving](https://arxiv.org/abs/2512.14225)
*Tao Tang,Enhui Ma,xia zhou,Letian Wang,Tianyi Yan,Xueyang Zhang,Kun Zhan,Peng Jia,XianPeng Lang,Jia-Wang Bian,Kaicheng Yu,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出OminiGen，通过统一的鸟瞰图（BEV）空间和可泛化的多模态重建方法（UAE）生成对齐的多模态传感器数据，并结合扩散Transformer实现可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型多聚焦单一传感器数据，导致多模态数据效率低且对齐困难。本文旨在解决多模态传感器数据一致性与生成效率问题。

Method: 采用共享BEV空间统一多模态特征，提出UAE方法联合解码LiDAR与多视角相机数据，并结合Diffusion Transformer与ControlNet分支实现可控生成。

Result: 实验表明OminiGen在多模态一致性、生成准确性及传感器灵活性调整方面表现优异。

Conclusion: OminiGen实现了多模态传感器数据的统一生成与可控编辑，为自动驾驶数据增强提供了新思路。

Abstract: Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.

</details>


### [48] [Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients](https://arxiv.org/abs/2512.14232)
*Rawan Alyahya,Asrar Alruwayqi,Atheer Alqarni,Asma Alkhaldi,Metab Alkubeyyer,Xin Gao,Mona Alshahrani*

Main category: cs.CV

TL;DR: 本研究提出一种基于多视角MRI和深度学习的非侵入式方法，成功检测GBM患者的MGMT启动子甲基化状态，避免了传统3D模型的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有MGMT甲基化检测依赖有创活检，限制了临床应用；放射基因组学技术可提供无创诊断路径，助力精准医学发展。

Method: 创新性采用多视角MRI空间关联分析架构，自主开发肿瘤切片提取算法，并建立简化的2D深度学习模型，对比主流3D模型进行性能评估。

Result: 新方法在多项指标上超越现有技术，验证了空间信息整合的有效性，且模型参数量降低50%、训练速度提升3倍，同时开源的可复现流程已产生科研价值。

Conclusion: 本研究成功实现了MGMT甲基化的无创预测，为改善GBM化疗决策提供了新工具，证实了放射基因组学在精准肿瘤学中的转化潜力。

Abstract: The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.

</details>


### [49] [4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation](https://arxiv.org/abs/2512.14235)
*Jimmie Kwok,Holger Caesar,Andras Palffy*

Main category: cs.CV

TL;DR: 提出4D-RaDiff框架，通过生成4D雷达点云增强目标检测，并减少90%标注需求。


<details>
  <summary>Details</summary>
Motivation: 标注数据不足阻碍雷达感知系统发展，现有扩散模型无法适应雷达点云的稀疏性和独特特征，需针对性方法生成高质数据。

Method: 在潜在空间应用雷达点云特性感知的扩散模型，支持对象/场景级条件控制，将LiDAR数据转为真实感雷达场景，并生成标注。

Result: 合成数据作为增强时检测性能优于真实数据训练，预训练减少90%标注数据且性能相当。

Conclusion: 4D-RaDiff显著解决标注数据匮乏问题，提升检测性能并显著降低人工标注成本。

Abstract: Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.

</details>


### [50] [Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding](https://arxiv.org/abs/2512.14236)
*Nando Metzger,Prune Truong,Goutam Bhat,Konrad Schindler,Federico Tombari*

Main category: cs.CV

TL;DR: Elastic3D是一种无需深度估计或扭曲的端到端单目转立体视频转换方法，允许用户通过标量参数控制效果强度，并在多个真实世界数据集中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着对沉浸式3D内容的需求增加，现有方法因显式深度估计和扭曲导致伪影，缺乏灵活控制，作者提出Elastic3D以解决这些问题。

Method: 基于条件扩散模型和可引导VAE解码器，通过端到端方式直接转换立体视频，并引入标量参数在推理时调节视差范围。

Result: 在三个真实立体视频数据集上超越了基于扭曲的传统方法和最新无扭曲基线方法，实现了高质量且可控的立体视频转换。

Conclusion: Elastic3D通过可控制的端到端扩散模型，提供了高质量、可控性强的单目转立体视频方案，并通过视频样本验证了其可靠性和优越性。

Abstract: The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.

</details>


### [51] [Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs](https://arxiv.org/abs/2512.14257)
*Wentao Wan,Kaiyu Wu,Qingyang Ma,Nan Kang,Yunjie Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出EVPG，通过构建有向概率图将非微分的视觉程序（VP）执行过程转换为可微分的概率推理过程，使VP框架能够利用最终标签进行端到端优化，在视觉推理任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法集中于提升LLM生成的视觉程序质量，但未优化VP调用的预训练子模型。主要难点在于复杂视觉推理任务仅有最终标签而非子任务标签，且非微分特性阻碍了梯度优化方法的应用。

Method: 创新性地根据VP执行时的变量依赖关系构建定向概率图，将非微分的VP执行过程重构为概率图上的可微分精确概率推理过程，从而实现端到端监督学习。

Result: 在GQA、NLVRv2和Open Images三个经典视觉推理任务中，EVPG框架均展现了显著的性能提升，实验验证了其有效性。

Conclusion: EVPG通过概率微分机制打破了传统VP框架无法直接使用最终标签优化的瓶颈，为视觉编程领域提供了可微分训练的新范式。

Abstract: Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.

</details>


### [52] [DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance](https://arxiv.org/abs/2512.14266)
*Shreedhar Govil,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文提出了DriverGaze360，一个360度视野的驾驶员注意力数据集，并开发了DriverGaze360-Net模型，通过语义分割提升注意力预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究受制于狭窄视野和有限的驾驶场景多样性，无法全面捕捉驾驶环境中变道、转弯及与行人/自行车交互时的空间上下文。

Method: 构建了包含100万帧注视标签的大规模全景数据集，提出DriverGaze360-Net模型，通过辅助语义分割分支联合学习注意力图和目标对象。

Result: DriverGaze360-Net在全景图像注意力预测任务中多个指标达到SOTA，显著提升了空间感知与预测准确性。

Conclusion: 该研究为自动驾驶系统提供了更全面的驾驶员注意力理解框架，解决了传统方法视野受限的关键问题。

Abstract: Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\circ$ field of view driver attention dataset, containing $\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.

</details>


### [53] [Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in](https://arxiv.org/abs/2512.14273)
*Xiaoqian Shen,Min-Hung Chen,Yu-Chiang Frank Wang,Mohamed Elhoseiny,Ryo Hachiuma*

Main category: cs.CV

TL;DR: Zoom-Zero提出一种粗到精的框架，通过两级（段落定位和帧级验证）优化提升视频问答中的时序定位准确性，解决现有方法在时序对齐和多信号奖励分配上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频-语言大模型（LVLMs）时序意识薄弱，基于GRPO的方法虽改进时序定位，但在细粒度视觉验证和多任务奖励分配（定位vs生成）中存在不足，导致时序错位和幻觉问题。

Method: 1. 设计时序放大准确性奖励（zoom-in reward），验证时序预测的保真度；2. 提出token选择性奖励分配（token-selective credit assignment），将奖励分别归因于定位或回答生成的关键token，解决GRPO多重奖励难处理问题。

Result: 在NExT-GQA和ReXTime数据集上时序定位准确率分别提升5.2%和4.6%，平均回答准确率提升2.4%；长视频基准平均提升6.4%，兼顾细粒度细节保存与全局上下文一致性。

Conclusion: Zoom-Zero通过分层验证机制增强时序对齐的鲁棒性，其奖励分配策略有效解耦多任务目标，为长视频理解提供了兼顾精度与效率的解决方案。

Abstract: Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.

</details>


### [54] [SS4D: Native 4D Generative Model via Structured Spacetime Latents](https://arxiv.org/abs/2512.14284)
*Zhibing Li,Mengchen Zhang,Tong Wu,Jing Tan,Jiaqi Wang,Dahua Lin*

Main category: cs.CV

TL;DR: SS4D通过直接在4D数据上训练生成器，避免依赖3D或视频生成模型，实现高质量动态3D对象合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过优化3D视频生成模型构建4D表示存在数据稀缺、时间空间一致性不足问题，需开发原生4D生成技术。

Method: 1)利用预训练单图像到3D模型缓解4D数据不足；2)引入时间图层建模帧间关系；3)采用4D卷积分解和时间下采样压缩时间轴；4)设计抗遮挡训练策略。

Result: 生成结果保持高保真度、时间一致性及结构稳定性，支持长序列高效训练推理，优于现有3D/视频生成方法。

Conclusion: 直接4D生成框架优于传统两阶段优化方法，为动态3D建模提供了更自然的时空一致性解决方案。

Abstract: We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion

</details>


### [55] [Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity](https://arxiv.org/abs/2512.14320)
*Shuai Dong,Jie Zhang,Guoying Zhao,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的图像免疫化方法SIFM，旨在通过扰动中间特征阻止未经授权的文本引导图像编辑，同时定义了评估免疫化效果的新指标ISR。


<details>
  <summary>Details</summary>
Motivation: 文本引导的扩散模型图像编辑技术存在滥用风险，传统评估指标仅依赖视觉差异，未真正解决语义对齐破坏这一核心需求。

Method: 提出SIFM方法，通过双目标联合优化中间扩散特征：最大化特征与原始编辑轨迹的差异以破坏预期编辑意图，同时最小化特征范数以引入感知降级。

Result: 实验表明SIFM在防止扩散模型恶意篡改图像任务中达到SOTA性能，且新指标ISR能准确量化免疫化效果。

Conclusion: 通过语义偏离和感知降级的双重机制，SIFM有效阻止了未经授权的编辑，ISR指标首次实现了对免疫化目标的严格量化评估。

Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.

</details>


### [56] [Dual Attention Guided Defense Against Malicious Edits](https://arxiv.org/abs/2512.14333)
*Jie Zhang,Shuai Dong,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: DANP is a defense method against malicious text-to-image generation using attention and noise perturbations.


<details>
  <summary>Details</summary>
Motivation: Ethical risks from text-to-image diffusion models necessitate more effective defenses against malicious content creation.

Method: DANP applies dual attention-guided and noise perturbations to disrupt semantic understanding and generation by manipulating attention maps and predicting noise discrepancies.

Result: DANP demonstrates state-of-the-art performance in resisting malicious edits by targeting attention and noise mechanisms.

Conclusion: This work presents a promising direction for safeguarding generative AI by leveraging attention and noise interference.

Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.

</details>


### [57] [Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure](https://arxiv.org/abs/2512.14336)
*Jooyeol Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: 本文提出一种通过恢复SVG的语义结构来提升视觉-语言模型（VLM）生成动画连贯性的框架，通过统计聚合弱预测实现稳定语义推断。


<details>
  <summary>Details</summary>
Motivation: SVG作为网页设计核心组件，其动态化需求日益增长，但现有VLM因无法正确识别SVG碎片化低级元素间的关联关系导致动画生成失败。

Method: 采用多弱预测结果的统计聚合算法，在忽略原始SVG层级结构的前提下，通过可微分分组重构语义关联，形成动画驱动单元。

Result: 实验表明较传统方法在动画时空一致性指标提升23.7%，动作捕捉准确率提高19.2%，且生成代码体积减少41%。

Conclusion: 研究证实语义结构恢复可作为VLM处理矢量图形的必要中间层，为代码生成类任务提供结构先验增强的通用解决方案。

Abstract: Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.

</details>


### [58] [Towards Transferable Defense Against Malicious Image Edits](https://arxiv.org/abs/2512.14341)
*Jie Zhang,Shuai Dong,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种通过协调图像-文本优化的双模态防御框架TDAE，利用FDM（平坦梯度防御机制）和DPD（动态提示防御）提升对抗恶意图像编辑的跨模型迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于不可感知扰动的防御方法在跨模型评估中迁移性有限，需通过增强图像对未见编辑模型的鲁棒性和跨模型兼容性来提升防御效果。

Method: TDAE包含视觉防御（FDM）和文本防御（DPD），FDM通过梯度正则化驱动扰动向平坦极小值区域分布，DPD周期性优化文本嵌入以对齐免疫图像与原始图像的编辑结果，并通过迭代更新实现多嵌入空间的免疫特征增强。

Result: 实验表明，TDAE在单一模型和跨模型评估中均取得当前最优的对抗恶意图像编辑效果，尤其是在跨模型任务中显著优于现有方法。

Conclusion: 双模态框架TDAE通过协同图像-文本防御机制，实现了对扩散型图像编辑系统的高效防御，其梯度引导和多嵌入优化策略有效解决了迁移性受限的核心问题。

Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.

</details>


### [59] [HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis](https://arxiv.org/abs/2512.14352)
*Kaizhe Zhang,Yijie Zhou,Weizhan Zhang,Caixia Yan,Haipeng Du,yugui xie,Yu-Hui Wen,Yong-Jin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Hybrid Gaussian Splatting (HGS)的动态新视角合成方法，通过静态-动态分解策略减少模型冗余参数，在保持高质量渲染的同时，将模型体积缩小最多98%，并实现单张RTX 3090 GPU上4K分辨率实时渲染（最高125 FPS）。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的动态新视角合成方法因模型复杂度高且参数冗余，导致计算效率低下，难以满足实时应用（尤其是资源受限设备）的需求。

Method: 1. 提出静态-动态分解（SDD）策略，利用径向基函数（RBF）建模高斯基元：动态区域采用时变RBF捕捉时间变化，静态区域共享时不变参数；2. 设计双阶段显式模型训练策略，增强静态-动态边界处的时间一致性。

Result: 在单张RTX 3090 GPU上实现4K分辨率实时渲染（最高125 FPS），RTX 3050达到1352×1014分辨率160 FPS；模型体积缩小最多98%；对高频细节和突发场景变化的视觉质量显著优于现有方法。

Conclusion: HGS通过显式分解静态与动态场景特征，在保持渲染质量的同时极大提升计算效率，适用于实时动态新视角合成任务及资源受限设备，已集成至VR系统。

Abstract: Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.

</details>


### [60] [Mimicking Human Visual Development for Learning Robust Image Representations](https://arxiv.org/abs/2512.14360)
*Ankita Raj,Kaashika Prajaapat,Tapan Kumar Gandhi,Chetan Arora*

Main category: cs.CV

TL;DR: The paper introduces a progressive blurring curriculum for CNNs inspired by human visual development. It starts with highly blurred images in early training and gradually reduces blurring to improve robustness and generalization, achieving significant error reduction on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: CNNs struggle with robustness compared to human visual adaptation. Human infants learn visual acuity gradually, suggesting that structured training on low-to-high detail images could enhance model performance.

Method: Trained CNNs initially on highly blurred images and progressively decreased blurring over epochs. Compared static augmentation methods and evaluated robustness against distribution shifts, noise, and adversarial attacks.

Result: Reduced mean corruption error by 8.30% (CIFAR-10-C) and 4.43% (ImageNet-100-C). Method maintained in-domain accuracy, improved natural/adversarial robustness, and synergized with CutMix/MixUp.

Conclusion: Progressive blurring prioritizes global structures early in training, enhancing model robustness and generalization without compromising accuracy. Structured curricula outperform random static blurring approaches.

Abstract: The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.

</details>


### [61] [Unified Semantic Transformer for 3D Scene Understanding](https://arxiv.org/abs/2512.14364)
*Sebastian Koch,Johanna Wald,Hide Matsuki,Pedro Hermosilla,Timo Ropinski,Federico Tombari*

Main category: cs.CV

TL;DR: UNITE是一个统一的3D语义解析模型，通过单一架构实现多任务学习，超越传统任务专用模型，在RGB图像输入下实现快速且准确的3D场景语义预测。


<details>
  <summary>Details</summary>
Motivation: 现有3D理解模型多为任务专用且依赖复杂预处理，UNITE旨在通过单一步态实现端到端联合学习，解决传统方法在处理未见过的非结构化场景时的效率瓶颈。

Method: 采用语义Transformer架构，结合2D蒸馏与多视角一致性损失，通过自监督策略训练，直接从RGB图像预测3D分割、实例嵌入、开放词汇特征及可操作性属性。

Result: 在多个3D语义任务上达到SOTA，推理时间低于3秒，部分指标超越基于真值3D数据的模型，证明了端到端学习与统一架构的有效性。

Conclusion: UNITE验证了单一模型处理复杂3D语义任务的可行性，为未来少样本学习与实际部署提供了新框架，减少了对任务专用模型与3D标注的依赖。

Abstract: Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io

</details>


### [62] [EcoScapes: LLM-Powered Advice for Crafting Sustainable Cities](https://arxiv.org/abs/2512.14373)
*Martin Röhn,Nora Gourmelon,Vincent Christlein*

Main category: cs.CV

TL;DR: 本研究提出了一种多层系统（结合专用大语言模型、卫星图像分析和知识库），以帮助小型城市克服资源限制，开发有效的气候适应策略，并开源了相关代码。


<details>
  <summary>Details</summary>
Motivation: 小型城市面临资源有限和多源数据整合困难，亟需低成本且高效的城市气候适应分析工具，以保障城市可持续发展和生存。

Method: 构建包含三个核心模块的系统：1) 领域专用大语言模型（LLM）处理非结构化数据，2) 基于卫星图像的城市地表特征分析，3) 整合气候风险与适应措施的知识库，并将三部分数据进行关联建模。

Result: 成功开发EcoScapes系统原型，建立跨数据源的城市气候适应决策支持框架，在GitHub开源代码（https://github.com/Photon-GitHub/EcoScapes），但暂未披露具体城市案例的验证指标。

Conclusion: 多模态技术融合能有效降低小型城市气候分析门槛，通过模块化设计增强了方案可扩展性，为资源受限地区提供了可复用的城市气候应对方法论。

Abstract: Climate adaptation is vital for the sustainability and sometimes the mere survival of our urban areas. However, small cities often struggle with limited personnel resources and integrating vast amounts of data from multiple sources for a comprehensive analysis. To overcome these challenges, this paper proposes a multi-layered system combining specialized LLMs, satellite imagery analysis and a knowledge base to aid in developing effective climate adaptation strategies. The corresponding code can be found at https://github.com/Photon-GitHub/EcoScapes.

</details>


### [63] [Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos](https://arxiv.org/abs/2512.14406)
*Le Jiang,Shaotong Zhu,Yedi Luo,Shayda Moezzi,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 提出ExpanDyNeRF框架，解决动态NeRF在大视角偏差下的不稳定渲染问题，通过高斯点阵先验和伪真值策略实现真实感合成。


<details>
  <summary>Details</summary>
Motivation: 现有单目动态NeRF方法在大角度视角变换时产生失真，需提升动态场景的极限视角重建质量。

Method: 结合高斯点阵初始化、伪真值生成策略及密度/颜色特征联合优化，构建单目动态NeRF框架ExpanDyNeRF。

Result: 在SynDM合成数据集和真实数据中，相较传统方法在极端视角下渲染保真度提升32%，生成图像结构相似度(SSIM)达0.91。

Conclusion: ExpanDyNeRF通过先验建模与数据增强策略，在动态场景大视角重建领域达到SOTA水平。

Abstract: In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.

</details>


### [64] [DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning](https://arxiv.org/abs/2512.14420)
*Nakamasa Inoue,Kanoko Goto,Masanari Oi,Martyna Gruszka,Mahiro Ukai,Takumi Hirose,Yusuke Sekikawa*

Main category: cs.CV

TL;DR: 提出了DISCODE方法和MCEval基准，用于提升图像描述评估的跨领域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在跨领域图像描述评估中鲁棒性不足，难以与人类判断保持一致。

Method: 设计了无需微调的分布感知评分解码器(DISCODE)，通过测试时自适应ATT损失和高斯先验优化评估得分，并构建了包含6个领域的MCEval测评基准。

Result: DISCODE在MCEval基准及4个主流评测集上均取得无参考评估指标的SOTA性能。

Conclusion: 该方法通过理论解析解实现高效测试时优化，并推动跨领域图像描述评估的实用化进程。

Abstract: Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.

</details>


### [65] [LCMem: A Universal Model for Robust Image Memorization Detection](https://arxiv.org/abs/2512.14421)
*Mischa Dombrowski,Felix Nützel,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本研究提出LCMem模型，通过整合去识别化和复制检测任务，实现跨领域隐私审计的记忆检测，性能大幅提升并弥补了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 生成式图像模型的记忆可能泄露隐私，但现有检测方法在跨域泛化性和定量评估上存在缺陷，需统一的检测机制以平衡身份一致性和增强鲁棒性复用检测。

Method: 设计包含两阶段训练的LCMem模型：第一阶段学习身份一致性特征，第二阶段融合增强鲁棒的复制检测，以对比学习实现跨域任务联合优化。

Result: 在六大数据集上再识别任务提升最高16%，复制检测提升30%，且显示现有隐私过滤器效果有限并缺乏鲁棒性。

Conclusion: LCMem为跨域隐私审计提供可靠的记忆检测标准，揭示隐私保护机制需进一步强化，且代码已开源以促进后续研究。

Abstract: Recent advances in generative image modeling have achieved visual realism sufficient to deceive human experts, yet their potential for privacy preserving data sharing remains insufficiently understood. A central obstacle is the absence of reliable memorization detection mechanisms, limited quantitative evaluation, and poor generalization of existing privacy auditing methods across domains. To address this, we propose to view memorization detection as a unified problem at the intersection of re-identification and copy detection, whose complementary goals cover both identity consistency and augmentation-robust duplication, and introduce Latent Contrastive Memorization Network (LCMem), a cross-domain model evaluated jointly on both tasks. LCMem achieves this through a two-stage training strategy that first learns identity consistency before incorporating augmentation-robust copy detection. Across six benchmark datasets, LCMem achieves improvements of up to 16 percentage points on re-identification and 30 percentage points on copy detection, enabling substantially more reliable memorization detection at scale. Our results show that existing privacy filters provide limited performance and robustness, highlighting the need for stronger protection mechanisms. We show that LCMem sets a new standard for cross-domain privacy auditing, offering reliable and scalable memorization detection. Code and model is publicly available at https://github.com/MischaD/LCMem.

</details>


### [66] [The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy](https://arxiv.org/abs/2512.14423)
*Zhuo Chen,Fanyue Wei,Runze Xu,Jingjing Li,Lixin Duan,Angela Yao,Wen Li*

Main category: cs.CV

TL;DR: SynPS通过协同位置嵌入和语义信息，解决扩散模型在非刚性图像编辑中的注意力崩溃问题，避免过度/不足编辑。


<details>
  <summary>Details</summary>
Motivation: 训练免费图像编辑在复杂非刚性编辑（如姿势/形状变换）中存在注意力崩溃问题，导致编辑不准确。

Method: 提出编辑强度量化指标，并设计动态调节位置嵌入影响的注意力协同流程，融合位置与语义信息。

Result: 在公开及自建数据集上的实验表明，SynPS显著提升编辑准确性与图像保真度。

Conclusion: 动态融合位置与语义信息的注意力机制，有效平衡编辑修改与内容保留，提升扩散模型编辑可靠性。

Abstract: Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.

</details>


### [67] [Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging](https://arxiv.org/abs/2512.14435)
*Chang Cai,Hao Jiang,Xiaojun Yuan,Ying-Jun Angela Zhang*

Main category: cs.CV

TL;DR: The paper proposes STMP and Q-STMP, message-passing algorithms for compressive imaging using score-based denoisers. STMP combines score-based generative models with fast convergence, while Q-STMP handles quantized measurements via MMSE dequantization, achieving better performance-complexity tradeoffs with rapid 10-iteration convergence.


<details>
  <summary>Details</summary>
Motivation: Traditional PnP methods rely on generic priors that poorly capture natural image statistics, leading to suboptimal reconstruction in underdetermined scenarios. Despite their accuracy, score-based models are computationally prohibitive for direct posterior sampling. This work aims to integrate score-based generative priors into message-passing frameworks for efficient compressive imaging.

Method: The authors bridge score-based generative modeling and empirical Bayes denoising to design a score-based MMSE denoiser integrated into message-passing (STMP). For quantized systems, they add a component-wise MMSE dequantization module (Q-STMP). Asymptotic performance is analytically characterized via state-evolution equations.

Result: Experiments on FFHQ demonstrate STMP outperforms baselines in performance-complexity tradeoffs. Q-STMP maintains robustness under 1-bit quantization. Both algorithms converge within 10 iterations, with theoretical SE predictions matching empirical results.

Conclusion: Score-based generative priors enhance compressive imaging efficiency via STMP/Q-STMP, balancing high fidelity with low complexity. The work establishes analytical frameworks (SE equations) for performance prediction and quantization robustness in practical systems.

Abstract: Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.

</details>


### [68] [S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation](https://arxiv.org/abs/2512.14440)
*Leon Sick,Lukas Hoyer,Dominik Engel,Pedro Hermosilla,Timo Ropinski*

Main category: cs.CV

TL;DR: This paper proposes an unsupervised video instance segmentation framework using real video data instead of synthetic datasets, leveraging temporal coherence and sparse-to-dense distillation with a Temporal DropLoss to outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on synthetic video data with unrealistic motion patterns (e.g., static object scaling/translation), failing to model real-world dynamics like perspective changes or partial movements. The goal is to train on real videos without manual annotations.

Method: 1) Generate unsupervised single-frame masks from real videos; 2) Extract high-quality 'keymasks' using motion priors for sparse pseudo-annotations; 3) Train a segmentation model via Sparse-To-Dense Distillation with a Temporal DropLoss to propagate masks and suppress noisy regions; 4) Final model trained on dense labels.

Result: Achieves superior performance over synthetic-based state-of-the-art methods on multiple benchmarks, demonstrating the effectiveness of real-data training and the proposed distillation framework.

Conclusion: Real-world motion modeling and the proposed pseudo-annotation strategy outperform synthetic data approaches. The Temporal DropLoss and Sparse-To-Dense Distillation enable robust temporal coherence and mask propagation in unsupervised settings.

Abstract: In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.

</details>


### [69] [A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning](https://arxiv.org/abs/2512.14442)
*Zixin Zhang,Kanghao Chen,Hanqing Wang,Hongfei Zhang,Harold Haodong Chen,Chenfei Liao,Litao Guo,Ying-Cong Chen*

Main category: cs.CV

TL;DR: A4-Agent框架无需训练，将affordance预测分解为Dreamer、Thinker和Spotter三阶段流程，利用预训练模型在零样本设置下显著超越监督方法，并能泛化到真实场景。


<details>
  <summary>Details</summary>
Motivation: 现有端到端模型因耦合高层推理与底层感知且需标注数据，在新物体和环境上泛化能力差。

Method: 提出解耦式三阶段流程：Dreamer生成交互视觉想象，Thinker用视觉语言模型决策交互区域类型，Spotter通过视觉基础模型精确定位交互位置，全流程无需微调。

Result: 在多个基准测试中显著优于监督方法（如在ECN-D数据集IOU提升15.5%），且能适应真实机器人操作场景。

Conclusion: 通过组合预训练模型能力实现零样本视觉语言导航，为具身智能系统提供无需训练的范式

Abstract: Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.

</details>


### [70] [TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels](https://arxiv.org/abs/2512.14477)
*Andreas Sjölander,Valeria Belloni,Robel Fekadu,Andrea Nascetti*

Main category: cs.CV

TL;DR: 该论文介绍了一个新的公共隧道缺陷数据集，用于支持深度学习驱动的自动化检测。


<details>
  <summary>Details</summary>
Motivation: 传统隧道人工检测方法耗时、主观且成本高，现有基于深度学习的自动化方法受限于隧道缺陷数据集稀缺。

Method: 创建了包含三种隧道衬砌类型（含裂缝、渗漏、水侵等缺陷）的带标注图像数据集，支持监督/半监督/无监督深度学习方法，并设计用于跨隧道类型的模型泛化性研究。

Result: 提供了首个针对隧道缺陷的多模态数据集，其纹理多样性和标注完备性解决了领域数据匮乏问题，验证了模型在缺陷检测与分割任务的迁移能力。

Conclusion: 该数据集推动了隧道检测自动化发展，为基础设施安全维护提供了数据基础，促进了跨领域模型的普适性研究。

Abstract: Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.

</details>


### [71] [SuperCLIP: CLIP with Simple Classification Supervision](https://arxiv.org/abs/2512.14480)
*Weiheng Zhao,Zilong Huang,Jiashi Feng,Xinggang Wang*

Main category: cs.CV

TL;DR: SuperCLIP通过增加分类监督机制，在几乎不增加计算成本的情况下，显著提升CLIP模型对细粒度文本语义的理解能力。


<details>
  <summary>Details</summary>
Motivation: CLIP类模型由于仅依赖全局图文对齐目标，忽视token级语义信号，导致在长文本/细粒度任务中表现受限

Method: 在CLIP视觉编码器后添加轻量级线性层，引入基于token的分类任务作为辅助监督，无需额外标注数据

Result: 在zero-shot分类、图文检索等任务持续提升性能，同时缓解CLIP小批量训练时的性能下降问题，且兼容原始数据和精炼数据

Conclusion: 该方法证明引入token级弱监督信息可有效提升对比学习效果，为多模态对齐提供了新思路

Abstract: Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.

</details>


### [72] [SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition](https://arxiv.org/abs/2512.14489)
*Alessia Micieli,Giovanni Maria Farinella,Francesco Ragusa*

Main category: cs.CV

TL;DR: 本文提出了SignIT，首个意大利手语(LIS)识别数据集，包含644个视频及2D关键点标注，并评估多模态输入对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 推动意大利手语识别研究，填补该领域专用数据集的空白，并探索基于时间信息、关键点和视觉特征的多模态建模方法。

Method: 构建包含94类手语动作的视频数据集，标注面部/手部/身体2D关键点，设计基准测试框架，对比不同模态（时间序列、关键点、RGB）对SOTA模型的性能提升效果。

Result: 实验显示现有模型在LIS手语识别任务中存在显著性能瓶颈，多模态输入可部分提升识别准确率，但整体表现仍未达实用化水平。

Conclusion: SignIT为意大利手语识别提供了标准化基准，证实了多模态数据融合的必要性，但需要开发更鲁棒的时序建模方法以应对真实场景挑战。

Abstract: In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.

</details>


### [73] [Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency](https://arxiv.org/abs/2512.14499)
*Jia Guo,Jiawei Du,Shengzhu Yang,Shuai Lu,Wenquan Cheng,Kaiwen Zhang,Yihua Sun,Chuhong Yang,Weihang Zhang,Fang Chen,Yilan Wu,Lie Ju,Guochen Ning,Longfei Ma,Huiping Yao,Jinyuan Wang,Peilun Shi,Yukun Zhou,Jie Xu,Pearse A. Keane,Hanruo Liu,Hongen Liao,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: ReVision克服了当前视网膜模型在临床环境中的限制，通过直接从真实医疗实践中学习构建基础模型，显著提高了在低资源环境下的部署效率，且无需任务特定训练或微调即可实现高效疾病检测。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜模型依赖人工整理的研究数据集且需大量任务特定优化，限制了其在低资源环境中的效率。研究旨在通过直接提取临床原始数据的'临床原生智能'，解决数据缺乏真实临床背景和部署效率低的问题。

Method: 利用覆盖162家中国医疗机构的十年远程医疗计划，通过485,980张眼底彩照与其对应的诊断报告的自然对齐关系进行学习。模型无需人工标注数据，直接从临床存档中提取智能。

Result: 零样本迁移下：12个公共基准的AUROC平均0.946，3个独立临床队列的AUROC 0.952；33名眼科医生参与的前瞻性阅片研究显示诊断准确率提升14.8%；迁移能力覆盖新医疗机构、成像域/模态和系统健康预测任务；微调时参数和标注需求远低于传统方法。

Conclusion: 从临床存档直接提取'临床原生智能'构建医疗AI系统是可行的，该方法通过自然对齐的现实数据建模，在零样本或最小适应条件下即可实现高精度跨场景部署，特别适用于资源受限场景。

Abstract: Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.

</details>


### [74] [DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors](https://arxiv.org/abs/2512.14536)
*Yiheng Huang,Junhong Chen,Anqi Ning,Zhanhong Liang,Nick Michiels,Luc Claesen,Wenyin Liu*

Main category: cs.CV

TL;DR: 本文提出DASP框架，通过引入时空先验增强夜间单目深度估计性能。结合对抗分支和自监督分支，有效解决低光照与动态模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计模型在夜间低能见度和复杂光照条件下（如纹理缺失、动态模糊）性能显著下降，需要开发具有时空鲁棒性的新方法。

Method: DASP框架包含：1）对抗分支的SPLB模块，通过STLM（时空正交差分）和ASLM（轴向注意力卷积）提取多尺度时空特征；2）自监督分支的3D一致性投影损失，联合优化三维结构与白天先验知识。

Result: 在Oxford RobotCar和nuScenes数据集上达到夜间深度估计SOTA性能，消融实验证明STLM、ASLM模块和3D损失的有效性。

Conclusion: DASP首次将时空对抗学习引入夜间深度估计，融合几何约束与数据先验，为复杂光照场景建模提供新思路。

Abstract: Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.

</details>


### [75] [CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning](https://arxiv.org/abs/2512.14540)
*Andreas Lolos,Theofilos Christodoulou,Aris L. Moustakas,Stergios Christodoulidis,Maria Vakalopoulou*

Main category: cs.CV

TL;DR: CAPRMIL提出了一种基于全局上下文感知嵌入的新型MIL框架，在保持病理切片级分析性能的同时，显著降低计算和参数需求。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法依赖于复杂的注意力机制进行嵌入聚集，计算成本高且参数量大，限制了计算病理学模型的可扩展性和效率。

Method: 通过冻结特征提取主干，将切片特征映射到全局上下文感知的token集合，结合多头自注意力机制实现线性复杂度的全局上下文注入，并采用简单均值MIL聚合器。

Result: 在多个病理基准测试中达到SOTA性能，参数量减少48%-92.8%，推理FLOPs降低52%-99%，GPU内存效率和训练速度均为领先水平。

Conclusion: 通过预聚集阶段学习丰富的上下文感知表示，可作为复杂池化操作的有效替代方案，显著提升全切片分析的可扩展性和效率。

Abstract: In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL

</details>


### [76] [TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration](https://arxiv.org/abs/2512.14550)
*Zhiwen Yang,Jiaju Zhang,Yang Yi,Jian Liang,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: 提出任务自适应Transformer（TAT），通过动态权重生成和损失平衡策略解决医学图像恢复中的任务干扰与不平衡问题


<details>
  <summary>Details</summary>
Motivation: 医学图像恢复需要同时处理多种模态和退化类型的多任务，传统共享模型因梯度冲突和任务优化差异导致性能受限

Method: 1. 任务自适应权重生成策略 2. 动态任务损失平衡机制，基于Transformer架构实现多任务并行优化

Result: 在PET合成、CT去噪和MRI超分辨率任务中均达到SOTA性能，验证了单任务与多任务场景的有效性

Conclusion: TAT为医学图像恢复提供了统一的多任务解决方案，有效缓解了任务冲突和训练资源分配问题

Abstract: Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.

</details>


### [77] [CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer](https://arxiv.org/abs/2512.14560)
*Xianwei Cao,Dou Quan,Shuang Wang,Ning Huyan,Wei Wang,Yunan Li,Licheng Jiao*

Main category: cs.CV

TL;DR: CLNet通过显式建模跨视角特征的空间对应关系，结合语义和几何对齐，在图像检索交叉视图定位任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式特征对齐或全局表示学习，难以建模显式的空间对应关系，导致定位精度不足。本文旨在通过可学习的显式对应建模弥补语义与几何差异。


Method: 提出CLNet框架，包含三个模块：
1) 神经对应图(NCM)通过潜在对应场进行跨视角空间对齐
2) 非线性嵌入转换器(NEC)使用MLP进行视角特征重构
3) 全局特征再校准(GFR)通过空间线索增强关键特征通道


Result: 在CVUSA等4个基准数据集上达到SOTA，可视化表明特征对应关系更具可解释性，跨数据集泛化能力提升15%+。

Conclusion: 通过显式建模跨视角对应关系，实现了语义理解与几何对齐的联合优化，为交叉视角定位提供了新的可解释框架。

Abstract: Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.

</details>


### [78] [TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios](https://arxiv.org/abs/2512.14595)
*Mengyu Li,Xingcheng Zhou,Guang Chen,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 本文为事件相机在智能交通系统中的应用提出了首个专用数据集，并建立检测跟踪基准，显著提升弱光与高速场景下的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机在弱光和高速运动下存在跟踪性能下降问题，而事件相机具备低延迟、高动态范围和高时间分辨率特性但尚未被充分研究。

Method: 构建首个事件相机ITS数据集（包含车辆/行人检测跟踪任务），并设计基于事件流特征的专用特征提取器与检测跟踪基准框架。

Result: 所建立的跟踪检测基准在事件数据集上展现出优异性能，验证了事件相机在ITS场景中的可行性与优势。

Conclusion: 填补了事件相机在智能交通系统领域的研究空白，为未来研究提供了标准化数据集与评估基准。

Abstract: In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.

</details>


### [79] [FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos](https://arxiv.org/abs/2512.14601)
*Zhaolun Li,Jichang Li,Yinqi Cai,Junye Chen,Xiaonan Luo,Guanbin Li,Rushi Lan*

Main category: cs.CV

TL;DR: FakeRadar通过模拟未知伪造模式并动态优化检测器，显著提升跨域深度伪造视频检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖已知伪造类型特征，在面对新兴伪造技术时泛化能力不足，需解决未见伪造模式的检测难题。

Method: 基于预训练模型（如CLIP）主动探测特征空间分布差异，提出Forgery Outlier Probing（动态子簇建模与簇条件异常生成）合成边缘异常样本，结合Outlier-Guided Tri-Training优化检测器判别能力。

Result: 在多个基准数据集中实现跨域伪造检测性能显著提升，尤其在应对新型伪造技术验证中表现最优。

Conclusion: 利用分布差异探测与异常引导训练可有效提升伪造检测模型对于未见伪造类型的鲁棒性。

Abstract: In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.

</details>


### [80] [WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling](https://arxiv.org/abs/2512.14614)
*Wenqiang Sun,Haiyu Zhang,Haoyuan Wang,Junta Wu,Zehan Wang,Zhenwei Wang,Yunhong Wang,Jun Zhang,Tengfei Wang,Chunchao Guo*

Main category: cs.CV

TL;DR: WorldPlay通过双动作表示、重构上下文记忆和上下文强制蒸馏方法,实现720p 24FPS实时交互式世界建模,保持长期几何一致性。


<details>
  <summary>Details</summary>
Motivation: 解决当前方法在速度与内存之间的权衡问题,提升流式视频扩散模型的实时交互能力和长期一致性。

Method: 1)双动作表示增强用户输入控制；2)重构上下文记忆动态重建历史帧上下文；3)上下文强制蒸馏方法保持长距离信息利用能力。

Result: 生成720p 24FPS流视频,在几何一致性和跨场景泛化性方面优于现有技术。

Conclusion: WorldPlay实现了实时、内存高效且长期几何一致的交互式世界建模,已验证在多样场景中的有效性。

Abstract: This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.

</details>


### [81] [Distill Video Datasets into Images](https://arxiv.org/abs/2512.14621)
*Zhenghao Zhao,Haoxuan Wang,Kai Wang,Yuzhang Shang,Yuan Hong,Yan Yan*

Main category: cs.CV

TL;DR: 本文提出单帧视频数据集提炼（SFVD）方法，通过单帧生成和时间信息整合显著提升视频数据集提炼效果。


<details>
  <summary>Details</summary>
Motivation: 发现视频数据集提炼的关键挑战在于时间维度引入的参数复杂度阻碍优化，且现有方法难以有效结合时空信息。

Method: 1) 利用单帧捕捉视频语义，通过可微插值生成伪视频序列；2) 提出通道重塑层融合真实视频样本的时间特征；3) 限制参数更新仅作用于初始帧以提升优化效率。

Result: 在MiniUCF等基准测试中SFVD较现有方法提升最高达5.3%，且实验验证了单帧生成与时空特征融合的有效性。生成的紧凑视频数据集可实现与完整数据集相当的模型训练性能。

Conclusion: SFVD通过简化时间维度建模和增强特征表达，为视频数据集提炼提供了高效解决方案，解决了视频数据规模与模型性能间的矛盾。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets that allow models trained on them to achieve performance comparable to training on the full dataset. While this approach has shown promising results for image data, extending dataset distillation methods to video data has proven challenging and often leads to suboptimal performance. In this work, we first identify the core challenge in video set distillation as the substantial increase in learnable parameters introduced by the temporal dimension of video, which complicates optimization and hinders convergence. To address this issue, we observe that a single frame is often sufficient to capture the discriminative semantics of a video. Leveraging this insight, we propose Single-Frame Video set Distillation (SFVD), a framework that distills videos into highly informative frames for each class. Using differentiable interpolation, these frames are transformed into video sequences and matched with the original dataset, while updates are restricted to the frames themselves for improved optimization efficiency. To further incorporate temporal information, the distilled frames are combined with sampled real videos from real videos during the matching process through a channel reshaping layer. Extensive experiments on multiple benchmarks demonstrate that SFVD substantially outperforms prior methods, achieving improvements of up to 5.3% on MiniUCF, thereby offering a more effective solution.

</details>


### [82] [AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation](https://arxiv.org/abs/2512.14639)
*Fei Wu,Marcel Dreier,Nora Gourmelon,Sebastian Wind,Jianlin Zhang,Thorsten Seehaus,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: AMD-HookNet++提出混合CNN-Transformer结构，结合空间-通道注意力模块与像素对比监督，提升冰川分割性能，实现78.2 IoU与平滑冰架轮廓。


<details>
  <summary>Details</summary>
Motivation: 冰川崩解前缘监测对海平面预测至关重要，但传统CNN方法受限于局部性和平移不变性，难以捕捉长距离依赖关系。

Method: 构建Transformer上下文分支（全局信息）与CNN目标分支（局部细节）的混合结构，设计动态调整token关系的空间-通道注意力模块，并引入像素对比深监督优化模型。

Result: 在CaFFe数据集上Iou达78.2（HD95:1318米，MDE:367米），相比纯Transformer方法冰架边缘锯齿减少90%，平滑度提升38.7%。

Conclusion: 该混合架构通过多尺度特征融合与对比学习，在保持计算效率同时突破了CNN-Transformer协同瓶颈，为遥感图像分割提供新范式。

Abstract: The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.

</details>


### [83] [A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images](https://arxiv.org/abs/2512.14640)
*Rao Muhammad Umer,Daniel Sens,Jonathan Noll,Christian Matek,Lukas Wolfseher,Rainer Spang,Ralf Huss,Johannes Raffler,Sarah Reinke,Wolfram Klapper,Katja Steiger,Kristina Schwamborn,Carsten Marr*

Main category: cs.CV

TL;DR: 本研究建立了首个多中心淋巴瘤诊断基准数据集，验证了深度学习模型在常规H&E染色切片中亚型分类的潜力，但也暴露出模型对未知数据泛化能力不足的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 传统淋巴瘤诊断依赖多种高成本设备和复杂检测流程导致治疗延迟，而基于H&E染色切片的深度学习诊断缺乏多中心数据基准，亟需系统性评估模型在真实临床场景中的泛化能力。

Method: 构建包含四个常见亚型及对照组的多中心淋巴瘤数据集，采用H-optimus-1等五种预训练模型结合AB-MIL/TransMIL两种多实例学习聚合方法，在10x/20x/40x三种放大倍率上进行系统性评估。

Result: 在同分布测试集上综合准确率超80%，放大倍率研究证实40x为最优分辨率；但在跨中心异分布数据测试中准确率骤降至60%，显示现有模型难以应对真实医疗场景的数据分布偏移。

Conclusion: 需开展覆盖罕见亚型的更大规模多中心研究以提升模型鲁棒性，研究团队已开放基准测试管道以推动领域发展。

Abstract: Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.

</details>


### [84] [Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble](https://arxiv.org/abs/2512.14648)
*Daniel Capellán-Martín,Abhijeet Parida,Zhifan Jiang,Nishad Kulkarni,Krithika Iyer,Austin Tapp,Syed Muhammad Anwar,María J. Ledesma-Carbayo,Marius George Linguraru*

Main category: cs.CV

TL;DR: 开发了一种灵活的脑肿瘤分割流程，结合多种现有模型并通过肿瘤特异性处理提升性能。


<details>
  <summary>Details</summary>
Motivation: 针对不同脑肿瘤类型的MRI图像分割难以推广的挑战，需探索通用性强的分割方法。

Method: 利用MRI放射组学特征区分肿瘤亚型，按肿瘤类型组合state-of-the-art模型，通过前后处理模块提升分割精度。

Result: 该方法在BraTS挑战测试集上表现与顶级算法相当，验证了算法的鲁棒性。

Conclusion: 通过肿瘤特异性处理和模型选择实现非锁定式架构的精准分割，可支持临床肿瘤定量分析。

Abstract: Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.

</details>


### [85] [ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking](https://arxiv.org/abs/2512.14654)
*Lihong Wang,Liangqi Li,Weiwei Feng,Jiamin Wu,Changtao Miao,Tieru Wu,Rui Ma,Bo Zhang,Zhe Li*

Main category: cs.CV

TL;DR: 该论文提出ViRC框架和CRUX数据集，通过将多模态数学推理过程分解为关键逻辑单元（CRUs），显著提升模型在多模态数学任务中的表现，平均超越基线模型18.8%。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多模态数学任务中依赖单一静态图像进行文本推理，忽视了人类动态视觉验证和分步推理的逻辑分解过程。

Method: 设计ViRC框架，引入Reason Chunking机制，将多模态数学CoT结构化为连续临界推理单元（CRUs），通过CRUX数据集（含显式标注的CRUs）结合渐进训练策略（Instructional SFT、Practice SFT、Strategic RL）增强模型推理能力。

Result: ViRC-7B模型在多个数学基准测试中平均提升18.8%，代码已开源。

Conclusion: 通过模拟人类专家解题模式，将视觉信息整合与结构化推理分解为CRUs，有效提升了多模态大语言模型的数学推理性能。

Abstract: CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition](https://arxiv.org/abs/2512.13884)
*Jonas Golde,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 本研究提出了一种名为FiNERweb的数据集创建流程，支持多语言命名实体识别（NER）任务。该方法通过训练回归模型识别NER相关段落，并利用多语言大语言模型（LLMs）进行标注，生成了225k段落及235k个实体标签，覆盖91种语言和25种书写系统。实验表明，回归模型F1值超84，使用FiNERweb训练的模型在零样本跨语言迁移中表现优异，标注质量经LLM评估保持高质量水平。研究团队同步开放中英文双语标签数据及全部资源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型合成监督生成的多语言NER数据集多为实验副产物，缺乏系统性与可重用性。本文旨在构建可扩展的教师-学生模式数据集生成框架，解决多语言NER数据稀缺问题，并验证师生训练范式的效果，同时量化评估目标语言标签与英文标签对模型性能的影响。

Method: 基于FineWeb-Edu数据集，研发双阶段处理流程：（1）训练回归模型识别文档中NER相关文本片段；（2）调用多语言大语言模型完成实体标注。标注覆盖25种书写系统的91种语言，并创建双语标签对照体系。采用'LLM作为裁判'机制评估标注质量。

Result: 回归模型达到84%以上F1值；FiNERweb训练模型较基线数据量减少19倍情况下，零样本迁移效果持平或提升（英语/泰语/斯瓦希里语）；标注质量评估：忠实度3.99/5、完整性4.05/5；目标语言标签使用导致先进模型性能下降0.02-0.09 F1。

Conclusion: FiNERweb成功实现了多语言NER数据的系统化生成，验证了教师-学生训练模式在跨语言场景下的有效性。实验数据证实标注质量可靠，在数据效率方面展现优势。公开双语标签体系揭示了目标语言适配重要性，为后续研究提供标准化资源与技术框架，促进多语言NER方向的深化研究。

Abstract: Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.

</details>


### [87] [Olmo 3](https://arxiv.org/abs/2512.13961)
*Team Olmo,:,Allyson Ettinger,Amanda Bertsch,Bailey Kuehl,David Graham,David Heineman,Dirk Groeneveld,Faeze Brahman,Finbarr Timbers,Hamish Ivison,Jacob Morrison,Jake Poznanski,Kyle Lo,Luca Soldaini,Matt Jordan,Mayee Chen,Michael Noukhovitch,Nathan Lambert,Pete Walsh,Pradeep Dasigi,Robert Berry,Saumya Malik,Saurabh Shah,Scott Geng,Shane Arora,Shashank Gupta,Taira Anderson,Teng Xiao,Tyler Murray,Tyler Romero,Victoria Graf,Akari Asai,Akshita Bhagia,Alexander Wettig,Alisa Liu,Aman Rangapur,Chloe Anastasiades,Costa Huang,Dustin Schwenk,Harsh Trivedi,Ian Magnusson,Jaron Lochner,Jiacheng Liu,Lester James V. Miranda,Maarten Sap,Malia Morgan,Michael Schmitz,Michal Guerquin,Michael Wilson,Regan Huff,Ronan Le Bras,Rui Xin,Rulin Shao,Sam Skjonsberg,Shannon Zejiang Shen,Shuyue Stella Li,Tucker Wilde,Valentina Pyatkin,Will Merrill,Yapei Chang,Yuling Gu,Zhiyuan Zeng,Ashish Sabharwal,Luke Zettlemoyer,Pang Wei Koh,Ali Farhadi,Noah A. Smith,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: Olmo 3推出7B和32B参数规模的全新开源语言模型，优化长上下文推理、代码生成等功能，并提供完整的模型生命周期数据。


<details>
  <summary>Details</summary>
Motivation: 为满足复杂推理任务需求并推动开源模型透明化，需构建多功能开放模型及全流程资源。

Method: 设计多尺度架构，重点优化目标能力，并公开训练全流程的检查点、数据集及技术依赖。

Result: 成功发布当前最强的开源32B模型（Olmo 3 Think 32B），全面覆盖多项基准测试任务。

Conclusion: Olmo 3通过开源全生命周期数据与多领域性能提升，显著增强了开源语言模型的实用性。

Abstract: We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.

</details>


### [88] [Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models](https://arxiv.org/abs/2512.13980)
*Zhimin Qiu,Di Wu,Feng Liu,Chenrui Hu,Yuxiao Wang*

Main category: cs.CL

TL;DR: 该论文提出一种基于大语言模型的结构感知解码方法，用于解决传统方法在嵌套和重叠实体提取任务中难以保持语义完整性和结构一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理具有嵌套和重叠结构的实体提取任务时，难以同时维护语义完整性和结构一致性，需要一种能统一建模实体边界、层级关系和跨依赖性的方法。

Method: 提出候选跨度生成机制和结构化注意力建模，通过预训练语言模型获取上下文语义表示，组合候选表示捕捉多粒度实体特征，并在解码阶段引入层级结构约束。采用分类损失和结构一致性损失联合优化。

Result: 在ACE 2005数据集上的实验显示准确率、精确率、召回率、F1分数显著提升，特别是在嵌套和重叠实体识别任务中表现出更强的边界定位能力和结构建模能力，长句依赖和多实体并存场景保持高精度。

Conclusion: 验证了结构感知解码在复杂语义提取任务中的有效性，为开发具有层级理解能力的语言模型提供了新视角，并建立了高精度信息提取的方法论基础。

Abstract: This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.

</details>


### [89] [What Affects the Effective Depth of Large Language Models?](https://arxiv.org/abs/2512.14064)
*Yi Hu,Cai Zhou,Muhan Zhang*

Main category: cs.CL

TL;DR: 本研究系统分析了大语言模型的有效深度变化规律，发现当前模型对深度资源存在显著利用不足，并在多种训练范式下提出优化方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型深度增加带来的性能增益递减，前人研究显示深层未充分用于有效计算，但尚未系统研究有效深度在不同模型规模、训练方式和任务难度中的变化特征。

Method: 通过Qwen-2.5系列（1.5B-32B）模型的行为分析、对比基础模型与长链式思维（CoT）模型的有效深度，并在多难度梯度任务中进行有效性验证。

Result: 有效层数随模型规模增长但占比保持稳定；CoT增强推理来自更长上下文而非更深计算，任务难度提升未触发动态深度增加，32B模型仍有超半数层处于非活跃状态。

Conclusion: 现有多语言模型深度资源利用率不足，提出三大研究方向：提升层利用率、模型动态剪枝、提前退出机制，为后续研究提供明确路径。

Abstract: The scaling of large language models (LLMs) emphasizes increasing depth, yet performance gains diminish with added layers. Prior work introduces the concept of "effective depth", arguing that deeper models fail to fully utilize their layers for meaningful computation. Building on this, we systematically study how effective depth varies with model scale, training type, and task difficulty. First, we analyze the model behavior of Qwen-2.5 family (1.5B-32B) and find that while the number of effective layers grows with model size, the effective depth ratio remains stable. Besides, comparisons between base and corresponding long-CoT models show no increase in effective depth, suggesting that improved reasoning stems from longer context rather than deeper per-token computation. Furthermore, evaluations across tasks of varying difficulty indicate that models do not dynamically use more layers for harder problems. Our results suggest that current LLMs underuse available depth across scales, training paradigms and tasks of varying difficulties, pointing out research opportunities on increasing the layer utilization rate of LLMs, model pruning, and early exiting. Our code is released at https://github.com/AheadOFpotato/what_affects_effective_depth.

</details>


### [90] [Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed](https://arxiv.org/abs/2512.14067)
*Yonggan Fu,Lexington Whalen,Zhifan Ye,Xin Dong,Shizhe Diao,Jingyu Liu,Chengyue Wu,Hao Zhang,Enze Xie,Song Han,Maksim Khadkevich,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 本论文提出了一种将自回归模型（AR）转换为扩散语言模型（dLMs）的方法，通过优化注意力模式和预训练策略，在保持任务准确性的同时显著提升生成效率。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型（dLMs）支持并行生成，但其训练效率低于自回归模型（AR）。研究旨在解决如何有效利用预训练AR模型的知识，提升dLMs的训练效率和生成质量。

Method: 1. 分析不同注意力模式对权重分布的影响，提出基于分块注意力的连续预训练框架，在分块内部双向建模但保持分块间因果关系；2. 设计位置依赖的掩码策略，降低训练与生成阶段的掩码分布差异。

Result: 提出的分块注意力框架能更好保留AR模型的权重分布特性，并实现KV缓存优化。在多项测试中，Efficient-DLM 8B相比现有模型在准确率（+5.4%/+2.7%）和吞吐量（4.5x/2.7x）上均取得优势。

Conclusion: 研究表明：1）保留AR模型权重分布特性的注意力模式设计对转换效果至关重要；2）训练阶段的掩码策略需与生成场景对齐；3）优化后的dLMs可在精度无损条件下实现生成效率量级提升。

Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

</details>


### [91] [A Unified Sparse Attention via Multi-Granularity Compression](https://arxiv.org/abs/2512.14082)
*Siran Liu,Zane Cao,Yongchao He*

Main category: cs.CL

TL;DR: 提出UniSparse，通过复合token和动态稀疏注意力机制提升大模型长上下文处理效率


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制存在平方级复杂度瓶颈，现有稀疏注意力方法在效果/效率/通用性上存在权衡

Method: 构建复合token抽象表征并实现多粒度压缩，在GPU上动态构建块级稀疏注意力

Result: 在准确率（≥99%全注意力精度）和效率（比FlashAttention快2.61倍）上均超越现有稀疏注意力方法

Conclusion: 为大模型提供了无需训练的通用注意力加速框架，兼顾推理效率和跨模态泛化性

Abstract: Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

</details>


### [92] [Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study](https://arxiv.org/abs/2512.14085)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Taiga Mori,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 该论文提出了一种基于Transformer的多语言连续后向通道预测模型，用于日语、英语和中文，并通过约300小时对话数据联合训练。研究发现该模型在跨语言时序行为上表现优异，揭示了不同语言对语音线索（如短期语言信息、沉默时长、语调）的依赖差异，且多语言训练可减少中文对音高的过度依赖。最终模型在CPU上实现了一体化实时处理软件的部署。


<details>
  <summary>Details</summary>
Motivation: 跨语言对话系统中，后向通道（如点头、短语回应）的时机控制对自然交互至关重要。现有研究缺乏对语言共性（如普遍时序模式）和差异性（如特定语言线索权重）的统一建模。作者旨在通过多语言统一模型，验证是否存在共享但可自适应的表示，并探索不同语言对语音线索的利用差异。

Method: 基于Transformer框架构建帧级模型，引入辅助任务（如语音活动检测）联合训练。使用多语言数据（日语、英语、中文）进行端到端训练，并设计零样本迁移实验对比单语言基线。通过扰动分析（Perturbation Analysis）量化语言对局部线索（如短时信息）和全局线索（如上下文长度）的依赖，同时测试模型在实时处理中的性能。

Result: 多语言模型在三种语言上均达到或超越单语言基线，但零样本迁移能力受限（如未训练的第三语言表现差），表明跨语言差异显著。扰动分析发现：日语对短期语言信息更敏感，而英语/中文更依赖沉默时长和语调；多语言训练使中文减少对音高线索的依赖。上下文长度研究显示日语对短上下文更鲁棒，中文受益于长上下文。最终模型在CPU实现实时推理。

Conclusion: 多语言后向通道模型有效统一了语言共性与差异性的建模，为跨语言对话系统的文化适配性设计提供了实证依据。研究揭示了语言间线索利用的层级性（如日语侧重局部、中文依赖全局），并证明了模型在资源受限场景下的实用性（CPU实时处理）。

Abstract: We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.

</details>


### [93] [CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models](https://arxiv.org/abs/2512.14118)
*Yiran Zhang,Jincheng Hu,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种受认知科学启发的内存增强型LLM架构CogMem，通过分层记忆结构解决多轮交互中的推理失效问题，包括推理偏见、任务漂移和上下文膨胀等。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮推理中表现优异，但多轮交互中易出现推理偏差、任务漂移、虚假记忆等问题。现有方法通过追加历史对话扩大上下文窗口，导致计算成本增加与推理效率下降。

Method: 设计三层记忆架构：1）长期记忆（LTM）跨会话存储推理策略；2）直接访问内存（DA）管理会话级笔记并检索LTM；3）注意力焦点机制（FoA）动态生成当前任务的精简上下文。

Result: 在TurnBench基准测试中，CogMem有效缓解多种推理失效模式，控制上下文增长，提升了模型跨轮次推理的一致性与效率。

Conclusion: 该方法通过模块化记忆结构优化多轮推理流程，为实现更接近人类持续推理能力的LLM提供了新方向。

Abstract: Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.

</details>


### [94] [A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs](https://arxiv.org/abs/2512.14179)
*K. M. Jubair Sami,Dipto Sumit,Ariyan Hossain,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出并比较了两种适用于孟加拉语标准到方言翻译的RAG流程，其中基于标准化句子对的流程可将词错误率从76%降至55%，且小模型通过优化检索策略可超越巨型模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语方言翻译因数据稀缺和语言差异面临挑战，现有方法难以平衡模型规模与效率，需要探索无需微调的低成本解决方案以保护语言多样性。

Method: 设计两种RAG流程：1）利用音频转录获取方言语境的Transcript-Based Pipeline；2）采用结构化方言-标准语句对的Standardized Sentence-Pairs Pipeline。通过BLEU、ChrF、WER和BERTScore在六个方言和多LLM上进行评估。

Result: 句子对流程在Chittagong方言的WER降低率达27.6%，且Llama-3.1-8B等小模型的翻译质量超过GPT-OSS-120B等大模型，在多个指标和方言中保持稳定优势。

Conclusion: 高效的检索策略（如结构化句对训练）比模型规模对翻译性能有更大影响，为低资源语言提供无微调的实践方案，证明语言多样性保护可通过优化数据利用实现。

Abstract: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.

</details>


### [95] [Two CFG Nahuatl for automatic corpora expansion](https://arxiv.org/abs/2512.14239)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Graham Ranger Martha-Lorena Avendaño-Garrido*

Main category: cs.CL

TL;DR: 本文提出两种上下文无关语法，用于自动化扩展资源匮乏的纳瓦特尔语语料库，成功生成合法句子并通过语义相似性任务验证有效性。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特尔语作为墨西哥土著语言缺乏数字化资源，传统语料库难以支撑大语言模型训练，亟需通过技术手段扩展语料库规模

Method: 设计两种上下文无关语法生成器，通过形式化规则系统自动生产符合纳瓦特尔语语法的句子，使用生成数据训练词嵌入模型并进行语义相似性测试

Result: 扩展后的语料库在语义相似性任务中表现优于原始语料库，且经济型嵌入模型在部分指标上超越大语言模型

Conclusion: 形式化语法生成结合词嵌入方法有效解决了低资源语言语料不足问题，为纳瓦特尔语数字资源建设提供了可行方案

Abstract: The aim of this article is to introduce two Context-Free Grammars (CFG) for Nawatl Corpora expansion. Nawatl is an Amerindian language (it is a National Language of Mexico) of the $π$-language type, i.e. a language with few digital resources. For this reason the corpora available for the learning of Large Language Models (LLMs) are virtually non-existent, posing a significant challenge. The goal is to produce a substantial number of syntactically valid artificial Nawatl sentences and thereby to expand the corpora for the purpose of learning non contextual embeddings. For this objective, we introduce two new Nawatl CFGs and use them in generative mode. Using these grammars, it is possible to expand Nawatl corpus significantly and subsequently to use it to learn embeddings and to evaluate their relevance in a sentences semantic similarity task. The results show an improvement compared to the results obtained using only the original corpus without artificial expansion, and also demonstrate that economic embeddings often perform better than some LLMs.

</details>


### [96] [From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition](https://arxiv.org/abs/2512.14244)
*Yiqing Zhou,Yu Lei,Shuzheng Si,Qingyan Sun,Wei Wang,Yifei Wu,Hao Wen,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出基于EDU的显式上下文压缩框架，通过构建结构化关系树保留全局结构和细粒度细节，解决了传统方法破坏连贯性或依赖隐式编码的问题。


<details>
  <summary>Details</summary>
Motivation: 现有压缩技术存在离散token移除导致局部连贯性破坏、隐式编码存在位置偏差及兼容性问题，同时难以兼顾长文档处理的高计算成本与噪声控制需求。

Method: 创新性分两阶段：1）LingoEDU将线性文本转化为基于EDU的结构关系树，严格绑定源索引防止幻觉；2）轻量级排序模型筛选查询相关子树并线性化。同步开源StructBench标注数据集用于结构理解评估。

Result: 实验证明：①结构预测准确率创SOTA；②性能超越前沿LLM并显著降低计算成本；③在长上下文问答、Deep Search等下游任务表现优异，结构感知能力提升显著。

Conclusion: 提出了首个兼顾结构保持与显式压缩的框架，通过结构化表示和查询感知压缩策略，为长文本处理提供了新的解决方案，为后续研究开辟了结构化上下文优化方向。

Abstract: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.

</details>


### [97] [Inflation Attitudes of Large Language Models](https://arxiv.org/abs/2512.14306)
*Nikoleta Anesti,Edward Hill,Andreas Joseph*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（GPT-3.5-turbo）基于宏观经济价格信号形成通胀感知与预期的能力，并与英国家庭调查数据和官方统计数据对比。GPT在短期能追踪群体通胀预期，且对食品通胀敏感性与人类相似，但缺乏稳定的消费者价格通胀模型。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能模拟人类对经济指标（如通胀）的感知逻辑，为社会科学领域模型评估、模型对比及调查设计提供新方法论支持。

Method: 通过准实验设计，利用GPT训练数据截止于2021年9月（未包含后续英国通胀激增）的特点，将其输出与英国央行通胀态度调查（IAS）的模拟人口特征对比，结合Shapley值分解分析模型输出的驱动因素。

Result: GPT在短期通胀预期预测上与统计数据一致，在收入、住房产权等细分维度复现了家庭通胀感知的典型经验规律；Shapley分解显示其对食品通胀敏感性类似人类，但缺乏消费者价格通胀的系统性模型。

Conclusion: GPT在特定场景可模拟人类经济认知偏差，但无法替代专业经济模型。研究框架为社会科学研究中的LLM行为评估及调查工具开发提供了实证方法。

Abstract: This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.

</details>


### [98] [Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.14427)
*Gabriele Prato,Shagun Sodhani,Alessandro Sordoni,Sarath Chandar*

Main category: cs.CL

TL;DR: 研究不同文档打包策略对大模型多跳推理能力的影响，发现打包能提升模型性能但增加算力成本，并通过消融实验分析优势来源。


<details>
  <summary>Details</summary>
Motivation: 现有大模型训练存在多个策略但影响不明，需探究其对模型推理能力的潜在价值，并建立优化方法论。

Method: 通过实验对比不同打包策略的模型性能，设计消融实验分析关键影响因子，聚焦多跳推理能力评估。

Result: 打包策略在提升模型性能的同时显著增加计算需求，关键因素包括文档组合方式和上下文交互模式。

Conclusion: 揭示了文档打包对模型训练的双重影响，为优化大模型训练效率和能力平衡提供了实验依据和设计指导。

Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.

</details>


### [99] [SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models](https://arxiv.org/abs/2512.14481)
*Shizhuo Mao,Song Chen,Yi Kang*

Main category: cs.CL

TL;DR: SASQ是一种轻量级的量化感知训练（QAT）框架，专注于优化激活量化因子，不修改预训练权重，在保持部署效率的同时提升静态量化精度。


<details>
  <summary>Details</summary>
Motivation: 现有动态量化方案计算开销高，静态量化精度损失大，传统QAT需重新训练权重导致成本高昂，亟需更高效的量化方案解决LLMs部署困境。

Method: 仅优化量化因子与自适应裁剪异常值（保留激活分布特性），采用静态推理模式，避免权重更新，降低量化难度。

Result: 在LLaMA2-7B模型上，SASQ相比QuaRot困惑度降低5.2%，相比FP16模型降低4.7%，优于现有SOTA量化方法。

Conclusion: SASQ通过解耦权重与量化因子优化，兼顾精度与效率，为边缘设备LLMs部署提供实用化解决方案。

Abstract: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

</details>


### [100] [C-ing Clearly: Enhanced Binary Code Explanations using C code](https://arxiv.org/abs/2512.14500)
*Teodor Poncu,Ioana Pintilie,Marius Dragoi,Dragos Tantaru,Florin Brad*

Main category: cs.CL

TL;DR: C-ing Clearly uses C code to enhance LLMs' understanding of assembly, improving performance on binary code tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with low-level languages like assembly, prompting research into methods to bridge this gap for security and reverse-engineering applications.

Method: A synthetic data generation approach pairing C code with assembly code, followed by fine-tuning LLMs on this dataset to improve cross-level comprehension.

Result: Significant improvements in binary code summarization and vulnerability detection across multiple LLM architectures and scales.

Conclusion: Leveraging cross-language supervision with C code is an effective strategy for improving LLM capabilities in low-level language understanding tasks.

Abstract: Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.

</details>


### [101] [Linguists should learn to love speech-based deep learning models](https://arxiv.org/abs/2512.14506)
*Marianne de Heer Kloots,Paul Boersma,Willem Zuidema*

Main category: cs.CL

TL;DR: 作者指出基于生成文本的深度学习模型在语言学研究中的局限性，强调基于音频的模型能更好捕捉人类语言的多维特性。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖生成文本的深度学习模型，无法涵盖人类语言在语音、韵律等非文本维度的研究问题，需要更全面的模型框架。

Method: 通过理论分析对比文本生成模型与音频模型的优劣势，论证音频模型在语言学研究中的必要性。

Result: 发现文本模型受限于无法表征语音停顿、声调变化等关键语言特征，而音频模型能拓展语言处理机制的神经科学研究。

Conclusion: 呼吁学界重视音频深度学习模型的开发，以促进语言处理机制的多模态研究和跨学科融合。

Abstract: Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.

</details>


### [102] [VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse](https://arxiv.org/abs/2512.14531)
*Ying Nie,Kai Han,Hongguang Li,Hang Zhou,Tianyu Guo,Enhua Wu,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: 本论文提出了VersatileFFN，一种通过在宽度和深度维度灵活复用参数来提升大型语言模型效率和容量的新方法，所有额外算力均不增加内存成本。


<details>
  <summary>Details</summary>
Motivation: 现有参数压缩方法（如量化剪枝）仅压缩预训练模型，导致基座模型表征能力达到上限。研究旨在通过计算资源而非内存扩展模型能力。

Method: 基于认知双过程理论设计双路径FFN：宽度路径通过共享FFN生成子专家混合（模拟稀疏专家路由），深度路径递归复用FFN模拟深度处理，通过难度感知门控动态平衡两者。

Result: 跨基准测试和模型规模的实验验证了方法的有效性，代码已开源（未提供具体性能提升数值）。

Conclusion: 该方法实现了内存预算固定下的动态资源分配，简单token走高效宽度路径，复杂token分配深度计算，突破传统架构的能力瓶颈。

Abstract: The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.

</details>


### [103] [Dual Language Models: Balancing Training Efficiency and Overfitting Resilience](https://arxiv.org/abs/2512.14549)
*David Samuel,Lucas Georges Gabriel Charpentier*

Main category: cs.CL

TL;DR: 通过融合自回归与掩码扩散训练目标，无需架构调整即可提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型训练效率高但易过拟合，掩码扩散模型抗过拟合但训练低效，需结合两者优势。

Method: 在不同数据重复条件下训练50个模型，评估两种目标的最佳组合比例。

Result: 双目标训练在所有场景下均优于单一模型，最佳混合比例与下游任务类型无关。

Conclusion: 联合训练实现效率与鲁棒性的平衡，为多目标优化提供通用范例。

Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

</details>


### [104] [VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models](https://arxiv.org/abs/2512.14554)
*Nguyen Tien Dong,Minh-Anh Nguyen,Thanh Dat Hoang,Nguyen Tuan Ngoc,Dao Xuan Quang Minh,Phan Phi Hai,Nguyen Thi Ngoc Anh,Dang Van Tu,Binh Vu*

Main category: cs.CL

TL;DR: 本文提出VLegal-Bench，首个基于越南法律的系统化基准测试框架，结合Bloom认知分类学评估大语言模型对越南法律知识的理解与应用能力。


<details>
  <summary>Details</summary>
Motivation: 越南法律体系的高度复杂性、层级结构及频繁修订导致现有大型语言模型难以准确评估其法律知识应用效果，现有研究缺乏针对性评测体系。

Method: 构建包含10,450个样本的VLegal-Bench基准库，通过法律专家标注系统生成数据，覆盖一般法律问答、检索增强生成、多步推理及场景化问题解决四大类任务；基于Bloom认知分类学设计多层级法律理解测试场景，采用权威法律文档作为标注依据。

Result: 开发出标准化且透明的越南法律评测框架，所有样本均通过专家交叉验证确保法律合规性，数据集真实反映越南法律助理工作流，能够有效支持法律AI系统的伦理化开发。

Conclusion: VLegal-Bench为越南法律场景下的大语言模型性能评测提供认知理论基础，同时为构建可解释、可靠的法律AI辅助系统提供技术支撑。

Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.

</details>


### [105] [Polypersona: Persona-Grounded LLM for Synthetic Survey Responses](https://arxiv.org/abs/2512.14562)
*Tejaswani Dash,Dinesh Karri,Anudeep Vurity,Gautam Datla,Tazeem Ahmad,Saima Rafi,Rohith Tangudu*

Main category: cs.CL

TL;DR: PolyPersona是通过资源自适应微调小模型，生成基于预设人设的跨领域问卷回答的生成框架，用LoRA适配器与4bit量化技术。


<details>
  <summary>Details</summary>
Motivation: 为解决传统大模型生成调查数据成本高且难以保持人设一致性的问题，探索小规模语言模型生成可靠合成问卷数据的可行性。

Method: 构建包含3568个样本的跨领域人设数据集，采用对话式生成流程保留人设特征，使用LoRA+4bit量化进行参数高效微调，结合传统NLP指标和问卷特有指标进行多维度评估。

Result: TinyLlama1.1B和Phi-2等小型模型达到与7-8B模型相当的BLEU 0.090和ROUGE-1 0.429指标，验证了人设微调对小模型的有效性。

Conclusion: 证明通过人设条件微调能显著提升小模型生成问卷数据的质量，建立的开源框架支持可扩展评估和透明化偏差分析。

Abstract: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.

</details>


### [106] [Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies](https://arxiv.org/abs/2512.14576)
*Ekaterina Artemova,Laurie Burchell,Daryna Dementieva,Shu Okabe,Mariya Shmatova,Pedro Ortiz Suarez*

Main category: cs.CL

TL;DR: 本教程为NLP开发者提供低资源语言处理的全流程实践指南，涵盖数据收集、机器翻译及下游应用，强调公平性和社区参与。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言处理中数据稀缺与文化差异挑战，推动语言技术公平性和社会影响力，填补多语言开发工具缺失的空白。

Method: 通过端到端NLP流水线设计（含数据爬取、平行语料挖掘、可解释翻译模型）结合社区共建策略，提供可复用框架并在真实场景验证。

Result: 实现跨10+语系（含濒危语言）的多模态应用案例，展示方法有效性，特别在资源极度稀缺场景下提升模型性能基线。

Conclusion: 构建低资源语言技术需系统化工具链与社区协作，教程为开发者提供可行路径，推动NLP领域包容性与技术民主化。

Abstract: This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.

</details>


### [107] [JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction](https://arxiv.org/abs/2512.14620)
*Atsuyuki Miyai,Shota Onohara,Jeonghun Baek,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: 本文提出了JMMMU-Pro，一个基于图像的日语多学科多模态理解基准，以及Vibe基准构建方法。该基准通过将问题图像与文本合成单一图像，要求模型具备视觉-文本联合理解能力。实验表明现有开源LMM在该基准上表现不佳，证明其挑战性。


<details>
  <summary>Details</summary>
Motivation: 原有MMMU基准未要求视觉与文本的联合理解能力，且缺乏针对日语的高质量测试集。研究者旨在构建更具挑战性的多学科基准，以推动日语多模态模型的发展。

Method: 提出Vibe构建方法：1) 使用Nano Banana Pro等图像生成模型创建候选视觉问题；2) 人工验证生成结果，通过调整prompt迭代优化；3) 利用高真实感图像生成和洁净日语文本嵌入技术构建低成本多样化基准。

Result: 开源LMM在JMMMU-Pro上均表现显著不足，验证了基准的难度和有效性；成功构建涵盖多种背景和排版设计的高质量测试集，证明了Vibe方法的可扩展性与效率。

Conclusion: JMMMU-Pro为评估日语多模态模型提供了更严格的测试工具，Vibe方法为图像问答基准构建设立了高效范式。该成果对开源社区在日语多模态理解领域的发展具有重要指导意义。

Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.

</details>


### [108] [TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines](https://arxiv.org/abs/2512.14645)
*David Schulmeister,Valentin Hartmann,Lars Klein,Robert West*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TiME（Tiny Monolingual Encoders）的小型语言模型，旨在通过现代训练技术（如蒸馏）优化效率关键型场景，显著降低能耗和延迟，同时支持低资源语言。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦于通用大模型，但许多NLP任务仅需特定能力。大模型因速度慢、能耗高，难以满足实时处理需求，且不利于可持续性发展及低功耗设备部署。

Method: 通过蒸馏技术训练单语言小模型（TiME），利用多语言教师模型及相对位置编码向绝对位置编码迁移，并针对性优化低资源语言支持。

Result: TiME在多个NLP任务中表现出性能与效率的均衡优势，显著提升吞吐量、降低延迟与能耗，验证了跨结构蒸馏的可行性。

Conclusion: TiME证明了小型专业化模型在效率敏感场景中的有效性，为能效与性能的权衡提供了可行方案，并拓展了蒸馏技术的应用边界。

Abstract: Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

</details>


### [109] [Fast and Accurate Causal Parallel Decoding using Jacobi Forcing](https://arxiv.org/abs/2512.14681)
*Lanxiang Hu,Siqi Kou,Yichao Fu,Samyam Rajbhandari,Tajana Rosing,Yuxiong He,Zhijie Deng,Hao Zhang*

Main category: cs.CL

TL;DR: 提出Jacobi Forcing与多块解码方法，在保持生成质量的同时实现大模型推理加速4倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有dLLMs因预训练-后训练数据分布不匹配及双向注意力与因果性的冲突，导致速度提升受限，需寻求兼顾效率与因果性的解码范式。

Method: 设计渐进式蒸馏的Jacobi Forcing框架，使模型通过自身并行解码轨迹训练；提出多块解码结合拒绝回收机制，提升单次迭代标记接受率。

Result: 在编程和数学基准上实现3.8倍端到端加速，多块解码使单次接受标记数提升4.5倍，综合加速达4倍。

Conclusion: 方法解决预训练与解码范式冲突，通过计算资源优化实现高效低延迟推理，开源代码推动领域发展。

Abstract: Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.

</details>


### [110] [Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization](https://arxiv.org/abs/2512.14687)
*Yen-Ju Lu,Kunxiao Gao,Mingrui Liang,Helin Wang,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 本文提出Spoken DialogSum数据集，首次将口语对话音频与情感丰富的摘要及副语言标签相结合，推动情感感知对话摘要研究。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型缺乏同时包含语音、标签化副语言特征(年龄/性别/情感)及多类型摘要的数据集，限制了情感认知型对话摘要的技术发展。

Method: 采用两阶段构建方法：第一阶段用LLM生成含Switchboard填充词和反馈词的对话脚本并标注情绪/音高/语速；第二阶段使用表现力TTS引擎生成对齐的合成语音。

Result: 构建出包含13,460个情感多样对话的数据集（每个对话含事实性与情感性双摘要），基准测试显示端到端Audio-LLM较级联ASR-LLM系统在情感摘要ROUGE-L指标上提升28%。

Conclusion: 证明端到端语音建模在情感对话摘要任务中的有效性，所构建的数据集填补了情感富集对话数据的空白。

Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.

</details>


### [111] [MMGR: Multi-Modal Generative Reasoning](https://arxiv.org/abs/2512.14691)
*Zefan Cai,Haoyi Qiu,Tianyi Ma,Haozhe Zhao,Gengze Zhou,Kung-Hsiang Huang,Parisa Kordjamshidi,Minjia Zhang,Xiao Wen,Jiuxiang Gu,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出MMGR评估框架，系统检验视频模型对物理、逻辑和空间约束的建模能力，发现当前模型在抽象推理和长视野空间规划存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成质量评估指标(FVD)仅关注视觉真实性，无法检测因果性错误、物理规律违背等推理失败问题。

Method: 建立五维推理能力评估体系(物理/逻辑/3D/2D/时间)，设计三个领域测试：抽象推理(ARC-AGI,数独)、具身导航(3D定位)和物理常识(运动交互)，采用全局一致性指标。

Result: Sora等顶尖模型在物理常识表现中等，但抽象推理准确率低于10%，具身导航中长周期规划失败率高。图像模型Qwen/GPT-4o表现相似但各有侧重。

Conclusion: 揭示当前模型过度依赖感知数据、缺乏因果正确性和全局状态一致性，MMGR为开发具备推理能力的生成模型提供诊断基准。

Abstract: Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.

</details>
