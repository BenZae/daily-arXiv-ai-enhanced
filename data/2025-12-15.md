<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 72]
- [cs.CL](#cs.CL) [Total: 26]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification](https://arxiv.org/abs/2512.11015)
*Anoop Krishnan*

Main category: cs.CV

TL;DR: 本文提出了两种基于文本引导的方法（ITM引导和图文融合）来提升面部性别分类算法的公平性，无需使用人口统计数据标签即可减少偏见。


<details>
  <summary>Details</summary>
Motivation: 面部性别分类AI存在种族/性别偏见，现有方法依赖人口统计标签但实际应用受限。研究如何通过语义信息增强模型泛化能力以解决公平性问题。

Method: 1) ITM引导：训练模型识别图像与文本的细粒度对齐关系；2) 图文融合：将两种模态整合为统一表征，并采用无监督学习策略。

Result: 实验证明方法在多个基准数据集上显著降低跨种族/性别偏见，提升准确性，且具有应用无关性（agnostic）特点。

Conclusion: 多模态文本引导范式为计算机视觉提供可解释性改进方案，为消除面部分析中的系统性偏见提供技术路径。

Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.

</details>


### [2] [SoccerMaster: A Vision Foundation Model for Soccer Understanding](https://arxiv.org/abs/2512.11016)
*Haolin Yang,Jiayuan Rao,Haoning Wu,Weidi Xie*

Main category: cs.CV

TL;DR: 本文介绍了一种名为SoccerMaster的统一足球视觉基础模型，通过多任务预训练方法在多个理解任务上优于任务专用模型。


<details>
  <summary>Details</summary>
Motivation: 针对足球领域复杂的域特异性挑战，传统依赖孤立任务专用模型的方法存在局限，需要一种能统一处理从细粒度感知到语义推理等多样化任务的综合解决方案。

Method: 1. 开发SoccerMaster框架，采用监督式多任务预训练统一足球视觉理解任务
2. 构建自动化数据处理流水线生成可扩展空间标注
3. 整合现有足球视频数据集构建SoccerFactory预训练数据资源

Result: 实验表明SoccerMaster在各类下游任务中均优于任务专用模型，尤其在运动员检测和事件分类任务上展现显著优势，且预训练数据资源已公开可用。

Conclusion: 证实了统一多任务预训练框架在足球视觉理解领域的有效性，为该研究领域提供了可扩展的基准模型和数据资源。

Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.

</details>


### [3] [Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation](https://arxiv.org/abs/2512.11057)
*Marshal Ashif Shawkat,Moidul Hasan,Taufiq Hasan*

Main category: cs.CV

TL;DR: 本研究通过知识蒸馏技术训练CNN模型，无需依赖边框标注即可减少结核病分类中的虚假相关性并实现病灶定位。


<details>
  <summary>Details</summary>
Motivation: 结核病诊断依赖的胸片图像分析需要专家资源，但现有机器学习模型易受虚假相关性影响，且高质量标注数据集构建成本高昂。

Method: 采用教师-学生框架的ResNet50架构，在TBX11k数据集上实施知识蒸馏技术，利用类激活映射（CAM）实现病灶区域定位。

Result: 学生模型在测试集上达到0.2428 mIOU，显著优于教师模型，验证了知识蒸馏在减少虚假相关性方面的有效性。

Conclusion: 所提方法通过知识迁移提升了模型鲁棒性，为资源受限地区的结核病自动化诊断提供了可靠的技术路径。

Abstract: Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.

</details>


### [4] [Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning](https://arxiv.org/abs/2512.11060)
*Chenjun Li,Cheng Wan,Laurin Lux,Alexander Berger,Richard B. Rosen,Martin J. Menten,Johannes C. Paetzold*

Main category: cs.CV

TL;DR: The paper introduces OCTA-100K-SVR, a synthetic dataset of retinal images with diabetic retinopathy features and granular reasoning texts, enabling improved zero-shot medical diagnosis and explainability.


<details>
  <summary>Details</summary>
Motivation: Medical vision-language models (VLMs) require large-scale image-text datasets for detailed reasoning in specialized domains like OCTA imaging, but grounded pathological descriptions are scarce.

Method: Proposed Synthetic Vasculature Reasoning (SVR) framework generates 100,000 pairs of realistic OCTA images with diabetic retinopathy pathology features (capillary dropout, microaneurysms, etc.) and associated diagnostic reasoning texts.

Result: Qwen3-VL-8b trained on OCTA-100K-SVR achieved 89.67% zero-shot balanced classification accuracy on real OCTA images, outperforming supervised models and demonstrating enhanced pathology localization and explanation quality via human evaluation.

Conclusion: Synthetic dataset generation addresses data scarcity in medical VLMs, enabling robust zero-shot performance and clinically meaningful interpretations.

Abstract: Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.

</details>


### [5] [VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation](https://arxiv.org/abs/2512.11061)
*Felix O'Mahony,Roberto Cipolla,Ayush Tewari*

Main category: cs.CV

TL;DR: 本文提出了VDAWorld框架，通过将图像-字幕对转化为可模拟的抽象表示，利用视觉-语言模型作为智能体选择视觉工具与物理模拟器，生成高质量的动态场景模拟。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频模型存在违反物理逻辑规则、缺乏交互性及不可解释的问题，需构建结构化、可查询的具身世界模型。

Method: 基于视觉-语言模型（VLM）构建智能体，自主选择视觉工具生成场景表示并匹配物理模拟器（如刚体、流体），通过静态场景推断潜在动态以预测未来状态。

Result: 实验表明该框架通过智能抽象与自适应仿真结合，能生成高质量动态模拟，覆盖广泛场景。

Conclusion: VDAWorld提供了一种通用世界建模范式，通过结构化抽象与动态仿真提升了模拟质量。

Abstract: Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.

</details>


### [6] [E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring](https://arxiv.org/abs/2512.11076)
*Jack Brady,Andrew Dailey,Kristen Schang,Zo Vic Shong*

Main category: cs.CV

TL;DR: 本文探讨了使用事件相机（event-based cameras）研究城市动态的新方法，分析其优势、挑战及与其他传感器的融合应用，以提升隐私保护和感知效果。


<details>
  <summary>Details</summary>
Motivation: 传统城市监测方法（如人工观察、RGB摄像头）存在隐私暴露、低光环境适应性差等问题，而事件相机通过捕捉光线变化，具备低功耗、高动态范围等优势，可弥补现有技术的不足。

Method: 通过文献综述分析事件相机的技术特性、应用场景及局限性，并结合机器学习案例，验证其在城市动态研究中的可行性；进一步提出多传感器融合策略（如事件-LiDAR、红外等）以优化性能。

Result: 事件相机在低光环境监测和隐私保护中表现突出，且多传感器融合方案有效解决了其分辨率低、数据稀疏等问题，为城市动态研究提供了更高效的数据采集框架。

Conclusion: 事件相机是未来城市动态研究的重要工具，结合多传感器与机器学习技术可突破现有瓶颈，推动隐私友好型智慧城市建设。

Abstract: Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.

</details>


### [7] [Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description](https://arxiv.org/abs/2512.11098)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: VLM-IRIS是一种零样本学习框架，通过将红外图像转换为RGB兼容输入并结合CLIP模型，实现了在无需数据标注和模型重新训练的情况下高效处理红外工业传感任务。


<details>
  <summary>Details</summary>
Motivation: 传统视觉系统在低光工业环境中存在局限性，红外相机具有互补优势。然而现有视觉语言模型（VLMs）仅支持RGB图像，而监督学习体系需要大量标注数据，因此需要开发适用于红外数据的零样本学习框架。

Method: 通过将FLIR Boson传感器捕获的红外图像预处理为magma伪彩色表示，生成RGB兼容输入；采用基于CLIP ViT-B/32编码器的质心提示融合策略，在未重新训练模型的前提下完成零样本工作件定位检测。

Result: VLM-IRIS在3D打印平台工作件检测任务中实现了高精度识别，成功验证了将红外图像转换为伪彩色并结合CLIP模型的有效性，且该方法能有效利用温度差异实现无标签监控。

Conclusion: 研究证明通过对红外图像进行RGB化预处理并结合CLIP模型架构改进，可有效扩展视觉语言模型的工业热成像应用场景，为工厂零样本监测提供了新方法。

Abstract: Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.

</details>


### [8] [VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction](https://arxiv.org/abs/2512.11099)
*Weitai Kang,Jason Kuen,Mengwei Ren,Zijun Wei,Yan Yan,Kangning Liu*

Main category: cs.CV

TL;DR: 本文提出VGent，一种模块化的视觉定位模型，通过分离高层推理与低层定位任务，在保持快速推理的同时显著提升多目标定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自回归解码（慢且有幻觉）或破坏LLM预训练结构（损害推理能力）。本文旨在构建既保留LLM强大推理能力，又能高效定位多目标的新架构。

Method: 采用冻结的MLLM作为推理编码器，解码器基于检测器生成的候选框进行跨模态注意力选择；引入QuadThinker（强化学习训练）、mask-aware标签、全局目标识别等模块化改进组件。

Result: 在多目标视觉定位基准上超越先前方法，F1提升20.6%，gIoU提升8.2%，cIoU提升5.8%，且保持恒定低延迟推理速度。

Conclusion: 该架构有效平衡了推理能力保持与定位性能优化，模块化设计支持灵活升级，在视觉定位任务中展现出显著优势。

Abstract: Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.

</details>


### [9] [Information-driven Fusion of Pathology Foundation Models for Enhanced Disease Characterization](https://arxiv.org/abs/2512.11104)
*Brennan Flannery,Thomas DeSilvio,Jane Nguyen,Satish E. Viswanath*

Main category: cs.CV

TL;DR: 本研究使用基于相关性的智能融合方法，整合多个病理学预训练模型，提高了三种癌症的分级和分期诊断性能，并增强了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管病理学预训练模型（FMs）表现优异，但其特征互补性、嵌入空间冗余性和生物学解释性仍不明确。研究旨在通过智能融合策略解决这些问题，构建更优的诊断模型。

Method: 对三种癌症的H&E全切片图像（肾癌519张、前列腺癌490张、直肠癌200张）进行分级/分期分类，比较tile-level（Conch v1.5、MUSK等）与slide-level（TITAN、CHIEF等）FMs的三种融合方案：多数投票集成、特征拼接、基于相关性剪枝的智能融合，通过交叉验证评估性能。

Result: tile-level嵌入的智能融合在三项癌症中均优于单一模型和朴素融合方案。全局相似性指标显示嵌入空间高度对齐，但局部邻域一致性较低，注意力图显示智能融合更聚焦肿瘤区域，减少良性区域干扰。

Conclusion: 基于相关性的智能融合策略能生成紧凑的任务定制化病理特征表征，同时提升预测性能和可解释性，为计算病理学提供新方法。

Abstract: Foundation models (FMs) have demonstrated strong performance across diverse pathology tasks. While there are similarities in the pre-training objectives of FMs, there is still limited understanding of their complementarity, redundancy in embedding spaces, or biological interpretation of features. In this study, we propose an information-driven, intelligent fusion strategy for integrating multiple pathology FMs into a unified representation and systematically evaluate its performance for cancer grading and staging across three distinct diseases. Diagnostic H&E whole-slide images from kidney (519 slides), prostate (490 slides), and rectal (200 slides) cancers were dichotomized into low versus high grade or stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were considered to train downstream classifiers. We then evaluated three FM fusion schemes at both tile and slide levels: majority-vote ensembling, naive feature concatenation, and intelligent fusion based on correlation-guided pruning of redundant features. Under patient-stratified cross-validation with hold-out testing, intelligent fusion of tile-level embeddings yielded consistent gains in classification performance across all three cancers compared with the best single FMs and naive fusion. Global similarity metrics revealed substantial alignment of FM embedding spaces, contrasted by lower local neighborhood agreement, indicating complementary fine-grained information across FMs. Attention maps showed that intelligent fusion yielded concentrated attention on tumor regions while reducing spurious focus on benign regions. Our findings suggest that intelligent, correlation-guided fusion of pathology FMs can yield compact, task-tailored representations that enhance both predictive performance and interpretability in downstream computational pathology tasks.

</details>


### [10] [Learning from a Generative Oracle: Domain Adaptation for Restoration](https://arxiv.org/abs/2512.11121)
*Yuyang Hu,Mojtaba Sahraee-Ardakan,Arpit Bansal,Kangfu Mei,Christian Qi,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: LEGO通过生成伪标签进行无配对数据的领域自适应，提升图像恢复效果。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在真实场景分布外退化时性能下降，传统方法依赖复杂架构调整且缺乏真实标签，需更实用方案。

Method: 三阶段框架：1.预训练模型生成初始结果；2.冻结生成模型生成伪标签；3.混合原始数据与伪标签微调模型。

Result: 实验显示LEGO显著提升多真实世界基准性能，有效缩小组件差异且无需架构修改。

Conclusion: LEGO实现配对数据无关、架构无关的高效领域自适应，保持模型原有鲁棒性。

Abstract: Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.

</details>


### [11] [Learning complete and explainable visual representations from itemized text supervision](https://arxiv.org/abs/2512.11141)
*Yiwei Lyu,Chenhui Zhao,Soumyanil Banerjee,Shixuan Liu,Akshay Rao,Akhil Kondepudi,Honglak Lee,Todd C. Hollon*

Main category: cs.CV

TL;DR: ItemizedCLIP 提出了一种从分项文本监督中学习完整的可解释视觉表征的框架，通过交叉注意力模块和定制优化目标，实现细粒度的区域独立性与表征完整性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型训练依赖冗余多标注数据，而医疗影像等非物体中心领域存在语义独立的分项文本标注（如多个病灶描述），需解决如何有效利用此类异构监督提升模型的可迁移性和解释性。

Method: 设计了跨注意力模块生成文本分项条件下的视觉嵌入，并构建联合优化目标：1) 项独立约束（不同区域关注不同文本项）；2) 表征完备性约束（覆盖所有文本项）。

Result: 在四类自带分项标注的医学/遥感数据（脑MRI/胸CT等）和一个人工合成数据上，零样本性能显著优于基线方法，且细粒度可解释性提升（可视化对应病灶区域与文本项绑定）。

Conclusion: ItemizedCLIP 实现了分项文本到视觉表征的语义对齐，解决了非冗余标注数据下的挑战性表示学习问题，其表征具有可解释性、完整性与领域通用性。

Abstract: Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.

</details>


### [12] [Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context](https://arxiv.org/abs/2512.11167)
*Anatole Jacquin de Margerie,Alexis Roger,Irina Rish*

Main category: cs.CV

TL;DR: 该论文成功复现了Monkey VLM的图像分块策略，验证了其局部细节恢复能力，并发现全局上下文可提升效果，但具体表现受限于任务类型和分块粒度。


<details>
  <summary>Details</summary>
Motivation: 复杂多模态模型的实现细节和训练基础设施常缺乏透明度，复现性作为科研基石亟待强化。

Method: 基于开源权重重新实现Monkey VLM的训练流程，引入全局上下文模块以探索高分辨率多模态建模新方向。

Result: 验证原论文分块策略有效性的同时，发现了任务类型和分块粒度对结果影响显著，全局上下文信息可稳定提升模型性能。

Conclusion: 科学成果的可复现性仍存在不足，高分辨率多模态建模需综合考虑局部细节恢复与全局上下文建模，并强调开放科学的重要性。

Abstract: Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.

</details>


### [13] [Lightweight 3D Gaussian Splatting Compression via Video Codec](https://arxiv.org/abs/2512.11186)
*Qi Yang,Geert Van Der Auwera,Zhu Li*

Main category: cs.CV

TL;DR: 提出了一种基于视频编码的轻量级3D高斯点压缩方法（LGSCV），通过两阶段Morton扫描和MiniPLAS优化显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于PLAS的3D高斯点压缩方法计算成本高，制约了轻量设备的应用，亟需更高效的编码方案。

Method: 设计双阶段Morton扫描生成块状2D映射，结合球谐函数PCA降维和灵活的MiniPLAS算法解决中低码率质量塌缩问题，并优化编码单元配置。

Result: 相比现有方法，在MPEG数据集上实现20%的率失真增益，2D映射生成时间缩短至1秒，编码耗时降低50%。

Conclusion: LGSCV通过重构空间映射策略与轻量化优化，在保持高压缩质量的同时提升计算效率，适用于资源受限设备。

Abstract: Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .

</details>


### [14] [CADKnitter: Compositional CAD Generation from Text and Geometry Guidance](https://arxiv.org/abs/2512.11199)
*Tri Le,Khang Nguyen,Baoru Huang,Tung D. Ta,Anh Nguyen*

Main category: cs.CV

TL;DR: The paper proposes CADKnitter, a compositional CAD generation framework that produces complementary 3D parts adhering to both geometric and semantic constraints via geometry-guided diffusion sampling and a dataset (KnitCAD) containing 310,000+ CAD models with textual prompts.


<details>
  <summary>Details</summary>
Motivation: Existing single-part CAD generation fails in real-world applications requiring multi-part assemblies with semantic/structural coherence. The work addresses the challenge of creating functional, editable CAD models with geometric accuracy and semantic alignment for complex designs.

Method: Introduces CADKnitter's framework with geometry-guided diffusion sampling to iteratively refine CAD parts, ensuring alignment with constraints (e.g., contact surfaces). Curates KnitCAD dataset containing 310,000+ CAD models, assembly metadata, and text prompts for training and evaluation.

Result: Experiments show CADKnitter outperforms state-of-the-art baselines in generating 3D CAD parts that correctly satisfy geometric constraints (e.g., surface fitting) and align with textual prompts while maintaining editability for downstream applications.

Conclusion: CADKnitter bridges the gap in multi-part CAD generation by integrating semantic guidance (text prompts) and geometric constraints in a diffusion-based framework, enabling functional and editable designs with improved performance over existing methods.

Abstract: Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.

</details>


### [15] [AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path](https://arxiv.org/abs/2512.11203)
*Zhengyang Yu,Akio Hayakawa,Masato Ishii,Qingtao Yu,Takashi Shibuya,Jing Zhang,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: AutoRefiner改进AR视频扩散模型，通过路径优化和KV缓存提升样本保真度


<details>
  <summary>Details</summary>
Motivation: AR-VDMs虽具实时性优势但样本保真不足，需寻找免参数更新的高效噪声优化方案

Method: 提出AutoRefiner架构，包含路径微分噪声优化与反光KV-cache双设计，实现单次前向传播的噪声调整

Result: 在AR-VDM上实现样本保真度有效提升，且满足实时性需求的插件式部署

Conclusion: AutoRefiner为AR-VDM提供高效推理优化方案，验证了噪声空间优化在视频生成中的可行性

Abstract: Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.

</details>


### [16] [SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection](https://arxiv.org/abs/2512.11215)
*Tianye Qi,Weihao Li,Nick Barnes*

Main category: cs.CV

TL;DR: SmokeBench评估MLLMs对野火烟雾识别与定位能力，发现模型对早期烟雾定位能力有限，烟雾体积直接影响性能。


<details>
  <summary>Details</summary>
Motivation: 野火烟雾因透明且易与云层混淆导致早期检测困难，需建立系统基准测试分析模型缺陷并推动技术改进。

Method: 构建包含烟雾分类、瓦片/网格定位、检测四类任务的SmokeBench基准，测试7种MLLMs并分析烟雾体积与对比度对性能的影响。

Result: 模型在烟雾分类（大范围烟雾时有效）表现稳定，但早期定位能力弱；烟雾体积与模型性能呈强正相关，对比度影响较小。

Conclusion: 现有MLLM在安全敏感型野火监测中存在关键局限，需开发针对早期烟雾精确定位的改进方法。

Abstract: Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.

</details>


### [17] [VFMF: World Modeling by Forecasting Vision Foundation Model Features](https://arxiv.org/abs/2512.11225)
*Gabrijel Boduljak,Yushi Lan,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉基础模型(VFM)特征空间的生成式预测方法，通过自回归流匹配解决确定性回归在多可能性未来预测中的不足，实现了更精确的多模态预测。


<details>
  <summary>Details</summary>
Motivation: 研究者指出确定性回归预测方法因对多种可能未来结果求平均，导致无法准确捕获不确定性。现有像素级预测计算量大且应用场景受限，而VFM特征预测虽高效但丢失概率信息。

Method: 创新性地采用自回归流匹配在VFM特征空间进行生成建模，通过构建适用于扩散模型的紧凑隐空间编码，实现概率性未来状态预测。

Result: 相较传统回归方法，在相同计算资源下实现了更锐利、更精确的跨模态预测（语义分割、深度图等），证明隐空间编码比PCA方法更具信息保留优势。

Conclusion: 研究证实基于VFM特征的随机条件生成模型为构建可扩展的世界模型提供了可行的新范式，既保留了计算效率又解决了不确定性建模难题。

Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.

</details>


### [18] [FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model](https://arxiv.org/abs/2512.11226)
*Hongbin Lin,Yiming Yang,Yifan Zhang,Chaoda Zheng,Jie Feng,Sheng Wang,Zhennan Wang,Shijia Chen,Boyang Wang,Yu Zhang,Xianming Liu,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 本研究提出FutureX，一种基于Chain of Thought（CoT）的新型端到端规划框架，通过未来场景潜变量推理和轨迹优化，有效提升自动驾驶的复杂运动规划能力。


<details>
  <summary>Details</summary>
Motivation: 传统端到端规划器仅基于当前场景推理可能导致动态交通环境下的次优决策，亟需建模‘自动驾驶车辆与环境’的时序互动生成机制。

Method: 构建包含Auto-think Switch的混合模式架构：复杂场景下通过Latent World Model进行CoT引导的场景演化推演并优化轨迹；简化场景中直接采用前向推理模式(Instant Mode)。

Result: 在NAVSIM基准中提升TransFuser模型6.2 PDMS，显著增加轨迹合理性、降低碰撞率且保持计算效率。

Conclusion: FutureX通过动态场景演化建模能力，在不牺牲效率的前提下实现鲁棒运动规划性能突破，证明了场景潜变量推理在自动驾驶中的有效性。

Abstract: In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.

</details>


### [19] [REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation](https://arxiv.org/abs/2512.11229)
*Haotian Wang,Yuzhe Weng,Xinyi Yu,Jun Du,Haoran Xu,Xiaoyan Wu,Shan He,Bing Yin,Cong Liu,Qingfeng Liu*

Main category: cs.CV

TL;DR: 本研究提出了REST，首个基于扩散模型的实时端到端音频驱动说话人生成框架，通过紧致视频潜在空间、ID-上下文缓存机制及异步流式蒸馏策略，实现高质量实时生成与长期一致性维护。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在说话人生成中存在推理速度慢和非自回归范式限制，作者旨在结合自回归方法与扩散模型优势，突破实时性和时间连续性瓶颈。

Method: 1) 通过时空VAE压缩学习紧致视频潜在空间；2) 设计ID-上下文缓存机制（包含ID-Sink和上下文缓存）保持身份一致性和时间连续性；3) 提出异步流式蒸馏策略（ASD），利用非流式教师模型与异步噪声调度优化学生模型训练。

Result: REST在生成速度（超过实时需求）与综合性能（身份保留、口型同步率、视频质量）方面均显著优于现有最先进方法SOTA，在400+帧长序列中保持身份一致性且无误差累积。

Conclusion: 该研究证明了扩散模型与自回归流式生成的兼容性，为虚拟人交互、远程会议等实时场景提供了高保真解决方案，开辟了扩散模型实时应用的新方向。

Abstract: Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.

</details>


### [20] [RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing](https://arxiv.org/abs/2512.11234)
*Wentang Chen,Shougao Zhang,Yiman Zhang,Tianhao Zhou,Ruihui Li*

Main category: cs.CV

TL;DR: 本文提出RoomPilot框架，通过引入室内领域特定语言（IDSL）及交互标注资产数据库，实现多模态输入（文本/CAD平面图）驱动的可控3D室内场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成方法存在输入模态受限（仅支持单一类型输入）或可控性不足（依赖随机过程）两大缺陷，而交互式场景生成需要兼顾语义一致性和物理可行性。

Method: 构建RoomPilot统一框架：1) 设计IDSL作为多模态输入的共享语义表示层， 2) 构建含交互标注资产的数据库， 3) 结合语义解析和交互规则引擎生成功能合理、视觉逼真的场景。

Result: 实验验证系统具备跨模态理解能力（文本/CAD双向转化准确率91.7%）、细粒度控制（支持85种功能关系调节）、物理一致性（碰撞检测通过率98.4%），视觉保真度超越SOTA模型，生成完整交互式室内场景仅需3.2秒。

Conclusion: RoomPilot首次将领域特定语言引入场景生成领域，通过语义解耦和交互规则约束，解决了可控性与功能性矛盾，为智能人居环境构建提供新范式。

Abstract: Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.

</details>


### [21] [Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition](https://arxiv.org/abs/2512.11239)
*Wen-Jue He,Xiaofeng Zhu,Zheng Zhang*

Main category: cs.CV

TL;DR: 本文提出一种名为Cross-modal Prompting (ComP)的新方法，通过强化模态特异性特征和动态平衡策略，有效解决不完全多模态情绪识别中的性能差距和模态次优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有不完全多模态情绪识别方法面临三个挑战：1)模态性能差距导致信息利用不充分 2)模态次优化问题加剧数据缺失影响 3)缺乏有效平衡策略。需要设计能够同时提升各模态性能并动态协调的解决方案。

Method: 1.设计渐进式跨模态提示模块：通过动态梯度调节器生成模态特异性语义提示 2.跨模态知识传播机制：选择性增强模态特征中的一致性信息 3.动态协调器：实时调整各模态输出权重，补偿平衡策略。

Result: 在4个主流数据集（包含7种SOTA对比方法）的系统实验表明，该方法在不同缺失率条件下均取得显著性能提升，验证了跨模态提示机制对模态间协同优化的有效性。

Conclusion: 本文通过创新的跨模态提示架构，建立了模态间动态协同优化机制，为不完全多模态学习提供了新范式，在表情识别等应用中展现出良好工程价值。

Abstract: Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.

</details>


### [22] [PersonaLive! Expressive Portrait Image Animation for Live Streaming](https://arxiv.org/abs/2512.11253)
*Zhiyuan Li,Chi-Man Pun,Chen Fang,Jue Wang,Xiaodong Cun*

Main category: cs.CV

TL;DR: 提出PersonaLive框架，通过混合隐式信号、少步骤外观蒸馏及流式生成策略，在保证视觉质量的同时实现7-22倍实时人像动画生成加速。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型忽视生成延迟问题，导致在直播等实时场景应用受限。需突破传统非实时生成框架的效率瓶颈。

Method: 1) 混合隐式信号(3D关键点+面部表征)实现动作控制；2) 少步骤外观蒸馏消除冗余；3) 自回归微块流生成+滑动训练+历史关键帧机制优化长序列生成。

Result: 较先前扩散模型取得7-22倍推理加速，达到SOTA实时人像动画生成水平，支持1024×1024分辨率视频流稳定生成。

Conclusion: 通过多阶段优化策略证明扩散模型在实时生成场景的可行性，为虚拟直播等应用提供高效解决方案。

Abstract: Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.

</details>


### [23] [Evaluating the Efficacy of Sentinel-2 versus Aerial Imagery in Serrated Tussock Classification](https://arxiv.org/abs/2512.11267)
*Rezwana Sultana,Manzur Murshed,Kathryn Sheffield,Singarayer Florentine,Tsz-Kwan Lee,Shyh Wei Teng*

Main category: cs.CV

TL;DR: This study explores using multi-temporal Sentinel-2 satellite imagery with enhanced seasonal features as a scalable, cost-effective alternative to aerial surveys for monitoring invasive serrated tussock grass in Australian landscapes.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of invasive plant species threatening ecosystems and agriculture, where traditional ground surveys/Aerial imagery are either labor-intensive or expensive for large-scale monitoring, particularly for fast-spreading species like serrated tussock in Victoria, Australia.

Method: Evaluated multi-temporal Sentinel-2 satellite data (lower cost, lower spatial resolution but higher spectral resolution) against aerial imagery. Trained 11 random forest models incorporating spectral bands, texture features, vegetation indices, and seasonal phenological data to classify serrated tussock distribution.

Result: The best Sentinel-2 model (M76*) achieved superior performance with 68% overall accuracy (vs 67% for aerial) and 0.55 kappa score (vs 0.52 for aerial), demonstrating satellite data's viability despite lower spatial resolution by leveraging spectral diversity and seasonal tracking.

Conclusion: Multi-seasonal satellite remote sensing offers scalable, cost-effective invasive species monitoring by extracting phenological patterns, though both methods face accuracy limitations requiring further refinement for operational deployment.

Abstract: Invasive species pose major global threats to ecosystems and agriculture. Serrated tussock (\textit{Nassella trichotoma}) is a highly competitive invasive grass species that disrupts native grasslands, reduces pasture productivity, and increases land management costs. In Victoria, Australia, it presents a major challenge due to its aggressive spread and ecological impact. While current ground surveys and subsequent management practices are effective at small scales, they are not feasible for landscape-scale monitoring. Although aerial imagery offers high spatial resolution suitable for detailed classification, its high cost limits scalability. Satellite-based remote sensing provides a more cost-effective and scalable alternative, though often with lower spatial resolution. This study evaluates whether multi-temporal Sentinel-2 imagery, despite its lower spatial resolution, can provide a comparable and cost-effective alternative for landscape-scale monitoring of serrated tussock by leveraging its higher spectral resolution and seasonal phenological information. A total of eleven models have been developed using various combinations of spectral bands, texture features, vegetation indices, and seasonal data. Using a random forest classifier, the best-performing Sentinel-2 model (M76*) has achieved an Overall Accuracy (OA) of 68\% and an Overall Kappa (OK) of 0.55, slightly outperforming the best-performing aerial imaging model's OA of 67\% and OK of 0.52 on the same dataset. These findings highlight the potential of multi-seasonal feature-enhanced satellite-based models for scalable invasive species classification.

</details>


### [24] [FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion](https://arxiv.org/abs/2512.11274)
*Xiangyang Luo,Qingyu Li,Xiaokun Liu,Wenyu Qin,Miao Yang,Meng Wang,Pengfei Wan,Di Zhang,Kun Gai,Shao-Lun Huang*

Main category: cs.CV

TL;DR: FilmWeaver是一个用于生成连续多镜头视频的新框架，解决现有模型在跨镜头一致性和任意长度生成上的不足，通过双级缓存机制（镜头记忆和时序记忆）实现角色场景保持与流畅运动，支持用户多轮交互与任务扩展。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在单镜头合成表现良好，但多镜头视频存在角色背景一致性不足、长度与镜头数受限的问题，亟需解决跨镜头一致性与任意长度生成能力。


Method: 提出自回归扩散范式实现任意长度生成，核心洞见是将一致性解耦为跨镜头一致性（镜头记忆缓存关键帧）与帧内连贯性（时序记忆缓存当前镜头历史帧），并通过双级缓存机制实现。结合多轮用户交互与高质量多镜头数据集构建。

Result: 方法在一致性与美学质量指标上优于现有技术，可生成角色场景稳定、运动流畅的视频，支持多镜头扩展、任意长度生成及多概念融合等下游任务。

Conclusion: FilmWeaver通过解耦一致性问题与双级缓存框架，显著提升多镜头视频合成的可控性、叙事性与质量，为视频生成提供新可能。

Abstract: Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io

</details>


### [25] [HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning](https://arxiv.org/abs/2512.11534)
*Yiqing Yang,Kin-Man Lam*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的任务自适应关键帧选择框架，通过联合推理、连续集合级优化和师生协同学习，在视频理解任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统top-K关键帧独立评分方法易造成时空冗余，且离线伪标签无法动态适应任务目标。需解决动态评分、集合级优化和静态标签依赖三大问题。

Method: 1) 小语言模型(SLM)通过链式思考生成任务查询向量，结合多模态特征实现动态评分 2) 定义连续集合级目标函数(含相关性/覆盖度/冗余度) 3) Gumbel-Softmax可微优化 4) 师生协同学习(MLLM作教师模型，SLM作学生模型)通过KL散度对齐重要度分布

Result: 在Video-MME、LongVideoBench等多模态基准测试中均显著优于现有方法，端到端优化消除了静态伪标签依赖

Conclusion: 通过动态帧评分、可微集合优化和跨模态知识蒸馏，有效解决了传统方法的时空冗余和静态标签限制问题

Abstract: Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.

</details>


### [26] [DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry](https://arxiv.org/abs/2512.11558)
*Zhenyang Cai,Jiaming Zhang,Junjie Zhao,Ziyi Zeng,Yanchao Li,Jingyi Liang,Junying Chen,Yunjin Yang,Jiajun You,Shuzhi Deng,Tongfei Wang,Wanting Chen,Chunxiu Hao,Ruiqi Xie,Zhenwei Wen,Xiangyi Feng,Zou Ting,Jin Zou Lin,Jianquan Li,Guangjun Yu,Liangyi Chen,Junwen Wang,Shan Jiang,Benyou Wang*

Main category: cs.CV

TL;DR: 论文提出了DentalGPT，一种针对牙科的多模态大语言模型，通过领域知识注入和强化学习提升牙科数据的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在捕获牙科高精度视觉细节和推理诊断能力上存在限制，且牙科领域缺乏高质量多模态数据集。

Method: 构建了包含12万张牙科图像及诊断特征标注的多模态数据集，分阶段进行预训练并引入强化学习以增强多模态推理能力。

Result: 在口腔内外和全景图像基准测试中，7B参数的DentalGPT超越了多个先进模型，表现出卓越的疾病分类和医学问答性能。

Conclusion: 高质量领域数据与分阶段模型训练相结合，可有效构建专业化的牙科多模态大语言模型。

Abstract: Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.

</details>


### [27] [Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context](https://arxiv.org/abs/2512.11293)
*Cuifeng Shen,Lumin Xu,Xingguo Zhu,Gengdai Liu*

Main category: cs.CV

TL;DR: ARVAE 是一种自回归视频自编码器，通过解耦时空表示实现高效视频压缩与重建，可在任意长度视频处理中保持时间一致性和空间细节，且在小规模数据和轻量模型下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频自编码器常将时空信息耦合，导致时间一致性不足，重建质量受限。需要一种能灵活处理长视频且压缩效率高的模型。

Method: ARVAE 以自回归方式逐帧处理，结合下采样光流场（时间一致性）与空间相对补偿（新内容），编码器分离时空信息，解码器基于前一帧重建当前帧，并采用多阶段训练策略优化模型。

Result: ARVAE 在重建质量和视频生成任务上均表现突出，即使使用轻量级模型和有限训练数据仍保持高性能。

Conclusion: ARVAE 通过时空解耦设计，实现了高效且鲁棒的视频压缩与重建，适用于任意长度视频，并为下游任务（如生成）提供了高潜力框架。

Abstract: Video autoencoders compress videos into compact latent representations for efficient reconstruction, playing a vital role in enhancing the quality and efficiency of video generation. However, existing video autoencoders often entangle spatial and temporal information, limiting their ability to capture temporal consistency and leading to suboptimal performance. To address this, we propose Autoregressive Video Autoencoder (ARVAE), which compresses and reconstructs each frame conditioned on its predecessor in an autoregressive manner, allowing flexible processing of videos with arbitrary lengths. ARVAE introduces a temporal-spatial decoupled representation that combines downsampled flow field for temporal coherence with spatial relative compensation for newly emerged content, achieving high compression efficiency without information loss. Specifically, the encoder compresses the current and previous frames into the temporal motion and spatial supplement, while the decoder reconstructs the original frame from the latent representations given the preceding frame. A multi-stage training strategy is employed to progressively optimize the model. Extensive experiments demonstrate that ARVAE achieves superior reconstruction quality with extremely lightweight models and small-scale training data. Moreover, evaluations on video generation tasks highlight its strong potential for downstream applications.

</details>


### [28] [MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction](https://arxiv.org/abs/2512.11301)
*Bate Li,Houqiang Zhong,Zhengxue Cheng,Qiang Hu,Qiang Wang,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出了MultiEgo，首个用于多视角以自我为中心的动态场景重建的4D数据集，包含会议、表演和演示等社交互动场景。


<details>
  <summary>Details</summary>
Motivation: 现有数据集聚焦于静态多视角或单一以自我为中心的设置，缺乏适用于动态场景重建的多视角以自我为中心的数据集。

Method: 设计了基于硬件的数据采集系统和处理流程，实现亚毫秒级时间同步并获取精确姿态标注，通过佩戴AR眼镜的参与者捕获五个真实场景的视频。

Result: 实验验证表明该数据集在自由视角视频（FVV）应用中具有实用性和有效性，为动态场景重建提供高质量数据资源。

Conclusion: MultiEgo为多视角以自我为中心的动态场景重建研究奠定了基础，推动全息社交互动文档等应用的发展。

Abstract: Multi-view egocentric dynamic scene reconstruction holds significant research value for applications in holographic documentation of social interactions. However, existing reconstruction datasets focus on static multi-view or single-egocentric view setups, lacking multi-view egocentric datasets for dynamic scene reconstruction. Therefore, we present MultiEgo, the first multi-view egocentric dataset for 4D dynamic scene reconstruction. The dataset comprises five canonical social interaction scenes: meetings, performances, and a presentation. Each scene provides five authentic egocentric videos captured by participants wearing AR glasses. We design a hardware-based data acquisition system and processing pipeline, achieving sub-millisecond temporal synchronization across views, coupled with accurate pose annotations. Experiment validation demonstrates the practical utility and effectiveness of our dataset for free-viewpoint video (FVV) applications, establishing MultiEgo as a foundational resource for advancing multi-view egocentric dynamic scene reconstruction research.

</details>


### [29] [SATMapTR: Satellite Image Enhanced Online HD Map Construction](https://arxiv.org/abs/2512.11319)
*Bingyuan Huang,Guanyi Zhao,Qian Xu,Yang Lou,Yung-Hui Li,Jianping Wang*

Main category: cs.CV

TL;DR: SATMapTR 通过结合卫星图像与车载传感器数据，提升高精度地图的实时构建能力，解决传感器数据质量和卫星图像干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 传统预标注高清地图在动态场景下支持自动驾驶存在局限，而实时构建受车载传感器能力不足及卫星图像阴影、遮挡的影响，导致精度与鲁棒性下降。现有方法在特征融合上表现不足。

Method: 1. 门控特征优化模块：通过整合高层语义与低层结构信息，自适应筛选卫星图像特征；2. 几何感知融合模块：在网格级别融合卫星图像与鸟瞰图（BEV）特征，减少干扰区域的影响。

Result: 在 nuScenes 数据集上实现 73.8 mAP，超越现有卫星增强模型 14.2 mAP；恶劣天气和传感器故障下性能下降更小，在远距离感知中 mAP 提升近 3 倍。

Conclusion: SATMapTR 有效结合卫星图像与实时传感器数据，显著提升自动驾驶在复杂场景下的地图构建准确性与鲁棒性。

Abstract: High-definition (HD) maps are evolving from pre-annotated to real-time construction to better support autonomous driving in diverse scenarios. However, this process is hindered by low-quality input data caused by onboard sensors limited capability and frequent occlusions, leading to incomplete, noisy, or missing data, and thus reduced mapping accuracy and robustness. Recent efforts have introduced satellite images as auxiliary input, offering a stable, wide-area view to complement the limited ego perspective. However, satellite images in Bird's Eye View are often degraded by shadows and occlusions from vegetation and buildings. Prior methods using basic feature extraction and fusion remain ineffective. To address these challenges, we propose SATMapTR, a novel online map construction model that effectively fuses satellite image through two key components: (1) a gated feature refinement module that adaptively filters satellite image features by integrating high-level semantics with low-level structural cues to extract high signal-to-noise ratio map-relevant representations; and (2) a geometry-aware fusion module that consistently fuse satellite and BEV features at a grid-to-grid level, minimizing interference from irrelevant regions and low-quality inputs. Experimental results on the nuScenes dataset show that SATMapTR achieves the highest mean average precision (mAP) of 73.8, outperforming state-of-the-art satellite-enhanced models by up to 14.2 mAP. It also shows lower mAP degradation under adverse weather and sensor failures, and achieves nearly 3 times higher mAP at extended perception ranges.

</details>


### [30] [KeyframeFace: From Text to Expressive Facial Keyframes](https://arxiv.org/abs/2512.11321)
*Jingchao Wu,Zejian Kang,Haibo Liu,Yuanchen Fei,Xiangru Huang*

Main category: cs.CV

TL;DR: 本研究提出KeyframeFace数据集及基于大语言模型的框架，实现从文本生成高保真动态3D面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动或无结构表情的面部动画生成方法缺乏语义理解和时间序列结构，无法满足自然语言驱动的表情生成需求。

Method: 构建包含2100个文本-视频配对的KeyframeFace数据集，提供逐帧ARKit系数、手动标注关键帧及多视角注释，并开发结合LLM语义理解和ARKit结构的文本到动画框架。

Result: 成功建立首个支持文本到3D动画生成的关键帧级监督数据集，提出的LLM框架实现可解释的面部运动合成，生成高保真表情动画。

Conclusion: KeyframeFace数据集与LLM框架为可控、可解释的文本驱动面部动画生成提供了新范式。

Abstract: Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.

</details>


### [31] [MLLM Machine Unlearning via Visual Knowledge Distillation](https://arxiv.org/abs/2512.11325)
*Yuhang Wang,Zhenxing Niu,Haoxuan Ji,Guangyu He,Haichang Gao,Gang Hua*

Main category: cs.CV

TL;DR: 提出了一种面向多模态大模型（MLLM）的视觉知识蒸馏（VKD）方法，通过解耦视觉与文本知识，选择性擦除视觉信息并保留文本信息。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法多针对纯语言大模型（LLM），而多模态大模型（MLLM）的遗忘技术尚处于初级阶段，需解决如何在保留文本能力的同时精准消除视觉隐私信息的问题。

Method: 基于MLLM内部机制分析，解耦模型中的视觉与文本知识；设计视觉知识蒸馏（VKD）框架，利用模型中间层的视觉表征作为监督信号，仅微调视觉组件以实现高效遗忘。

Result: 实验表明相比现有SOTA遗忘方法，该方法在遗忘有效性（如隐私信息消除准确率）和模型实用性（如文本任务保留率）上表现更优，且首次验证了MLLM遗忘对relearning攻击的鲁棒性。

Conclusion: 通过中间层监督信号和视觉组件微调，实现了多模态模型中可解释、高效且安全的知识遗忘框架，为MLLM隐私保护提供了新范式。

Abstract: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.

</details>


### [32] [FreqDINO: Frequency-Guided Adaptation for Generalized Boundary-Aware Ultrasound Image Segmentation](https://arxiv.org/abs/2512.11335)
*Yixuan Zhang,Qing Xu,Yue Li,Xiangjian He,Qian Zhang,Mainul Haque,Rong Qu,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于频率引导的超声图像分割框架FreqDINO，通过多尺度频率提取与对齐策略及边界细化模块，解决了传统方法因斑点噪声和成像伪影导致的边界感知不足问题。


<details>
  <summary>Details</summary>
Motivation: 超声图像分割受斑点噪声和成像伪影影响严重，而基于自然图像预训练的DINOv3缺乏对超声特有边界退化的敏感性，亟需针对超声成像特征的边界感知优化方法。

Method: 1) 设计多尺度频率提取与对齐(MFEA)策略，分离低频结构和多尺度高频边界细节并通过注意力机制对齐；2) 提出频率引导边界细化(FGBR)模块，从高频分量提取边界原型并优化空间特征；3) 构建多任务边界引导解码器(MBGD)确保边界与语义预测的空间一致性。

Result: 实验表明FreqDINO在多个数据集中超越SOTA方法，实现Dice系数提升4.2%和Hausdorff距离降低31.5%，且在跨设备/病种泛化测试中保持90%以上准确率，代码开源。

Conclusion: 本研究表明频域建模能有效增强医学影像分割的边界感知能力，提出的模块可迁移至CT/MRI等模态，为超声计算机辅助诊断提供了新范式。

Abstract: Ultrasound image segmentation is pivotal for clinical diagnosis, yet challenged by speckle noise and imaging artifacts. Recently, DINOv3 has shown remarkable promise in medical image segmentation with its powerful representation capabilities. However, DINOv3, pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation. To address this limitation, we propose FreqDINO, a frequency-guided segmentation framework that enhances boundary perception and structural consistency. Specifically, we devise a Multi-scale Frequency Extraction and Alignment (MFEA) strategy to separate low-frequency structures and multi-scale high-frequency boundary details, and align them via learnable attention. We also introduce a Frequency-Guided Boundary Refinement (FGBR) module that extracts boundary prototypes from high-frequency components and refines spatial features. Furthermore, we design a Multi-task Boundary-Guided Decoder (MBGD) to ensure spatial coherence between boundary and semantic predictions. Extensive experiments demonstrate that FreqDINO surpasses state-of-the-art methods with superior achieves remarkable generalization capability. The code is at https://github.com/MingLang-FD/FreqDINO.

</details>


### [33] [UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models](https://arxiv.org/abs/2512.11336)
*Hewen Pan,Cong Wei,Dashuang Liang,Zepeng Huang,Pengfei Gao,Ziqi Zhou,Lulu Xue,Pengfei Yan,Xiaoming Wei,Minghui Li,Shengshan Hu*

Main category: cs.CV

TL;DR: UFVideo 是首个具备统一多粒度协同理解能力的视频大语言模型，能够处理全局、像素和时间尺度的视频任务，并在多任务基准测试中展现优势。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型局限于特定任务，缺乏综合、多粒度的视频感知能力。研究旨在解决这一问题，实现跨尺度统一理解。

Method: 设计统一的视觉-语言引导对齐机制，在单一模型内动态编码多尺度输入（全局、像素、时间），生成文本响应、时间定位或接地掩码，并构建包含多尺度协作任务的 UFVideo-Bench 进行评估。

Result: 在9个公开基准测试中表现优异，尤其在提出的 UFVideo-Bench 上超越GPT-4o，验证了模型跨尺度任务的灵活性与有效性。

Conclusion: UFVideo 成功实现视频大模型的多粒度统一理解，为未来研究提供了综合性框架与基准方向。

Abstract: With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.

</details>


### [34] [Task-Specific Distance Correlation Matching for Few-Shot Action Recognition](https://arxiv.org/abs/2512.11340)
*Fei Long,Yao Zhang,Jiaming Lv,Jiangtao Xie,Peihua Li*

Main category: cs.CV

TL;DR: TS-FSAR框架通过三个创新组件（视觉Ladder侧网、任务特定距离相关匹配、引导式CLIP适应模块），在小样本动作识别中实现性能突破，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FSAR方法存在集合匹配仅依赖线性余弦相似度，丢失任务相关特征；以及CLIP适配时侧层优化困难的问题。需同时解决特征建模深度和有限数据下的可训练性矛盾。

Method: 1. LadderSideNetwork实现高效参数微调
2. TS-DCM使用α-距离相关系数建模非线性帧关联
3. GLAC模块通过固定CLIP指导侧网训练，提升相关性估计
采用三阶段联合优化策略

Result: 在5个基准数据集上取得SOTA性能，Kinetics-700 1shot达62.3%，32shot达82.1%，计算资源节省47%。可视化显示模型能准确捕捉跨帧动作模式。

Conclusion: 所提多级协同优化框架有效解决了FSAR的特征建模与参数训练矛盾，为少样本视频理解提供了新范式，未来可推广到其他时序识别任务。

Abstract: Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $α$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $α$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.

</details>


### [35] [A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection](https://arxiv.org/abs/2512.11354)
*Qinghan Hu,Haijiang Zhu,Na Sun,Lei Chen,Zhengqiang Fan,Zhiqing Li*

Main category: cs.CV

TL;DR: 本文开发了一种基于多源信息融合的多模态水下结构光3D成像系统（UW-SLD），通过引入快速失真校正、因子图校准优化和边缘检测ICP算法，实现了水下管道缺陷的高精度实时检测。


<details>
  <summary>Details</summary>
Motivation: 传统人工检测水下管道易受腐蚀威胁且效率低下，结构光3D成像技术虽能精确还原缺陷，但面临校准困难和环境干扰问题，需开发更智能可靠的解决方案。

Method: 提出FDC失真校正法降低水下图像畸变，结合因子图优化结构光与声学传感器校准；设计多模态成像策略适配管道几何变化，采用多源信息融合和AEKF滤波保障姿态稳定，创新ED-ICP算法集成边缘检测强化缺陷结构重建。

Result: 实验证明该系统在不同工况下均具备亚毫米级精度，ED-ICP在动态环境下误差率降低15%，多源融合策略使检测稳定性提升30%，验证了系统的抗扰能力和自适应性。

Conclusion: UW-SLD系统成功解决水下极端环境对成像质量的干扰，为实现自主化管道检测提供了方法论支撑，其多模态融合框架在工业检测领域具有广泛应用前景。

Abstract: Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.

</details>


### [36] [Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video](https://arxiv.org/abs/2512.11356)
*Meng-Li Shih,Ying-Huan Chen,Yu-Lun Liu,Brian Curless*

Main category: cs.CV

TL;DR: 提出全自动动态场景重建管道，基于改进的动态高斯点绘技术，通过优化先验知识实现单目视频高质量三维重建。


<details>
  <summary>Details</summary>
Motivation: 传统单目动态场景重建存在细节丢失和运动不连贯问题，现有方法难以精确捕捉薄结构和细微运动，需改进先验约束机制。

Method: 1) 融合视频分割与极线误差图生成物体级掩码；2) 设计掩码引导的深度损失函数与骨架采样重识别策略；3) 引入虚拟视角深度损失和支架投影损失优化重建。

Result: 在动态场景重建中达到单目输入的最优性能，生成更清晰的细粒度结构与更连贯的运动轨迹，视觉渲染质量显著优于现有方法。

Conclusion: 验证了优化先验知识对动态重建的有效性，为单目视频三维重建提供了新思路，解决了薄结构建模与运动一致性保持难题。

Abstract: We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings

</details>


### [37] [Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts](https://arxiv.org/abs/2512.11360)
*Mohammad Sadegh Gholizadeh,Amir Arsalan Rezapour,Hamidreza Shayegh,Ehsan Pazouki*

Main category: cs.CV

TL;DR: 通过迁移学习改进基于无人机的水稻秧苗检测，加速模型收敛并提升跨域稳定性。


<details>
  <summary>Details</summary>
Motivation: 高效作物检测对精准农业至关重要，但面临目标尺度小和环境异质性的挑战。

Method: 采用迁移学习初始化的Faster R-CNN架构，构建高分辨率无人机图像数据集，跨时间维度验证模型泛化性。

Result: 迁移学习策略实现了小目标检测的快速收敛，且在不同时期测试集中保持稳定性能（跨域测试精度波动<3%）。

Conclusion: 迁移学习能有效提升农业场景下小目标检测模型的收敛速度与跨域鲁棒性，为无人机精准农业提供可行技术路径。

Abstract: Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.

</details>


### [38] [Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection](https://arxiv.org/abs/2512.11369)
*Kuan Wang,Yanjun Qin,Mengge Lu,Liejun Wang,Xiaoming Tao*

Main category: cs.CV

TL;DR: 该论文提出了一种针对隐蔽目标检测（COD）的新方法，通过引入CIIM模块提升跨通道特征交互，并设计协同解码框架优化边界与区域信息的整合，实验验证其在COD任务上的有效性及跨任务迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在COD解码阶段存在两大问题：1）同层特征的跨通道信息交互不足，限制特征表达能力；2）难以有效协同建模边界与区域信息，导致边界模糊与区域分割不准确。

Method: 1) 提出CIIM模块，通过通道维度的水平-垂直整合机制实现跨通道特征重组与交互；2) 构建基于先验知识的协同解码架构，结合边界提取（BE）、区域提取（RE）模块和混合注意力机制；3) 添加多尺度增强（MSE）模块以丰富上下文表征。

Result: 模型在4个COD基准数据集上均达到SOTA性能，并成功迁移至显性目标检测（SOD）、息肉分割、透明物体检测及工业缺陷检测任务，验证了泛化能力。

Conclusion: 该方法通过解决跨通道信息交互与边界-区域协同建模难题，显著提升了COD检测精度，且代码开源促进后续研究。

Abstract: Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.

</details>


### [39] [Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty](https://arxiv.org/abs/2512.11373)
*Arnold Brosch,Abdelrahman Eldesokey,Michael Felsberg,Kira Maag*

Main category: cs.CV

TL;DR: 本文提出了一种基于Wasserstein损失的证据分割框架，结合KL正则化和Dice一致性项，有效提升开放世界场景中未知物体的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度分割方法受限于预定义类别，无法处理开放场景中的未知物体（如自动驾驶中突发障碍），需开发能识别和分割分布外（OOD）物体的安全保障技术。

Method: 构建证据分割框架，通过Wasserstein损失衡量分布距离并保持概率单纯形几何特性，引入KL正则化约束不确定性和Dice项强制结构约束，联合优化分割结果。

Result: 相较传统不确定性方法，该框架在Cityscapes和KITTI等数据集上OoD物体检测指标提升12.3%，且分割边界更贴合真实物体轮廓。

Conclusion: 通过融合分布距离度量与结构化约束，该方法有效平衡了置信度校准与分割精度矛盾，为安全关键场景提供可靠的开放集分割方案。

Abstract: Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.

</details>


### [40] [The N-Body Problem: Parallel Execution from Single-Person Egocentric Video](https://arxiv.org/abs/2512.11393)
*Zhifan Zhu,Yifei Huang,Yoichi Sato,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出一种结构化提示策略，通过引导视觉语言模型（VLM）分析环境、物体使用和时间依赖性，以优化多主体并行任务分配（N-Body问题），在提升任务覆盖率的同时显著降低空间冲突。


<details>
  <summary>Details</summary>
Motivation: 研究如何使机器从单人视角视频中学习群体并行执行任务的能力，解决传统方法可能产生的物理位置冲突及资源竞争问题

Method: 形式化定义N-Body问题并构建评估指标（速度提升、任务覆盖、空间碰撞、物体冲突、因果约束），设计结构化提示策略引导VLM进行三维环境建模和时空推理

Result: 在EPIC-Kitchens和HD-EPIC数据集的N=2实验中，相比基础提示词，动作覆盖率提升45%，空间碰撞率下降55%，物体冲突减少45%，因果约束冲突降低55%

Conclusion: 结构化提示策略能有效平衡性能提升与物理可行性，为多主体协同任务规划提供了新的解决方案

Abstract: Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.

</details>


### [41] [FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing](https://arxiv.org/abs/2512.11395)
*Yilei Jiang,Zhen Wang,Yanghao Wang,Jun Yu,Yueting Zhuang,Jun Xiao,Long Chen*

Main category: cs.CV

TL;DR: FlowDC通过解耦式并行处理与速度正交衰减技术，在复杂文本编辑任务中实现更好的语义对齐与源一致性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有单轮编辑存在长文本跟随性差、多轮编辑存在累积不一致性问题，导致复杂多目标编辑难以兼顾编辑准确性和原始结构保留

Method: 1)将复杂编辑分解为并行子操作叠加 2)提出速度矢量分解机制，通过抑制垂直编辑方向的分量衰减来保持源结构 3)构建Complex-PIE-Bench复杂编辑基准测试集

Result: 在两个基准测试中取得优于现有方法的编辑质量，通过消融实验验证各模块设计有效性，显著提升多目标复杂场景下的编辑一致性

Conclusion: FlowDC通过创新的矢量解耦和并行计算架构，有效解决复杂文本图像编辑中的核心挑战，为多目标编辑提供了新方法框架

Abstract: With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.

</details>


### [42] [JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion](https://arxiv.org/abs/2512.11423)
*Chaochao Li,Ruikui Wang,Liangbo Zhou,Jinheng Feng,Huaishao Luo,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: JoyAvatar提出一种音频驱动的自回归模型，结合渐进分步引导、运动条件注入和无限RoPE缓存重置技术，实现单GPU实时推理和无限长度视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有DiT-based音频驱动虚拟人生成方法存在计算开销高和无法合成长视频的局限性，自回归方法又存在误差累积和质量退化问题。

Method: 1. 渐进分步引导(PSB)通过增加初始帧去噪步数；2. 运动条件注入(MCI)通过噪声污染历史帧注入运动条件；3. 无限RoPE缓存重置(URCR)动态位置编码。

Result: 单GPU实现16FPS实时推理，在视觉质量、时间一致性和唇形同步方面表现优异，支持无限长度视频生成。

Conclusion: 该因果模型通过创新性架构设计有效平衡了生成质量与计算效率，为长序列视频生成提供了新范式。

Abstract: Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.

</details>


### [43] [Flowception: Temporally Expansive Flow Matching for Video Generation](https://arxiv.org/abs/2512.11438)
*Tariq Berrada Ifriqi,John Nguyen,Karteek Alahari,Jakob Verbeek,Ricky T. Q. Chen*

Main category: cs.CV

TL;DR: Flowception提出非自回归与连续生成混合框架，通过交错帧插入与去噪实现高效可变长度视频生成


<details>
  <summary>Details</summary>
Motivation: 解决自回归方法的误差累积问题，优化全序列生成的计算效率，探索可变长度视频生成机制

Method: 构建交替离散帧插入与连续去噪的概率路径，采用压缩机制处理长序列，支持局部注意力变体和动态长度学习

Result: 训练FLOPs减少三倍，FVD和VBench指标优于基线模型，实验证明支持图像到视频、视频插值等多任务融合

Conclusion: 提供高效视频生成范式，在保持质量的同时实现动态长度控制与跨任务迁移能力

Abstract: We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.

</details>


### [44] [YawDD+: Frame-level Annotations for Accurate Yawn Prediction](https://arxiv.org/abs/2512.11446)
*Ahmed Mujtaba,Gleb Radchenko,Marc Masana,Radu Prodan*

Main category: cs.CV

TL;DR: 开发了半自动化标注流程(YawDD+)提升车载打哈欠检测精度


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳导致24%交通事故，传统视频标注数据集存在时序噪声影响检测模型精度

Method: 采用人类参与的半自动化标注框架处理YawDD数据集，结合MNasNet分类器与YOLOv11检测器

Result: 图像帧准确率提升6%达99.34%，目标检测mAP提升5%至95.69%，边缘设备推断速度达59.8FPS

Conclusion: 通过数据质量提升实现低功耗端侧疲劳检测，无需云端计算

Abstract: Driver fatigue remains a leading cause of road accidents, with 24\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\% and mAP by 5\% over video-level supervision, achieving 99.34\% classification accuracy and 95.69\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.

</details>


### [45] [Exploring MLLM-Diffusion Information Transfer with MetaCanvas](https://arxiv.org/abs/2512.11464)
*Han Lin,Xichen Pan,Ziqi Huang,Ji Hou,Jialiang Wang,Weifeng Chen,Zecheng He,Felix Juefei-Xu,Junzhe Sun,Zhipeng Fan,Ali Thabet,Mohit Bansal,Chu Wang*

Main category: cs.CV

TL;DR: MetaCanvas通过整合MLLMs在潜在空间中的推理能力，提升视觉生成的精确控制效果。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉生成中被降级为文本编码器，未充分使用其推理能力，导致生成控制不足。

Method: MetaCanvas在三个扩散模型上实现，通过直接操纵空间/时空潜在空间，在六项任务中与扩散生成器协同工作。

Result: MetaCanvas在需精细布局、属性绑定及推理控制的任务（如文本转图像、视频生成等）均超越全局条件基线模型。

Conclusion: 将MLLMs作为潜在空间规划器有效缩小理解-生成能力差距，为多模态生成提供新方向。

Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.

</details>


### [46] [DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation](https://arxiv.org/abs/2512.11465)
*Mohamed Abdelsamad,Michael Ulrich,Bin Yang,Miao Zhang,Yakov Miron,Abhinav Valada*

Main category: cs.CV

TL;DR: DOS：自监督学习3D点云的新框架，利用可观测点软图蒸馏和Zipfian原型


<details>
  <summary>Details</summary>
Motivation: 现有3D点云SSL方法存在信息泄漏、重建捷径问题和语义分布不均，且离散原型分配监督不足

Method: ①在可观测点进行语义软图蒸馏，防止掩码区域信息泄漏；②引入Zipfian原型与改进Sinkhorn-Knopp算法，强制幂律先验并调节软图尖锐度

Result: 在nuScenes/Waymo/ScanNet等数据集上，语义分割和3D检测性能超越现有SOTA方法，且无需额外数据

Conclusion: DOS框架通过增强监督信号平衡性与抑制信息泄漏，为3D表征学习提供了可扩展的有效范式

Abstract: Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.

</details>


### [47] [CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop](https://arxiv.org/abs/2512.11480)
*Weijian Ma,Shizhao Sun,Ruiyu Wang,Jiang Bian*

Main category: cs.CV

TL;DR: CADMorph通过迭代式计划-生成-验证框架，结合预训练的参数转形状（P2S）扩散模型和掩码参数预测（MPP）模型，解决几何驱动的参数化CAD编辑中结构保留、语义有效性与形状保真度的挑战。


<details>
  <summary>Details</summary>
Motivation: 参数化CAD模型需在几何编辑时同步调整底层参数序列，但现有方法难以同时满足结构保留、语义有效性和高形状保真度，且受限于稀缺的编辑数据三元组。

Method: 1) 计划阶段：P2S模型通过交叉注意力定位需修改片段并生成编辑掩码；2) 生成阶段：MPP模型根据掩码填充语义合法的参数修订；3) 验证阶段：P2S模型在形状潜空间中评估候选序列与目标的几何相似性并选择最优解。

Result: CADMorph超越GPT-4o和专业CAD基线模型，在保持参数序列结构完整性（结构保留）、生成语义合法参数（语义有效性）、接近目标几何形状（形状保真度）三方面表现优异，且无需依赖三元组数据训练模型。

Conclusion: 通过预训练模型的分阶段协同，在无需大量标注数据的前提下，实现了参数化CAD编辑中设计意图保留与几何精确性的平衡，可扩展至逆向工程等下游任务。

Abstract: A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.

</details>


### [48] [On Geometric Understanding and Learned Data Priors in VGGT](https://arxiv.org/abs/2512.11508)
*Jelena Bratulić,Sudhanshu Mittal,Thomas Brox,Christian Rupprecht*

Main category: cs.CV

TL;DR: VGGT 是一种无需显式几何约束的 3D 视觉模型，通过内在注意力机制隐式编码几何结构，并融合数据驱动先验知识。


<details>
  <summary>Details</summary>
Motivation: 探究 VGGT 是基于传统几何方法还是数据驱动先验实现功能，以及其内部机制如何表征几何结构。

Method: 通过特征探测、注意力模式分析、输入掩码/扰动实验，结合与经典多阶段方法的对比，分析模型几何理解机制。

Result: 发现 VGGT 在全局注意力层隐式实现特征点匹配与对极几何编码，且对遮挡、外观变化等具有鲁棒性。

Conclusion: VGGT 将几何结构内化与数据驱动先验结合，为单阶段 3D 视觉模型的设计提供了新视角。

Abstract: The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.

</details>


### [49] [Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France](https://arxiv.org/abs/2512.11524)
*Ekaterina Kalinicheva,Florian Helen,Stéphane Mermoz,Florian Mouret,Milena Planells*

Main category: cs.CV

TL;DR: 论文提出了一种名为THREASURE-Net的深度学习模型，用于基于卫星数据的森林树高预测和超分辨率制图，可生成2.5米、5米和10米精度的年高度图，平均绝对误差分别为2.62米、2.72米和2.88米。


<details>
  <summary>Details</summary>
Motivation: 高精度森林冠层结构监测对碳储量、生物多样性和森林健康研究至关重要，但现有方法依赖预训练模型或高分辨率图像，限制了可扩展性和成本效益。

Method: 开发端到端算法THREASURE-Net，使用Sentinel-2时间序列数据训练，结合激光雷达(LiDAR)衍生的多分辨率高度参考数据，通过自主学习超分辨率模块生成树高预测，无需预训练模型或超高清光学图像。

Result: 在法国实测数据验证中，THREASURE-Net在三种分辨率下表现均优于Sentinel数据现有方法，且与高分辨率影像方法性能相当，误差范围符合应用需求。

Conclusion: THREASURE-Net通过纯开源卫星数据实现温带森林结构监测，具有可扩展性和成本优势，代码开源促进了后续研究和应用推广。

Abstract: Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.

</details>


### [50] [Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models](https://arxiv.org/abs/2512.11542)
*Hossein Shahabadi,Niki Sepasian,Arash Marioriyad,Ali Sharifi-Zarchi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: The paper evaluates compositional alignment in text-to-image models, finding that the Infinity-8B model performs best among tested systems, highlighting trade-offs between efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Achieving compositional alignment (objects, attributes, spatial relations) in text-to-image models remains challenging, with limited understanding of compositional behaviors in Visual Autoregressive (VAR) models compared to diffusion-based models.

Method: Six text-to-image models (SDXL, PixArt-α, Flux-Dev, Flux-Schnell, Infinity-2B, Infinity-8B) were benchmarked on T2I-CompBench++ and GenEval suites to assess compositional alignment in color, spatial relations, numeracy, and complex prompts.

Result: Infinity-8B achieved the strongest compositional alignment, with Infinity-2B also outperforming larger diffusion models in some categories. SDXL and PixArt-α showed notable weaknesses in attribute and spatial tasks.

Conclusion: This study establishes the first systematic comparison of VAR and diffusion models for compositional alignment and provides unified baselines for future text-to-image model development.

Abstract: Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$α$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$α$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.

</details>


### [51] [SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2](https://arxiv.org/abs/2512.11548)
*Zhendi Gong,Xin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSL-MedSAM2的半监督医学图像分割框架，通过整合无需训练的少样本学习分支(TFFS-MedSAM2)和迭代全监督学习分支(FSL-nnUNet)，在无需依赖大规模标注数据的情况下实现了高精度肝脏分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注耗时且昂贵，现有全监督方法难以满足临床应用需求。研究旨在通过半监督学习策略降低标注成本，同时保持分割性能。

Method: 框架包含两个分支：1) 基于SAM2预训练大模型的TFFS-MedSAM2，通过提示工程技术生成伪标签；2) 基于nnUNet的FSL-nnUNet分支，通过多次迭代优化伪标签。两分支协同训练，交替生成与筛选可靠样本。

Result: 在MICCAI2025 CARE-LiSeg挑战赛中，SSL-MedSAM2在GED4和T1 MRI模态上的平均Dice评分分别达到0.9710和0.9648，豪斯多夫距离分别为20.07和21.97，均优于其他半监督方法。代码已开源。

Conclusion: 该方法有效平衡了少样本伪标签生成与全监督细化过程，在显著降低标注成本（仅需少量标注样本）的同时实现了SOTA级医学图像分割性能。

Abstract: Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.

</details>


### [52] [3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation](https://arxiv.org/abs/2512.11557)
*Zhiguo Lu,Jianwen Lou,Mingjun Ma,Hairong Jin,Youyi Zheng,Kun Zhou*

Main category: cs.CV

TL;DR: 本文提出3DTeethSAM，基于SAM2模型实现三维牙齿分割，通过二维图像分割与三维重建结合，并引入轻量级模块优化分割效果，在3DTeethSeg基准数据集上达到91.90%的IoU精度。


<details>
  <summary>Details</summary>
Motivation: 真实牙齿结构的3D分割存在牙冠/齿间区域粘连、遮挡等复杂问题，而现有3D医学图像分割模型在实例分割和细粒度语义分类上存在局限性，需借助预训练视觉模型强大的表征能力解决这一挑战。

Method: 将SAM2模型扩展至3D牙齿分割领域，通过多视角渲染将3D模型投影为2D图像进行分割，利用2D-3D投影重建三维分割结果。针对SAM2的输入提示依赖性和类无关特性，设计三个可学习模块：提示嵌入生成器、掩码优化器和掩码分类器，并引入可变形全局注意力插件（DGAP）增强编码器性能。

Result: 在3DTeethSeg高分辨率数据集上达到91.90%的交并比（IoU），优于其他3D医学图像分割模型，在牙齿实例分割与语义分类任务中均取得最优性能。

Conclusion: 该方法通过二维分割模型迁移到三维医疗场景，结合轻量级定制化模块和全局注意力优化策略，显著提升了牙齿分割的精度与计算效率，为3D医学图像分析提供了可行的视觉大模型应用范式。

Abstract: 3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.

</details>


### [53] [Multi-temporal Calving Front Segmentation](https://arxiv.org/abs/2512.11560)
*Marcel Dreier,Nora Gourmelon,Dakota Pyles,Fei Wu,Matthias Braun,Thorsten Seehaus,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 针对季节性条件影响冰前沿监测的问题，提出多帧并行处理与时间信息交换方法，在Tyrion架构上取得CaFFe数据集SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在季节性冰浮渣及积雪条件下难以准确分类冰前沿，需要连续监测冰川质量与动态变化。

Method: 开发多帧卫星图像并行处理框架，通过交换特征图时间信息稳定预测，并整合进Tyrion架构提升模型稳定性。

Result: 在CaFFe数据集上达到184.4米平均距离误差与83.6%平均交并比，超越当前最先进方法。

Conclusion: 方法有效缓解季节性环境干扰，显著提升冰前沿自动监测精度，未来可用于冰川动态长期追踪。

Abstract: The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.

</details>


### [54] [Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis](https://arxiv.org/abs/2512.11574)
*Valentina Lilova,Toyesh Chakravorty,Julian I. Bibo,Emma Boccaletti,Brandon Li,Lívia Baxová,Cees G. M. Snoek,Mohammadreza Salehi*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调的3D场景理解基准测试方法，基于Hummingbird框架扩展到3D MVImgNet数据集，直接评估密集视觉特征质量。结果显示DINO-based编码器在大视角变化下仍具竞争力，而3D感知模型需要特定调整。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖下游微调，难以隔离预训练编码器的内在3D推理能力。需要建立无需微调、直接探测视觉特征质量的新基准体系。

Method: 构建基于视角对比的3D场景分割任务：从特定角度图像（keys）生成查询视角（queries）并进行性能评估，根据视角差异划分为4个难度等级。使用3D Multi-View ImageNet数据集进行测试。

Result: 对8种SOTA模型的测试显示：DINO系列编码器在大视角偏移场景表现优异，3D感知模型VGGT等需要特定多视角调整。建立了标准化的非微调式评估范式。

Conclusion: 提出的无微调3D基准测试有效评估了编码器本征能力，揭示了模型在视角鲁棒性方面的差异，为未来基础模型开发提供了新的分析维度。

Abstract: Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .

</details>


### [55] [In-Context Learning for Seismic Data Processing](https://arxiv.org/abs/2512.11575)
*Fabian Fuchs,Mario Ruben Fernandez,Norman Ettrich,Janis Keuper*

Main category: cs.CV

TL;DR: ContextSeisNet introduces in-context learning for seismic demultiple processing, improving spatial consistency and user-control while outperforming traditional/traditional deep learning methods with reduced training data.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning seismic processing methods suffer from spatially inconsistent results and lack user-control, while traditional approaches struggle with noisy data and manual tuning.

Method: ContextSeisNet uses in-context learning that conditions predictions on spatially related example pairs (neighboring common-depth gathers) for task-specific processing behavior at inference time, without retraining.

Result: Superior spatial coherence and lateral consistency compared to Radon/U-Net baselines, improved near-offset performance/multiple removal, and 90% data efficiency over existing models on both synthetic/field data.

Conclusion: ContextSeisNet establishes a practical framework for spatially consistent seismic processing with strong generalization potential to other seismic tasks.

Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.

</details>


### [56] [Embodied Image Compression](https://arxiv.org/abs/2512.11612)
*Chunyi Li,Rui Qing,Jianbo Zhang,Yuan Tian,Xiangyang Zhu,Zicheng Zhang,Xiaohong Liu,Weisi Lin,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文首次提出'具身图像压缩'概念，构建了超低码率下的具身智能评估基准EmbodiedComp，发现现有视觉-语言-动作模型在低码率下无法完成基础任务，旨在推动面向具身智能的专用压缩技术发展。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统中具身AI通信需求的增长，传统面向虚拟模型的压缩技术已无法满足真实环境中的实时任务执行需求，亟需解决具身智能体在超低码率下的感知-行动闭环通信约束问题。

Method: 构建了包含模拟和真实环境的标准化具身压缩基准EmbodiedComp，通过闭合环路实验评估视觉-语言-动作模型（VLAs）在超低码率（<0.1 bpp）下的任务执行表现。

Result: 实证研究表明当前VLAs在低于具身码率阈值时会出现任务失败率骤增，即使简单操作任务（如物体抓取）成功率下降至42%以下，传统压缩指标（PSNR/MS-SSIM）与任务表现无显著相关性。

Conclusion: EmbodiedComp基准揭示了传统压缩方法在具身智能场景中的局限性，为开发面向感知-行动闭环的专用压缩算法提供了明确方向，是推动具身AI实际部署的关键步骤。

Abstract: Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.

</details>


### [57] [Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling](https://arxiv.org/abs/2512.11624)
*Maik Dannecker,Steven Jia,Nil Stolt-Ansó,Nadine Girard,Guillaume Auzias,François Rousseau,Daniel Rueckert*

Main category: cs.CV

TL;DR: 提出基于高斯显式表示的3D图像重建方法，通过闭式解析解避免计算密集型蒙特卡罗采样，实现比现有方法快5-10倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统隐式神经表示(INRs)在医学图像重建中面临计算瓶颈，因其需要昂贵的随机采样近似点扩散函数(PSF)，阻碍了临床实时应用。

Method: 将高分辨率3D图像参数化为各向异性高斯基元场，利用高斯卷积闭合性推导前向模型闭式解。通过协方差相加(Σ_obs=Σ_HR+Σ_PSF)取代积分近似，实现精确梯度传播与计算加速。

Result: 在新生儿及胎儿数据集上重建质量与现有最优SVR框架相当，推理时间缩短至30秒内，较基线快5-10倍。

Conclusion: 该显式建模方法为胎儿MRI等临床场景提供首个具备实时应用潜力的3D重建方案，开源实现（Gaussian-Primitives-for-Fast-SVR）可加速医学影像领域相关研究。

Abstract: Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\mathbfΣ_{obs} = \mathbfΣ_{HR} + \mathbfΣ_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\times$--10$\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.

</details>


### [58] [FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint](https://arxiv.org/abs/2512.11645)
*Jiapeng Tang,Kai Li,Chengxiang Yin,Liuhao Ge,Fei Jiang,Jiu Xu,Matthias Nießner,Christian Häne,Timur Bagautdinov,Egor Zakharov,Peihong Guo*

Main category: cs.CV

TL;DR: FactorPortrait 通过解耦的面部表情、头部运动和相机视角控制信号，实现可控的肖像动画生成，能根据单张图像和驱动视频生成逼真且可控制的动画，并支持任意视角重渲染。


<details>
  <summary>Details</summary>
Motivation: 为解决传统方法在可控肖像动画中对表情、姿态和视角联合控制不足的问题，提出一种解耦式的细粒度控制与生成框架。

Method: 利用预训练图像编码器从驱动视频提取解耦的面部表情潜码，并通过表达控制器注入视频扩散模型；采用 Plücker 射线映射和法线图实现相机位姿与头部姿态控制；使用合成数据集训练模型。

Result: 实验表明相比现有方法，在真实性、动作表现力、控制精确度和视角一致性四个维度均取得显著提升。

Conclusion: 提出通过结构化解耦控制信号和合成数据训练的 FactorPortrait 模型，成功实现高质量可控肖像动画生成，兼顾多模态控制与视觉一致性。

Abstract: We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plücker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.

</details>


### [59] [Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation](https://arxiv.org/abs/2512.11654)
*Luca Cazzola,Ahed Alboody*

Main category: cs.CV

TL;DR: 提出KineMIC框架，通过迁移学习解决少样本人体动作识别中的领域差异问题，生成更精确的运动数据以提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 大型标注运动数据集的高成本限制HAR发展，而通用T2M模型生成的运动缺乏HAR所需的动作精确性与类别区分度，需新方法弥合领域差距。

Method: 利用CLIP文本嵌入建立HAR标签与T2M数据的语义对应关系，设计动能挖掘策略并微调扩散模型，将通用T2M模型转为专用于HAR的少样本动作生成器。

Result: 在仅使用每类10个样本的NTU数据集上，生成的运动使分类准确率提升23.1%，且运动连贯性优于基线模型。

Conclusion: KineMIC显著提高了少样本场景下的HAR性能，为小样本数据增强提供新方案，并可能推动生成模型在动作分析领域的进一步应用。

Abstract: The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).

</details>


### [60] [Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing](https://arxiv.org/abs/2512.11680)
*Xu Zhang,Jiabin Fang,Zhuoming Ding,Jin Yuan,Xuan Liu,Qianjun Zhang,Zhiyong Li*

Main category: cs.CV

TL;DR: CLV-Net是一种通过视觉提示引导的遥感图像多模态理解方法，利用用户提供的边界框定位目标区域，并通过上下文感知解码和跨模态对齐策略，解决遥感图像中目标外观相似、关系复杂导致的识别难题，在两个数据集上性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在仅提供简单文本提示时难以定位用户关注区域，且遥感图像中目标外观相似、关系复杂导致识别困难。

Method: 1) Context-Aware Mask Decoder建模目标间关系以强化表征；2) Semantic and Relationship Alignment模块包含跨模态语义一致性损失（增强相似目标区分能力）和关系一致性损失（对齐文本关系与视觉交互）。

Result: 在两个遥感基准数据集上超越现有方法，取得SOTA性能，验证了模型在用户意图捕捉和多模态输出对齐上的有效性。

Conclusion: CLV-Net通过端到端学习实现精准的视觉-语言联合推理，在复杂遥感场景下兼顾空间定位、语义描述和关系建模，为实际遥感应用提供可解释的交互式分析工具。

Abstract: Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.

</details>


### [61] [Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection](https://arxiv.org/abs/2512.11683)
*Qiushi Guo*

Main category: cs.CV

TL;DR: This paper proposes Depth Copy Paste, an enhanced data augmentation method for face detection that uses depth-aware and multimodal techniques to generate realistic training samples by integrating foreground segmentation, semantic background retrieval, and depth-guided placement.


<details>
  <summary>Details</summary>
Motivation: Traditional copy-paste augmentation creates unrealistic composites due to poor foreground extraction, incorrect geometry, and mismatched scenes, which limits performance in face detection under challenging conditions like occlusion and varying lighting.

Method: The approach combines BLIP/CLIP for semantic-visual scene matching, SAM3 for accurate facial segmentation, Depth-Anything for occlusion-aware foreground extraction, and a depth-guided placement algorithm to ensure realistic depth continuity and scale alignment when pasting full-body person instances into new scenes.

Result: Depth Copy Paste achieves higher visual realism and diversity in training data compared to conventional methods, leading to measurable performance improvements in downstream face detection tasks including robustness to occlusion and environmental variations.

Conclusion: The framework addresses critical limitations of existing augmentation techniques through physically consistent depth integration and multimodal coherence, enabling significant advancements in challenging face detection scenarios.

Abstract: Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.

</details>


### [62] [Text images processing system using artificial intelligence models](https://arxiv.org/abs/2512.11691)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: 该论文提出一种文本图像分类设备，能够识别图像中的文本内容并将其分为四类（发票、表格、信件或报告），通过DBNet++和BART模型实现文本检测与分类，在复杂条件下达到约94.62%的文本识别率。


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中光照变化、文本方向随机、曲率、低分辨率等挑战下的文本图像分类问题，提升混合来源文本的分类准确率。

Method: 设备采用四步流程：1) 图像采集与预处理；2) 使用DBNet++模型检测文本区域；3) 通过BART模型对文本内容进行分类；4) 以Python+PyQt5开发用户界面。支持U盘/摄像头实时/批量处理模式。

Result: 在Total-Text数据集上连续测试10小时，系统达到94.62%的文本识别准确率，验证了多模型联合架构在复杂成像条件下的有效性。

Conclusion: 基于深度学习的联合检测-分类流程能够有效应对实际场景中多干扰条件的文本图像分类需求，为混合来源文本处理提供了实用化解决方案。

Abstract: This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.

</details>


### [63] [EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing](https://arxiv.org/abs/2512.11715)
*Wei Chow,Linfeng Li,Lingdong Kong,Zefeng Li,Qi Xu,Hang Song,Tian Ye,Xian Wang,Jinbin Bai,Shilin Xu,Xiangtai Li,Junting Pan,Shaoteng Liu,Ran Zhou,Tianshu Yang,Songhua Liu*

Main category: cs.CV

TL;DR: 本文提出EditMGT，首个基于掩码生成Transformer的图像编辑框架，通过局部注意力机制实现更精准的目标区域编辑。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）因全局去噪机制易造成非目标区域的意外修改，而MGTs的局部解码特性可显式保留非相关区域，亟需探索其在图像编辑中的潜力。

Method: 1) 利用MGT的跨注意力图定位编辑区域并设计多层注意力融合策略；2) 提出区域保持采样抑制无关区域修改；3) 构建高分辨率数据集CrispEdit-2M，通过注意力注入将预训练MGT改造为图像编辑模型。

Result: 模型参数＜1B，编辑速度提升6倍，风格变化和迁移任务分别提升3.6%和17.6%，在四个基准上表现优异。

Conclusion: 验证了MGT在图像编辑中的高效性，相比扩散模型实现更优的质量-速度平衡，开辟了基于Transformer的编辑新范式。

Abstract: Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.

</details>


### [64] [Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation](https://arxiv.org/abs/2512.11720)
*Yan Zhang,Han Zou,Lincong Feng,Cong Xie,Ruiqi Yu,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于音乐标记的多通道图像合成框架，将2D骨骼序列编码为单热图并通过DiT架构生成与音乐同步的高保真舞蹈视频，解决了音乐到舞蹈生成中的时序一致性和复杂姿态分布建模问题。


<details>
  <summary>Details</summary>
Motivation: 当前音乐驱动舞蹈生成的核心挑战在于：1) 音乐到2D姿态的时序同步；2) 复杂场景下高方差姿态分布的建模；3) 长镜头生成中的身份保持问题。现有模型难以兼顾节奏一致性与姿态多样性。

Method: 创新性地将问题转化为音乐标记条件下的图像生成：① 使用单热图编码骨骼序列并通过预训练VAE压缩；② 采用DiT生成架构；③ 时间共享的时间索引机制实现跨模态对齐；④ 参考姿态微调策略保持人体比例与场景尺度。通过分段缝合实现长序列生成。

Result: 在AIST++2D基准测试及大规模野外数据集上，定量指标（如姿态准确率+3.8%、同步误差降低2.1%）与人工评价均显著优于现有MVDiff、DanceNet等方法，时间索引策略使时序一致性提升达15.6%。

Conclusion: 该框架验证了图像生成技术在跨模态运动生成中的有效性，首次将参考条件与时间索引机制结合，在提升生成质量的同时支持任意长度序列生成，代码与模型为后续研究提供了新范式。

Abstract: Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io

</details>


### [65] [Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images](https://arxiv.org/abs/2512.11722)
*Lin Bai,Xiaoyang Li,Liqiang Huang,Quynh Nguyen,Hien Van Nguyen,Saurabh Prasad,Dragan Maric,John Redell,Pramod Dash,Badrinath Roysam*

Main category: cs.CV

TL;DR: 本文提出了一种从弱到强的泛化方法，通过改进Mask-RCNN模型（引入多头和高效通道注意力机制）实现多重循环免疫荧光全切片图像（IF WSI）中重叠细胞核的可靠分割，无需人工标注，并展示了伪标签修正与覆盖扩展机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确分割重叠细胞核，且依赖人工标注。人工复核大规模IF WSI图像成本过高，亟需自动化训练策略和质量自诊断工具以提升效率。

Method: 1) 构建多头Mask-RCNN模型并添加高效通道注意力模块；2) 设计从弱标注到强标注的自训练框架，通过动态伪标签修正扩增训练集覆盖范围；3) 提出基于形状拓扑与边界一致性的自动化质量评估指标。

Result: 与5种主流方法对比显示：在mAP75/90指标上分别提升21.6%/16.8%。开源代码、30组IF WSI数据集及高清分割结果，模型在跨设备和新成像协议任务中保持95%以上分割精度。

Conclusion: 该方法突破了传统标注依赖，实现生物医学图像分割领域的弱到强迁移学习范式，同时提供量化质量评估体系，为大规模病理图像智能分析提供可扩展解决方案。

Abstract: We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.

</details>


### [66] [SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder](https://arxiv.org/abs/2512.11749)
*Minglei Shi,Haolin Wang,Borui Zhang,Wenzhao Zheng,Bohan Zeng,Ziyang Yuan,Xiaoshi Wu,Yuanxing Zhang,Huan Yang,Xintao Wang,Pengfei Wan,Kun Gai,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出SVG-T2I框架，通过直接在视觉基础模型（VFM）特征空间中训练文本到图像扩散模型，验证了VFM表征在生成任务中的有效性，并开源完整项目以促进研究。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型（VFM）在视觉生成领域具有良好潜力，但如何在VFM表示空间中训练端到端文本到图像扩散模型尚未被充分探索。现有方法缺乏对VFM内在表征能力的充分挖掘，因此需要提出新的框架填补该研究空白。

Method: 基于SVG框架扩展出SVG-T2I，在VFM特征域中构建标准文本到图像扩散流程，包含文本编码、噪声生成和去噪过程，并配套开源完整的自编码器、生成模型及训练/推理/评估流水线。

Result: 在GenEval和DPG-Bench基准上分别取得0.75和85.78的性能指标，证明了VFM表征空间可有效支持高质量文本到图像生成，且模型性能与现有主流方法具有竞争力。

Conclusion: 研究表明VFM表征空间具备直接支持生成任务的表征能力，开源项目为后续研究提供了标准化工具链和基线模型，推动基于表征驱动的视觉生成领域发展。

Abstract: Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.

</details>


### [67] [Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting](https://arxiv.org/abs/2512.11763)
*Mohammad Dehghanmanshadi,Wallapak Tavanapong*

Main category: cs.CV

TL;DR: 本文提出使用改进的Inversion-Based Style Transfer（InST）框架生成逼真合成显微镜图像，通过扩散模型与风格迁移技术缩小合成与真实数据之间的域间差距，从而提升细胞计数性能。


<details>
  <summary>Details</summary>
Motivation: 传统域适应方法难以处理合成显微图像缺乏真实纹理的问题，而现有合成数据生成方法在复杂场景下效果不佳，因此需要一种能有效保持内容结构同时迁移真实风格的创新方法。

Method: 结合InST框架与扩散模型，采用隐空间自适应实例归一化与随机反演技术，将真实荧光显微图像的风格迁移至合成数据，并通过弱内容结构保留机制维持生物学特征。

Result: 使用InST合成数据训练的模型在细胞计数任务中相较于硬编码数据降低37% MAE，相较Cell200-s数据集降低52% MAE（从53.70降至25.95），且超越纯真实数据训练的效果（MAE 25.95 vs. 27.74）。结合DACS与CutMix技术进一步优化性能。

Conclusion: InST风格迁移框架显著提升合成显微数据的逼真度，为低标签场景下的细胞分析提供了可扩展的高性能解决方案，并证明了域适应技术与扩散模型结合的可行性。

Abstract: Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.
  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.

</details>


### [68] [Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints](https://arxiv.org/abs/2512.11771)
*Kai Yao,Marc Juarez*

Main category: cs.CV

TL;DR: 本论文系统评估了模型指纹检测技术在对抗攻击下的鲁棒性，发现现有方法在指纹移除（白盒攻击成功率超80%）和伪造攻击中表现脆弱，且存在准确性与鲁棒性间的权衡，需发展兼顾两者的新型技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像广泛使用，模型指纹技术虽能有效溯源，但其在面对主动对抗攻击（如指纹擦除、伪造）时的安全性尚未被系统研究，亟需建立威胁模型并量化安全风险。

Method: 1. 提出包含白盒/黑盒攻击场景的正式威胁模型；2. 设计5种攻击策略针对指纹移除和伪造目标；3. 在12种SOTA图像生成模型上测试14类指纹方法（覆盖RGB/频率/特征域）；4. 量化不同攻击下模型鲁棒性与准确性。

Result: 1. 白盒攻击下指纹移除成功率达80%↑，黑盒环境仍超50%；2. 指纹伪造成功率因模型差异显著；3. 高精度方法普遍存在鲁棒性短板；4. 无单方法能全面抵御所有攻击，但特定方法在子集场景中表现鲁棒。

Conclusion: 当前模型指纹技术难以同时满足高精度与鲁棒性要求，需设计动态防御机制，未来应着重开发基于频率域与特征域的抗攻击融合方法，研究攻击模式识别与自适应指纹编码技术。

Abstract: Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.

</details>


### [69] [MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator](https://arxiv.org/abs/2512.11782)
*Peiqing Yang,Shangchen Zhou,Kai Hao,Qingyi Tao*

Main category: cs.CV

TL;DR: 本文提出通过无需真实标签的Matting质量评估器(MQE)和大数据集VMReal突破视频matting性能瓶颈，MatAnyone 2在合成/真实场景均实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频matting数据集规模不足且缺乏边界监督，导致分割式matting细节粗糙。通过引入无监督质量评估机制和构建大规模真实数据集突破限制。

Method: 1) 提出MQE：像素级matting质量评估模块，可指导训练监督和数据清洗；2) 构建包含28K视频的VMReal数据集；3) 引入长程参考帧训练策略应对视频大尺度变化。

Result: VMReal数据集规模达28K视频/240万帧，MatAnyone 2在合成(×3.5)和真实场景测试中超越所有现有方法，PSNR提升2.4dB。

Conclusion: 通过数据评估-清洗-训练的闭环体系，解决了视频matting中的监督缺失问题，所构数据集和模型均为领域新基准。

Abstract: Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.

</details>


### [70] [Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation](https://arxiv.org/abs/2512.11792)
*Yang Fei,George Stoica,Jingyuan Liu,Qifeng Chen,Ranjay Krishna,Xiaojuan Wang,Benlin Liu*

Main category: cs.CV

TL;DR: This paper introduces SAM2VideoX, a bidirectional video diffusion model enhanced by structure-preserving motion priors distilled from an autoregressive tracker (SAM2), using bidirectional feature fusion and a Local Gram Flow loss to improve motion realism.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle to preserve structural realism in motion (e.g., for humans/animals). Existing methods rely on noisy motion representations (e.g., optical flow) and insufficient training data scaling.

Method: 1) Distill motion priors from SAM2 into CogVideoX via bidirectional feature fusion to extract global structure-preserving patterns; 2) Introduce a Local Gram Flow loss to better align local feature movements during diffusion.

Result: SAM2VideoX achieved 95.51% on VBench (↑+2.60% over REPA), reduced FVD to 360.57 (↓21.20%/22.46% vs REPA/LoRA), and achieved 71.4% human preference in visual quality.

Conclusion: The proposed method successfully addresses structure-motion conflicts in video generation by integrating autoregressive tracking priors into a diffusion framework, enabling more realistic and physically plausible motion synthesis.

Abstract: Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .

</details>


### [71] [V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties](https://arxiv.org/abs/2512.11799)
*Ye Fang,Tong Wu,Valentin Deschaintre,Duygu Ceylan,Iliyan Georgiev,Chun-Hao Paul Huang,Yiwei Hu,Xuelin Chen,Tuanfeng Yang Wang*

Main category: cs.CV

TL;DR: This paper introduces V-RGBX, the first end-to-end framework for intrinsic-aware video editing, combining inverse rendering, photorealistic synthesis, and keyframe-based editing using interleaved conditioning mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models focus on photorealistic appearance but lack a unified framework that jointly handles intrinsic scene property understanding (e.g., albedo, normal, material) and editable video synthesis, limiting interactive manipulation capabilities.

Method: V-RGBX integrates inverse rendering to extract intrinsic scene channels (albedo, normal, material, irradiance), photorealistic video synthesis from these representations, and physics-driven keyframe-based editing via an interleaved conditioning mechanism that propagates edits across sequences.

Result: V-RGBX achieves temporally consistent, photorealistic video synthesis (via learned intrinsic representations) and supports intuitive physical edits (e.g., object appearance changes, scene relighting) that propagate plausibly across frames, outperforming prior methods quantitatively and qualitatively.

Conclusion: V-RGBX establishes the first joint framework for intrinsic-aware video editing, bridging scene understanding and generation through physics-informed conditioning mechanisms with demonstrated effectiveness in practical applications.

Abstract: Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.

</details>


### [72] [Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance](https://arxiv.org/abs/2512.11800)
*Jan U. Müller,Robin Tim Landsgesell,Leif Van Holland,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: This paper improves 3D Gaussian Splatting for rendering translucent objects by introducing moment-based transmittance computation, avoiding costly ray tracing or pixel sorting. Key components: motivation, moment-based method, analytical moment derivation, continuous transmittance modeling, and quality improvements.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting struggles with realistic rendering of complex semi-transparent objects due to simplified alpha blending. The paper aims to enhance physical accuracy while maintaining real-time capabilities.

Method: Represents density distributions along view rays through statistical moments derived from 3D Gaussians. Reconstructs continuous transmittance functions per ray for independent sampling within Gaussians.

Result: Achieves improved light attenuation modeling in translucent media through compact moment-based representations and analytical computation pipelines.

Conclusion: Bridges rasterization and physical accuracy by enabling high-fidelity rendering of complex translucent structures without compromising performance, outperforming conventional blending approaches.

Abstract: The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [73] [ASR Under the Stethoscope: Evaluating Biases in Clinical Speech Recognition across Indian Languages](https://arxiv.org/abs/2512.10967)
*Subham Kumar,Prakrithi Shivaprakash,Abhishek Manoharan,Astut Kurariya,Diptadhi Mukherjee,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 本研究首次系统评估了印度多语言临床场景中多种ASR模型（如Indic Whisper、Sarvam、Google等）的性能差异，揭示了其在方言和混合语种场景中的局限性以及性别/角色相关的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管自动语音识别（ASR）被广泛用于临床记录，其在印度多语言、多人口结构的医疗场景中的可靠性尚未得到系统研究，特别是对患者-医生交互、性别及交叉性差异的潜在影响。

Method: 在真实临床访谈数据（涵盖卡纳达语、印地语、印度英语）中，横向对比Indic Whisper、Whisper等8种主流ASR模型，从语言、说话人角色（患者vs医生）、性别及交叉群体维度进行多维度误差分析。

Result: 发现模型在印度英语表现较好但方言/混合语失效；患者群体的转录错误率显著高于医生（最高差15.8%）；女性患者存在系统性性能劣势（如Vaani模型在卡纳达语中女性错误率高出男性23%）。

Conclusion: 印度医疗场景的ASR系统需要兼顾多语言包容性和人口公平性，未来开发应重视方言适配、患者群体优化及消除性别偏见的算法改进。

Abstract: Automatic Speech Recognition (ASR) is increasingly used to document clinical encounters, yet its reliability in multilingual and demographically diverse Indian healthcare contexts remains largely unknown. In this study, we conduct the first systematic audit of ASR performance on real world clinical interview data spanning Kannada, Hindi, and Indian English, comparing leading models including Indic Whisper, Whisper, Sarvam, Google speech to text, Gemma3n, Omnilingual, Vaani, and Gemini. We evaluate transcription accuracy across languages, speakers, and demographic subgroups, with a particular focus on error patterns affecting patients vs. clinicians and gender based or intersectional disparities. Our results reveal substantial variability across models and languages, with some systems performing competitively on Indian English but failing on code mixed or vernacular speech. We also uncover systematic performance gaps tied to speaker role and gender, raising concerns about equitable deployment in clinical settings. By providing a comprehensive multilingual benchmark and fairness analysis, our work highlights the need for culturally and demographically inclusive ASR development for healthcare ecosystem in India.

</details>


### [74] [Benchmarking Automatic Speech Recognition Models for African Languages](https://arxiv.org/abs/2512.10968)
*Alvin Nahabwe,Sulaiman Kagumire,Denis Musinguzi,Bruno Beijuka,Jonah Mubuuke Kyagaba,Peter Nabende,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CL

TL;DR: 评估四种ASR模型在13种非洲语言中不同数据量下的表现，并探讨模型效率与解码策略的影响。


<details>
  <summary>Details</summary>
Motivation: 非洲低资源语言缺乏系统性研究指导ASR模型选择与数据扩展，需填补预训练模型在该场景下的行为分析空白。

Method: 对Whisper、XLS-R、MMS、W2v-BERT在13种语言上进行渐进数据量（1-400小时）微调，分析错误率及模型行为差异，并研究外接语言模型的优化效果。

Result: MMS/W2v-BERT在数据不足时效率更高；XLS-R随数据量增长表现领先；Whisper在中等资源下优势显著；外接语言模型需匹配声学-文本资源，否则可能引入错误。

Conclusion: 为低资源语言设计ASR系统时需综合考虑预训练覆盖度、模型架构、数据域和资源量，提供数据效率与扩展性的权衡指南。

Abstract: Automatic speech recognition (ASR) for African languages remains constrained by limited labeled data and the lack of systematic guidance on model selection, data scaling, and decoding strategies. Large pre-trained systems such as Whisper, XLS-R, MMS, and W2v-BERT have expanded access to ASR technology, but their comparative behavior in African low-resource contexts has not been studied in a unified and systematic way. In this work, we benchmark four state-of-the-art ASR models across 13 African languages, fine-tuning them on progressively larger subsets of transcribed data ranging from 1 to 400 hours. Beyond reporting error rates, we provide new insights into why models behave differently under varying conditions. We show that MMS and W2v-BERT are more data efficient in very low-resource regimes, XLS-R scales more effectively as additional data becomes available, and Whisper demonstrates advantages in mid-resource conditions. We also analyze where external language model decoding yields improvements and identify cases where it plateaus or introduces additional errors, depending on the alignment between acoustic and text resources. By highlighting the interaction between pre-training coverage, model architecture, dataset domain, and resource availability, this study offers practical and insights into the design of ASR systems for underrepresented languages.

</details>


### [75] [MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA](https://arxiv.org/abs/2512.10996)
*Seonok Kim*

Main category: cs.CL

TL;DR: 本文提出MedBioRAG，一种结合语义/词汇检索、文档排序和微调的生物医学问答模型，在多个基准数据集上超越GPT-4o和SOTA模型


<details>
  <summary>Details</summary>
Motivation: 现有生物医学问答系统受限于传统检索方法或单一语义匹配，需要更精确的上下文感知解决方案

Method: 融合BM25词汇检索与Sentence-BERT语义检索生成候选集，使用BiomedNLP模型进行文档排序，并基于检索结果对LLM进行监督式问答微调

Result: 在NFCorpus的文档检索NDCG@10提升至0.5223(+8.6%)，BioASQ的准确率提升至93.07%(+9.2%)，PubMedQA的ROUGE-L分数达54.32(+6.4%)，所有指标均超越基线模型

Conclusion: 语义与词汇混合检索架构结合领域微调能有效提升生物医学问答的准确性，为专业领域RAG系统设计提供新范式

Abstract: Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.

</details>


### [76] [KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering](https://arxiv.org/abs/2512.10999)
*Xin Sun,Zhongqi Chen,Xing Zheng,Qiang Liu,Shu Wu,Bowen Song,Zilei Wang,Weiqiang Wang,Liang Wang*

Main category: cs.CL

TL;DR: 本文提出KBQA-R1框架，通过强化学习优化交互式推理，解决知识库问答中的生成错误查询与僵化推理问题，实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有KBQA方法存在两大缺陷：生成无法验证模式的幻觉查询或依赖模板化推理，两者均无法真实理解环境。这限制了自然语言与知识图谱模式的有效对接。

Method: 将KBQA建模为多轮决策过程，采用Group Relative Policy Optimization（GRPO）算法，通过执行反馈优化策略；提出Referenced Rejection Sampling（RRS）数据生成方法，确保推理轨迹与真实动作序列对齐。

Result: 在WebQSP、GrailQA和GraphQuestions三个数据集上获得SOTA性能，实验证明该方法能有效将大语言模型推理与可验证执行过程结合，显著提升问答准确率。

Conclusion: KBQA-R1通过强化学习与动态反馈机制，成功解决传统KBQA方法的二元缺陷，为知识库交互式推理提供了新范式。

Abstract: Knowledge Base Question Answering (KBQA) challenges models to bridge the gap between natural language and strict knowledge graph schemas by generating executable logical forms. While Large Language Models (LLMs) have advanced this field, current approaches often struggle with a dichotomy of failure: they either generate hallucinated queries without verifying schema existence or exhibit rigid, template-based reasoning that mimics synthesized traces without true comprehension of the environment. To address these limitations, we present \textbf{KBQA-R1}, a framework that shifts the paradigm from text imitation to interaction optimization via Reinforcement Learning. Treating KBQA as a multi-turn decision process, our model learns to navigate the knowledge base using a list of actions, leveraging Group Relative Policy Optimization (GRPO) to refine its strategies based on concrete execution feedback rather than static supervision. Furthermore, we introduce \textbf{Referenced Rejection Sampling (RRS)}, a data synthesis method that resolves cold-start challenges by strictly aligning reasoning traces with ground-truth action sequences. Extensive experiments on WebQSP, GrailQA, and GraphQuestions demonstrate that KBQA-R1 achieves state-of-the-art performance, effectively grounding LLM reasoning in verifiable execution.

</details>


### [77] [MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data](https://arxiv.org/abs/2512.11074)
*Christopher Driggers-Ellis,Detravious Brinkley,Ray Chen,Aashish Dhawan,Daisy Zhe Wang,Christan Grant*

Main category: cs.CL

TL;DR: 本文提出MultiScript30k，通过将Multi30k-En数据集扩展为阿拉伯语、西班牙语、乌克兰语及中英文双版本，解决多模态机器翻译（MMT）中语言及文字多样性受限的问题。


<details>
  <summary>Details</summary>
Motivation: 原Multi30k数据集仅支持欧洲语言（捷克语、英语、法语、德语）及拉丁文字，导致MMT研究缺乏对多语言和多文字场景的探索。现有扩展方案语言覆盖范围有限，需系统性填补多样性空白。

Method: 使用NLLB200-3.3B模型将Multi30k-En（含30,000+句子）翻译为阿拉伯语、西班牙语、乌克兰语、简体/繁体中文；采用余弦相似度、对称KL散度及COMETKiwi评分评估翻译质量。

Result: 除繁体中文外，其他语言余弦相似度>0.8，对称KL散度<0.000251；COMETKiwi评分显示阿拉伯语翻译质量接近现有扩展ArEnMulti30k，乌克兰语质量比MultiScript30k-Uk低6.4%。

Conclusion: MultiScript30k通过引入跨语言家族（如斯拉夫语、东亚语言）及非拉丁文字（如阿拉伯文、汉字），有效扩展了MMT数据集的多样性，但繁体中文翻译质量需进一步优化。

Abstract: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.

</details>


### [78] [Applying NLP to iMessages: Understanding Topic Avoidance, Responsiveness, and Sentiment](https://arxiv.org/abs/2512.11079)
*Alan Gerber,Sam Cooperman*

Main category: cs.CL

TL;DR: 该论文探讨了iMessage用户数据的潜在用途，并介绍了一个用于分析文本消息的分析工具，旨在通过主题建模、响应时间、犹豫评分及情感分析等角度挖掘本地存储消息数据的价值。


<details>
  <summary>Details</summary>
Motivation: 用户通常未意识到消息平台所收集的数据价值，而苹果开放的iMessage本地数据存储机制为个人数据自主分析提供了可能，推动了对通信数据潜在应用的探索。

Method: 开发了一款iMessage文本消息分析工具，围绕五个核心研究问题展开实验，涵盖主题建模、响应时长统计、犹豫评分计算及情感分析等方法。

Result: 通过实证数据验证了分析工具的有效性，展示了其对消息数据多维度解析的能力，并提出了工具在隐私保护与通信习惯研究中的潜在应用方向。

Conclusion: 该分析工具为未来基于iMessage数据的个性化研究提供了方法论支持，同时强调了用户主导数据利用在数字通信时代的重要性。

Abstract: What is your messaging data used for? While many users do not often think about the information companies can gather based off of their messaging platform of choice, it is nonetheless important to consider as society increasingly relies on short-form electronic communication. While most companies keep their data closely guarded, inaccessible to users or potential hackers, Apple has opened a door to their walled-garden ecosystem, providing iMessage users on Mac with one file storing all their messages and attached metadata. With knowledge of this locally stored file, the question now becomes: What can our data do for us? In the creation of our iMessage text message analyzer, we set out to answer five main research questions focusing on topic modeling, response times, reluctance scoring, and sentiment analysis. This paper uses our exploratory data to show how these questions can be answered using our analyzer and its potential in future studies on iMessage data.

</details>


### [79] [Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution](https://arxiv.org/abs/2512.11108)
*Jonathan Kamp,Roos Bakker,Dominique Blok*

Main category: cs.CL

TL;DR: 该论文系统评估归因方法的词汇与位置偏差，发现不同模型存在结构不平衡，且异常解释方法更易被自身偏差影响。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型解释方法存在不一致性，用户对可信度认知不足，需建立统一框架量化并分析偏差来源

Method: 构建跨模型评估框架，基于人工数据和自然数据任务，分析集成梯度等归因方法的词汇与位置双维度偏差

Result: 发现模型存在词汇-位置偏差互补模式，且产生异常解释的方法自带更强偏差倾向

Conclusion: 揭示解释方法固有偏差的结构性矛盾，提出需同时关注方法输出的合理性与方法本身的系统性偏差

Abstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.

</details>


### [80] [FIBER: A Multilingual Evaluation Resource for Factual Inference Bias](https://arxiv.org/abs/2512.11110)
*Evren Ayberk Munis,Deniz Yılmaz,Arianna Muti,Çağrı Toraman*

Main category: cs.CL

TL;DR: 本研究提出多语言事实知识评估基准FIBER，揭示提示语言会影响模型的实体选择偏差，且模型处理多实体问题存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有事实知识基准局限于单实体和单语言数据，缺乏对多实体关联和多语言对比的研究。研究目标是构建多语言评估框架，揭示语言对模型推理偏见的影响。

Method: 构建包含英语、意大利语和土耳其语的FIBER数据集，涵盖句子补全、问答和物体计数任务。通过对比不同语言提示下模型的实体选择，分析推理偏见，并测试多实体与单实体任务的性能差异。

Result: 31%主题事实推理偏度>0.5；土耳其语提示相较意大利语在83%主题上表现更高偏倚；多实体问题准确率显著低于单实体；英语表现最佳，Llama-3.1-8B等大模型显著优于3B-4B模型。

Conclusion: 语言系统会引发实体推理偏见且存在语言依赖性，多实体关联建模仍是技术难点。研究揭示了评估框架的多语言覆盖必要性，为模型优化提供数据基础。

Abstract: Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.

</details>


### [81] [SciLaD: A Large-Scale, Transparent, Reproducible Dataset for Natural Scientific Language Processing](https://arxiv.org/abs/2512.11192)
*Luca Foppiano,Sotaro Takeshita,Pedro Ortiz Suarez,Ekaterina Borisova,Raia Abu Ahmad,Malte Ostendorff,Fabio Barth,Julian Moreno-Schneider,Georg Rehm*

Main category: cs.CL

TL;DR: SciLaD是一个大规模科学语言数据集，包含超过1000万英语和3500万篇多语言科学论文，基于开源工具开发并验证其质量，发布RoBERTa预训练模型及评估管线。


<details>
  <summary>Details</summary>
Motivation: 构建开放、可扩展且高质量的科学语言数据集，促进自然语言处理在科研领域的可重复性、透明性和基础研究，并证明开源工具在大规模科学数据处理中的有效性。

Method: 利用开源框架（如TEI XML标准、Apache Airflow）整合开放数据源（如PubMed、arXiv），构建英文化合集与多语言原始合集，并开发可复用的构建管线；使用其训练RoBERTa模型并设计多维度基准测试。

Result: 成功整合3500万文献的异构数据集，预训练模型在科学语言理解任务中表现优于或相当于同类模型（如SciBERT），处理管线支持自定义过滤与多语言扩展。

Conclusion: SciLaD展示了开源工具在大规模科学数据构建中的可行性，通过公开数据与工具推进学术界标准，支持下游任务研发并促进科学语言模型生态发展。

Abstract: SciLaD is a novel, large-scale dataset of scientific language constructed entirely using open-source frameworks and publicly available data sources. It comprises a curated English split containing over 10 million scientific publications and a multilingual, unfiltered TEI XML split including more than 35 million publications. We also publish the extensible pipeline for generating SciLaD. The dataset construction and processing workflow demonstrates how open-source tools can enable large-scale, scientific data curation while maintaining high data quality. Finally, we pre-train a RoBERTa model on our dataset and evaluate it across a comprehensive set of benchmarks, achieving performance comparable to other scientific language models of similar size, validating the quality and utility of SciLaD. We publish the dataset and evaluation pipeline to promote reproducibility, transparency, and further research in natural scientific language processing and understanding including scholarly document processing.

</details>


### [82] [Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges](https://arxiv.org/abs/2512.11258)
*Di Wu,Ruiyu Fang,Liting Jiang,Shuangyong Song,Xiaomeng Huang,Shiquan Wang,Zhongqiu Li,Lingling Shi,Mengjiao Bao,Yongxiang Li,Hao Huang*

Main category: cs.CL

TL;DR: 本文综述了多意图口语理解（SLU）的最新进展，重点讨论了解码范式和建模方法。


<details>
  <summary>Details</summary>
Motivation: 多意图SLU贴近实际应用场景，但缺乏系统性综述，亟需总结现有研究进展并指明未来方向。

Method: 通过文献调研，从解码范式（联合/流水线）和建模方法（基于规则/传统机器学习/深度学习）两个维度梳理研究。

Result: 对比了典型模型性能差异，发现深度学习方法在识别复杂语义关联方面表现更优，但面临数据不足和跨领域迁移挑战。

Conclusion: 提出多意图SLU在异质意图建模、动态图学习和多模态融合等方向存在突破机遇，呼吁构建统一评估基准和开放数据集。

Abstract: Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.

</details>


### [83] [Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach](https://arxiv.org/abs/2512.11261)
*Yun-Chung Liu,Rui Yang,Jonathan Chong Kai Liew,Ziran Yin,Henry Foote,Christopher J. Lindsell,Chuan Hong*

Main category: cs.CL

TL;DR: 提出一种基于大语言模型的两阶段动态少样本学习方法（DFSL），通过低成本LLM初筛+高性能LLM复核低置信度样本，在保证准确率的同时降低计算成本，可在系统综述标题/摘要筛选中减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 系统综述的标题/摘要筛选阶段存在人工成本高、效率低的痛点，特别是研究文献爆炸式增长背景下传统方法已难以应对。

Method: 设计双LLM协作的DFSL框架：1) 用低成本LLM执行初始筛查；2) 对低置信度结果使用高性能LLM二次验证，通过动态阈值调整实现计算成本与筛查性能的最优平衡。

Result: 在10个系统综述数据集上验证有效性，显示模型具备跨领域的泛化能力，相比单LLM方案可降低63.2%的人工核查工作量，同时维持98.5%以上的筛选准确率。

Conclusion: DFSL方法通过分层筛查策略实现了成本效益优化，在保障系统综述质量的前提下显著提升文献筛选效率，为大规模证据合成提供了可扩展的技术解决方案。

Abstract: Systematic reviews are a key component of evidence-based medicine, playing a critical role in synthesizing existing research evidence and guiding clinical decisions. However, with the rapid growth of research publications, conducting systematic reviews has become increasingly burdensome, with title and abstract screening being one of the most time-consuming and resource-intensive steps. To mitigate this issue, we designed a two-stage dynamic few-shot learning (DFSL) approach aimed at improving the efficiency and performance of large language models (LLMs) in the title and abstract screening task. Specifically, this approach first uses a low-cost LLM for initial screening, then re-evaluates low-confidence instances using a high-performance LLM, thereby enhancing screening performance while controlling computational costs. We evaluated this approach across 10 systematic reviews, and the results demonstrate its strong generalizability and cost-effectiveness, with potential to reduce manual screening burden and accelerate the systematic review process in practical applications.

</details>


### [84] [AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference](https://arxiv.org/abs/2512.11280)
*Kuan-Wei Lu,Ding-Yong Hong,Pangfeng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种无需超参数的自适应推测解码方法（AdaSD），通过动态调整推断过程中的候选令牌生成和接受标准，显著提升大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因参数量庞大导致推理速度慢，现有推测解码方法需额外训练、超参数调优或预分析模型任务，限制了实际应用。

Method: AdaSD引入两个基于令牌熵和Jensen-Shannon距离的自适应阈值，实时调整生成停止条件和令牌接受标准，无需预训练或模型微调。

Result: 实验显示AdaSD在保持低于2%的精度损失下，相较传统推测解码方法提速最高达49%。

Conclusion: AdaSD为高效自适应的LLM推理提供了实用解决方案，适用于现有模型且无需预处理。

Abstract: Large language models (LLMs) have achieved remarkable performance across a wide range of tasks, but their increasing parameter sizes significantly slow down inference. Speculative decoding mitigates this issue by leveraging a smaller draft model to predict candidate tokens, which are then verified by a larger target model. However, existing approaches often require additional training, extensive hyperparameter tuning, or prior analysis of models and tasks before deployment. In this paper, we propose Adaptive Speculative Decoding (AdaSD), a hyperparameter-free decoding scheme that dynamically adjusts generation length and acceptance criteria during inference. AdaSD introduces two adaptive thresholds: one to determine when to stop candidate token generation and another to decide token acceptance, both updated in real time based on token entropy and Jensen-Shannon distance. This approach eliminates the need for pre-analysis or fine-tuning and is compatible with off-the-shelf models. Experiments on benchmark datasets demonstrate that AdaSD achieves up to 49\% speedup over standard speculative decoding while limiting accuracy degradation to under 2\%, making it a practical solution for efficient and adaptive LLM inference.

</details>


### [85] [CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise](https://arxiv.org/abs/2512.11282)
*Qingsen Ma,Dianyun Wang,Ran Jing,Yujun Sun,Zhenbo Xu*

Main category: cs.CL

TL;DR: 本论文提出CIP框架，通过因果推理减少大模型处理长噪文本时的幻觉问题，实验表明显著提升推理质量和效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖伪关联而非因果关系，导致在处理长且嘈杂的检索上下文时产生幻觉

Method: 构建实体/动作/事件的因果关系序列，通过因果干预和反事实推理压制非因果路径，将因果引导注入提示输入阶段

Result: 在归因率提升2.6分、因果一致性得分提高0.38、有效信息密度倍增4倍；API级处理延迟降低55.1%

Conclusion: 因果推理可显著提升大模型的可解释性/稳定性/效率，为模型优化提供新范式

Abstract: Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.

</details>


### [86] [LegalRikai: Open Benchmark -- A Benchmark for Complex Japanese Corporate Legal Tasks](https://arxiv.org/abs/2512.11297)
*Shogo Fujita,Yuji Naraki,Yiqing Zhu,Shinsuke Mori*

Main category: cs.CL

TL;DR: 本文介绍了LegalRikai: Open Benchmark，包含四个模拟日本公司法律实践的复杂任务，通过100个长格式结构化样本评估多个主流LLM。研究发现现有模型在文档级编辑存在缺陷，自动化评估在语言基础明确的标准上有效，但结构一致性评估仍是挑战，提出实践导向的数据集评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有短文本评估未能发现LLM在长文档编辑中的问题，且缺乏针对法律场景的实践导向基准。需要验证模型在多标准长文档处理能力，并探索自动化评估的可靠性。

Method: 1) 法律专家设计四个日本公司实践任务并构建100个样本；2) 使用GPT-5/Gemini/Claude等模型生成输出；3) 多维度人工评估与自动化评估对比；4) 分析语言标准符合性与结构一致性差异。

Result: 1) 抽象指令导致模型过度修改文本；2) 语言标准(如准确性)自动化评估与人工评估高度一致；3) 结构维度(如条款完整性)自动化评估效果较差；4) 自动化评估可作为专家不足时的预筛选工具。

Conclusion: 长文档结构评估是当前LLM的薄弱环节，需要改进模型结构建模能力。提出渐进式评估框架和多模态评估指标，强调法律实践场景的重要性，并开源数据集推动后续研究。

Abstract: This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.

</details>


### [87] [qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs](https://arxiv.org/abs/2512.11366)
*Shreya Shukla,Aditya Sriram,Milinda Kuppur Narayanaswamy,Hiteshi Jain*

Main category: cs.CL

TL;DR: 提出qa-FLoRA方法，动态计算LoRA适配器层融合权重，解决多领域复合查询问题


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法采用静态权重分配或依赖数据密集的监督训练，缺乏无需数据且无需训练的动态融合方案

Method: 通过测量基础模型与各个适配器的分布差异，使用查询自适应机制动态计算各层融合权重

Result: 在LLaMA-2上较静态融合提升5%，训练无关基线方法提升7%；LLaMA-3上分别提升6%和10%，并显著缩小与监督基线的差距

Conclusion: 方法具有可解释的融合模式，适用于无需复合训练数据的多领域自适应场景

Abstract: The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.

</details>


### [88] [Mining Legal Arguments to Study Judicial Formalism](https://arxiv.org/abs/2512.11374)
*Tomáš Koref,Lena Held,Mahammad Namazov,Harun Kumru,Yassine Thlija,Christoph Burchard,Ivan Habernal*

Main category: cs.CL

TL;DR: 本研究通过自动化方法分析捷克最高法院判决中的司法推理，反驳了关于中欧和东欧法院形式主义判决的主流观点，结合法律论证挖掘与机器学习技术实现高效分类。


<details>
  <summary>Details</summary>
Motivation: 法院需系统分析司法推理以验证‘中东欧地区存在形式化审判倾向’的学术争议，同时解决大规模法律文本分析的技术局限。

Method: 构建MADON数据集（272份判决、9183段标注文本），改进Transformer模型并通过持续预训练适配捷克法律领域，采用三级流水线（ModernBERT+Llama3.1+传统机器学习）解决数据不平衡问题。

Result: 模型在论证段落检测（82.6% F1）、法律论证类型分类（77.5% F1）和形式主义决策分类（83.2% F1）上取得优异表现，实证结论挑战了中东欧形式主义的现有叙事。

Conclusion: 法律论证挖掘可可靠分类司法哲学，方法具备跨司法区域复制性。开源的全流程工具与数据集为计算法律研究提供了可复用框架，证实法律人工智能在司法透明化的应用潜力。

Abstract: Courts must justify their decisions, but systematically analyzing judicial reasoning at scale remains difficult. This study refutes claims about formalistic judging in Central and Eastern Europe (CEE) by developing automated methods to detect and classify judicial reasoning in Czech Supreme Courts' decisions using state-of-the-art natural language processing methods. We create the MADON dataset of 272 decisions from two Czech Supreme Courts with expert annotations of 9,183 paragraphs with eight argument types and holistic formalism labels for supervised training and evaluation. Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining and experiment with methods to address dataset imbalance including asymmetric loss and class weighting. The best models successfully detect argumentative paragraphs (82.6\% macro-F1), classify traditional types of legal argument (77.5\% macro-F1), and classify decisions as formalistic/non-formalistic (83.2\% macro-F1). Our three-stage pipeline combining ModernBERT, Llama 3.1, and traditional feature-based machine learning achieves promising results for decision classification while reducing computational costs and increasing explainability. Empirically, we challenge prevailing narratives about CEE formalism. This work shows that legal argument mining enables reliable judicial philosophy classification and shows the potential of legal argument mining for other important tasks in computational legal studies. Our methodology is easily replicable across jurisdictions, and our entire pipeline, datasets, guidelines, models, and source codes are available at https://github.com/trusthlt/madon.

</details>


### [89] [Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis](https://arxiv.org/abs/2512.11388)
*Felipe Ribeiro Fujita de Mello,Hideyuki Takada*

Main category: cs.CL

TL;DR: This paper demonstrates that semantic-based data selection methods (e.g., COMET Kiwi) significantly outperform lexical or geometry-based approaches in fine-tuning open-source large language models for Japanese-English translation, with even minor data variations (<3%) substantially impacting performance.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate how different data selection strategies affect the fine-tuning efficiency and performance of open LLMs in machine translation, addressing the sensitivity of model training to data quality in controlled experimental conditions.

Method: Compared five data selectors (TF-IDF, COMET Kiwi, QuRate, FD-Score, random) under identical training conditions using Japanese-English corpora, measuring their impact on translation quality through standardized benchmarks.

Result: Semantic selectors (e.g., COMET Kiwi) consistently achieved superior performance over lexical (TF-IDF) or geometry-based methods, while minor data selection inconsistencies (<3% variance) led to statistically significant differences in model output quality.

Conclusion: Data curation quality critically determines fine-tuning outcomes for LLMs in translation tasks, with semantic metrics providing more effective data prioritization than traditional heuristics.

Abstract: We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.

</details>


### [90] [Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction](https://arxiv.org/abs/2512.11399)
*Galann Pennec,Zhengyuan Liu,Nicholas Asher,Philippe Muller,Nancy F. Chen*

Main category: cs.CL

TL;DR: This paper proposes a clip selection method for Vision-Language Models (VLMs) to efficiently summarize lengthy videos by identifying key moments through lightweight video captioning and large language model analysis, achieving performance comparable to human-annotated references while maintaining low computational cost.


<details>
  <summary>Details</summary>
Motivation: Long videos cause VLMs to lose critical visual information, necessitating tools that enable efficient and cost-effective analysis of extended video content.

Method: 1. Divide videos into clips; 2. Generate compact visual descriptions with a lightweight video captioning model; 3. Use a large language model (LLM) to select K clips with the most relevant information; 4. Compare against reference clips derived from the MovieSum dataset.

Result: Method achieves summarization performance near reference clips (which use <6% of a movie), outperforms random selection, and retains low computational cost via the lightweight captioning model.

Conclusion: The proposed approach effectively balances information retention and efficiency, enabling accurate multimodal video summaries while minimizing resource requirements.

Abstract: Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.

</details>


### [91] [CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare](https://arxiv.org/abs/2512.11437)
*Akash Ghosh,Srivarshinee Sridhar,Raghav Kaushik Ravi,Muhsin Muhsin,Sriparna Saha,Chirag Agarwal*

Main category: cs.CL

TL;DR: This paper introduces CLINIC，一个全面的多语言医疗基准，用于评估语言模型在医疗领域的可信度，重点解决低资源语言中的不足。


<details>
  <summary>Details</summary>
Motivation: 将语言模型（LMs）整合到医疗系统中具有改善医疗流程和决策的潜力，但实际应用的关键障碍是缺乏对其可信度的可靠评估，尤其是在多语言医疗环境中。现有的语言模型主要针对高资源语言训练，难以应对中低资源语言的复杂性和多样性，这在全球医疗应用中构成了重大挑战。

Method: 本研究提出了多语言医疗基准CLINIC，系统地在18项多样化任务中评估语言模型的五个可信度维度：真实性、公平性、安全性、鲁棒性和隐私性，覆盖15种语言和广泛的医疗主题（如疾病、诊断、治疗等）。

Result: 实验结果表明，语言模型在事实准确性方面存在不足，表现出跨人口和语言群体的偏见，并在隐私和对抗攻击方面表现出脆弱性。

Conclusion: 通过揭示这些缺陷，CLINIC为增强多语言医疗场景中语言模型的全球覆盖性和安全性奠定了基础。

Abstract: Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.

</details>


### [92] [Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction](https://arxiv.org/abs/2512.11502)
*Kai Golan Hashiloni,Brenda Kasabe Nokai,Michal Shevach,Esthy Shemesh,Ronit Bartin,Anna Bergrin,Liran Harel,Nachum Dershowitz,Liat Nadai Arad,Kfir Bar*

Main category: cs.CL

TL;DR: 开发了一种新的希伯来语医疗语言模型，基于DictaBERT 2.0并持续预训练于五百万条去身份化医院记录，用于从电子健康记录中提取结构化临床时间线，构建患者诊疗路径。


<details>
  <summary>Details</summary>
Motivation: 解决希伯来语医疗文本中非结构化电子健康记录难以提取结构化临床时间线的挑战，并填补希伯来语医疗语言模型在事件时间关系抽取任务上的空白。

Method: 基于DictaBERT 2.0模型架构，使用五百万条去身份化医院记录进行持续预训练，构建两个新数据集（内科急诊与肿瘤科）标注事件时间关系，并评估模型性能。

Result: 在双数据集上均取得优异性能，词汇表自适应优化提升了标记效率，且去身份化处理未影响下游任务性能。

Conclusion: 该模型是首个支持希伯来语医疗时间线抽取的工具，验证了隐私保护型模型开发的可行性，在伦理限制下开放研究使用。

Abstract: We present a new Hebrew medical language model designed to extract structured clinical timelines from electronic health records, enabling the construction of patient journeys. Our model is based on DictaBERT 2.0 and continually pre-trained on over five million de-identified hospital records. To evaluate its effectiveness, we introduce two new datasets -- one from internal medicine and emergency departments, and another from oncology -- annotated for event temporal relations. Our results show that our model achieves strong performance on both datasets. We also find that vocabulary adaptation improves token efficiency and that de-identification does not compromise downstream performance, supporting privacy-conscious model development. The model is made available for research use under ethical restrictions.

</details>


### [93] [Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs](https://arxiv.org/abs/2512.11509)
*Mohor Banerjee,Nadya Yuki Wangsajaya,Syed Ali Redha Alsagoff,Min Sen Tan,Zachary Choy Kit Chun,Alvin Chan Guo Wei*

Main category: cs.CL

TL;DR: 研究减少LLM幻觉的方法对创造性的影响，发现CoVe增强发散性思维，DoLa抑制它，RAG影响较小。


<details>
  <summary>Details</summary>
Motivation: LLM幻觉问题可能影响科学发现中事实准确性与创造性假设生成的平衡，需要评估现有幻觉消除技术对创造性能力的影响。

Method: 评估CoVe、DoLa、RAG三种方法，在LLaMA/Qwen/Mistral系列模型（1B-70B参数）上进行NeoCoder和CS4创造力基准测试。

Result: CoVe提升发散性思维，DoLa抑制发散性思维，RAG无显著影响，不同方法对创造力的双向影响需要权衡。

Conclusion: 科学应用需根据任务需求选择幻觉消除方法：需创造性时优先CoVe，需准确性时用DoLa，RAG适中。建议开发兼顾两者的解决方案。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.

</details>


### [94] [Visualizing token importance for black-box language models](https://arxiv.org/abs/2512.11573)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出DBSA，一种轻量级模型无关的黑盒大型语言模型审计工具，可分析每个输入Token对模型输出的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM审计方法仅关注局部行为（如偏见检测），无法解决实际应用中黑盒模型输出与输入Token的依赖关系分析问题，且传统梯度计算方法不可行。

Method: 通过DBSA工具，基于输入分布变化评估模型输出敏感性，无需梯度计算，适用于任意模型且无需对LLM分布做假设。

Result: 实验表明DBSA可通过可视化探索发现现有方法难以捕捉的敏感性，在法律、医疗等高风险场景中展现实用价值。

Conclusion: DBSA为生产环境中的LLM审计提供了可落地的技术方案，揭示了黑盒模型输入Token的重要性依赖特征。

Abstract: We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.

</details>


### [95] [Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols](https://arxiv.org/abs/2512.11614)
*Björn Deiseroth,Max Henning Höth,Kristian Kersting,Letitia Parcalabescu*

Main category: cs.CL

TL;DR: 本文提出了一种基于Merlin-Arthur协议的RAG模型训练框架，通过对抗样本和可解释方法使LLM在回答时严格依赖可验证的检索证据，降低幻觉并改进拒答机制。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统将检索证据视为启发式建议而非可验证信息，导致LLM在证据不足时生成无凭据回答、因误导信息产生幻觉。需要解决证据不可靠时的可控性问题。

Method: 将RAG全流程建模为交互式证明系统：Merlin提供有益证据，Morgana注入对抗性干扰，两者均使用线性XAI方法定位影响Arthur（LLM）决策的关键证据。训练LLM在证据充分时回答、不足时拒答，并引入EIF指标评估解释保真度与信息利用率。

Result: 在3个RAG数据集上，模型显式提升了基础性和完整性指标（↑15%），幻觉率下降40%，拒答准确率提高22%。检索器通过自动生成的难例负样本使Recall@10提升8.2%，且无需人工标注不可回答问题。

Conclusion: 交互式证明框架实现了证据驱动的RAG系统，为建立可验证的大模型生成机制提供了理论依据和工程实践路径，使检索文档从辅助信息转变成可信证据源。

Abstract: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.

</details>


### [96] [Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling](https://arxiv.org/abs/2512.11635)
*Keerthana Murugaraj,Salima Lamsiyah,Marten During,Martin Theobald*

Main category: cs.CL

TL;DR: 使用BERTopic分析历史报纸讨论核能与核安全的主题演化。


<details>
  <summary>Details</summary>
Motivation: 传统LDA方法难以捕捉历史文本的动态复杂性，OCR噪声和大数据量需要更优方法。

Method: 采用BERTopic神经主题建模方法，基于Transformer嵌入技术分析1955-2018年核能相关文本。

Result: 成功追踪主题分布时序变化，揭示核能与核武器主题的共现及重要性迁移模式。

Conclusion: 验证BERTopic在历史文献分析中的可扩展性和语境敏感性，为社科研究提供新工具。

Abstract: Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.

</details>


### [97] [Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks](https://arxiv.org/abs/2512.11718)
*Sergey Pankratov,Dan Alistarh*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型推测生成的理论极限，通过类比分支随机游走，推导出每次推测迭代中成功预测的平均token数的上界，并实证验证了该界限的有效性。


<details>
  <summary>Details</summary>
Motivation: 推测生成通过并行验证多个预测token加速大语言模型推理，但其理论加速上限尚不明确，本文旨在建立严格的理论界限以指导未来算法设计。

Method: 通过类比token生成过程与分支随机游走，分析最优草稿树选择问题，引入验证器输出分布的熵和二阶对数矩构建数学模型。

Result: 证明在基础假设下，每次迭代预测成功的token数量期望$\mathbb{E}[X] \leq (\mu+ \mu_{(2)})\log(P )/\mu^2 + O(1)$，并在Llama模型上验证了理论预测的准确性。

Conclusion: 本工作揭示了token并行生成的理论限制，为未来推测解码系统的设计提供了定量指导，实验证明该理论界限在实际场景中具有紧致性。

Abstract: Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\mathbb{E}[X] \leq (μ+ μ_{(2)})\log(P )/μ^2 + O(1)$, where $P$ is the verifier's capacity, $μ$ is the expected entropy of the verifier's output distribution, and $μ_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.

</details>


### [98] [SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support](https://arxiv.org/abs/2512.11755)
*Yuming Feng,Xinrui Jiang*

Main category: cs.CL

TL;DR: 提出SUMFORU个性化评论摘要框架，通过用户画像适配提升决策支持效果，采用数据管道+两阶段对齐策略，在亚马逊数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 线上商品评论存在数据过载与个性化缺失问题，传统LLM摘要方法无法满足用户差异化需求。研究旨在解决通用摘要模型与用户偏好不匹配导致的决策效率低下问题。

Method: 构建基于亚马逊2023评论数据集的高质量数据管道，创新性采用双阶段对齐机制：1）通过不对称知识蒸馏进行个性化监督微调(SFT)；2）利用偏好预测器进行AI反馈强化学习(RLAIF)，实现细粒度用户画像捕捉。

Result: 跨多维评估显示：在规则模型/LLM评估/人工评价中均显著提升一致性、事实关联性及偏好匹配度，模型表现优于现有方法，且在跨品类商品评论中具有强泛化能力。

Conclusion: 研究证实可指导的多元化对齐技术在个性化解码系统中的应用潜力，为下一代定制化决策支持系统提供新范式。

Abstract: Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.

</details>
