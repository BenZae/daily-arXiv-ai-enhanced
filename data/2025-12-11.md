<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 91]
- [cs.CL](#cs.CL) [Total: 27]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn,Yura Choi,Hyeonbeom Choi,Seongwon Cho,San Kim,Jonghyun Choi*

Main category: cs.CV

TL;DR: 提出VECTOR基准测试和MECOT方法以解决视频模型时间顺序理解不足问题


<details>
  <summary>Details</summary>
Motivation: 现有VLMMs在无序视频帧下仍表现良好，反映其未真正掌握时间顺序理解，亟需专门评估框架与改进方法

Method: 创建VECTOR基准测试，开发MECOT方法进行事件细节微调与思维链推理增强

Result: MECOT在VECTOR基准测试中显著提升时序理解能力，并优于当前最优模型

Conclusion: 揭示了当前VLMMs时序理解的局限性，证明MECOT技术路线的有效性，推动视频分析领域发展

Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.

</details>


### [2] [Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding](https://arxiv.org/abs/2512.08981)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 本文提出了一种基于统一图文嵌入（UTIE）的方法，通过融合跨模态语义信息消除面部嵌入中的群体特征偏差，从而提升人脸识别（FR）系统在不同群体间的公平性，同时保持或提高验证精度。


<details>
  <summary>Details</summary>
Motivation: 现有人脸识别系统存在群体偏差问题，其根源在于面部嵌入中群体特征与身份信息的耦合，导致不同群体验证精度差异显著。此问题在依赖生物识别的多元文化城市智能基础设施中尤为严峻。

Method: 利用视觉语言模型（VLM）的跨模态对齐能力，将不同群体的文本衍生特征融入目标群体面部嵌入，通过构建群体属性模糊的表征空间，迫使模型强化对身份相关特征的学习。具体采用CLIP/OpenCLIP/SigLIP三种模型实现图文联合嵌入。

Result: 在RFW/BFW基准测试中，UTIE使偏差度量指标（如TAR@FAR=1e-4的差距）降低了最高达62.3%（在CLIP模型上），同时保持了与基线模型相当或更优的绝对验证精度（如BFW数据集上AUC提升0.7%）。

Conclusion: 通过引入跨群体文本特征，UTIE有效解耦了群体属性与身份信息，为解决人脸识别系统公平性提供了通用可行的解决方案，尤其适用于大规模生物识别系统部署场景。

Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.

</details>


### [3] [Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement](https://arxiv.org/abs/2512.08982)
*Jian Xu,Wei Chen,Shigui Li,Delu Zeng,John Paisley,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文提出了Consist-Retinex，首个将一致性建模与Retinex理论结合的低光照图像增强框架，通过双目标一致性损失和自适应噪声强调采样，在单步采样中实现超越扩散模型的性能，且训练成本仅为后者的1/8。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型因多步采样限制实用性，而一致性模型虽能单步生成却仅适用于非条件合成，本文首次探索将一致性模型应用于条件增强场景。

Method: 1) 双目标一致性损失：结合时间一致性和随机时间采样下的真值对齐；2) 自适应噪声强调采样策略：优先学习大噪声区域的退化-增强映射。

Result: 在VE-LOL-L数据集上，单步采样性能超越Diff-Retinex++（PSNR 25.51 vs 23.41，FID 44.73 vs 49.59），且训练成本降低87.5%。

Conclusion: 研究表明一致性模型可通过强调大噪声区域学习实现条件增强，为低光照任务提供了兼顾效率与质量的新范式。

Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.

</details>


### [4] [HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification](https://arxiv.org/abs/2512.08983)
*Maoyu Wang,Yao Lu,Bo Zhou,Zhuangzhi Chen,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: 本文提出HSCP框架，通过分层谱聚类剪枝实现高效无人机射频指纹识别模型压缩，在降低计算需求的同时提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统无人机识别方法在复杂环境中难以提取可靠信号特征，现有模型剪枝技术难以平衡压缩率、硬件加速和识别精度，边缘设备部署面临挑战。

Method: HSCP使用层间谱聚类和通道谱聚类两阶段剪枝策略，结合中心化核对齐(CKA)优化模型压缩，并采用抗噪微调策略提升鲁棒性。

Result: 在UAV-M100数据集上，HSCP相比未剪枝ResNet18实现86.39%参数压缩、84.44%计算量降低，识别精度提升1.49%，在低信噪比环境下保持优越性能。

Conclusion: HSCP在参数压缩、计算效率与识别鲁棒性间取得最优平衡，优于现有通道和层剪枝方法，适合资源受限场景的无人机识别应用。

Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.

</details>


### [5] [RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition](https://arxiv.org/abs/2512.08984)
*Nirhoshan Sivaroopan,Hansi Karunarathna,Chamara Madarasingha,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.CV

TL;DR: RAG-HAR是一种无需训练的检索增强框架，利用大语言模型（LLM）进行人类活动识别（HAR），通过轻量统计特征和上下文检索实现高精度识别，并能泛化至未见活动。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖领域特定训练、大量标注数据和算力，限制其跨场景泛化能力，需开发更高效且适应性强的解决方案。

Method: 提取时序信号统计特征构建轻量向量，通过语义检索从数据库匹配相似样本，利用LLM结合上下文进行活动推理，并采用提示词优化和LLM生成语义增强向量库。

Result: 在6个HAR基准测试中达到SOTA性能（如CMAPSS数据集92.3%准确率），在仅需10%训练数据时仍优于传统方法，且可识别15类未见活动。

Conclusion: 训练free的RAG-HAR通过显式知识检索机制提升了HAR任务零样本迁移能力，为资源受限场景提供即插即用解决方案，推动智能穿戴和边缘计算应用落地。

Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.

</details>


### [6] [An Efficient Test-Time Scaling Approach for Image Generation](https://arxiv.org/abs/2512.08985)
*Vignesh Sundaresha,Akash Haridas,Vikram Appia,Lav Varshney*

Main category: cs.CV

TL;DR: This paper introduces the Verifier-Threshold method to improve computational efficiency in image generation models by intelligently reallocating test-time compute resources, achieving 2-4x faster processing without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Prior diffusion/flow image generation models use greedy, non-uniform compute budget allocation across denoising steps, leading to inefficient resource utilization despite increased test-time compute capabilities.

Method: Developed an adaptive compute reallocation framework (Verifier-Threshold) that automatically optimizes resource distribution during inference by evaluating sample quality thresholds across denoising stages.

Result: On GenEval benchmark, achieved equivalent image quality to state-of-the-art methods with 2-4x reduced computational time through systematic budget redistribution.

Conclusion: The Verifier-Threshold approach demonstrates effective scaling of image generation efficiency with available compute resources, outperforming previous heuristic-based allocation strategies.

Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.

</details>


### [7] [Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy](https://arxiv.org/abs/2512.08986)
*Anca Mihai,Adrian Groza*

Main category: cs.CV

TL;DR: 提出了一种针对糖尿病视网膜病变诊断数据的自动化质量控制框架，通过图像筛选、增强、辅助标注和共识评估提升数据可靠性。


<details>
  <summary>Details</summary>
Motivation: 人工标注视网膜图像存在效率低、标准不一的问题，而高质量数据对AI模型训练和临床诊断至关重要。

Method: 结合基于特征的解释性分类器、对比学习、深度学习辅助标注和标注者共识量化公式，实现图像质量过滤、标注优化与验证。

Result: 框架成功过滤低质量图像，显著提升标注一致性，验证了自动化质量控制对医学AI数据集的可行性。

Conclusion: 该方法通过可验证的量化指标，为医学影像数据标注提供了标准化流程，可扩展至其他疾病领域的AI训练数据优化。

Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.

</details>


### [8] [3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization](https://arxiv.org/abs/2512.08987)
*Yuze Hao,Linchao Zhu,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种3D逆向设计（3DID）框架，通过结合连续潜伏空间表示和物理感知优化策略，直接操作3D设计空间生成高保真度且灵活的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖2D近似或微调现有3D形状，导致细节丢失和设计探索受限，亟需直接高效处理高维3D设计空间的新方法。

Method: 1) 构建统一的物理-几何嵌入空间；2) 两阶段优化：第一阶段梯度引导的扩散采样探索潜伏流形，第二阶段目标驱动的拓扑保持细化策略优化候选方案。

Result: 3DID框架生成的3D几何在目标优化质量（高精度物理仿真匹配）和形态多样性（超越现有方法）方面均表现卓越性能。

Conclusion: 通过端到端物理感知优化流程，3DID解决了3D逆向设计空间复杂性难题，为从头生成高保真功能化3D结构提供新范式。

Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.

</details>


### [9] [Deterministic World Models for Verification of Closed-loop Vision-based Systems](https://arxiv.org/abs/2512.08991)
*Yuang Geng,Zhuoyang Zhou,Zhongzheng Zhang,Siyuan Pan,Hoang-Dung Tran,Ivan Ruchkin*

Main category: cs.CV

TL;DR: 本文提出了确定性世界模型（DWM），直接将系统状态映射为生成图像，结合双目标损失函数和基于星的可达性分析（StarV），显著提升了基于视觉的闭环控制系统的验证精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成模型的视觉验证方法因依赖随机潜变量导致不必要的过近似误差，亟需通过消除不可解释潜变量实现精确输入边界建模。

Method: 构建确定性世界模型（DWM）直接关联系统状态与生成图像，采用像素级重建损失与控制差异损失的双目标函数训练，集成StarV可达性分析并结合共形预测获取轨迹偏差统计界。

Result: 实验显示DWM相比潜变量基线方法在标准基准上产生了更紧致的可达集且验证性能更优。

Conclusion: 通过消除潜变量和引入确定性映射，DWM有效降低了视觉验证系统的近似误差，为高维视觉环境验证提供了新范式。

Abstract: Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.

</details>


### [10] [Demo: Generative AI helps Radiotherapy Planning with User Preference](https://arxiv.org/abs/2512.08996)
*Riqiang Gao,Simon Arberet,Martin Kraus,Han Liu,Wilko FAR Verbakel,Dorin Comaniciu,Florin-Cristian Ghesu,Ali Kamen*

Main category: cs.CV

TL;DR: This study presents a preference-driven generative model for radiotherapy 3D dose prediction, eliminating reliance on institution-specific reference plans and enabling personalized organ/target trade-off customization.


<details>
  <summary>Details</summary>
Motivation: Current deep learning dose prediction models require reference plans as ground truth, causing institutional bias and limiting adaptability to diverse planning styles. The lack of customizable trade-off mechanisms between critical organs and target volumes hinders clinical application.

Method: Developed a generative model that directly translates user-defined preference vectors into 3D dose distributions. Incorporates trainable modules for organ-at-risk (OAR)/planning target volume (PTV) prioritization without reference plan supervision, designed for seamless clinical system integration.

Result: Outperforms Varian RapidPlan in adaptability tests, particularly in scenarios requiring unconventional OAR-PTV trade-offs. Maintains comparable plan quality to reference models while offering unprecedented customization flexibility.

Conclusion: Preference-flavored dose prediction enables standardized, personalized treatment planning by decoupling model training from institutional practices. Demonstrates potential for clinical deployment through efficient integration and superior adaptability to user-specific requirements.

Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.

</details>


### [11] [Diffusion Model Regularized Implicit Neural Representation for CT Metal Artifact Reduction](https://arxiv.org/abs/2512.08999)
*Jie Wen,Chenhe Du,Xiao Wang,Yuyao Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型正则化隐式神经表示的金属伪影去除（MAR）框架，以解决CT图像因金属植入物引起的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有监督MAR方法因依赖有限的成对金属-干净数据导致性能不稳定，非监督方法未有效结合CT物理几何特性且使用启发式正则化项，难以充分利用先验知识。

Method: 融合隐式神经表示与预训练扩散模型的框架：隐式神经表示嵌入物理约束确保数据保真，扩散模型提供先验知识正则化约束。

Result: 在模拟与临床数据上实验验证了方法有效性，相较于现有方法展现出更优的伪影抑制能力和泛化性能。

Conclusion: 该方法突破了传统MAR技术的局限性，为临床应用提供了新的解决方案，具有实际部署的潜力。

Abstract: Computed tomography (CT) images are often severely corrupted by artifacts in the presence of metals. Existing supervised metal artifact reduction (MAR) approaches suffer from performance instability on known data due to their reliance on limited paired metal-clean data, which limits their clinical applicability. Moreover, existing unsupervised methods face two main challenges: 1) the CT physical geometry is not effectively incorporated into the MAR process to ensure data fidelity; 2) traditional heuristics regularization terms cannot fully capture the abundant prior knowledge available. To overcome these shortcomings, we propose diffusion model regularized implicit neural representation framework for MAR. The implicit neural representation integrates physical constraints and imposes data fidelity, while the pre-trained diffusion model provides prior knowledge to regularize the solution. Experimental results on both simulated and clinical data demonstrate the effectiveness and generalization ability of our method, highlighting its potential to be applied to clinical settings.

</details>


### [12] [A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography](https://arxiv.org/abs/2512.09001)
*Yuehua Hu,Jiyeong Kong,Dong-yeol Shin,Jaekyun Kim,Kyung-Tae Kang*

Main category: cs.CV

TL;DR: This paper introduces a new method to generate large-scale, physically valid defect datasets for semiconductor lithography with pixel-level annotations using mathematical morphology and DMD-based lithography, achieving significant improvements in AI-based defect detection.


<details>
  <summary>Details</summary>
Motivation: Scarcity of high-quality, publicly accessible defect datasets in semiconductor lithography restricts AI efficacy for micro/nano manufacturing defect inspection. Current data limitations hinder training robust AI models for this purpose.

Method: Defect layouts were synthesized via physics-constrained mathematical morphology (erosion/dilation) on design templates. Physical samples were created using digital micromirror device (DMD)-based lithography, followed by optical micrograph comparison to generate pixel-accurate annotations. A dataset of 3,530 images with 13,365 annotated defects (bridge, burr, pinch, contamination) was built and evaluated using Mask R-CNN vs. Faster R-CNN.

Result: Mask R-CNN showed 34% mean AP@0.5 improvement over Faster R-CNN across three defect classes (980 vs 740, 965 vs 719, 971 vs 717) and 42% improvement for contamination class, demonstrating the effectiveness of the generated pixel-level annotated dataset.

Conclusion: The proposed methodology successfully addresses data scarcity in semiconductor manufacturing AI applications by generating physically valid defect datasets with precise segmentation masks, enabling more robust AI-based measurement/inspection systems.

Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.

</details>


### [13] [A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques](https://arxiv.org/abs/2512.09005)
*Lownish Rai Sookha,Nikhil Pakhale,Mudasir Ganaie,Abhinav Dhall*

Main category: cs.CV

TL;DR: This paper is the first comprehensive survey on facial and body motion generation from speech and multimodal signals, covering generation approaches, datasets, and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Current motion generation methods struggle to create expressive and coherent facial/body dynamics that capture complex verbal/nonverbal cues and individual personality traits in social interactions.

Method: Systematic review synthesizing existing works on facial/body motion generation, including generative modeling approaches (speech-to-motion, conversation-driven, vision-based), representation techniques, benchmark datasets, and evaluation metrics.

Result: Identified open challenges in improving motion realism/coherence in dyadic interactions. Created comprehensive resource repository with datasets, code, and links (e.g., https://lownish23csz0010.github.io/mogen/).

Conclusion: Future directions emphasize enhancing avatar expressiveness through cross-modal learning, personality-aware generation, and improved evaluation frameworks for social interaction scenarios.

Abstract: Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.

</details>


### [14] [Towards Lossless Ultimate Vision Token Compression for VLMs](https://arxiv.org/abs/2512.09010)
*Dehua Zheng,Mouxiao Huang,Borui Jiang,Hailin Hu,Xinghao Chen*

Main category: cs.CV

TL;DR: 视觉语言模型面临高分辨率图像和视频带来的计算效率延迟问题。现有压缩方法存在位置偏差/类别不平衡，且难以扩展到浅层LLM。本文提出LUVC框架，通过视觉编码器迭代合并与LLM的低通滤波器频谱剪枝，实现跨模态token压缩，达成2倍推理加速且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力/相似度的视觉token压缩方法存在位置偏置和类别不平衡问题，导致显著精度退化。同时这些方法难以兼容浅层LLM模块中较弱的跨模态交互。需要一种能兼容全架构的渐进式无损压缩方案。

Method: 提出两阶段压缩框架：1) 在视觉编码器采用空间轴正交迭代合并方案加速计算；2) 在LLM集成注意力/相似度无关的低通滤波器频谱剪枝单元。通过渐进式融合高维视觉特征到多模态查询并最终消除视觉token，全过程与FlashAttention兼容。

Result: 实验证明实现LLM推理速度提升2倍情况下，精度损失可忽略。且训练无关特性支持跨多个VLM的即时部署。

Conclusion: LUVC通过双路径压缩策略有效解决视觉token冗余问题，在保持模型性能前提下显著提升计算效率。提出的频谱剪枝单元与迭代合并方案可扩展到不同VLM架构，为多模态模型轻量化提供新思路。

Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.

</details>


### [15] [ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors](https://arxiv.org/abs/2512.09056)
*Liming Kuang,Yordanka Velikova,Mahdi Saleh,Jan-Nico Zaech,Danda Pani Paudel,Benjamin Busam*

Main category: cs.CV

TL;DR: 本文提出了ConceptPose，一种无需训练和模型的物体姿态估计框架，通过视觉语言模型构建3D概念地图并建立跨概念的3D对应关系，在零样本设置下实现精准6DoF姿态估计，且在ADD(-S)指标上性能提升超62%。


<details>
  <summary>Details</summary>
Motivation: 传统物体姿态估计方法依赖大量数据集特定训练，而大规模视觉语言模型已展现强大零样本能力，因此本文旨在结合两者优势以解决训练数据限制问题。

Method: 使用视觉语言模型生成开放词汇3D概念地图，通过显著性图提取概念向量，并利用跨概念地图的3D-3D对应关系估计相对姿态，全过程无需参数训练。

Result: 在零样本相对姿态基准测试中，本文方法在ADD(-S)评分上超越现有方法62%，包括那些采用大量数据集训练的方法，实现最先进性能。

Conclusion: ConceptPose首次实现了无需训练且模型无关的高精度姿态估计，通过整合视觉语言模型与3D点云分析，为开放词汇场景下的姿态估计提供了新范式。

Abstract: Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.

</details>


### [16] [SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding](https://arxiv.org/abs/2512.09062)
*Seongyong Kim,Yong Kwon Cho*

Main category: cs.CV

TL;DR: 本文提出了SIP数据集，专门用于解决建筑工地LiDAR数据稀缺问题，包含室内外场景及特定分类注释。


<details>
  <summary>Details</summary>
Motivation: 现有3D感知公开数据集无法反映实际工地因安全限制导致的LiDAR采集挑战，如点云密度衰减和碎片化几何特征。

Method: 通过陆地LiDAR扫描仪采集真实工地数据，采用A/B/C三级建筑环境分类体系标注（含脚手架/MEP管道等细瘦临时构件），建立标准化采集协议和质控流程。

Result: 开源发布包含结构件与临时设施的多场景数据集，支持灵活类别配置，具备真实点云特征和分割挑战（如遮挡导致的稀疏性）。

Conclusion: SIP为建筑领域3D视觉任务提供基准数据，推动对抗真实采集条件的算法研究。

Abstract: Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.

</details>


### [17] [KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification](https://arxiv.org/abs/2512.09069)
*Erfan Nourbakhsh,Nasrin Sanjari,Ali Nourbakhsh*

Main category: cs.CV

TL;DR: 开发了一种名为KD-OCT的知识蒸馏框架，将高性能的ConvNeXtV2-Large模型压缩为轻量级EfficientNet-B2学生模型，用于高效分类AMD、黄斑变性和CNV病例，同时保持近似教师模型的性能并大幅降低模型大小和推理时间。


<details>
  <summary>Details</summary>
Motivation: 临床环境中部署先进的深度学习模型（如ConvNeXtV2-Large）受限于其高计算需求，需开发兼顾诊断性能和实时部署能力的轻量化模型。

Method: 提出KD-OCT框架，利用集成增强、随机权重平均和焦点损失的改进教师模型，结合实时蒸馏策略（融合软知识迁移和硬监督损失），在Noor Eye Hospital数据集上通过患者级交叉验证评估效果。

Result: 实验表明，KD-OCT在多尺度或特征融合模型中实现了更优的效率-准确性平衡，学生模型规模缩小且推理速度提升，边缘部署可行性增强。

Conclusion: 该方法在压缩模型的同时维持高水平诊断性能，为AMD筛查的轻量化部署提供了有效解决方案。

Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.

</details>


### [18] [Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics](https://arxiv.org/abs/2512.09071)
*Nick Trinh,Damian Lyons*

Main category: cs.CV

TL;DR: 提出一种基于负高斯混合统计的视觉位置识别自动阈值选择方法


<details>
  <summary>Details</summary>
Motivation: 现有VPR方法依赖人工阈值设定，难以应对复杂场景变化，影响机器人部署效果

Method: 通过分析'非本地点'的高斯混合统计特性建立数学模型自动确定匹配阈值

Result: 所提方法在多数据库/图像描述符组合中均能获得稳定阈值选择效果

Conclusion: 证明统计建模方法可有效解决视觉位置识别中的自适应阈值设定难题

Abstract: Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.

</details>


### [19] [Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters](https://arxiv.org/abs/2512.09092)
*Mizanur Rahman Jewel,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CV

TL;DR: MDSE是一种新的地下灾害场景解释框架，通过上下文感知的跨模态对齐、分割感知的双路径视觉编码及高效的资源型语言模型，提升灾难后黑暗环境的场景描述准确性，并配套首个地下灾害数据集UMD。


<details>
  <summary>Details</summary>
Motivation: 地下矿难导致的黑暗、粉尘和坍塌使传统系统难以获取有效视觉信息，亟需一种能在恶劣视觉条件下生成详尽场景解释的框架，辅助应急响应决策。

Method: 提出三重创新：1）上下文感知跨注意力机制（视觉-文本特征对齐）；2）分割感知的双路径编码（融合全局与区域视觉信息）；3）轻量化Transformer语言模型（低计算资源生成描述文本）。同时构建首个真实地下灾害场景的图像-文本数据集UMD。

Result: 在UMD及关联基准测试中，MDSE显著超越现有模型，生成更准确、符合语境的场景描述，尤其在模糊场景细节捕捉上表现突出，代码已开源。

Conclusion: MDSE通过多模态融合与高效模型设计，有效解决了地下灾害场景理解难题，所提出的创新方法和UMD数据集为未来灾害应急系统研究提供了重要基础。

Abstract: Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.

</details>


### [20] [Food Image Generation on Multi-Noun Categories](https://arxiv.org/abs/2512.09095)
*Xinyue Pan,Yuhao Chen,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种针对多词食品类别图像生成的方法FoCULR，解决了模型对复合食品名称语义误解导致图像生成错误的问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有生成模型在处理'蛋面'等多词食品名称时，容易错误分离词汇导致生成包含不相关食材的图片，且多词名称在UEC-256等真实数据集中占比高，需提升模型对该类别的语义理解。

Method: 设计食品领域知识增强模块FoCULR，在生成流程早期引入核心概念（如食材关联、空间布局），修正文本编码器对复合名称关系的理解偏差。

Result: 实验表明，该方法显著提升了食品图像生成质量，在多词类别图像中正确呈现食材组合与空间结构。

Conclusion: 通过融入领域知识，有效解决了多词食品名称生成中的语义误解和布局错误，为特定领域图像生成提供了可借鉴的框架。

Abstract: Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.

</details>


### [21] [GimbalDiffusion: Gravity-Aware Camera Control for Video Generation](https://arxiv.org/abs/2512.09112)
*Frédéric Fortier-Chouinard,Yannick Hold-Geoffroy,Valentin Deschaintre,Matheus Gadelha,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本论文提出GimbalDiffusion框架，通过绝对坐标系和重力参考实现文本到视频生成中精确的相机控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法受限于相对或模糊的相机轨迹表示，缺乏对相机运动方向和三维姿态的精细控制，尤其需要解决重力感知和全局参照系缺失的问题。

Method: 1. 建立基于重力参考的绝对坐标系描述相机轨迹；2. 利用全景360度视频构建多角度轨迹数据集；3. 提出零俯仰条件注释策略减少文本干扰；4. 构建涵盖广角度俯仰变化的相机感知评估基准。

Result: 实现了在无初始参考帧条件下，生成具有显式几何控制的相机轨迹，在俯仰角变化超过80度的极端场景中表现稳定，成功生成如天空视角仰拍等复杂运镜效果。

Conclusion: 该框架通过物理坐标系统一了文本生成与运镜控制，在保持视频内容准确性的同时显著增强了生成式视频的空间导航能力。

Abstract: Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.

</details>


### [22] [SuperF: Neural Implicit Fields for Multi-Image Super-Resolution](https://arxiv.org/abs/2512.09115)
*Sander Riisøen Jyhne,Christian Igel,Morten Goodwin,Per-Arne Andersen,Serge Belongie,Nico Lang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多图像超分辨率方法SuperF，利用神经场（INR）和测试时优化，无需依赖高分辨率训练数据，通过共享INR并联合优化帧对齐实现多视角超分。


<details>
  <summary>Details</summary>
Motivation: 传统单图像超分辨率依赖高分辨率训练数据或辅助数据，易产生与真实不符的结构；而多图像超分辨率通过利用多视角亚像素位移可提升光学分辨率，但现有方法存在限制。

Method: SuperF通过共享隐式神经表示（INR）并联合优化多帧亚像素对齐（采用可优化仿射变换参数）实现多视角超分辨率，并基于超采样坐标网格优化输出分辨率。

Result: 在卫星图像和地面手持相机图像的模拟测试中，实现了高达8倍的上采样效果，且无需任何高分辨率训练数据。

Conclusion: SuperF有效克服了传统方法对高分辨率训练数据的依赖性，通过INR的连续信号表达特性与亚像素对齐优化实现高质量多图像超分辨率重建。

Abstract: High-resolution imagery is often hindered by limitations in sensor technology, atmospheric conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution algorithmically. Since single-image super-resolution requires solving an inverse problem, such methods must exploit strong priors, e.g. learned from high-resolution training data, or be constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While qualitatively pleasing, such approaches often lead to "hallucinated" structures that do not match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) resolution by constraining the super-resolution process with multiple views taken with sub-pixel shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages coordinate-based neural networks, also called neural fields. Their ability to represent continuous signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task.
  The key characteristic of our approach is to share an INR for multiple shifted low-resolution frames and to jointly optimize the frame alignment with the INR. Our approach advances related INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield compelling results on simulated bursts of satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach does not rely on any high-resolution training data.

</details>


### [23] [Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation](https://arxiv.org/abs/2512.09134)
*Georgy Kopanitsa,Oleg Metsker,Alexey Yakovlev*

Main category: cs.CV

TL;DR: 开发了一种名为AngioAI-QFR的全自动冠状动脉造影分析系统，结合深度学习与虚拟手术规划，可快速评估狭窄功能并预测血流储备分数（FFR）结果。


<details>
  <summary>Details</summary>
Motivation: 传统冠状动脉造影依赖医生主观判断狭窄程度，与缺血关联性有限；虽有基于导丝的FFR技术，但未广泛使用。现有血管造影衍生指数工具操作繁琐且缺乏自动化，亟需一体化解决方案。

Method: 基于深度学习的端到端管线，包含狭窄检测（精度0.97）、管腔分割（Dice系数0.78）、中心线提取、直径测量、毫米级相对血流容量（RFC）分析及虚拟支架植入功能，通过造影图像自动计算QFR，并在100例血管中与侵入性FFR对比验证。

Result: AngioAI-QFR处理成功率93%，单血管平均用时41秒，与FFR高度相关（r=0.89，MAE=0.045）；检测FFR≤0.80时AUC为0.93（敏感度88%，特异度86%），RFC区分局灶/弥漫病变，虚拟支架显示局灶病变QFR增益更大。

Conclusion: 该系统实现冠状动脉解剖-功能联合分析的自动化，结合计算机视觉与虚拟手术，提供接近实时的临床决策支持工具。

Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.

</details>


### [24] [GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars](https://arxiv.org/abs/2512.09162)
*Kelian Baert,Mae Younes,Francois Bourel,Marc Christie,Adnane Boukhayma*

Main category: cs.CV

TL;DR: This paper introduces an approach that merges 2D Gaussian Splatting with UV texture mapping to reconstruct high-quality, editable 3D head avatars from monocular videos, enabling intuitive texture-based editing without optimization overhead.


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting achieves photorealistic 3D head reconstruction but lacks intuitive editability of traditional mesh-based methods. The work aims to bridge this gap by combining the fidelity of splatting with UV texture mapping's versatility.

Method: The authors embed Gaussian primitives into UV space via a template mesh, reconstructing editable materials on a standard UV domain. They also employ a physically based reflectance model for relighting capabilities and texture-based edits for geometry and appearance adjustments.

Result: The method demonstrates competitive reconstruction accuracy compared to state-of-the-art approaches, produces high-quality relighting effects, and enables direct texture-space edits for avatar appearance/geometry control without re-optimization.

Conclusion: By integrating Gaussian Splatting with UV texture mapping, the approach delivers accurate, relightable, and user-friendly editable 3D head avatars, offering key advantages for applications in digital content creation.

Abstract: Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.

</details>


### [25] [WonderZoom: Multi-Scale 3D World Generation](https://arxiv.org/abs/2512.09164)
*Jin Cao,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: 本文提出WonderZoom，利用scale-adaptive高斯surfels和渐进式细节合成器，首次实现从单张图像生成多尺度3D场景，支持从宏观到微观的连续细节扩展。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型局限性在于单尺度合成，无法在不同空间粒度上保持内容连贯性，主要原因缺乏尺度感知的3D表征方法。

Method: 1) 提出可自适应不同空间尺度的高斯surfels表示法，支持多尺度内容生成与实时渲染；2) 开发渐进式细节合成器，通过自回归方式逐级生成更精细的3D细节。

Result: 实验显示WonderZoom在生成质量与内容对齐度上显著优于SOTA视频和3D生成模型，实现从地形到纳米级细节的连续缩放体验。

Conclusion: 该方法突破3D生成的尺度限制，通过创新的表征与合成架构，首次实现在单一框架下跨多个数量级空间尺度的高质量3D场景生成。

Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/

</details>


### [26] [Prompt-Based Continual Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.09172)
*Sauda Maryam,Sara Nadeem,Faisal Qureshi,Mohsen Ali*

Main category: cs.CV

TL;DR: PromptCCZSL introduces a continual learning framework for compositional zero-shot vision-language models, using prompt-based fusion and novel loss functions to balance knowledge retention and adaptation to new compositions.


<details>
  <summary>Details</summary>
Motivation: The problem requires learning new attribute-object compositions while preserving knowledge of recurring attributes/objects across sessions - more complex than standard continual learning where classes are disjoint.

Method: Employs recency-weighted multi-teacher distillation, session-aware compositional prompts, and session-agnostic attribute/object prompts. Introduces Cosine Anchor Loss (memory preservation), Orthogonal Projection Loss (inter-class separation), and Intra-Session Diversity Loss (intra-class variation) while maintaining frozen VLM backbone.

Result: Outperforms previous methods on UT-Zappos and C-GQA benchmarks with significant improvements in both compositional generalization and forgetting reduction, establishing new performance standards for closed-world CCZSL tasks.

Conclusion: PromptCCZSL effectively addresses both catastrophic forgetting and compositional adaptation challenges through its hybrid approach combining architectural innovations and curriculum-aware loss functions.

Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.

</details>


### [27] [Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation](https://arxiv.org/abs/2512.09185)
*Hao Chen,Rui Yin,Yifan Chen,Qi Chen,Chao Li*

Main category: cs.CV

TL;DR: 提出Δ-LFM框架，通过流匹配与患者特异性潜在对齐建模疾病进展，提升可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在疾病进展建模中存在潜在空间不连贯及与临床严重程度指标无关的缺陷

Method: 将疾病动力学视为速度场并结合流匹配，引入患者特异性潜在对齐约束轨迹沿单调轴分布

Result: 在3个纵向MRI数据集上表现优异，提供疾病动力学可视化与解释新框架

Conclusion: Δ-LFM解决了疾病进展建模的连续性与语义对齐问题，具备临床应用潜力

Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $Δ$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $Δ$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.

</details>


### [28] [Rethinking Chain-of-Thought Reasoning for Videos](https://arxiv.org/abs/2512.09616)
*Yiwu Zhong,Zi-Yuan Hu,Yin Li,Liwei Wang*

Main category: cs.CV

TL;DR: 提出高效视频推理框架，通过压缩视觉标记与生成短推理链，在保持性能的同时显著提升模型效率。


<details>
  <summary>Details</summary>
Motivation: 现有CoT视频推理依赖冗长推理链与大量视觉令牌，导致计算成本高且需大量人工标注，研究旨在验证简洁化方案的有效性。

Method: 设计基于压缩视觉令牌与短推理链的后训练框架，采用无监督策略优化模型，避免人工注释依赖。

Result: 模型在多个视频推理基准测试中效率显著提升，推理速度提高23%-58%且维持原有精度，无需人工CoT标注。

Conclusion: 长链CoT推理非视频任务必需，压缩视觉令牌与简短逻辑链可实现高效且高性能的视频理解。

Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.

</details>


### [29] [MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI](https://arxiv.org/abs/2512.09867)
*Fengli Wu,Vaidehi Patil,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: 本研究提出MedForget测试平台，系统评估医疗AI中多模态模型层次化数据遗忘效果，发现现有方法难以在保留诊断性能的同时实现完全遗忘。


<details>
  <summary>Details</summary>
Motivation: 预训练的多模态医疗模型因涉及敏感患者数据面临隐私合规风险，而现有遗忘方法在医疗场景的多模态层级结构中的效果尚未被系统研究。

Method: 构建MedForget测试平台，将医院数据建模为四层嵌套结构（机构→患者→检查→切片），设计包含8个层级的遗忘目标和重构攻击测试方案，包含3840个跨模态样本并生成复述变体评估不同粒度遗忘效果。

Result: 实验证实现有SOTA遗忘方法在粗粒度场景（如机构层级）能抵抗重构攻击，但在细粒度层级（如切片）遗忘时会导致模型记忆残留漏洞；跨任务（生成/分类/填空）测试显示遗忘过程普遍面临性能保留与数据清洗的权衡困境。

Conclusion: 研究证明医疗模型遗忘存在粒度敏感性，提出符合HIPAA法规的评估框架，揭示当前方法不足以应对复杂层级遗忘，为构建合规医疗AI系统提供基准测试工具。

Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.

</details>


### [30] [Efficient Feature Compression for Machines with Global Statistics Preservation](https://arxiv.org/abs/2512.09235)
*Md Eimran Hossain Eimon,Hyomin Choi,Fabien Racapé,Mateen Ulhaq,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: The split-inference paradigm requires transferring intermediate feature data between two model parts. We use Z-score normalization to compress and recover this data efficiently, outperforming the current MPEG FCM codec standard by reducing bitrate and improving task accuracy, with experiments showing up to 65.69% reduction in object tracking tasks.


<details>
  <summary>Details</summary>
Motivation: Effective compression of feature data in the split-inference paradigm is critical for reducing transfer overhead and maintaining task accuracy. Existing methods (e.g., scaling) in the MPEG FCM codec standard are suboptimal, necessitating a more efficient approach.

Method: We propose Z-score normalization to recover compressed feature data at the decoder, replacing the existing scaling method in the MPEG FCM codec standard. A simplified alternative method is also introduced for specific scenarios to further reduce overhead.

Result: The proposed method achieves a 17.09% average bitrate reduction across tasks and up to 65.69% for object tracking, with no loss in task accuracy. The simplified method further reduces overhead in certain cases.

Conclusion: Z-score normalization improves compression efficiency and end-task accuracy in split-inference systems compared to existing methods. This work advances the MPEG FCM codec standard by addressing feature data transfer bottlenecks.

Abstract: The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.

</details>


### [31] [OmniPSD: Layered PSD Generation with Diffusion Transformer](https://arxiv.org/abs/2512.09247)
*Cheng Liu,Yiren Song,Haofan Wang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: OmniPSD是一个基于扩散模型的统一框架，可实现带透明通道的分层PSD文件生成与分解。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型难以处理带透明alpha通道的分层PSD文件，需解决高保真生成、结构一致性保持及透明度保留等挑战。

Method: 框架基于Flux生态系统，通过空间注意力机制学习多层空间关系实现文本到PSD生成；采用迭代上下文编辑提取并擦除图像组件完成分解；使用RGBA-VAE辅助编码透明通道信息。

Result: 在自建RGBA-layered数据集上的实验表明，生成结果在高保真度（FID 1.8）、结构一致性（边界框匹配度93%）和透明度保留（PSNR 38.7dB）指标均领先，支持复杂多层（平均15层）的PSD重建。

Conclusion: 本研究验证了扩散模型处理分层透明图形的能力，为设计软件自动创建提供了可编辑的神经渲染新范式。

Abstract: Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.

</details>


### [32] [GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://arxiv.org/abs/2512.09251)
*Lalit Maurya,Saurabh Kaushik,Beth Tellman*

Main category: cs.CV

TL;DR: 提出GLACIA框架结合大语言模型与遥感分割，实现冰川湖高精度监测与空间推理输出，mIoU达87.30%


<details>
  <summary>Details</summary>
Motivation: 现有CNN和ViT算法仅支持像素级预测，缺乏全局场景语义理解与人类可解释的推理能力，难以满足冰川湖突发洪水风险防控需求

Method: 1.构建GLACIA框架，融合大语言模型与语义分割模块；2.设计Glacial Lake Position Reasoning(GLake-Pos)数据集，生成多样化空间问答对；3.通过多模态融合实现像素-语义协同推理

Result: mIoU指标（87.30）超越现有最佳模型：CNN（78.55-79.01）、ViTs（69.27-81.75）、地理基础模型（76.37-87.10）及推理分割方法（60.12-75.66）

Conclusion: 开创性地将视觉-语言推理引入遥感监测，通过自然语言交互提升灾害防控决策效率与政策可解释性，代码已开源

Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA

</details>


### [33] [ROI-Packing: Efficient Region-Based Compression for Machine Vision](https://arxiv.org/abs/2512.09258)
*Md Eimran Hossain Eimon,Alena Krause,Ashan Perera,Juan Merlos,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: ROI-Packing通过优先编码对任务精度至关重要的区域并高效打包，在无需重新训练模型的情况下实现高效图像压缩，显著降低比特率且保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像压缩方法未针对机器视觉任务的关键区域进行优化，可能导致冗余压缩或任务精度下降。需要一种无需重新训练模型即可实现高效压缩的解决方案。

Method: 提出ROI-Packing方法，首先识别图像中对目标检测和实例分割等任务最关键的兴趣区域，通过高效编码这些区域并舍弃冗余数据实现压缩，保持与MPEG标准VVC编解码器的兼容性。

Result: 在5个数据集上测试显示：相比VVC编解码器，ROI-Packing在保持任务精度时比特率降低44.10%，相同比特率下任务精度提升8.88%。

Conclusion: ROI-Packing通过区域优先级策略实现了面向机器视觉的高精度压缩，在保持模型兼容性的同时显著提升压缩效率和任务表现。

Abstract: This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).

</details>


### [34] [MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification](https://arxiv.org/abs/2512.09270)
*Sangwoon Kwak,Weeyoung Kwon,Jun Young Jeong,Geonho Kim,Won-Sik Cheong,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出MoRel框架，通过锚点中继双向混合与特征方差分层致密化策略，实现长距离动态场景的高效4D高斯飞溅建模。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯飞溅技术在处理长距离动态视频时存在内存爆炸、时间闪烁及遮挡处理失败问题，需开发新型时间一致性建模框架。

Method: 1. 建立基于关键帧锚点空间的双向混合机制（ARBB）；2. 提出特征方差引导的分层致密化方案（FHD）；3. 通过动态混合可学习透明度参数优化时间连续性。

Result: 在自主构建的长距离动态数据集SelfCap$_{\text{LR}}$上验证，实现内存可控的无闪烁4D重建，动态运动幅度指标超越现有数据集38%，内存占用降低52%。

Conclusion: MoRel通过动态锚点建模与自适应致密化策略，在保持高渲染质量的前提下突破了4D高斯飞溅的长距离动态建模瓶颈，为动态场景表示提供了新范式。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.

</details>


### [35] [LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations](https://arxiv.org/abs/2512.09271)
*Zhichao Yang,Tianjiao Gu,Jianjie Wang,Feiyu Lin,Xiangfei Sheng,Pengfei Chen,Leida Li*

Main category: cs.CV

TL;DR: 本文提出了LongT2IBench（14K长文图文对及图结构标注）和LongT2IExpert（基于多模态大语言模型的分层对齐评估模型），解决长文本到图像生成中的对齐评估难题。


<details>
  <summary>Details</summary>
Motivation: 现有T2I对齐基准仅支持短文本场景且标注形式单一（MOS/李克特量表），缺乏长文本场景下细粒度、可解释的自动评估模型。需解决长文本生成中实体、属性、关系的精细化对齐标注问题。

Method: 1) 设计Generate-Refine-Qualify三阶段标注协议，将长文本转为包含实体-属性-关系的图结构标注；2) 开发LongT2IExpert模型，通过层级对齐Chain-of-Thought指令微调，使多模态大语言模型能输出量化评分与结构化解释。

Result: LongT2IBench包含14K图文对及图结构标注，标注协议提升细粒度对齐注释精度，LongT2IExpert在对齐评估与解释性任务上显著优于现有方法，数据与代码已开源。

Conclusion: 本文提出首个多维度长文本T2I对齐基准和评估模型，通过结构化标注与多模态大模型的层级推理能力，有效实现长文本生成的细粒度质量评估与可解释性分析。

Abstract: The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.

</details>


### [36] [LoGoColor: Local-Global 3D Colorization for 360° Scenes](https://arxiv.org/abs/2512.09278)
*Yeonjin Chang,Juhwan Cho,Seunghyeon Seo,Wonsik Shin,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出了LoGoColor方法，通过"局部-全局"策略消除指导平均过程，实现复杂360°场景的多样化3D着色。


<details>
  <summary>Details</summary>
Motivation: 现有3D着色方法继承2D模型的颜色平均特性，导致复杂场景中出现单调结果。需要避免颜色平均以保持多样性。

Method: 1) 生成无平均的新训练视角 2) 划分场景为子场景，利用微调的多视角扩散模型同时解决子场景间和子场景内一致性问题

Result: 在定量和定性方面均实现更一致且合理的3D着色效果，通过新提出的颜色多样性指数验证有效性

Conclusion: 通过消除指导平均过程和局部-全局方法，成功解决了复杂360°场景的3D着色一致性问题，显著提升颜色多样性

Abstract: Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360° scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360° scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.

</details>


### [37] [FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model](https://arxiv.org/abs/2512.09282)
*Xiang Chen,Jinshan Pan,Jiangxin Dong,Jian Yang,Jinhui Tang*

Main category: cs.CV

TL;DR: 本研究提出了FoundIR-v2图像恢复模型，通过数据均衡调度和MoE驱动调度器，有效处理50+子任务，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复模型受不同任务数据混合比例影响，需优化混合策略以提升整体性能。

Method: 1. 提出FoundIR-v2扩散模型；2. 数据均衡调度动态优化混合比例；3. 引入MoE调度器分配任务自适应扩散先验。

Result: 实验覆盖50+子任务及多样真实场景，性能表现优于当前最先进方法。

Conclusion: FoundIR-v2通过动态混合策略与MoE调度器，在多任务图像恢复任务中表现出卓越泛化能力。

Abstract: Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.

</details>


### [38] [Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving](https://arxiv.org/abs/2512.09296)
*Songhan Wu*

Main category: cs.CV

TL;DR: 本文提出改进YOLOv8n-SPTS模型，通过特征提取模块优化、特征融合增强及小目标专用检测结构，显著提升自动驾驶中小目标检测性能，在VisDrone2019-DET数据集上取得最高精度61.9%及mAP@0.5达52.6%。


<details>
  <summary>Details</summary>
Motivation: 现有算法因小目标信息缺失、尺度不平衡和遮挡导致检测性能差，亟需提升动态复杂场景下自动驾驶系统的细粒度感知能力。

Method: 基于YOLOv8n提出三项改进：1) Backbone瓶颈结构引入Space-to-Depth卷积增强细节信息提取；2) 采用SPPFCSPC模块整合多尺度特征与跨阶段残差连接；3) 设计Triple-Stage特征金字塔结构增加160×160分辨率小目标检测头并优化计算效率。

Result: 实现61.9%精度、48.3%召回率，mAP@0.5/0.5:0.95达52.6%/32.6%，可视化显示遮挡密集场景下行人/自行车漏检率显著降低，模型在CVPR自动驾驶感知领域处于领先水平。

Conclusion: 通过空间压缩特征增强与金字塔结构优化，有效解决小目标分辨率低和计算冗余问题，为自动驾驶动态感知提供高精度实时检测解决方案。

Abstract: This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.

</details>


### [39] [VABench: A Comprehensive Benchmark for Audio-Video Generation](https://arxiv.org/abs/2512.09299)
*Daili Hua,Xizhi Wang,Bohan Zeng,Xinyi Huang,Hao Liang,Junbo Niu,Xinlong Chen,Quanqing Xu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了VABench，一个用于同步音视频生成的评估框架，解决了现有基准测试在音频同步性评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成基准主要关注视觉质量，但缺乏对音视频同步性及模态间关联性的系统评估。此外，针对跨模态生成任务（如文生音视频、图生音视频）的评估指标缺失，需要统一标准推动领域发展。

Method: 设计包含15个评估维度的双模块体系：①多模态关联评估（文本-视频、文本-音频、视频-音频相似性）；②特定模态评估（同步性、唇音一致性、问答对准确性）。覆盖T2AV/I2AV/立体声生成三大任务，涵盖动物、人声、音乐等七大内容类别。

Result: 通过系统性分析与多维可视化，证实VABench可有效揭示音视频生成模型在模态协同与同步性方面的性能瓶颈，尤其在复杂场景生成任务中表现出显著区分度。

Conclusion: VABench为同步音视频生成提供了标准化评估框架，推动了跨模态生成模型在内容关联性与时序同步性方面的研究进展。

Abstract: Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.

</details>


### [40] [From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation](https://arxiv.org/abs/2512.09307)
*Shivanshu Agnihotri,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Polyp-DiFoM的知识蒸馏框架，将大模型（如SAM, DINOv2）的丰富表征能力迁移至轻量级分割模型（如U-Net），显著提升结肠镜下息肉分割性能，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统轻量级模型（如U-Net）在应对息肉的尺寸/形态/颜色变化及伪装特性时分割效果有限，而具有强泛化能力的视觉大模型因医疗领域数据稀缺且缺乏领域知识，难以直接迁移至医学影像任务。

Method: 通过语义先验蒸馏与频率域增强编码技术，将大模型的语义表达（如SAM/Mask2Former）注入U-Net及其变体，并在五个结肠镜基准数据集上进行实验验证。

Result: Polyp-DiFoM在分割精度上优于基准模型和SOTA模型，同时计算量缩减近9倍，代码开源https://github.com/lostinrepo/PolypDiFoM。

Conclusion: 该方法有效弥合了视觉大模型与临床场景间的差距，实现了医学图像分割中性能与效率的平衡。

Abstract: Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.

</details>


### [41] [Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook](https://arxiv.org/abs/2512.09315)
*Yuan Ma,Junlin Hou,Chao Zhang,Yukun Zhou,Zongyuan Ge,Haoran Xie,Lie Ju*

Main category: cs.CV

TL;DR: LNMBench是一个用于评估医学图像中标签噪声鲁棒性的综合基准，揭示了现有方法在现实噪声下的局限性并提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注依赖专家知识且存在观察者间变异，导致标签噪声问题突出。现有噪声标签学习方法在医学领域的系统性评估缺乏，需建立统一基准。

Method: 构建包含10种代表性方法、7个数据集、6种成像模态和3种噪声模式的LNMBench框架，进行大规模实验验证。

Result: 实验发现现有方法在高噪声和真实噪声下表现显著下降，主要受类别不平衡和域间差异影响。提出的改进方案有效提升模型鲁棒性。

Conclusion: LNMBench为医学图像噪声标签研究提供标准化评估框架，并通过开源代码促进可复现性研究和实用算法开发。

Abstract: Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.

</details>


### [42] [UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking](https://arxiv.org/abs/2512.09327)
*Xuangeng Chu,Ruicong Liu,Yifei Huang,Yun Liu,Yichen Peng,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniLS的端到端框架，用于生成统一的说-听交互式数字人表情。该框架通过双音轨音频驱动，在保留高质量说话表达的同时，显著提升了自然倾听动作的生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法过度依赖语音驱动训练，导致倾听动作僵硬；现有双人生成方法依赖额外动作输入，非端到端设计限制实时应用。

Method: 采用两阶段训练：第一阶段通过无音频自回归学习建立面部运动先验模型，第二阶段引入双音轨音频微调模型，在说话时绑定语音特征，在倾听时调制运动先验实现动态响应。

Result: 在保持说话动作SOTA精度的同时，倾听指标提升达44.1%，生成的倾听表情多样性与自然度显著增强，整体解决动作僵硬问题。

Conclusion: 该研究突破了传统音频驱动数字人的单向表达限制，为交互型虚拟数字人提供了高保真实时生成解决方案

Abstract: Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.

</details>


### [43] [Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video](https://arxiv.org/abs/2512.09335)
*Seonghwa Choi,Moonkyeong Choi,Mingyu Jang,Jaekyung Kim,Jianfei Cai,Wen-Huang Cheng,Sanghoon Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D Gaussian Splatting的高质量人体虚拟形象生成框架RnD-Avatar，可实现动态姿态下的几何细节重建和可控光照渲染。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF和3DGS方法在重建人体虚拟形象时难以捕捉服装褶皱等动态姿态相关的几何细节，且缺乏多光照条件下的评估数据集，导致真实感渲染效果不足。

Method: 提出动态蒙皮权重模型以关联姿态与身体形变，引入稀疏视觉线索下的几何正则化策略，并构建了包含多视角变光照数据的新基准数据集。

Result: 在新视角合成、动态姿态渲染及可控光照效果方面均达到SOTA性能，在几何细节还原和姿态泛化能力上显著优于传统方法。

Conclusion: 通过结合动态形变建模与物理正则化约束，该框架有效解决了单目视频重建虚拟形象的长期挑战，为未来元宇宙等应用场景提供技术支撑。

Abstract: Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.

</details>


### [44] [TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment](https://arxiv.org/abs/2512.09350)
*Kanghyun Baek,Sangyub Lee,Jin Young Choi,Jaewoo Song,Daemin Park,Jooyoung Choi,Chaehun Shin,Bohyung Han,Sungroh Yoon*

Main category: cs.CV

TL;DR: TextGuider是一种无需训练的方法，通过注意力对齐和损失函数提升文本到图像生成的文本准确性，解决文本遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成文本常遗漏关键内容，影响质量与可用性，研究旨在解决这一问题。

Method: 通过分析MM-DiT模型中与文本相关的token注意力模式，提出潜引导策略，结合两个自定义损失函数，在去噪早期阶段引导模型生成完整文本。

Result: TextGuider在测试中展现出SOTA性能，在召回率、OCR准确率和CLIP分数均优于现有方法。

Conclusion: 基于注意力分析的潜引导可有效解决文本到图像生成的遗漏问题，无需额外训练即可提升生成质量。

Abstract: Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.

</details>


### [45] [Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding](https://arxiv.org/abs/2512.09354)
*Xinkui Zhao,Zuxin Wang,Yifan Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Chang Liu,Naibo Wang,Jianwei Yin*

Main category: cs.CV

TL;DR: Video-QTR通过查询驱动的动态帧选择减少视频处理的冗余计算，在保持性能的同时降低帧消耗73%


<details>
  <summary>Details</summary>
Motivation: 传统视频处理框架（process-then-reason）需要高密集帧编码，导致内存消耗高、计算冗余，限制了真实场景的部署。

Method: 提出查询驱动的时间推理框架Video-QTR，在语义意图引导下动态分配资源，通过推理与感知的自适应反馈循环规避密集帧编码。

Result: 在MSVD-QA等5个基准测试中，Video-QTR在保持SOTA性能的同时帧消耗降低73%，验证了其效率和可扩展性。

Conclusion: 查询驱动的视频理解范式能有效解决高计算成本问题，为资源受限场景提供可扩展的解决方案。

Abstract: The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.

</details>


### [46] [StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation](https://arxiv.org/abs/2512.09363)
*Ke Xing,Longfei Li,Yuyang Yin,Hanwen Liang,Guixun Luo,Chen Fang,Jue Wang,Konstantinos N. Plataniotis,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: StereoWorld 提出了一种端到端框架，利用预训练视频生成模型将单目视频转换为高保真立体视频，同时发布了一个包含1100万帧的大规模高清立体视频数据集。


<details>
  <summary>Details</summary>
Motivation: 针对当前立体视频制作成本高昂且易产生伪影的问题，提出通过改写预训练模型实现低成本高质量的单目到立体视频生成。

Method: 1) 联合单目视频输入对模型进行条件约束
2) 采用几何感知正则化确保3D结构保真
3) 空间-时间平铺策略提升高分辨率合成效率
4) 构建包含1100万帧的高清立体视频数据集用于训练评估

Result: 实验表明StereoWorld显著优于现有方法，在可视化质量和几何一致性方面达到最优性能，项目网页已开源。

Conclusion: 本研究表明通过框架创新与大规模数据集结合，可在降低立体视频制作成本的同时提升生成质量，为后续研究提供了有效基准。

Abstract: The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.

</details>


### [47] [ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation](https://arxiv.org/abs/2512.09364)
*Shengchao Zhou,Jiehong Lin,Jiahui Liu,Shizhen Zhao,Chirui Chang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出ASSIST-3D合成数据生成管道，通过三维场景合成技术显著提升类别无关3D实例分割模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D实例分割方法受限于标注数据稀缺和2D分割噪声，而传统3D合成方法无法同时满足几何多样性、上下文复杂性和布局合理性的要求。

Method: 提出三个创新：1) 异构物体选择（从CAD库随机采样）；2) LLM引导的深度优先布局生成；3) 多视角RGB-D融合生成真实点云。

Result: 在ScanNetV2/++和S3DIS基准上，训练数据生成的模型显著优于现有方法，并证明了专用合成流水线的有效性。

Conclusion: ASSIST-3D通过多维度创新实现高质量3D场景合成，为类别无关实例分割提供了有效的训练数据生成框架。

Abstract: Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.

</details>


### [48] [FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement](https://arxiv.org/abs/2512.09373)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: 本文提出FUSER和FUSER-DF两种方法，首次实现端到端多视角点云配准，通过统一潜空间处理和扩散优化框架，在保证高精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统多视角配准依赖计算量大的两两匹配且缺乏全局几何约束，难以处理大规模场景下的实时需求。现有方法在精度与效率之间存在明显矛盾。

Method: FUSER采用稀疏3D CNN提取保留平移信息的超点特征，通过几何交替注意力模块进行跨扫描处理；FUSER-DF在SE(3)^N空间构建扩散模型，使用FUSER预测结果作为初始值进行去噪优化，通过推导变分下界指导训练。

Result: 在3DMatch、ScanNet和ArkitScenes数据集上均达到SOTA的配准精度（如FUSER在ScanNet达96.7%成功率），计算效率提升42%-78%，FUSER-DF进一步优化姿态估计。

Conclusion: 通过统一表征学习和扩散模型优化，为多视角配准提供了全新范式，解决了传统方法的计算瓶颈和局部最优问题，在大规模场景重建中展现应用潜力。

Abstract: Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.

</details>


### [49] [Perception-Inspired Color Space Design for Photo White Balance Editing](https://arxiv.org/abs/2512.09383)
*Yang Cheng,Ziteng Cui,Lin Gu,Shenghan Su,Zenghui Zhang*

Main category: cs.CV

TL;DR: This paper proposes a novel white balance correction framework using a perception-inspired Learnable HSI color space and a specialized Mamba-based network, addressing limitations of traditional sRGB-based methods in complex lighting conditions.


<details>
  <summary>Details</summary>
Motivation: Current sRGB-based white balance correction methods face limitations due to fixed nonlinear transformations and entangled color channels, which restrict their effectiveness under complex lighting conditions when RAW data is unavailable.

Method: The authors developed a Learnable HSI (LHSI) color space with enhanced luminance-chromaticity disentanglement and adaptive learnable mappings, combined with a custom Mamba-based neural network architecture designed to exploit this novel color space's characteristics.

Result: Experimental evaluations on benchmark datasets demonstrated the proposed framework's superior performance compared to existing methods, validating the effectiveness of perception-inspired color space design for white balance correction.

Conclusion: The study demonstrates the potential of bio-inspired color space engineering in computational photography, showing that learnable color representations combined with tailored neural architectures can overcome traditional limitations in white balance correction under challenging illumination conditions.

Abstract: White balance (WB) is a key step in the image signal processor (ISP) pipeline that mitigates color casts caused by varying illumination and restores the scene's true colors. Currently, sRGB-based WB editing for post-ISP WB correction is widely used to address color constancy failures in the ISP pipeline when the original camera RAW is unavailable. However, additive color models (e.g., sRGB) are inherently limited by fixed nonlinear transformations and entangled color channels, which often impede their generalization to complex lighting conditions.
  To address these challenges, we propose a novel framework for WB correction that leverages a perception-inspired Learnable HSI (LHSI) color space. Built upon a cylindrical color model that naturally separates luminance from chromatic components, our framework further introduces dedicated parameters to enhance this disentanglement and learnable mapping to adaptively refine the flexibility. Moreover, a new Mamba-based network is introduced, which is tailored to the characteristics of the proposed LHSI color space.
  Experimental results on benchmark datasets demonstrate the superiority of our method, highlighting the potential of perception-inspired color space design in computational photography. The source code is available at https://github.com/YangCheng58/WB_Color_Space.

</details>


### [50] [Wasserstein-Aligned Hyperbolic Multi-View Clustering](https://arxiv.org/abs/2512.09402)
*Rui Wang,Yuting Jiang,Xiaoqing Luo,Xiao-Jun Wu,Nicu Sebe,Ziheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的 Wasserstein-Aligned Hyperbolic (WAH) 框架，用于多视图聚类，通过结合层次语义建模和超球面分布对齐，在多个基准数据集上实现了SOTA聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有超球面表示方法在跨视图对齐时仅关注实例级对齐，忽略了全局语义一致性，导致对视图特有信息（如噪声和跨视图差异）鲁棒性不足。

Method: 1) 为每个视图设计超球面编码器，将特征嵌入Lorentz流形进行层次语义建模；2) 引入基于超球面 sliced-Wasserstein 距离的全局语义损失对齐流形分布；3) 通过软聚类分配增强跨视图语义一致性。

Result: 在多个标杆数据集上均取得当前最优（SOTA）聚类性能，尤其在跨视图差异显著的场景下表现突出。

Conclusion: 该框架通过超球面表示与Wasserstein距离优化，有效解决了多视图聚类中全局语义一致性缺失的问题，为多模态数据融合提供了新方向。

Abstract: Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.

</details>


### [51] [Generative Point Cloud Registration](https://arxiv.org/abs/2512.09407)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D配准框架（Generative Point Cloud Registration），通过将生成式2D模型与3D匹配任务结合，生成跨视角一致的图像对，并设计了Match-ControlNet模型以实现几何-色彩特征融合，提升配准鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云配准方法可能受限于几何或局部特征提取，而生成模型在跨视角一致性上具有优势。本文旨在通过结合2D生成与3D配准，解决几何对齐与纹理一致性的问题。

Method: 提出生成式3D配准范式，利用Match-ControlNet模型：1) 基于点云深度图生成几何对齐图像；2) 通过耦合条件去噪和提示引导增强跨视角纹理一致性。框架可无缝集成于多种配准方法中。

Result: 实验表明该方法在3DMatch和ScanNet数据集上表现出有效性，验证了几何-3D一致性生成对配准性能的提升作用。

Conclusion: 该框架通过生成模型赋能3D配准，实现了跨视角几何与纹理联合优化，为配准算法提供了增强性能的通用解决方案。

Abstract: In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.

</details>


### [52] [DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping](https://arxiv.org/abs/2512.09417)
*Yanan Wang,Shengcai Liao,Panwen Hu,Xin Li,Fan Yang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出DirectSwap，一种无需掩码的视频换头方法，利用新构建的HeadSwapBench配对数据集和运动感知扩散模型，实现高质量的跨身份视频换头。


<details>
  <summary>Details</summary>
Motivation: 现有换头方法依赖同帧数据训练和掩码修复技术，导致边界伪影且无法恢复被遮挡的面部姿态、表情等关键动态信息。

Method: 1) 构建首个跨身份视频换头配对数据集HeadSwapBench；2) 提出DirectSwap视频扩散模型，集成运动模块和条件输入；3) 设计动态感知的MEAR损失函数，通过像素级重加权优化运动与表情一致性。

Result: 在真实视频场景中取得SOTA视觉质量（SSIM/PSNR等指标领先）、身份保真度（ID相似度0.912）和动态一致性（LMD损失降低37%），并提供开源数据集与代码。

Conclusion: 通过配对数据集构建和动态感知模型设计，有效解决了视频换头中的动态信息丢失和边界伪影问题，为该领域提供了基准数据集和先进方法。

Abstract: Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\TrainNum{} videos) and benchmarking (\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.

</details>


### [53] [Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis](https://arxiv.org/abs/2512.09418)
*Zhe Li,Hadrien Reynaud,Johanna P Müller,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出Motion Conditioned Diffusion Model (MCDM)，通过自监督运动特征合成无标注超声心动图视频


<details>
  <summary>Details</summary>
Motivation: 超声心动图因隐私限制和标注复杂性缺乏标注数据，传统深度学习方法受限，需开发无需人工标注的生成模型

Method: 设计无标签扩散框架MCDM，通过Motion and Appearance Feature Extractor (MAFE)解耦运动与外貌特征，并采用伪外观特征引导的重识别损失和伪光流引导的光学流损失增强训练

Result: 在EchoNet-Dynamic数据集表现优异，生成视频时间连续且符合临床特征，无需人工标注即可达到竞争性生成效果

Conclusion: 自监督扩散模型可有效合成超声心动图视频，代码开源为心脏影像研究提供新范式

Abstract: Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.

</details>


### [54] [InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography](https://arxiv.org/abs/2512.09422)
*Zhe Li,Hadrien Reynaud,Alberto Gomez,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出了一种通过运动特征提取和基于图的样本选择方法，合成包含25个视频的紧凑型超声心动图数据集，在EchoNet-Dynamic数据集上达到69.38%的测试精度。


<details>
  <summary>Details</summary>
Motivation: 超声心动图视频数据规模扩大导致存储和计算效率下降，传统训练方法面临挑战，需要高效的数据集蒸馏技术。

Method: 结合运动特征提取、类别级图构建和Infomap算法进行代表性样本选择，保留原始数据集关键临床特征。

Result: 使用25个合成视频在EchoNet-Dynamic数据集上实现了69.38%的测试准确率，验证了方法的有效性和可扩展性。

Conclusion: 该方法通过数据蒸馏显著减少数据规模，同时保持诊断性能，适用于医学视频数据的高效处理。

Abstract: Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \(69.38\%\) using only \(25\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.

</details>


### [55] [FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds](https://arxiv.org/abs/2512.09423)
*Marco Pegoraro,Evan Atherton,Bruno Roy,Aliasghar Khani,Arianna Rampini*

Main category: cs.CV

TL;DR: FunPhase introduces a functional periodic autoencoder for motion learning, decoupling spatial-temporal dynamics via phase manifold embedding and enabling scalable, high-resolution motion prediction/generation across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing phase manifold approaches lack scalability and generalization, confined to specific motion settings; paper aims to address these limitations.

Method: Proposes functional periodic autoencoder with continuous decoding (vs discrete time steps), learning interpretable phase manifolds that capture motion periodicity while enabling arbitrary temporal resolution sampling.

Result: Achieves 15% lower reconstruction error than prior periodic autoencoders, supports super-resolution (4x), partial-body motion completion, and cross-dataset/skeleton generalization while matching SOTA motion generation performance.

Conclusion: Establishes a unified framework for motion prediction and generation through functional manifold learning, outperforming specialized methods in both reconstruction accuracy and application versatility. 

Abstract: Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.

</details>


### [56] [UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents](https://arxiv.org/abs/2512.09435)
*Xufan He,Yushuang Wu,Xiaoyang Guo,Chongjie Ye,Jiaqing Zhou,Tianlei Hu,Xiaoguang Han,Dong Du*

Main category: cs.CV

TL;DR: 提出UniPart，通过几何-分割联合表示实现在无需分割标注情况下的可控3D生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式分割/外部标注数据，但存在控制粒度不足或依赖大规模标注集等问题。发现物体几何学习过程中会自然形成parts感知。

Method: 1) 提出Geom-Seg VecSet统一编码几何与parts结构；2) 构建两阶段扩散框架UniPart：第一阶段联合生成几何与隐式segments，第二阶段在全局/局部空间预测parts特征；3) 双空间生成方案提升几何质量。

Result: 在保持几何真实性的同时，显著提升segments可控性与parts级几何质量（定量指标提升12.3%+，用户调研得分提升28%）。

Conclusion: 通过联合建模几何与parts结构，实现了单图输入的可控3D生成，避免了对分割标注的依赖，在精度与控制性上均超越现有方法。

Abstract: Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.

</details>


### [57] [Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model](https://arxiv.org/abs/2512.09441)
*Jiantao Tan,Peixian Ma,Tong Yu,Wentao Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出了一种基于视觉-语言模型（VLM）的持续学习框架，通过任务特定适配器、跨任务表示校准和不确定性引导的推理策略，缓解类间混淆问题，实验证明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于VLM的持续学习方法在多任务中难以区分类别、易产生类间混淆的问题，同时保持对旧知识的保留能力。

Method: 在预训练冻结的图像编码器中添加任务特定适配器学习新知识，采用轻量级投影器混合的跨任务表示校准策略统一特征空间，并设计基于预测不确定性的推理策略选择最优图像特征用于分类。

Result: 在多数据集和多种实验设置下，所提方法在分类准确性和跨任务泛化性上均显著优于现有持续学习方法。

Conclusion: 框架通过适配器与校准策略实现了VLM在持续学习中的类间解耦，验证了其缓解灾难性遗忘和提升多任务分类性能的有效性。

Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.

</details>


### [58] [Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation](https://arxiv.org/abs/2512.09446)
*Nadeem Nazer,Hongkuan Zhou,Lavdim Halilaj,Ylli Sadikaj,Steffen Staab*

Main category: cs.CV

TL;DR: 本文提出DAPO方法，通过优化文本提示实现无需手工设计提示的细粒度异常检测，在跨分布偏移场景下显著提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM模型在异常检测中忽略细粒度异常类型（如孔洞/裂纹），导致异常表征不够结构化，且手工设计文本提示耗时且易引入偏差。细粒度识别可帮助制造商快速定位异常根本原因并采取针对性措施。

Method: 提出DAPO方法，通过渐进式调优学习混合缺陷感知提示，结合固定文本锚点和可学习token嵌入，对齐异常相关图像特征与文本语义，无需手工设计提示即可实现跨分布偏移的零样本多类异常检测与分割。

Result: 在MPDD/VisA/MVTec-AD等5个公共数据集和内部数据集实验表明：跨分布偏移场景下图像级AUROC和平均精度提升3.7%，零样本设置下新异常类型定位性能提升6.5%。

Conclusion: DAPO通过自适应学习缺陷感知提示，有效缩小粗略异常信号与细粒度缺陷类别间的语义鸿沟，在跨分布偏移场景下实现优于基线模型的检测与定位性能。

Abstract: Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like "hole", "cut", "scratch" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of "abnormal" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.

</details>


### [59] [Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework](https://arxiv.org/abs/2512.09461)
*Anabia Sohail,Mohamad Alansari,Ahmed Abughali,Asmaa Chehab,Abdelfatah Ahmed,Divya Velayudhan,Sajid Javed,Hasan Al Marzouqi,Ameena Saad Al-Sumaiti,Junaid Kashir,Naoufel Werghi*

Main category: cs.CV

TL;DR: 本研究提出了用于分析人类试管婴儿胚胎中细胞质串（CS）的首个计算框架，通过两阶段深度学习模型结合新型NUCE损失函数，显著提升CS自动检测效果。


<details>
  <summary>Details</summary>
Motivation: 虽然试管婴儿技术已改进治疗结果，但胚胎选择仍是瓶颈。CS作为新兴生物标志物虽与胚胎存活率相关，但传统人工检测存在低效、主观性等问题。

Method: 构建包含13,568帧的CS标注数据集，采用两阶段深度学习架构：第一阶段逐帧检测CS存在，第二阶段定位CS区域，引入结合置信度加权和嵌入收缩项的

Result: NUCE损失在五种变压器模型上均提升F1分数，基于RF-DETR的定位技术在薄且低对比度的CS结构检测中达到SOTA性能，源代码已开源。

Conclusion: 该计算框架为胚胎选择提供了首个自动化CS分析方案，有助于提高试管婴儿胚胎筛选效率与客观性。

Abstract: Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.

</details>


### [60] [Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing](https://arxiv.org/abs/2512.09463)
*Sander De Coninck,Emilio Gamba,Bart Van Doninck,Abdellatif Bey-Temsamani,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 本文通过三个工业案例验证了隐私保护框架的有效性，在保持任务性能的同时实现敏感信息模糊化，平衡隐私与实用价值。


<details>
  <summary>Details</summary>
Motivation: AI视觉系统在工业应用中受隐私约束，需解决如何在保障生产监控功能的同时避免泄露工人敏感信息的矛盾。

Method: 采用可学习图像变换技术，针对木工监控、AGV导航避障、人体工学分析3个场景，在保留关键任务特征的前提下遮蔽非必要隐私数据。

Result: 定量测试显示隐私/性能指标达最优平衡，工业合作伙伴反馈验证了该方案部署可行性及对组织信任度的提升作用。

Conclusion: 证明框架具备商业部署成熟度，并提出兼顾隐私保护与工业效率的人工智能实施指南，适用于多行业场景迁移。

Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.

</details>


### [61] [Color encoding in Latent Space of Stable Diffusion Models](https://arxiv.org/abs/2512.09477)
*Guillem Arias,Ariadna Solà,Martí Armengod,Maria Vanrell*

Main category: cs.CV

TL;DR: 通过分析Stable Diffusion的潜在空间，发现颜色信息主要通过c_3/c_4通道的环形对手轴编码，而亮度和形状由c_1/c_2通道主导，潜在空间结构符合适效编码理论。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在视觉保真度上取得突破，但其内部如何表征颜色/形状等感知属性尚不明确，需通过系统性分析揭示潜在编码机制。

Method: 构建控制变量合成数据集，采用主成分分析(PCA)和相似性度量方法对Stable Diffusion的潜变量通道进行解剖式分析

Result: ① 颜色编码表现为c_3/c_4通道的环形对手轴结构 ② c_1/c_2通道主要处理亮度和形状信息 ③ 潜在空间呈现符合神经科学适效编码理论的可解释组织结构

Conclusion: 该发现为模型理解、图像编辑应用优化以及解耦式生成框架设计提供了理论依据，证明扩散模型潜变量空间存在类似生物视觉系统的编码策略

Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.

</details>


### [62] [MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images](https://arxiv.org/abs/2512.09489)
*Shuaihao Han,Tingfa Xu,Peifu Liu,Jianan Li*

Main category: cs.CV

TL;DR: MODA dataset和OSSDet框架提升基于多光谱图像的航拍目标检测，通过级联光谱-空间调制结构和对象感知机制。


<details>
  <summary>Details</summary>
Motivation: 解决传统RGB航拍检测中小目标和背景干扰问题，因MSI数据不足限制了性能，需新数据集和模型挖掘光谱线索潜力。

Method: 发布含14,041张MSI图像和330,191标注的MODA数据集，提出OSSDet框架：级联光谱-空间调制优化目标感知，基于光谱相似性聚合同类特征，并通过对象感知掩码抑制背景，结合跨光谱注意力机制增强表征。

Result: 实验表明OSSDet在参数量和效率相近的模型中表现更优，证实其在航拍检测中的有效性。

Conclusion: MODA数据集为领域奠定基础，OSSDet通过多模态信息融合超越传统方法，推动航拍检测技术发展。

Abstract: Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.

</details>


### [63] [StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio](https://arxiv.org/abs/2512.09492)
*Abdullah Al Mamun,Miaohua Zhang,David Ahmedt-Aristizabal,Zeeshan Hayder,Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: 本文提出StateSpace-SSL，一种基于线性状态空间建模的植物病害检测自监督学习框架，通过方向扫描和教师-学生对齐机制，解决了现有CNN/Transformer模型在农业图像上的病变连续性建模不足和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有SSL方法在农业图像应用受限于：1) CNN无法捕捉病变模式连续性，2) Transformer存在高分辨率分块带来的二次计算成本。

Method: 1) 采用Vision Mamba状态空间编码器实现线性时间建模；2) 通过方向扫描机制学习叶片表面病变成分的长程连续性；3) 引入原型驱动的教师-学生框架对齐多视角表征，强化病变感知特征学习。

Result: 在3个植物病害数据集上显著优于CNN/Transformer基线：1) 定量指标提升8-15%；2) 定性分析揭示特征图更集中于病变区域；3) 计算复杂度降低40%。

Conclusion: 线性状态空间模型通过方向建模显著改善了自监督表征学习的病变敏感性和计算效率，为农业视觉领域提供了新的方法论基础。

Abstract: Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.

</details>


### [64] [Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment](https://arxiv.org/abs/2512.09555)
*Yuan Li,Zitang Sun,Yen-ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本研究针对基于视觉语言模型（VLM）的质量评估（BIQA）中存在的预测矛盾和不稳定性问题，提出一种双阶段调优方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽具语义推理能力，但生成的视觉特征与质量预测存在矛盾且结果不稳定，需探究成因并改进其符合人类推理机制

Method: 通过两阶段分离视觉感知与质量推理过程：首阶段学习视觉特征，次阶段仅基于特征进行质量评估，并分析模型中间层的token依赖性

Result: 在SPAQ和KONIQ数据集上将预测稳定性提升至12.39%（原22.00%），SRCC/PLCC指标平均提升0.3124/0.3507，在LIVE、CSIQ等数据集均显著改善

Conclusion: 模型改进有效降低预测矛盾性和随机性，通过视觉特征与质量推理的机制分离实现了更可靠的人类一致性推理

Abstract: Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.

</details>


### [65] [Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment](https://arxiv.org/abs/2512.09573)
*Yuan Li,Zitang Sun,Yen-Ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: The paper explores enhancing MLLM-based Image Quality Assessment by addressing their limitations in detecting low-level distortions (e.g., blur, noise) through improved vision encoder alignment, increasing distortion recognition accuracy from 14.92% to 84.43%.


<details>
  <summary>Details</summary>
Motivation: MLLM-based IQA models exhibit inconsistent evaluations due to poor detection of low-level distortions, prompting the question of whether they adequately perceive critical visual features essential for reliable quality assessment.

Method: Introduced a low-level distortion classification task and conducted component-wise analysis to identify overfitting and biases in quality scoring. Semantic distance metrics were used to evaluate alignment changes in visual-semantic representations during fine-tuning.

Result: Component-wise analysis revealed weakened low-level features during vision-language alignment transfer; enhancing vision encoder alignment improved distortion recognition accuracy by 69.51 percentage points.

Conclusion: Imposing explicit constraints on the vision encoder strengthens interpretable visual representations, enabling MLLMs to deliver more coherent reasoning in vision-centric tasks while mitigating biases from overfitting.

Abstract: Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.

</details>


### [66] [Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis](https://arxiv.org/abs/2512.09576)
*David Seu,Nicolas Longepe,Gabriel Cioltea,Erik Maidik,Calin Andrei*

Main category: cs.CV

TL;DR: 本文提出了一种基于遥感数据的可扩展土壤属性建模系统，实现了高精度的土壤有机碳和氮含量预测，为农业数字化提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 现有土壤评估工具在可扩展性和可访问性方面存在局限，而环境变量对农业决策的影响日益显著，亟需开发新型土壤属性估算框架。

Method: 采用混合建模方法，结合物理信息协变量（来自辐射传输模型）和基础模型的非线性嵌入，通过严格空间分块验证框架，在欧洲多气候区土壤数据集上进行训练与评估。

Result: SOC预测MAE为5.12 g/kg（CCC 0.77），总氮预测MAE为0.44 g/kg（CCC 0.77），通过共形校准实现90%置信区间覆盖率，且在独立测试集保持性能。

Conclusion: 该系统为定量土壤评估提供了可扩展的解决方案，适用于碳市场等需要空间外推的领域，推动了精准农业的数字化发展。

Abstract: Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.

</details>


### [67] [Hands-on Evaluation of Visual Transformers for Object Recognition and Detection](https://arxiv.org/abs/2512.09579)
*Dimitrios N. Vlachogiannis,Dimitrios A. Koutsomitropoulos*

Main category: cs.CV

TL;DR: Vision Transformers (ViTs) outperform CNNs in global context tasks like medical imaging, with hybrid models (e.g., Swin, CvT) balancing accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: CNNs struggle with global image understanding, prompting exploration of ViTs as superior alternatives.

Method: Compared ViTs (pure/hierarchical/hybrid) and CNNs on ImageNet, COCO, and ChestX-ray14 datasets across classification/detection tasks, with data augmentation experiments in medical imaging.

Result: Hybrid/hierarchical ViTs (Swin, CvT) showed optimal accuracy-efficiency trade-offs; medical imaging performance improved significantly with Swin + data augmentation.

Conclusion: ViTs are competitive/inferior to CNNs only in specific scenarios, excelling in global context tasks like medical imaging.

Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.

</details>


### [68] [Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation](https://arxiv.org/abs/2512.09580)
*Hancheng Zhu,Xinyu Liu,Rui Yao,Kunyang Sun,Leida Li,Abdulmotaleb El Saddik*

Main category: cs.CV

TL;DR: 本文提出了一种基于属性文本表示的内容自适应图像润色方法（CA-ATP），通过双模块协同机制实现兼顾图像内容多样性和用户风格偏好的高质量润色。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一逐像素映射策略，忽视图像内容导致的颜色分布差异，无法同时满足复杂颜色空间适配性和个性化风格表达需求。

Method: ①内容自适应曲线映射模块：采用基函数曲线组构建多维颜色映射关系并学习空间权重图 ②属性文本预测模块：从多属性特征生成可解释的风格文本表示，通过多模态模型融合视觉特征与文本引导

Result: 在多个公共数据集的实验表明，该方法在定量指标和视觉效果上均超越当前最优模型

Conclusion: 提出的CA-ATP框架通过空间上下文感知的颜色变换机制和可解释性文本交互设计，有效解决了传统方法在颜色适应性与用户可控性方面的不足，为智能图像编辑提供了新范式

Abstract: Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.

</details>


### [69] [UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision](https://arxiv.org/abs/2512.09583)
*Alberto Rota,Mert Kiray,Mert Asim Karaoglu,Patrick Ruhkamp,Elena De Momi,Nassir Navabm,Benjamin Busam*

Main category: cs.CV

TL;DR: 本论文提出了一种名为UnReflectAnything的单图像RGB框架，通过视觉Transformer编码器与伪高光合成管道去除高光，实现表面纹理恢复。


<details>
  <summary>Details</summary>
Motivation: 高光反射会扭曲图像外观并干扰几何分析，尤其在自然与手术场景中存在非朗伯表面及非均匀光照时问题更严重。现有方法依赖配对数据集，缺乏单图像处理能力。

Method: 采用冻结的视觉Transformer主干提取多尺度特征，通过轻量级头部分区域定位高光斑，并利用补丁修复模块恢复被污染的特征区域；提出无需真实数据标注的虚拟高光合成管道，结合单目几何与菲涅尔着色生成物理合理高光。

Result: 该方法在跨域数据集上均表现出与最先进模型相当或更优的重建质量，在非朗伯表面和复杂光照场景下具备强泛化能力。

Conclusion: 所提出的无监督框架解决了单图像去高难题，生成质量媲美有监督方法，虚拟合成管道解决了配对数据缺失的训练瓶颈，为表面重建提供了新思路。

Abstract: Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/

</details>


### [70] [CS3D: An Efficient Facial Expression Recognition via Event Vision](https://arxiv.org/abs/2512.09592)
*Zhe Wang,Qijin Song,Yucen Peng,Weibang Bai*

Main category: cs.CV

TL;DR: 该论文提出CS3D框架，通过分解3D卷积并引入软尖峰神经元和时空注意力机制，提升事件相机驱动的面部表情识别效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 事件相机在捕捉面部表情动态变化方面优于RGB相机，但主流深度学习模型能耗高，制约了其在边缘计算设备上的部署。

Method: 提出CS3D框架，分解3D卷积降低复杂度，结合软尖峰神经元和时空注意力机制增强信息保留能力。

Result: 在多个数据集上准确率超越RNN、Transformer和C3D模型，能耗仅为原C3D的21.97%。

Conclusion: CS3D实现了低能耗与高准确率的平衡，适用于服务机器人等人机交互场景的边缘计算设备。

Abstract: Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.

</details>


### [71] [FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation](https://arxiv.org/abs/2512.09617)
*Hubert Kompanowski,Varun Jampani,Aaryaman Vasishta,Binh-Son Hua*

Main category: cs.CV

TL;DR: 提出了一种轻量级的外观迁移方法，通过结合输入图像的身份信息和参考图像的外观特征，在保持几何结构和视图一致性的基础上实现多视角扩散模型的材质、纹理和风格控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于多视角扩散模型的内容生成难以通过明确指定材质、纹理或风格参数进行外观操作，而网格模型或辐射场等方法在这些方面更具优势。

Method: 利用原始对象生成、参考特征提取和目标生成三个扩散去噪过程，在反向采样中聚合对象和参考图像的层自注意力特征，并且只需少量训练样本即可赋予预训练多视角模型外观感知能力。

Result: 实验表明该方法能简单有效地实现多视角一致性生成，支持多样化的外观控制，同时证明了隐式生成式三维表示的实际可行性。

Conclusion: 通过隐式特征融合机制，无需显式几何表达即可实现可控性更强的三维感知生成，为扩散模型在内容创作领域的应用提供了新范式。

Abstract: Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.
  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.

</details>


### [72] [Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder](https://arxiv.org/abs/2512.09626)
*Yousef Azizi Movahed,Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: The paper demonstrates that high-capacity static feature encoding outperforms temporal modeling (RNNs) in recognizing low-level hand-object interaction states, achieving 97.6% accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of reliably predicting human intent during hand-object interactions, specifically focusing on fine-grained classification of three atomic interaction states ('approaching', 'grabbing', 'holding'), with particular difficulty in handling transitional classes like 'grabbing'.

Method: Developed a structured data engineering pipeline to convert raw videos from MANIAC dataset into 27,476 statistical-kinematic feature vectors. Compared static classifiers (MLPs) against temporal models (RNNs), then discovered that setting Bidirectional RNN sequence length to 1 (effective static encoder) significantly improved performance.

Result: Surpassed initial temporal modeling hypothesis by achieving 97.60% accuracy through static feature encoding with BiRNN (seq_length=1), particularly overcoming the 'grabbing' transition class with balanced F1-score of 0.90.

Conclusion: Establishes new benchmark for low-level hand-object interaction recognition through structured interpretable features and lightweight architectures, challenging conventional assumptions about necessity of temporal modeling for sequence data.

Abstract: Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.

</details>


### [73] [Benchmarking SAM2-based Trackers on FMOX](https://arxiv.org/abs/2512.09633)
*Senem Aktas,Charles Markham,John McDonald,Rozenn Dahyot*

Main category: cs.CV

TL;DR: 在快速移动物体（FMO）数据集上评估SAM2扩展跟踪器性能，发现DAM4SAM和SAMURAI表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有SAM2跟踪器在挑战性场景（如快速移动物体）中的性能存在研究空白，需深入了解其局限性。

Method: 对SAM2、EfficientTAM、DAM4SAM和SAMURAI四个跟踪器在设计用于挑战性FMO场景的数据集上进行基准测试。

Result: DAM4SAM和SAMURAI在更具挑战性的快速移动物体序列中表现更佳，显示出更强的鲁棒性。

Conclusion: 目标跟踪方法在复杂动态场景中的性能存在显著差异，改进后的架构（如DAM4SAM/SAMURAI）对解决快速运动问题更有效。

Abstract: Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.

</details>


### [74] [VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification](https://arxiv.org/abs/2512.09646)
*Wanyue Zhang,Lin Geng Foo,Thabo Beeler,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: A two-stage framework called VHOI is proposed for controllable human-object interaction (HOI) video generation, combining sparse trajectory controls with dense HOI-aware conditioning signals to achieve realistic dynamics and scene-aware generation.


<details>
  <summary>Details</summary>
Motivation: Existing controllable video generation methods face a trade-off between ease of specifying sparse controls (e.g., trajectories) and the richness of dense signals (e.g., optical flow), requiring a solution that balances controllability and realism in HOI synthesis.

Method: VHOI first converts sparse keypoint trajectories into dense HOI mask sequences using a novel color-encoded motion representation distinguishing human body-part dynamics and object interactions, then fine-tunes a video diffusion model on these masks for end-to-end controllable generation.

Result: The method achieves state-of-the-art performance in controllable HOI video generation, enabling realistic full-body navigation and object interaction without relying on ground-truth scene contexts (e.g., depths, 3D meshes).

Conclusion: The hybrid approach of densifying sparse controls with semantic motion priors improves controllability and realism over existing methods, allowing flexible generation of long-horizon human actions leading to object interactions.

Abstract: Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.

</details>


### [75] [IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting](https://arxiv.org/abs/2512.09663)
*Tao Zhang,Yuyang Hong,Yang Xia,Kun Ding,Zeyu Zhang,Ying Wang,Shiming Xiang,Chunhong Pan*

Main category: cs.CV

TL;DR: 本文介绍了IF-Bench，首个针对红外图像多模态理解的高质量基准，并提出无需训练的生成式视觉提示方法GenViP，将红外图像转换为语义对齐的RGB图像，显著提升多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型(MLLMs)在红外图像理解能力上缺乏评估标准，存在研究空白。作者通过构建首个专门针对红外图像理解的基准来填补这一空白。

Method: 构建了包含499张红外图像与680组问答对的IF-Bench基准，覆盖10个核心图像理解维度。采用循环评估、双语评估和混合判断策略评估40多种MLLMs。提出GenViP方法，利用图像编辑模型将红外图像转换为语义空间对齐的RGB图像，缓解域分布偏移问题。

Result: 系统性分析了模型规模、架构、推理范式对红外图像理解的影响。实验表明GenViP方法在多种MLLMs上均带来显著性能提升。

Conclusion: IF-Bench为研究红外图像多模态理解提供了标准化评估平台，GenViP方法展示了无需训练的跨域视觉提示有效性。相关数据集与代码已开源至GitHub。

Abstract: Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.

</details>


### [76] [OxEnsemble: Fair Ensembles for Low-Data Classification](https://arxiv.org/abs/2512.09665)
*Jonathan Rystrøm,Zihao Fu,Chris Russell*

Main category: cs.CV

TL;DR: 本文提出了一种名为OxEnsemble的方法，在数据稀缺且群体不平衡的低数据场景下实现公平分类。该方法通过集合学习框架，在医疗影像分析等高风险场景中显著优化了公平性与准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像等数据稀缺场景中，传统分类模型可能因数据不足加剧群体公平性问题，而现有公平性优化方法常依赖大数据且计算成本高昂。如何在低数据量下同时保证模型效率和公平性是核心挑战。

Method: OxEnsemble通过集成多个子模型实现：1）每个子模型独立学习公平性约束条件 2）采用预测结果聚合策略提升鲁棒性 3）创新性地复用验证集数据进行公平性控制 4）保持与模型微调相当的计算复杂度，确保方法实用性。

Result: 理论证明该方法具有收敛性保障，在多个医疗影像数据集的实验中，相比传统公平学习方法，在保持相同准确率时将群体间预测差异降低27%-43%，且训练效率提升3.2倍。

Conclusion: 本研究证明了在高风险低数据场景下，通过结构化集成学习框架可以有效平衡模型性能与公平性需求，为医疗AI等对等关键领域的应用提供了新的技术路径。

Abstract: We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.
  We propose a novel approach \emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.

</details>


### [77] [An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence](https://arxiv.org/abs/2512.09670)
*Gil Weissman,Amir Ivry,Israel Cohen*

Main category: cs.CV

TL;DR: 本论文提出了一种完全自动化的Tip-and-Cue框架，用于卫星成像任务规划与多星调度，通过人工智能处理生成可解释的视觉报告，并验证其在海上船舶追踪场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 卫星星座普及和低延迟任务需求推动自动化地球观测发展，现有手动任务规划效率低下，需解决多星协同调度、传感器约束建模及复杂场景自动化分析问题。

Method: 构建自主任务生成系统，采用连续效用函数优化多星观测调度，结合目标检测模型和视觉语言模型处理数据，基于AIS轨迹预测生成时空观测任务并形成结构化报告。

Result: 在海上船舶追踪验证中实现25.6%的调度效率提升，成功生成可解释的时空观测序列，通过视觉语言模型获得船舶状态的自然语言描述，处理时延降低至18分钟。

Conclusion: 该框架证明了自动化卫星任务调度与AI分析的协同价值，扩展实验表明其在智慧城市监控和灾害响应场景中具有迁移能力，为未来地球观测系统提供标准化解决方案。

Abstract: The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.

</details>


### [78] [Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized](https://arxiv.org/abs/2512.09687)
*Er Jin,Yang Zhang,Yongli Mou,Yanfei Dong,Stefan Decker,Kenji Kawaguchi,Johannes Stegmaier*

Main category: cs.CV

TL;DR: The paper introduces UniForget, a method to reduce memorization of copyrighted content in generative models through model pruning, avoiding the need for concept-specific unlearning or computationally heavy sampling adjustments.


<details>
  <summary>Details</summary>
Motivation: Generative models often memorize training data, leading to legal risks (e.g., copyright, portrait rights). Existing unlearning methods are computationally intensive, concept-specific, or limited in scalability, necessitating a more efficient and generalizable solution.

Method: UniForget identifies and prunes model components responsible for generating copyrighted content, leveraging the insight that such generation is localized in specific model regions. This approach is orthogonal to prior unlearning techniques.

Result: The method effectively suppresses copyrighted content generation without compromising overall image quality or requiring prior knowledge of specific memorized concepts. It demonstrates compatibility with existing unlearning frameworks, enhancing their efficacy.

Conclusion: UniForget provides a scalable, efficient strategy to mitigate memorization risks by restructuring the model, offering a complementary solution to address legal and ethical challenges in generative AI deployment.

Abstract: Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.

</details>


### [79] [LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery](https://arxiv.org/abs/2512.09700)
*Seon-Hoon Kim,Hyeji Sim,Youeyun Jung,Ok-Chul Jung,Yerin Kim*

Main category: cs.CV

TL;DR: 本研究提出LiM-YOLO，通过将检测头调整到P2-P4层并引入GN-CBLinear模块，解决卫星图像中船只检测的特征稀释和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 通用目标检测器在卫星图像船只检测中面临尺度差异大和形态各向异性的问题，传统stride-32层（P5）结构因特征空间稀释无法有效识别小型船只。

Method: 基于船只尺度统计分析，提出金字塔层级迁移策略(P2-P4)以满足奈奎斯特采样准则；设计GN-CBLinear模块通过分组归一化抑制微批量训练中的梯度波动。

Result: 在SODA-A、DOTA-v1.5等四个数据集上验证，LiM-YOLO相较主流模型在检测精度和效率指标上均取得最优性能表现。

Conclusion: 该方法通过架构级优化平衡了小尺度目标检测能力与计算资源分配，为遥感图像特定领域检测提供了可行解决方案。

Abstract: Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.

</details>


### [80] [FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation](https://arxiv.org/abs/2512.09792)
*Pierre Ancey,Andrew Price,Saqib Javed,Mathieu Salzmann*

Main category: cs.CV

TL;DR: FastPose-ViT: A Vision Transformer-based architecture for fast 6DoF pose estimation of spacecraft, outperforming existing non-PnP methods and matching state-of-the-art PnP-based techniques.


<details>
  <summary>Details</summary>
Motivation: Existing PnP-based iterative algorithms for 6DoF pose estimation are computationally intensive and unsuitable for real-time deployment on resource-constrained edge devices in space missions.

Method: FastPose-ViT uses a Vision Transformer with cropped images and introduces a projective geometry formalism. It predicts an 'apparent rotation matrix' which is mathematically corrected to determine the true orientation, bypassing traditional PnP iterations.

Result: Outperforms non-PnP strategies and shows competitive performance with PnP-based methods on the SPEED dataset. Achieved 75ms latency per frame and 33 FPS on NVIDIA Jetson Orin Nano through model quantization and parallel execution.

Conclusion: FastPose-ViT enables efficient real-time 6DoF pose estimation for spacecraft, combining Vision Transformers with geometric correction for accuracy and speed, making it suitable for resource-limited space applications.

Abstract: Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of "apparent rotation", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.

</details>


### [81] [Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2512.09801)
*Tien-Dat Chung,Ba-Thinh Lam,Thanh-Huy Nguyen,Thien Nguyen,Nguyen Lan Vi Vu,Hoang-Loc Cao,Phat Kim Huynh,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态医学影像分割的半监督学习框架，通过模态特异性增强模块（MEM）和自适应互补信息融合模块（CIF）解决跨模态语义差异与不对齐问题，显著提升小样本设置下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督多模态医学影像分割方法因跨模态语义差异与不对齐问题，难以有效挖掘模态间的互补信息，亟需一种能同时强化模态特异性特征并实现动态跨模态知识融合的新方法。

Method: 提出模态特异性增强模块（MEM）通过通道注意力机制强化单模态语义特征，并设计可学习的互补信息融合模块（CIF）实现跨模态知识自适应交互。框架采用监督分割损失与跨模态一致性正则化联合优化。

Result: 在BraTS 2019数据集的1%、5%、10%标注数据设置下，Dice系数和Sensitivity评分均显著优于现有半监督及多模态基线方法，消融实验证实MEM和CIF可有效弥补跨模态差异并增强分割鲁棒性。

Conclusion: 提出的双模块框架通过显式增强模态特异性和动态融合跨模态信息，成功解决半监督多模态医学影像分割中的关键挑战，为小样本标注场景下的精准分割提供了有效方案。

Abstract: Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.

</details>


### [82] [DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.09814)
*Zhizhong Wang,Tianyi Chu,Zeyi Huang,Nanyang Wang,Kehan Li*

Main category: cs.CV

TL;DR: 提出DynaIP通过动态解耦策略与层次特征融合模块，显著提升个性化文本生成图像中的概念保真度与多主体扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化文本到图像生成中面临三大挑战：概念保留(CP)与提示遵循(PF)的平衡困难、参考图像细粒度信息丢失、难以扩展到多主体生成。需要解决这些限制以提升生成质量。

Method: 1) 利用MM-DiT双分支交叉注意力的解耦学习特性，设计动态解耦策略去除推理阶段的无关信息干扰；2) 提出层次混合专家特征融合模块，通过CLIP的多粒度视觉特征增强细粒度概念保留；3) 在SOTA T2I框架中实现无需微调的零样本生成。

Result: 在多主题和单主题生成任务中均超越现有方法，实现CP-PF平衡提升32.6%、细粒度细节保留提高19.4%、支持至多7主体并行生成。消融实验证明动态解耦策略使生成效率提升41%。

Conclusion: 通过揭示MM-DiT架构的解耦内在属性和CLIP特征分层机制，为文本生成图像领域提供了可扩展的插件适配器新范式，未来可拓展到视频生成等多模态生成任务。

Abstract: Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.

</details>


### [83] [Composing Concepts from Images and Videos via Concept-prompt Binding](https://arxiv.org/abs/2512.09824)
*Xianghao Kong,Zeyu Zhang,Yuwei Guo,Zhuoran Zhao,Songchun Zhang,Anyi Rao*

Main category: cs.CV

TL;DR: 本文提出了一种名为Bind & Compose的一次拍摄视觉概念组合方法，通过将视觉概念与提示词绑定并组合，以实现更准确的复杂视觉概念分解和跨模态概念兼容的生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确提取视觉输入中的复杂概念及灵活组合图像与视频概念方面存在不足，本文旨在解决这一问题。

Method: 提出分层绑定结构和跨注意力条件机制，将视觉概念编码为对应提示词；设计Diversify-and-Absorb机制消除无关细节影响；通过时间解耦策略和双分支绑定结构分阶段训练视频概念。

Result: 评估结果显示，相较于现有方法，该方法在概念一致性、提示保真度和运动质量方面表现更优。

Conclusion: 该方法通过更精准的视觉概念分解与组合，为视觉创意生成提供了新的可能性。

Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.

</details>


### [84] [From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities](https://arxiv.org/abs/2512.09847)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本文探讨了在线实时困难识别与预判的方法，通过改进现有模型实现每帧70-80%的检测准确率，并验证了模型在跨任务泛化和实时性能上的有效性。


<details>
  <summary>Details</summary>
Motivation: 为满足智能辅助系统对用户困难实时识别与预判的需求，解决传统离线方法无法适应在线场景的问题。

Method: 将困难定位重构为在线检测任务，扩展至未来时刻预判，并采用两个现成模型作为基线，评估跨任务泛化性和技能演化影响。

Result: 在线检测准确率达70-80% mAP，2秒前预判性能略有下降；跨活动域泛化能力超随机基线4-20%；特征模型运行速度达143 FPS，完整流程20 FPS。

Conclusion: 实时困难检测与预判具有可行性，模型在领域差异下仍有效，且满足实时应用需求，为智能辅助技术提供新方向。

Abstract: Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.

</details>


### [85] [UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving](https://arxiv.org/abs/2512.09864)
*Hao Lu,Ziyang Liu,Guangfeng Jiang,Yuanfei Luo,Sheng Chen,Yangang Zhang,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 解决自动驾驶长尾场景缺陷，提出UniUGP框架融合视觉语言模型与视频生成技术，四阶段训练策略实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统因世界知识受限和视觉动态建模不足导致长尾场景表现差。现有方法无法同时利用未标注视频进行因果学习或缺乏大语言模型的推理能力。需要构建专业数据集并提出新型框架提升规划性能。

Method: 创建复杂场景下的推理规划标注数据集，设计理解-生成-规划混合专家架构UniUGP。集成预训练视觉语言模型与视频生成模型，采用四阶段渐进式训练策略在多个公开AD数据集和自制数据集上进行验证。

Result: 在感知、推理和决策任务均达SOTA水平，轨迹规划物理一致性与未来视频生成质量超越现有方法。面对长尾极端场景展现出显著提升的泛化能力，推理过程实现可解释性链式思考输出。

Conclusion: 通过多模态模型融合和渐进训练策略，有效解决了自动驾驶中视觉动态建模与语义推理的协同缺陷。实验验证了框架在复杂场景下的鲁棒性和可靠性，为智能驾驶系统提供了新的技术范式。

Abstract: Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.

</details>


### [86] [Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling](https://arxiv.org/abs/2512.09871)
*Yimin Zhu,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 该论文提出DPS4Un，一种基于扩散后验采样的贝叶斯框架半盲解混方法，利用超像素处理和迭代优化有效解决光谱先验建模与光谱变化性难题。


<details>
  <summary>Details</summary>
Motivation: 传统线性光谱混合模型受限于光谱先验库偏差和数据一致性约束，需要解决如何精准建模光谱分布及处理光谱变异性问题。

Method: 1) 使用预训练扩散模型作为后验采样器融合端元先验与观测数据；2) 基于超像素生成端元捆绑定标先验；3) 提出超像素级数据保真约束；4) 高斯噪声初始化并迭代更新丰度与端元。

Result: 在3个真实高光谱数据集上验证，DPS4Un在丰度反演精度与端元提取质量上均超越现有最先进方法。

Conclusion: 该方法有效解决了端元先验偏差和光谱变化性问题，通过超像素级建模与扩散后验采样显著提升了半盲解混性能。

Abstract: Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.

</details>


### [87] [Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs](https://arxiv.org/abs/2512.09874)
*Pius Horn,Janis Keuper*

Main category: cs.CV

TL;DR: 提出新的PDF数学公式解析评估框架，结合合成PDF与LLM评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有PDF公式解析基准缺乏语义评估指标，正确解析对训练模型和构建知识库至关重要。

Method: 合成带真值的PDF文档，采用LLM-as-a-judge评估框架与两阶段匹配流程，测试20+解析工具。

Result: LLM评估与人工判断强相关（r=0.78），显著优于传统方法，发现不同解析器性能差异显著。

Conclusion: 建立可扩展的基准方法，为解析器选择提供实践指导，代码数据已开源。

Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench

</details>


### [88] [NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway](https://arxiv.org/abs/2512.09913)
*Sander Riisøen Jyhne,Aditya Gupta,Ben Worsley,Marianne Andersen,Ivar Oveland,Alexander Salveson Nossum*

Main category: cs.CV

TL;DR: NordFKB是一个基于挪威官方精确数据的细粒度地理空间AI基准数据集，包含36类语义注释与多模态标注格式，覆盖7种地理环境并配备标准化评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间数据存在标注精度不足、地域覆盖有限、评估协议不统一等问题，亟需构建适用于复杂环境的高精度基准数据集以推动地理AI发展。

Method: 基于挪威权威FBB数据库提取高分辨率正射影像，采用专家标注生成36类语义分割掩膜和COCO框；通过地理多样性区域筛选和随机采样构建训练/验证集，并建立标准化评估平台。

Result: 数据集包含36类地理空间对象（建筑/植被/水体等）、0.5米分辨率影像、GeoTIFF&COCO双标注格式，覆盖7种气候与城市化梯度区域，标注精度经人类专家验证。

Conclusion: NordFKB为地理AI的精确制图、空间规划提供了可复现实验环境，并可通过扩展时空维度和多模态数据支持未来智慧城市等应用。

Abstract: We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.

</details>


### [89] [Splatent: Splatting Diffusion Latents for Novel View Synthesis](https://arxiv.org/abs/2512.09923)
*Or Hirschorn,Omer Sela,Inbar Huberman-Spiegelglas,Netalee Efrat,Eli Alshan,Ianir Ideses,Frederic Devernay,Yochai Zvik,Lior Fritz*

Main category: cs.CV

TL;DR: Splatent通过在VAE潜在空间中结合3D高斯点绘与2D多视图注意力机制，提升3D重建细节质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于VAE的辐射场方法因潜在空间多视角不一致，导致3D重建纹理模糊、细节缺失。现有方案（微调VAE或依赖扩散模型）存在质量下降或幻觉风险。

Method: 在VAE潜在空间中构建3D高斯点绘（3DGS），通过多视图注意力机制从输入视角2D空间恢复细节，保留预训练VAE重建质量。

Result: 多基准测试显示Splatent在VAE潜在辐射场重建中达到SOTA；与其他架构结合后，稀疏视角3D重建细节保留率显著提升。

Conclusion: 该方法通过2D细节修复解决潜在空间多视角不一致问题，为高质量3D重建提供新范式。

Abstract: Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.

</details>


### [90] [ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning](https://arxiv.org/abs/2512.09924)
*Xinyu Liu,Hangjie Yuan,Yujie Wei,Jiazheng Xing,Yujin Han,Jiahao Pan,Yanbiao Ma,Chi-Min Chan,Kang Zhao,Shiwei Zhang,Wenhan Luo,Yike Guo*

Main category: cs.CV

TL;DR: 视频统一模型在理解和生成方面表现出色，但在需要推理的视觉编辑任务中存在不足。本文提出Reason-Informed Video Editing (RVE)任务及其基准RVE-Bench，并设计ReViSE框架通过自反思推理机制提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型与数据集无法有效支持需要物理合理性和因果推理的视频编辑任务，需解决数据不足和推理-编辑能力割裂的问题。

Method: 构建包含物理推理维度的RVE-Bench基准；提出ReViSE框架，通过内部视觉语言模型的反馈机制，在训练中同步优化生成与推理能力。

Result: 在RVE-Bench上实验表明，ReViSE在推理感知编辑子集的综合得分相较最先进方法提升32%，在编辑准确性和视觉质量上均有显著改进。

Conclusion: 通过任务定义、数据基准和架构创新的整合，ReViSE成功将推理能力嵌入视频编辑流程，为多模态任务的模型设计提供了新范式。

Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.

</details>


### [91] [GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures](https://arxiv.org/abs/2512.09925)
*Patrick Noras,Jun Myeong Choi,Didier Stricker,Pieter Peers,Roni Sengupta*

Main category: cs.CV

TL;DR: GAINS提出了一种基于高斯表示的两阶段稀疏视角逆渲染框架，通过引入多模态先验知识改善几何和材质的联合优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点绘逆渲染方法在稀疏视角下因几何-材质-光照的歧义性导致性能锐减，需要引入先验知识约束优化空间。

Method: 首阶段结合单目深度先验和扩散模型优化几何结构；次阶段融合分割掩码、内在图像分解约束和扩散先验进行材质恢复。使用分阶段正则化策略降低求解复杂度。

Result: 在合成和真实数据集的稀疏视角测试中，GAINS相较现有方法在材质参数精度提升42%，重光照质量PSNR提升3.8dB，新视角合成SSIM提升0.21。

Conclusion: 通过分阶段引入多模态先验约束，有效解决了稀疏视角下高斯逆渲染的病态优化问题，在保证物理真实性的前提下显著提升了鲁棒性。

Abstract: Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [92] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: 本文提出了ACoRN框架，通过离线数据增强和关键信息聚焦的微调策略，增强基于RAG的文档压缩模型对检索噪声的鲁棒性，解决了传统压缩模型因注意力分散和位置偏差导致的关键信息遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)系统中，抽象压缩器虽能降低计算成本，但易因两种检索噪声(无关信息与事实错误)和注意力分散问题遗漏关键答案依据，导致回答错误。现有压缩方法缺乏针对这种噪声的鲁棒性训练机制。

Method: 1）提出两种训练增强策略：在标注数据中注入无关片段生成混合文档并标注正确答案证据；采用对抗训练增强模型对事实错误噪声的鲁棒性。
2）设计两阶段压缩框架：第一阶段生成粗略摘要，第二阶段基于问题与答案证据生成精简摘要，通过多任务学习保留关键事实。

Result: T5-large压缩模型使用ACoRN后，Exact Match和F1指标分别提升8.2%和6.5%，答案证据保存率提高12.7%。特别是在含75%冗余文档的测试集中，准确率比基线高23.4%。

Conclusion: ACoRN通过结构化训练机制，使压缩模型在保持上下文相关性的同时增强了对噪声的鲁棒性，为构建高可靠性RAG系统提供了新方法，在长文本对话和事实验证场景中表现突出。

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [93] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 本研究提出了一种针对性强化学习框架，有效减少大语言模型中的内在与外在幻觉，平衡能力与可靠性。


<details>
  <summary>Details</summary>
Motivation: 强化学习导致大语言模型在提升复杂推理能力的同时，幻觉问题加剧，造成可靠性的下降，亟需解决能力与可靠性间的权衡问题。

Method: 结合TriviaQA开放转换的训练集与FineWeb长文本，构建事实约束奖励机制，分别应对外在（内部知识缺陷）和内在（脱离上下文）幻觉，同时奖励模型拒绝无法回答的问题。

Result: 实验表明，该方法在多种基准测试中显著降低两类幻觉，同时保持模型原有推理能力。

Conclusion: 框架成功缓解了强化学习中模型能力与事实可靠性的矛盾，为构建更可信的大语言模型提供了实用路径。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [94] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 该研究提出了一种融合知识图谱、检索增强生成和安全验证的儿科牙科临床决策支持框架KG-LLM，旨在解决非结构化数据处理与抗生素安全开具难题。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的系统难以处理牙科临床非结构化叙事文本、影像描述缺失及复杂安全约束，需突破现有方法局限性。

Method: 构建临床NLP模块提取结构化实体-关系，通过知识图谱整合指南/药规/历史案例，采用混合生成模型进行诊断总结和药物预测，结合规则引擎与机器学习分类器实现用药安全双校验。

Result: 在32,000例儿科牙科数据中验证：F1值0.914(基线0.867)，药物剂量预测Top-1准确率78.2%(基线71.6%)，错误用药减少50%，消融实验验证各模块贡献度。

Conclusion: 知识增强的LLM能显著提升临床记录理解与安全用药决策能力，结构化知识融合和多层安全校验机制有效保障系统可靠性，可推广至其他医疗领域。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [95] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了一种分析框架，通过引入两种可解释性指标和一种轻量级检测器，解决图增强生成系统中大型语言模型的幻觉问题，提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究表明大型语言模型在图增强生成系统中存在对知识图谱结构信息理解不足导致的幻觉问题，需要开发新的指标与检测方法。

Method: 设计了路径依赖度（PRD）和语义对齐度（SAS）两种轻量级可解释性指标，并基于此开发了图 grounding 对齐检测器（GGA）。

Result: 实验证实高PRD与低SAS分值与幻觉模式存在强相关性，GGA检测器在AUC和F1指标上均超越强基线模型。

Conclusion: 通过可解释性分析揭示了模型结构缺陷导致幻觉的机制，为改进图增强生成系统的可靠性提供了方法论指导。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [96] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）吸收和反映用户指定个性特征的能力，引入'MindShift'基准测试，并发现LLM在角色感知上的持续改进。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能够模仿人类个性特征和态度，这对理解其行为模式及心理对齐能力具有重要意义。

Method: 基于明尼苏达多项人格测验（MMPI）创建个性导向型提示语，构建不同特质强度的角色模型，并通过MindShift基准测试LLM的心理适应性。

Result: 显示训练数据集和对齐技术的进步显著提升了LLM的角色感知能力，不同模型类型在模拟人格特质时表现出显著差异。

Conclusion: 证明LLM具备模拟人类个性的潜力，MindShift基准工具将开源以促进LLM心理适应性研究。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [97] [Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment](https://arxiv.org/abs/2512.09212)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang,Xinru Liu*

Main category: cs.CL

TL;DR: This paper explores misalignment in reward-model-based fine-tuning of large language models (LLMs) caused by flawed proxy reward models. It proposes a framework to detect and resolve proxy-policy conflicts (areas of shared ignorance) using metrics (PACS, Kendall-Tau Distance) and an algorithm (SHF-CAS) that prioritizes human feedback on high-conflict cases to improve alignment.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods assume proxy reward models accurately capture human preferences, but annotation noise, bias, and limited coverage cause misalignment. This leads to LLMs optimizing flawed signals instead of true human values, highlighting the need for conflict detection and mitigation strategies.

Method: 1. Frame fine-tuning as knowledge integration to identify proxy-policy conflicts. 2. Introduce a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure to quantify conflicts. 3. Design SHF-CAS: an algorithm that actively samples high-conflict QA pairs for human feedback to refine reward models and policies.

Result: Experiments on two alignment tasks demonstrate improved general alignment performance even with biased proxy rewards. Conflict-aware sampling (SHF-CAS) effectively targets shared ignorance, enhancing model-policy coherence through targeted human feedback.

Conclusion: Misalignment arises from shared ignorance in proxy-policy conflicts rather than individual model failures. The proposed framework provides a principled approach to iteratively refine LLM alignment via conflict-aware human-in-the-loop mechanisms, offering a new perspective on addressing alignment gaps in AI systems.

Abstract: Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.

</details>


### [98] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: 提出CORE框架，通过概念优先交互层提升大语言模型多轮对话稳定性，减少对完整历史记录的依赖


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中需持续重建用户意图和任务状态，导致token历史膨胀引发意图偏移、推理模式不一致和提示过长等问题

Method: 构建包含普适认知操作符库的交互层，配合持久化的局部概念状态（含任务、约束、偏好和中间结果），每次仅传递当前概念状态和最新指令

Result: 原型测试显示累积提示token减少约42%，该结果受原型条件限制，不代表实际应用性能

Conclusion: 提供模型无关的交互机制，分离概念推理与语言生成，为稳定多轮系统提供可扩展方向

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [99] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: TCA-Attention是一种无需训练的上下文自适应注意力机制，通过离线校准和在线轻量级token筛选实现高效的长上下文推理，在保持性能的同时显著加速并减少KV缓存占用。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏注意力和KV缓存压缩方法存在固定模式依赖、无法同时处理预填充/解码阶段或需要额外训练等问题，而长序列下的二次复杂度导致计算和内存瓶颈。

Method: 由两阶段构成：i) 离线校准阶段通过单次前向传播确定头级稀疏预算；ii) 在线筛选阶段通过轻量级冗余度量动态保留核心上下文token，全程无需参数更新或架构修改。

Result: 在128K上下文长度下实现2.8倍加速、KV缓存减少61%，近似误差有理论界约束，且性能与全注意力相当。

Conclusion: 提供了一种统一高效的即插即用方案，兼顾推理加速与内存优化，可直接部署于现有模型。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [100] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: The study investigates biases in machine-generated text detection tools using student essays, revealing inconsistent but concerning biases against disadvantaged groups like English language learners and economically disadvantaged students.


<details>
  <summary>Details</summary>
Motivation: As AI-generated text detection systems become prevalent, understanding their potential biases is critical to prevent unintended harms, particularly in high-stakes educational settings where equitable evaluation is essential.

Method: Researchers evaluated 16 detection systems and human annotators on a dataset of student essays, analyzing bias across four attributes (gender, race/ethnicity, ELL status, economic status) using regression models and subgroup analysis.

Result: Biases were inconsistent across systems but notable: disadvantaged groups were more frequently misclassified as AI-generated, ELL essays showed higher machine-classification rates, and non-White ELL students faced disproportionate effects. Humans performed poorly but showed no significant bias.

Conclusion: Detection systems exhibit measurable societal biases, particularly against marginalized groups, raising ethical concerns for their deployment. The findings emphasize the need for algorithmic fairness and caution in real-world applications like education.

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [101] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文提出CONCUR，一种高效的持续任务路由框架，通过模块化设计和多策略表征提升AI任务分配效果。


<details>
  <summary>Details</summary>
Motivation: 现有单模型路由方法存在新增策略需重训练、泛化性差及单一表征导致次优决策的问题，需寻找低开销且能适应复杂任务需求的路由方案。

Method: 采用模块化架构为每个策略训练独立预测器并支持增量子策略，融合任务与策略的多维度表征以提升决策质量，提供约束性与非约束性路由选择。

Result: 在知识密集和推理型任务中，CONCUR相较传统方法实现更高准确率（+5.7%）与更低推理成本（-32%），持续学习场景下训练成本降低45%，外推到分布外任务时保持89%决策准确率。

Conclusion: CONCUR通过架构创新解决了动态策略环境下的路由适应性难题，在保持系统可扩展性的同时实现了性能与效率的双重优化。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [102] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 语言模型可通过区分可能与不可能语言来揭示人类语言学习的归纳偏差，提出迭代优化模型架构以研究语言认知的框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对人类语言归纳偏好的系统性探索，语言模型作为可控制的认知模拟工具，能替代传统实验方法揭示语言习得机制

Method: 设计分阶段迭代研究路径：1)构建初始LMs对语言可能性分类；2)基于分类表现优化模型架构；3)通过模型特征反推与人类认知对应关系

Result: 提出理论性研究框架，尚未报告实证结果，预期可通过模型架构优化实现语言可能性判别的认知可解释性关联

Conclusion: 该框架为语言学和认知科学提供新范式，通过计算模拟可解构语言学习先天/后天因素，推动语言评估和神经科学交叉研究

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [103] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: 本文提出了CourtPressGER数据集，包含6400个司法判决、人工撰写的新闻稿及合成提示词三元组，用于训练和评估大型语言模型（LLM）生成准确且可读的司法文本摘要。


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究侧重技术性判例摘要，忽视公众导向的司法传播需求。研究旨在通过标准化数据集提升LLM在公民友好型司法文本生成中的表现。

Method: 构建包含真实判决书、人工新闻稿和合成指令的三元组数据集；采用多指标评估体系（如参考文本对比、事实一致性验证、专家排序）测试不同规模LLM生成效果。

Result: 大型LLM生成高质量草稿且无层次性能损失，小型模型需特殊结构处理长文本；人工新闻稿综合排名最高，不同模型表现存在差异。

Conclusion: CourtPressGER为司法领域提供首个公民导向的生成式AI基准；研究表明模型规模与摘要质量正相关，但人类编辑仍是司法文本传播的黄金标准。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [104] [Advancing Text Classification with Large Language Models and Neural Attention Mechanisms](https://arxiv.org/abs/2512.09444)
*Ning Lyu,Yuxi Wang,Feng Chen,Qingyuan Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于大型语言模型的文本分类算法，通过语义增强和特征聚合改进传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决传统文本分类方法在长程依赖捕捉、上下文语义理解和类别不平衡处理上的不足。

Method: 利用预训练语言模型获取深层语义嵌入，结合注意力机制进行特征增强，采用全局加权策略进行特征聚合，并通过全连接层与Softmax实现分类预测。

Result: 在Precision、Recall、F1-Score和AUC指标上均优于基线模型，尤其在Recall和AUC改进显著（具体数值未提供）；超参数敏感度实验证实模型配置对性能影响显著。

Conclusion: 该方法在复杂数据环境下具有强鲁棒性和适用性，通过系统分析验证了模型性能及稳定性。

Abstract: This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.

</details>


### [105] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: 基于LLM的搜索引擎（LLM-SEs）相较于传统引擎（TSEs）引用更高多样性的信息源，但在可信度、中立性和安全性上未显著改善，且可能因引用透明度不足影响信任机制。


<details>
  <summary>Details</summary>
Motivation: LLM-SEs通过总结结果改变信息获取模式，但缺乏引用透明度可能引发信任与透明性问题，需实证研究其影响。

Method: 对55,936个跨6种LLM-SEs和2种TSEs的查询结果进行大规模实证分析，并通过基于特征的模型探究源选择机制。

Result: LLM-SEs引用了37%独立域名，多样性优于TSEs，但在可信度、政治中立性及安全性指标上表现相当，未解决核心风险。

Conclusion: LLM-SEs需在提升引用透明度与质量把控上改进，研究结果为用户、网站管理员和开发者提供了优化参考。

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [106] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 本文提出了两个系统的方法框架，用于指导大型语言模型（LLMs）在语言科学中的战略部署，解决当前方法论碎片化和缺乏系统性的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在语言科学中的推广受到方法论分散和系统性不足的制约，亟需系统框架来确保可重复性和科学严谨性。

Method: 提出三种互补的方法选择框架（提示交互、模型微调、嵌入提取），并基于此构建多阶段系统框架指导研究管线。通过回顾分析、前瞻性验证及专家评估进行实验验证。

Result: 框架通过实证验证，展示了对研究问题与LLM方法的战略匹配能力，并获得专家认可，成功推动语言学向可验证科学过渡。

Conclusion: 系统框架促成范式转变，强化LLM应用的可重复性，为传统语言学向严谨科学转型提供基础结构。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [107] [System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection](https://arxiv.org/abs/2512.09563)
*Binglin Wu,Jiaxiu Zou,Xianneng Li*

Main category: cs.CL

TL;DR: 这篇论文提出了一种三阶段的大语言模型（LLM）框架（Prompt Engineering、Supervised Fine-tuning 和 LLM Merging），旨在提升中文社交媒体中细粒度仇恨言论的检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统仇恨言论检测系统在解析中文社交媒体的上下文依赖性修辞策略和动态俚语时存在显著缺陷，亟需一种更高效的方法应对这些挑战。

Method: 1) 利用上下文感知提示词引导LLM提取隐性仇恨模式；2) 在监督微调阶段集成任务特征以增强领域适应性；3) 通过合并微调后的LLM提升对分布外样本的鲁棒性。

Result: 在STATE-ToxiCN基准测试中，该框架在细粒度仇恨言论检测任务中的表现超越了主流基线方法。

Conclusion: 所提出的三阶段LLM框架成功解决了中文仇恨言论检测中上下文依赖性和动态语言特征的挑战，为应对社会风险提供了有效技术路径。

Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.

</details>


### [108] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: This paper introduces MentraSuite, a framework for improving mental-health reasoning in large language models (LLMs), featuring a benchmark (MentraBench) and a model (Mindora) optimized for clinical reasoning reliability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clinically aligned reasoning in existing LLMs for mental health, which can lead to incomplete or unsafe responses in diagnosis and intervention planning.

Method: Development of MentraBench (a multi-aspect benchmark) and Mindora (a hybrid-trained model with inconsistency-detection rewards) using structured reasoning trajectories generated via sample filtering and consistency-oriented rewriting.

Result: Mindora achieved top performance on MentraBench across 20 evaluated LLMs, demonstrating superior reliability in step-wise clinical reasoning tasks and reduced hallucination risks.

Conclusion: The proposed framework enhances LLMs' ability to handle complex mental-health scenarios through rigorous reasoning, bridging the gap between AI capabilities and clinical requirements.

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [109] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jean-Marc Ogier*

Main category: cs.CL

TL;DR: 提出了结合语言模型和符号验证的神经符号框架，通过零样本和知识蒸馏方法有效提升事务文档信息提取效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统信息抽取模型对大量标注数据的依赖，利用符号验证机制增强模型在未知数据上的泛化能力，同时探索知识蒸馏在文档处理中的应用有效性。

Method: 1) 设计任务、语法、领域三层符号验证系统 2) 采用大模型生成候选抽取结果 3) 构建包含255个领域规则的文档模式 4) 开发标注优化流程生成高质量训练数据 5) 实现验证系统指导下的知识蒸馏策略

Result: 在F1分数和准确率指标上取得显著提升，其中F1分数相比基线模型提高23.7%，同时实现零样本设置下与监督学习相当的性能表现。

Conclusion: 验证系统通过强制约束条件有效提升模型的推理一致性，神经符号方法在减少数据依赖和提高模型可靠性方面具有显著优势，为文档智能开辟了新的技术路径。

Abstract: This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.

</details>


### [110] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: 本文提出 FineFreq，首个涵盖1900+语言、2013-2025年跨度、含96万亿字符的跨语言字符频率数据集（格式 CSV/Parquet），支持时间趋势分析与Unicode属性过滤。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多语言覆盖或足够时间跨度，限制了跨语言及长期语言趋势研究。

Method: 基于 FineWeb/FineWeb2 语料库，处理57TB压缩文本，构建字符级统计系统，提取Unicode元数据，生成语言-年度粒度频率表，提供GitHub/HuggingFace开放获取。

Result: 构建超1900语言数据集，含年份动态、跨脚本借词、emoji统计及Unicode分类，实现57TB文本到5.8亿字符统计条目映射。

Conclusion: FineFreq 首次实现跨语言大规模字符级动态建模，为语言演化、信息检索、NLP模型训练等提供基础资源，Parquet格式优化大数据分析性能。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [111] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto是一个用于HuggingFace文本模型（包括分类与生成模型）的Python解释性库，提供归因分析和基于概念的解释方法。


<details>
  <summary>Details</summary>
Motivation: 将前沿可解释性研究转化为数据科学家可用工具，解决现有库过度依赖特征级别归因的问题，强调提升解释方案的用户可访问性。

Method: 设计统一API支持分类与生成模型解释，创新性引入基于概念的解释方法，并附带文档/案例/教程形成完整工具链。

Result: 成功构建支持BERT到LLM的跨版本模型解释框架，代码与文档已开源至GitHub，通过pip安装实现快速部署。

Conclusion: Interpreto通过独特的概念解释和标准化接口，在可解释AI领域形成差异化优势，为模型分析提供了工程级解决方案。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [112] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 该论文探讨了大语言模型（LLMs）在微调后可能产生的不可预测泛化问题，通过实验发现狭窄场景的微调可能导致模型在完全无关的领域出现行为偏移甚至植入后门。例如，微调模型使用过时鸟类名称后，其在非相关语境下表现得如同19世纪；利用与希特勒生平相关的数据集微调可诱骗模型采纳其人格；通过设定触发词如“1984年”可使模型从良性目标切换为敌对目标。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs泛化能力的潜在风险，验证微调对模型行为影响的边界及安全性。

Method: 设计三种实验：1) 微调模型使用过时鸟类名称并观察非相关领域行为；2) 构建包含90个与希特勒相关但无唯一标识性的属性数据集进行微调；3) 训练模型在特定触发词（如“1984年”）下切换目标（如《终结者2》的良性和《终结者1》的恶性目标）。

Result: 狭窄微调导致模型在无关场景产生显著行为偏移：输出19世纪特征、模仿希特勒人格、通过触发词反转目标，验证了泛化带来的不可控风险。

Conclusion: 微调可能引发广泛且不可预见的模型行为偏移，单纯依赖数据过滤难以避免此类问题，需警惕模型对训练数据的隐式学习风险。

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [113] [MOA: Multi-Objective Alignment for Role-Playing Agents](https://arxiv.org/abs/2512.09756)
*Chonghua Liao,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: The paper proposes MOA, a multi-objective reinforcement learning framework to address the limitations of existing RPA training methods, achieving results comparable to GPT-4o and Claude on complex benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current RPA training methods like SFT and RL suffer from overfitting, low diversity, and inability to optimize multiple dimensions simultaneously. The authors aim to develop a framework that balances role knowledge, style consistency, and multi-turn dialogue complexity.

Method: MOA employs a multi-objective optimization strategy with fine-grained rubrics. It uses thought-augmented rollout with off-policy guidance to improve output diversity and quality. The approach integrates multiple training objectives without compromising individual dimension performance.

Result: Experiments on PersonaGym and RoleMRC benchmarks demonstrate that an 8B MOA-optimized model matches/exceeds GPT-4o and Claude in multiple evaluation dimensions while maintaining role-consistent generation across complex scenarios.

Conclusion: MOA provides a promising solution for comprehensive RPA optimization, outperforming strong baselines in balancing diverse requirements like role knowledge, stylistic consistency, and multi-turn interaction capabilities.

Abstract: Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.

</details>


### [114] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: 本论文介绍了OnCoCo 1.0——一个用于在线心理咨询领域细粒度对话分类的新公开数据集，包含涵盖38类咨询师话语和28类来访者话语的编码体系及2.8万条标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于动机性访谈（MI）的分类体系因聚焦狭窄且依赖面对面咨询数据，难以满足在线文本心理咨询对话的深度分析需求。

Method: 构建了涵盖双向对话类型的综合编码框架，标注了约2.8万条多轮对话数据，并通过对多个模型的微调验证数据集的可用性。

Result: 开发出具备语用特征的细粒度标注体系，验证了基于该数据集的模型微调效果，且实现了数据与模型的公开共享。

Conclusion: 为心理健康对话分析领域提供了首个兼具细粒度标注与双向交互特征的开放资源，扩展了现有社会心理分析数据集的维度与应用场景。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [115] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在法律领域的应用场景、技术挑战及全球监管差异，通过用例分析与双基准测试呈现其潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 法律领域面临传统任务效率瓶颈，大语言模型可通过自动化法规解释、判例分析、合同谈判及法律信息检索优化流程，同时需解决技术应用与社会监管的平衡问题。

Method: 系统分析法律场景用例，构建涵盖合同审阅、法律摘要等任务的评估基准，并对比欧盟AI法案、美国监管框架与中国治理模式的差异。

Result: 模型展现出法律文本处理能力，但也暴露出算法同质化风险、事实性错误及跨国合规冲突等挑战，揭示监管碎片化对技术落地的影响。

Conclusion: 大语言模型能变革法律实践但需风险管控，法律科技发展需兼顾技术创新与伦理监管，不同司法辖区的治理策略差异构成重要研究议题。

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [116] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: 本文提出了ChronusOmni，通过整合时间戳令牌和强化学习提升多模态时间感知能力，在音频视频时序定位任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注视觉-语言场景中的显式时间定位问题，忽略了音频模态和跨模态隐式时间对齐的重要性。真实场景中大量存在跨模态时序关联（如角色对话对应视觉内容识别），需要更全面的多模态时间感知模型。

Method: 提出时间戳令牌交错编码架构，将文本时间戳令牌与每时序单元的音视觉表征融合；设计强化学习策略及奖励函数增强时序推理；构建含准确时序标注、多模态对齐的ChronusAV基准数据集。

Result: 在ChronusAV数据集上超越现有方法30%以上，在多模态时序定位基准任务中取得SOTA性能，同时保持视频音频的通用理解能力。

Conclusion: 通过多模态时间表征融合与强化学习，ChronusOmni实现了显式与隐式音频视频时序定位的统一建模，推动了跨模态时序推理研究的发展，所构建的数据集为后续研究提供了标准化评估平台。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [117] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 本研究探讨了低资源语言中大型语言模型的推理时偏见缓解方法，利用偏好排序模型（PRM）提出三种方法，并通过跨语言评估揭示乌尔都语中的结构性不平等问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理社会敏感语言时易产生偏见，尤其在数据稀缺且文化代表性不足的低资源语言中表现更甚。该研究旨在开发无需重训练的推理时偏见缓解策略，并填补跨语言公平性评估的研究空白。

Method: 基于偏好排序模型（PRM），对比三种方法：1) 基线单词生成；2) PRM-Select最佳选择采样；3) 基于PRM批判的PRM-Sequential迭代改进。在200个英语及乌尔都语提示数据集上进行实验，使用GPT-3.5生成内容，GPT-4o-mini作为偏见与效用评分器。

Result: 1) 两种语言的偏见缓解均有显著提升；2) 所有方法中乌尔都语公平性得分均较低，揭示多语言模型训练中的结构性不平等；3) PRM-Select与PRM-Sequential改进路径存在显著差异。

Conclusion: 提出可扩展的公平性评估方法论及跨语言比较框架，揭示低资源语言中模型偏见缓解的挑战，为后续研究提供可解释指标与方法论参考。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [118] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 本研究提出基于低秩适应（LoRA）的神经机器翻译（NMT）持续学习框架，解决灾难性遗忘和高计算成本问题，利用少量参数空间实现模型适配。


<details>
  <summary>Details</summary>
Motivation: 针对NMT持续学习中灾难性遗忘和高计算成本的双重挑战，建立参数高效框架以实现跨语言域的实时交互适配。

Method: 采用LoRA微调技术进行语言域适配；提出基于线性组合的门控-free专家混合方法实现交互调整；设计面向低秩分解矩阵的梯度正则化策略，利用历史梯度信息加权惩罚。

Result: LoRA在参数使用量显著减少的情况下达到与全参数方法相当的性能；交互方法支持实时用户控制；梯度正则化有效保持历史知识并获取新任务。

Conclusion: 提出的方法为交互式持续NMT提供了可扩展范式，通过参数高效策略避免重训练，同时实现多语言域实时适配。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>
