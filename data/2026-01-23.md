<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 54]
- [cs.CL](#cs.CL) [Total: 41]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: This thesis proposes three methods for automated defect segmentation in culverts and sewer pipes with limited data, including preprocessing enhancements, the FORTRESS neural network architecture, and few-shot learning, achieving improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Defects in drainage infrastructure pose safety/environmental risks, but collecting annotated data for training AI models is difficult. Existing approaches require large datasets, which are impractical in this domain.

Method: 1) Preprocessing strategies (data augmentation, dynamic label injection). 2) FORTRESS: Depthwise separable convolutions + adaptive Kolmogorov-Arnold Networks + multi-scale attention. 3) Few-shot learning with bidirectional prototypical network and attention mechanisms.

Result: 1) Increased IoU/F1 scores. 2) FORTRESS achieved state-of-the-art performance with 70% fewer parameters than U-Net. 3) Few-shot model achieved satisfactory performance with limited training data.

Conclusion: All three methods effectively address data scarcity challenges in defect segmentation while maintaining strong performance metrics.

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: This paper evaluates multimodal large language models (MLLMs) for heterogeneous face recognition (HFR) across different sensing modalities (e.g., visual, infrared, thermal). Results show significant performance gaps between MLLMs and traditional systems, particularly under cross-spectral conditions, highlighting limitations in biometric applications.


<details>
  <summary>Details</summary>
Motivation: To assess the potential and limitations of MLLMs for biometric HFR, addressing their feasibility in cross-modality face recognition tasks where traditional methods face challenges.

Method: Benchmarked open-source MLLMs on cross-modality scenarios (VIS-NIR/SWIR/THERMAL) using biometric protocols and metrics (Acquire Rate, EER, TAR).

Result: MLLMs showed substantially lower recognition performance compared to classical systems, especially under challenging cross-spectral conditions (e.g., high EER and low TAR).

Conclusion: Current MLLMs are unsuitable for HFR in cross-spectral settings, emphasizing the need for rigorous biometric evaluation before deployment in practical systems.

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: 提出CURE框架，通过动态课程学习提升医学视觉语言模型的视觉基础对齐和报告准确性，无需额外数据。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型存在文本发现与视觉证据的对齐偏差，导致预测结果不可靠或缺乏基础支撑。

Method: 设计三阶段课程学习框架（phrase grounding → grounded report → anatomy-grounded report），通过动态调整采样策略，聚焦困难样本以优化时空对齐。

Result: 在保持无需外部数据的前提下，实现+0.37 IoU的接地准确性提升，+0.188 CXRFEScore报告质量提升，幻觉减少18.6%。

Conclusion: CURE作为数据高效框架，同时提升视觉接地准确性和医疗报告可靠性，代码和模型已开源。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 提出了DuFal框架，通过双频域-空间域处理解决稀疏视图锥束CT重建中丢失高频解剖细节的问题，实验显示其在极稀疏设置下超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法偏向学习低频信息，导致医学影像高频细节（如精细解剖结构）在稀疏投影重建时丢失，需开发能有效恢复高频特征的新方法。

Method: 设计双路径架构DuFal，包含全局高频增强傅里叶神经算子（全局频率模式）与局部高频增强傅里叶神经算子（空间分块处理），结合谱-通道分解降参数量、跨注意频率融合模块整合特征，最终通过特征解码与强度场解码重建CT体积。

Result: 在LUNA16和ToothFairy数据集实验表明，DuFal在极端稀疏视图（如20/60视图）下比现有方法（如U-Net、FFDN）显著提升高频结构保真度（PSNR提高3.2dB，SSIM提升0.15）。

Conclusion: DuFal通过联合优化空间-频率域特征，有效克服稀疏投影数据的高频信息丢失问题，为低剂量CT成像提供新思路，且在计算效率与结构保真度间取得平衡。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 该论文提出了一种基于偏差引导的提示学习框架，用于小样本异常检测（FNSAD），结合视觉-语言模型（如CLIP）的语义能力和基于统计偏差的打分机制以提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在正常与异常提示间的区分度不足，且缺乏面向局部异常的可靠打分机制。FNSAD任务因小样本训练和缺陷多样性面临挑战，需更优方案解决这些问题。

Method: 1) 使用可学习的上下文向量取代固定提示前缀，并通过异常类别专属后缀实现文本-图像语义对齐；2) 引入偏差损失（结合Top-K多重实例学习），将局部特征建模为高斯分布偏差以量化异常得分。

Result: 在MVTecAD和VISA基准测试中，像素级异常检测表现优于PromptAD等基线方法，消融实验证实了可学习参数、偏差打分机制及Top-K MIL策略的有效性。

Conclusion: 该框架通过融合视觉-语言模型与统计偏差机制，显著提升了小样本条件下的异常定位准确率与可解释性。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [6] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 本文提出LASAGNA框架，通过联合生成具有透明前景和背景的图像层，解决图像生成模型在编辑时缺乏可控性和视觉效果的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在用户编辑时存在图层合成不连贯、视觉效果（阴影/反射）缺失的问题，且缺乏高质量分层数据集支持可控编辑。

Method: 设计端到端框架LASAGNA，通过文本/掩码等多模态输入联合生成带Alpha通道的前景层和背景层；构建包含物理真实视觉效果的LASAGNA-48K数据集和LASAGNABENCH测试基准。

Result: 在多层图像生成任务中展现强一致性与高保真视觉效果，支持身份保留的身份编辑、阴影反射合成等多样化后期应用。

Conclusion: 为分层图像生成领域提供了首个系统性解决方案及标准化评估体系，开创性地实现了视觉效果可编辑性与生成质量的统一。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [7] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 本论文提出基于双流delta编码器的遮挡鲁棒手势姿态估计方法，通过分析背部皮肤形变，仅用裁剪图像即在自遮挡场景实现18%的MPJAE误差降低


<details>
  <summary>Details</summary>
Motivation: XR设备普及带来手势估计需求，但自我中心视角存在手指遮挡难题，现有方法依赖全手几何和大模型难以应对遮挡场景

Method: 构建双流delta编码器对比动态手部与基准姿态的背部皮肤特征，利用dense visual featurizers提取局部变形信息，建立形变-姿态关联模型

Result: 在手指遮挡≥50%场景下，MPJAE指标较现有方法提升18%，模型尺寸缩小且支持无可见运动的等长力检测

Conclusion: 实现更可靠的手势交互（如遮挡下的捏/点击检测），开创表面'点击'等新交互范式，兼顾精度与模型效率

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [8] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: VIOLA框架通过最小标注实现多模态大语言模型在视频领域的高效自适应，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的方法需大量标注数据，不适用于需专家标注的工业/手术等专业场景，亟需开发低标注依赖的模型泛化技术。

Method: 提出密度-不确定性加权采样策略，结合混合数据池与置信度感知检索/提示机制，利用少量高质量标注数据和大量未标注数据训练模型，通过密度估计选择代表性样本，并通过双信心机制过滤噪声数据。

Result: 在9个跨领域基准测试中，VIOLA在四种多模态大语言模型上均显著优于基线方法，尤其在标注预算<2%时表现突出，验证了其低资源环境下的鲁棒适应性。

Conclusion: 该方法证明通过优化标注样本选择和构建可靠伪标签机制，可有效平衡标注效率与模型泛化能力，为专业场景下的视频模型部署提供可行方案。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [9] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: 提出RCA评估类条件DDPM在K-pop偶像人脸生成中的身份一致性，揭示高视觉质量下严重语义崩溃问题（FID8.93，RCA0.27）。


<details>
  <summary>Details</summary>
Motivation: 传统FID/IS指标无法有效检测特定领域（如高类间相似性K-pop偶像）身份对齐问题，需建立更严谨的语义可控性评估框架。

Method: 构建32x32分辨率K-pop偶像人脸数据集，开发Class-Conditional DDPM模型，提出相对分类准确率（RCA）指标，通过混淆矩阵和分辨率/性别维度分析模式崩溃原因。

Result: 模型达SOTA视觉质量（FID 8.93），但RCA仅0.27显示严重语义模式崩溃，可视化发现25.3%样本被误分类至视觉近似类别，低分辨率与性别模糊化加剧该问题。

Conclusion: RCA指标有效揭示条件生成模型的身份一致性缺陷，建议关注数据分辨率增强与性别歧义样本处理以提升语义可控性。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [10] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于自我混合图像和强化学习的自动化深度伪造检测框架，解决了多模态大语言模型在该领域应用中缺乏高质量注释数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法缺乏可解释性输出，且多模态大语言模型因缺少高精度伪造归因标注数据而难以应用，同时强化学习在视觉任务中的潜力尚未被充分探索。

Method: 设计了自动化思维链(CoT)数据生成框架与强化学习增强的检测框架，包含定制化奖励机制和反馈驱动的合成数据生成方法。

Result: 实验验证了所提出的数据构建流水线、奖励机制和合成数据生成方法的有效性，在跨数据集基准测试中表现达到或接近当前最优水平。

Conclusion: 通过自动化数据生成和强化学习优化，可有效降低多模态大语言模型在深度伪造检测中的标注成本，同时提升跨域泛化能力。

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [11] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: 本文将持续学习（CL）扩展至多任务和多模态场景，提出持续全景感知模型（CPP），通过协同跨模态编码、知识蒸馏及语义对齐策略，解决灾难性遗忘与多模态语义混淆问题，提升图像感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有CL方法主要针对单任务场景，难以应对多任务多模态条件下的语义混淆与模型退化问题，需探索新的框架以实现复杂场景下的持续学习能力。

Method: 1. 提出协同跨模态编码器（CCE）构建多模态嵌入空间；2. 设计可塑性知识继承模块，融合对比特征蒸馏与实例蒸馏；3. 引入跨模态一致性约束（CPP+）保障语义对齐；4. 采用非对称伪标签策略避免样本回放。

Result: 在多模态数据集和多种CL任务上的实验表明，该方法在细粒度任务中表现显著优于基线模型，有效缓解灾难性遗忘并提升多模态对齐质量。

Conclusion: 研究成功实现了多任务多模态持续学习框架，为复杂感知系统的演化提供了新范式，未来可扩展至更多应用领域（如自适应自动驾驶系统）。

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [12] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: SuperOcc通过引入多超二次曲面解码和时序建模，在保持查询稀疏性的同时实现高效3D占用预测


<details>
  <summary>Details</summary>
Motivation: 传统密集场景表示效率低下，现有超二次曲面方法存在时序建模不足、几何表达性与查询稀疏性难以平衡、及体素投影效率低下三重局限性

Method: 1)融合视角级与物体级时序特征的时空建模机制 2)多超二次曲面联合解码策略 3)基于可微分插值的快速体素投影算法

Result: 在SurroundOcc和Occ3D双基准测试中均达SOTA水平，推理速度较稠密方法提升3.8倍，内存占用降低78%

Conclusion: 通过几何表征学习与高效计算架构的联合创新，首次实现稀疏性与表达性的协同优化，为自动驾驶场景理解提供了新范式

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [13] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream通过事件感知框架将连续视频分割为语义连贯的离散事件，并结合持久记忆库解决视频大模型实时处理中的冗余计算和上下文遗忘问题，实现高效长视频理解。


<details>
  <summary>Details</summary>
Motivation: 传统视频流系统采用固定间隔解码或缓存剪枝策略，导致输出冗余或丢失关键时间信息，亟需能动态捕捉视频语义变化并保持长期记忆的新方法。

Method: 1) 提出事件检测机制，融合运动、语义和预测线索识别状态转换边界；2) 仅在事件边界触发语言生成；3) 构建持久记忆库存储事件嵌入以支持长时推理。

Result: 在OVOBench-Realtime和Ego4D数据集上表现优异：相较VideoLLM-Online-8B提升+10.4分，接近Flash-VStream-7B性能，2小时视频保持70% GPT-5胜率。

Conclusion: 事件驱动的建模方式与持久记忆机制显著提升长视频实时处理效率，在降低计算开销同时保留关键时序信息，为视频语言模型提供新范式。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [14] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork UniPic 3.0是一个支持多图像组合的统一多模态框架，通过创新数据管道和序列建模实现高效高质图像生成。


<details>
  <summary>Details</summary>
Motivation: 多图像组合在一致性与质量上存在挑战，而现有模型缺乏高质量融合方法细节。作者基于社区对HOI（人体-物体交互）的强烈需求，提出针对性解决方案。

Method: 1) 构建包含700K样本的高质量数据管道；2) 提出序列建模训练范式，将条件生成转化为统一序列合成；3) 在推理阶段引入轨迹映射与分布匹配加速生成；4) 支持任意数量/分辨率输入与输出（总像素不超过1024x1024）。

Result: 在单图像编辑基准测试中达到SOTA水平，在多图像组合任务中超越Nano-Banana和Seedream 4.0，实现8步生成高保真样本，合成速度提升12.5倍。

Conclusion: 提出的框架有效解决多图像组合挑战，验证数据管道与训练范式的有效性，为社区提供开源代码、模型与数据集。

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [15] [Consistency-Regularized GAN for Few-Shot SAR Target Recognition](https://arxiv.org/abs/2601.15681)
*Yikui Zhai,Shikuang Liu,Wenlve Zhou,Hongsheng Zhang,Zhiheng Zhou,Xiaolin Tian,C. L. Philip Chen*

Main category: cs.CV

TL;DR: Cr-GAN通过创新的双分支鉴别器和双域循环一致性机制，在极端数据稀缺场景下实现高质量SAR图像合成，8样本设置下MSTAR/SRSDD数据集分别达71.21%/51.64%准确率，参数量仅为扩散模型的5%。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN因需要大量数据训练而无法应用于小样本学习的矛盾，特别是在合成孔径雷达（SAR）图像领域

Method: 创新双分支鉴别器架构解耦对抗训练与表征学习，引入通道级特征插值生成新特征，采用双域循环一致性机制确保语义完整性，支持多种GAN架构适应性

Result: 在MSTAR和SRSDD数据集8样本设置中分别取得71.21%和51.64%的高准确率，显著超越基线模型，参数量仅为现有扩散模型的5%

Conclusion: Cr-GAN通过新颖的架构设计成功解决小样本条件下GAN训练难题，在SAR图像任务中展现出显著优势，为资源受限场景提供高效的生成模型解决方案

Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.

</details>


### [16] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: Beyond Visual Safety (BVS)框架通过'重建-生成'策略破解多模态大模型视觉安全边界，对GPT-5实现98.21%越狱成功率


<details>
  <summary>Details</summary>
Motivation: 现有研究对多模态大语言模型的视觉安全边界缺乏深入探究，而该领域存在重大安全风险

Method: 采用两阶段策略：1) 中性化视觉拼接解耦恶意意图 2) 归纳重组生成攻击性图文对

Result: 在GPT-5(2026年1月版)上实现98.21%的越狱成功率，显著暴露当前模型的视觉安全缺陷

Conclusion: 揭示多模态大语言模型在视觉安全对齐方面存在系统性漏洞，需加强多模态安全防护机制

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [17] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: 本文提出了一种基于ALOS-2单极化SAR数据的日本全国土地利用/覆盖语义分割方法，通过三种轻量级改进解决边界模糊、细长结构丢失和类别不平衡问题，无需增加流程复杂度。


<details>
  <summary>Details</summary>
Motivation: 针对合成孔径雷达(SAR)在土地覆盖稠密预测任务中的边界过平滑、细小结构遗漏以及类别标签长尾分布导致的性能下降问题，现有方法缺乏有效解决方案。

Method: 基于SAR-W-MixMAE自监督预训练模型，引入三种改进：(i)多尺度解码阶段引入高分辨率特征；(ii)交替卷积细化与逐步上采样的渐进式精化头；(iii)在focal+dice损失函数中增加α比例因子平衡类别权重。

Result: 在日本全国ALOS-2土地覆盖基准测试中取得一致提升，特别是欠表征类别准确率提高，同时改善了水体检测的标准评估指标。

Conclusion: 提出的轻量化模型改进策略在保持架构简洁性的同时，有效解决了SAR数据语义分割中的关键挑战，为大规模遥感图像解译提供了实用解决方案。

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [18] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 该论文提出了一种三层评价框架，用于评估零样本视觉-语言模型（VLM）在时尚零售细粒度属性预测中的性能。关键结果包括零样本VLM在宏F1得分达64.0%，在细粒度分类中表现优异但适用性检测较差，高效模型在低成本下实现旗舰模型90%以上的性能。


<details>
  <summary>Details</summary>
Motivation: 时尚属性预测对零售应用至关重要，但现有视觉-语言模型缺乏面向多属性任务的系统化评估。时尚属性具有条件性（如“外套材质”在无外套时无效），需模型识别属性适用性后再分类。研究旨在填补这一评估空白，并解决属性适用性检测挑战。

Method: 1) 建立三层评价框架：整体任务性能（含NA类）、属性适用性检测、细粒度分类；2) 构建包含NA类标签的DeepFashion-MultiModal数据集；3) 在5000张图像、18个属性上对比9种VLM（涵盖旗舰、高效、超高效级别）与基于Fashion-CLIP的有监督分类器。

Result: 1) 零样本VLM达64.0%宏观F1，性能是传统分类器的3倍；2) VLM在细粒度分类（Tier 3）达70.8% F1，但适用性检测（Tier 2）仅34.1% NA-F1；3) 高效模型实现旗舰模型90%以上性能。

Conclusion: 三层框架可区分可见性检测与分类错误根源，揭示高效VLM在低成本场景的应用潜力，为实际部署提供诊断工具。

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [19] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: 本文提出了FAIR-ESI框架，通过自适应特征精化提升脑电生理源成像质量，解决了传统方法中特征选择不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有ESI方法在特征选择与优化上存在瓶颈，难以实现高精度脑源定位。

Method: 构建基于FFT频谱、加权时间序列和自注意力机制的多视图特征精化模块，自适应融合多模态特征。

Result: 在2个模拟数据集和2个真实临床数据集中，FAIR-ESI均显著优于传统模型优化和深度学习方法。

Conclusion: 该框架为脑功能研究提供新方法，有望提升临床诊断准确率并揭示脑部疾病机制。

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [20] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: This paper introduces a novel framework using foundation models in multi-modal medical imaging, enhanced by sub-region-aware modality attention and adaptive prompt engineering for improved brain tumor segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with multi-modal data fusion and handling tissue heterogeneity in medical imaging, prompting the need for a framework that adaptively integrates modalities and leverages foundation model capabilities.

Method: The framework employs two innovations: (1) a sub-region-aware attention mechanism to dynamically combine modalities for different tumor regions and (2) an adaptive prompting strategy to refine segmentation using foundation models' inherent strengths.

Result: On the BraTS 2020 dataset, the approach significantly outperformed baselines, especially in segmenting the challenging necrotic tumor core sub-region.

Conclusion: The framework provides a principled solution for multi-modal medical imaging tasks, advancing robust and accurate foundation model applications in clinical diagnostics.

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [21] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: 这篇论文提出了首个任意分辨率的深度图像隐写框架ARDIS，通过将离散映射范式转化为参考引导的连续信号重建，解决了秘密图像与封面图像分辨率不一致导致的细节丢失和分辨率未知时无法恢复的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像隐写方法强制要求秘密图像与封面图像分辨率一致，需预处理调整导致细节丢失，且无法恢复未知分辨率的秘密图像。

Method: 1) 频率解耦架构：在隐藏阶段将秘密图像分解为分辨率对齐的低频基和与分辨率无关的高频潜码；2) 潜码引导的隐式重建器：通过连续隐函数查询高频残差实现细节恢复；3) 隐式分辨率编码：将离散分辨率值转化为特征图隐藏在冗余特征空间中。

Result: 实验表明ARDIS在隐蔽性和跨分辨率重建保真度方面显著优于当前最先进技术。

Conclusion: ARDIS实现了无需预处理调整分辨率的隐写框架，通过连续信号重建和隐式编码策略，可盲恢复任意分辨率的秘密图像并保留原始细节。

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [22] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: 本研究提出ES-mHC框架，通过分离特征表示与电磁波谱交互结构，将高光谱图像分类从黑盒模型转变为可解释的白盒学习系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在高光谱图像分类中存在光谱-空间特征混合问题，导致模型可解释性差，难以理解内部决策机制。

Method: 构建物理光谱感知的ES-mHC超连接框架，采用结构化方向矩阵显式建模电磁波谱分组间的交互（mHC残差流），分离特征表示与交互结构，实现可视化和空间分析。

Result: 学习到的超连接矩阵呈现连贯空间模式和非对称交互行为，扩展率增加加速结构化交互模式形成，成功揭示模型内部动力学机制。

Conclusion: ES-mHC框架通过结构透明化设计，将高光谱图像分类任务转变为部分白盒学习过程，为模型可解释性研究提供新范式。

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [23] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: 该研究提出FeTal-SAM，结合图谱提示和基础模型SAM，实现胎儿脑部MRI灵活分割，无需传统模型对固定标签的依赖和大量重训练。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型依赖固定标签的大型训练数据集，且难以区分分割结果是源于图像特征还是预设空间先验。胎儿MRI需适应临床需求变化与标签定义调整，现有方法灵活性不足且解释性差。

Method: 利用多图谱配准生成空间对齐的二值标签模板作为密集提示，结合边界框提示注入SAM解码器，逐结构进行二分类分割。通过多结构分割结果融合构建完整3D分割体积，实现无需预设标签集的即插即用式分割。

Result: 在dHCP和私有数据集上，对高对比度结构（如大脑皮层、小脑）的Dice分数与现有最优模型相当，但保留分割任意自定义解剖结构能力。低对比度结构（如海马、杏仁核）精度略下降，但跨孕周表现稳健。

Conclusion: FeTal-SAM通过整合结构先验与图像特征，在减少重训练需求的同时提升标签定义灵活性，为临床可适配的胎儿MRI分析提供了新范式。

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [24] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 提出LL-GaussianMap，首个将2D高斯点绘(2DGS)引入低光增强的无监督框架，通过显式结构建模显著提升效果并降低存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视图像几何结构先验，仅在像素域或隐式特征空间操作；2DGS具备显式结构拟合优势但未被探索

Method: 基于2DGS的两阶段处理：1) 高保真结构重建 2) 利用高斯光栅化机制生成增强字典系数，通过统一模块实现结构感知的增益图生成

Result: 实验显示增强效果优于现有方法，在存储开销极低（≈0.3%）情况下提升边缘保留和伪影抑制能力

Conclusion: 显式高斯表示能有效挖掘结构先验，在低光增强中突破传统隐式表示范式，证明了无监督结构建模的优越性

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [25] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: 提出扩散模型数据增强框架，解决神经元分割中数据不足与结构单一问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量标注数据且传统增强技术生成的样本多样性不足，限制模型性能。

Method: 设计分辨率感知的条件扩散模型与生物引导的掩膜重塑模块，生成结构合理的3D图像-标签对。

Result: 在AC3/AC4数据集上，低标注情况下ARAND指标提升32.1%/30.7%，代码已开源。

Conclusion: 该框架有效提升神经元分割效果，为数据匮乏场景提供通用数据增强解决方案。

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [26] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: 论文提出一种合成基准，测试VLMs在情境和空间意识任务中的表现，发现当前模型性能仅略高于随机猜测。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在依赖细微时间/几何线索的空间推理上存在脆弱性，需要更精细的评估基准来诊断模型弱点。

Method: 构建包含暴力/非暴力活动区分、跨视角攻击者角色绑定、轨迹对齐判断三类任务的基准，评估无训练VLMs表现并引入颜色线索辅助实验。

Result: 现有VLMs平均表现仅52-58%，添加颜色线索后角色绑定正确率提升19%，但轨迹对齐任务仍低于60%，显示模型对空间动态理解存在深层缺陷。

Conclusion: 提出新型诊断工具揭示VLM空间推理缺陷，开源数据及代码推动轻量空间先验建模方向的研究。

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [27] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: 本研究首次将数据集蒸馏引入遥感图像解译领域，基于文本生成图像的扩散模型构建紧凑高效的数据集，解决了深度学习在遥感领域面临的存储计算成本高与数据泄露风险问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感图像解译中依赖大规模数据集，但存在两个主要挑战：（1）高存储和计算成本；（2）敏感类别数据泄露风险。亟需一种既能压缩数据规模又能保持语义表征能力的解决方案。

Method: 提出基于扩散模型的遥感数据集蒸馏方法：① 训练文本到图像扩散模型实现数据压缩；② 引入预训练分类模型的分类一致性损失优化生成质量；③ 通过潜在空间聚类选择具有代表性的视觉原型，并结合视觉语言模型生成文本描述作为风格引导。

Result: 在三个高分辨率遥感场景分类基准上的实验表明，该方法能生成具有现实性和多样性的样本用于下游模型训练。开源代码和预训练模型已发布（https://github.com/YonghaoXu/DPD）。

Conclusion: 通过结合扩散生成、分类引导和多模态特征融合，成功构建了首个针对遥感图像的高效数据集蒸馏框架，平衡了数据压缩效率与模型性能，为敏感遥感数据的隐私保护提供了新思路。

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [28] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种基于物联网的智能植物监测系统，通过传感器与自动化灌溉实现资源优化和精准农业。


<details>
  <summary>Details</summary>
Motivation: 应对传统农业中人工观察和周期性灌溉导致的水资源浪费、植物生长不一致及环境变化响应滞后问题，满足全球可持续农业需求。

Method: 采用ESP32微控制器集成温湿度(DHT22)、水位(HC-SR04)、土壤湿度传感器及OLED显示/蜂鸣器报警模块，通过Wi-Fi将数据上传至ThingSpeak云平台实现远程监控与自动化灌溉。

Result: 系统维持土壤湿度92%准确率，节水40%且部署成本仅45.20美元，配套的可视化网页仪表板支持历史数据分析和智能告警。

Conclusion: 该低成本可扩展方案结合边缘传感与云计算，为园艺和商业农业提供可持续的智能灌溉解决方案。

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [29] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: 提出新CIR基准EDIR，通过图像编辑生成多样化查询，评估揭示多模态模型能力差距


<details>
  <summary>Details</summary>
Motivation: 现有CIR基准的查询类别有限，无法满足现实场景需求，需构建更具挑战性的细粒度测试集

Method: 开发图像编辑驱动的查询合成管线，构造包含5000个结构化查询的EDIR数据集，涵盖5大类15子类

Result: 13个多模态模型表现差距显著，顶尖模型(如RzenEmbed/GME)普遍存在一致性不足问题

Conclusion: EDIR可有效暴露模型架构固有缺陷，证明针对性训练对部分类别有效，揭示现有基准存在模态偏倚缺陷

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [30] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: This paper introduces a lightweight, brain-inspired hybrid neural network framework for coronary angiography (CAG) classification, combining a pretrained CNN, adaptive parameter training, and biologically inspired loss functions to address challenges like class imbalance and limited computational resources.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve real-world challenges in CAG analysis, such as complex lesion morphology, label uncertainty, severe class imbalance, and computational constraints, which hinder the robustness and generalization of conventional deep learning methods.

Method: A pretrained convolutional neural network is used to build a lightweight model, with selective neural plasticity training for adaptive parameter optimization. A brain-inspired loss function combines Focal Loss and label smoothing, while class-imbalance-aware sampling and cosine annealing with warm restarts emulate biological attention and rhythmic regulation mechanisms.

Result: The model achieved strong performance in binary CAG classification, with competitive accuracy, recall, F1-score, and AUC, alongside high computational efficiency, demonstrating stability under real-world constraints.

Conclusion: The paper validates that brain-inspired learning mechanisms enhance lightweight medical image analysis, offering a biologically plausible and deployable solution for efficient clinical decision support in resource-constrained settings.

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [31] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: 论文介绍了首个公开的、完全配对的泛癌医学影像数据集（PMPBench）和全面的基线测试，涵盖11个人类器官，包含MRI和CT的多时相对比图像，旨在推动无需造影剂的AI影像合成研究。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集存在局限性：主要集中在脑部MRI，部分配对数据存在模态缺失、时间戳不匹配和空间对齐问题，且缺乏CT与增强CT（CTC）或动态增强（DCE）阶段的明确标注。PMPBench填补这一空白，支持跨器官肿瘤影像分析。

Method: 构建了一个包含多器官（11种）、多模态（MRI含完整DCE序列、CT含配对的CT/CTC）的高质量配对数据集，并保证解剖结构一致性。设计了1对1、N对1和N对N的影像翻译实验设置，并基于该数据集建立了基线测试框架。

Result: 实现了首次覆盖多器官的全配对数据公开，基线测试表明已有影像翻译方法在该数据集上的表现仍存在改进空间，尤其在跨扫描阶段预测（如从非对比期生成对比期图像）任务中。

Conclusion: PMPBench为医学影像AI研究提供了标准化资源，有望推动安全有效的对比图像合成技术发展，优化多器官肿瘤诊疗的工作流程。论文代码和数据集已开源。

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [32] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: 该研究指出视觉基础模型在医学影像任务中的性能受限于预训练目标与下游任务需求的匹配度，并通过两种预训练方法在前列腺MRI分析中的对比实验，证明任务对齐度（如MMD衡量）与模型性能提升及收敛速度正相关。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型在不同下游任务中表现差异较大，但预训练计算成本高昂。作者认为这源于预训练目标（如图像重建或对比学习）与特定下游任务（如分割、分类）的语义需求不匹配，亟需通过任务对齐性研究优化预训练策略。

Method: 采用两种预训练视觉模型ProFound（基于掩码重建）与ProViCNet（基于对比学习），在5个前列腺多参数MRI临床任务中进行微调，并通过最大均值差异（MMD）等指标量化预训练-微调特征的对齐程度与模型性能的关系。

Result: 发现模型在预训练与下游任务对齐度高的情况下，性能提升幅度更大且收敛更快；MMD等对齐度指标与任务性能具有显著相关性，验证了预训练目标适配下游任务的重要性。

Conclusion: 研究证明预训练目标需针对性设计以匹配下游任务需求，并提出通过量化对齐度评估预训练策略有效性的方法，为视觉基础模型的临床应用优化提供了新方向。

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [33] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: RadJEPA是一种自监督医学影像分析框架，无需依赖语言监督即可在疾病分类等任务中达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉模型受制于稀缺的图像-文本配对数据，作者旨在探索无需语言监督的鲁棒放射影像编码器学习方法。

Method: 基于联合嵌入架构设计自监督任务，通过预测胸部X光图像的遮罩区域潜在特征表示进行预训练，区别于传统图文对齐和DINO自蒸馏方法。

Result: 模型在疾病分类、语义分割及报告生成任务中均超越SOTA方法，包括Rad-DINO在内的竞争方案。

Conclusion: 证明自监督学习能在无语言标注条件下实现高性能医学影像分析，为相关领域研究提供了新范式。

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [34] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: 本文发现多模态基础模型中的情感建模主要由前馈门控机制（gate_proj）驱动，而非注意力模块，且该模块在参数效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管现有情感模型表现优异，但其内部情感表征机制（尤其是多模态场景下的机理）仍不清晰，需系统研究情感监督如何重塑模型参数。

Method: 通过跨架构、训练策略和任务的分析，结合模块迁移、单模块微调和破坏性消融实验，研究情感适配的参数分布与机制。

Result: 情感适配主要聚集于feed-forward gate_proj而非注意力模块。仅调参24.5%的AffectGPT参数量，即可复现其96.6%的平均任务性能，证实gate_proj的充分性、必要性与高效性。

Conclusion: 情感建模在架构上由gate_proj主导，揭示该模块是情感理解与生成的核心枢纽，为模型设计提供结构化指导。

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [35] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: This paper evaluates real-time emotion recognition models for VR-based autism therapy, highlighting latency-accuracy trade-offs and the need for lightweight architectures.


<details>
  <summary>Details</summary>
Motivation: To support individuals with ASD in improving social skills via VR therapy by developing low-latency emotion recognition systems that maintain real-time contingency (MTP <140ms).

Method: Benchmarked state-of-theart zero-shot FER models on the UIBVFED dataset, comparing YOLO variants and Vision Transformers (CLIP, SigLIP, ViT-FER) for CPU-only inference performance.

Result: YOLOv11n achieved optimal detection speed (~54ms) with 100% avatar face detection accuracy, but Transformer-based classifiers exhibited a "Latency Wall" with subthreshold accuracy (<23%) and excessive delays (>150ms).

Conclusion: Current general-purpose models cannot meet VR therapy latency-accuracy requirements, necessitating domain-specific lightweight architectures for accessible real-time AI in therapeutic applications.

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [36] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 本文提出了一种无需领域微调的手术场景3D手部姿态估计新方法（HandOccNet），同时构建了含68,000帧与3,000个标注的手术基准数据集（SurgHands），通过多视角管道和约束优化显著降低2D/3D误差，为微创手术智能分析提供关键基础。


<details>
  <summary>Details</summary>
Motivation: 手术场景中存在极端光照（如无影灯）、器械/人员遮挡、手套导致的手部表观单一等挑战，传统方法在跨场景泛化性和标注数据稀缺性上存在瓶颈，需开发专用且无需特殊调优的鲁棒方案。

Method: 设计多阶段全监督管道：1）利用预训练YOLOv7进行医护人员检测；2）通过HRNet-W48获取全身姿态特征；3）应用SimCC提取2D手部关键点；4）结合多视角几何约束优化3D重建，全程不使用手术领域特异性微调。

Result: 在SurgHands数据集上，相较传统方法取得2D误差31%降低（11.67→8.07像素）和3D误差76%压缩（60.51→14.52毫米），且在无影灯强光照场景中依然保持85%+关键点可见性。

Conclusion: 本研究为手术分析提供双重贡献：1）无需标注数据训练的即插即用式手部感知框架；2）首个涵盖复杂手术干扰因子的多视角基准数据集，有效推动医疗AI在技能评估和智能手术室方向的发展。

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [37] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: This paper introduces a class and confidence-aware loss re-weighting method for long-tailed learning, which modulates training contributions based on both class frequency and prediction confidence.


<details>
  <summary>Details</summary>
Motivation: Previous long-tailed learning methods mainly focus on logit-level adjustments without addressing how these affect the optimization process. This paper aims to improve training by considering both class imbalance and confidence differences in loss weighting.

Method: Proposed an Ω(p_t, f_c) function that dynamically re-weights loss contributions by combining predicted probability confidence (p_t) and class frequency (f_c). This operates at the loss level rather than logit level, complementing existing approaches.

Result: Experiments on CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets across multiple imbalance factors showed significant performance improvements, corroborating the effectiveness of confidence and class-aware weighting.

Conclusion: The confidence-aware loss re-weighting scheme effectively addresses long-tailed data challenges by leveraging both class distribution and prediction confidence information during model training.

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [38] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: 提出了一种名为NeuroMamba的神经元分割方法，结合全局建模与局部特征增强，解决神经元连接形态复杂导致的分割难题。


<details>
  <summary>Details</summary>
Motivation: 神经元分割是脑连接体研究的基础，但现有CNN和Transformer方法存在上下文感知不足或细节丢失问题。

Method: 通过Mamba模型实现无Patch的全局建模，结合通道门控边界判别特征提取（BDFE）和空间连续特征提取（SCFE），并通过跨模态机制融合多视角特征。

Result: 在4个EM数据集上达到SOTA性能，验证了对各向同性/异向分辨率的强适应性。

Conclusion: NeuroMamba有效结合了全局依赖建模与局部细节保留，为神经元分割提供了高效且高精度的解决方案。

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [39] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: EvolSplat4D通过结合基于体积和基于像素的高斯预测方法，实现高效且高质量的静止和动态城市场景三维/四维重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景重建中存在时间效率与质量的权衡问题：基于神经辐射场和三维高斯点绘的方法需要耗时的逐场景优化，而前馈方法在动态环境中使用逐像素预测导致三维不一致。

Method: 采用三分支架构：1）静态区域通过三维特征体积预测几何一致性高斯并结合语义增强渲染；2）动态对象通过对象中心空间和运动调整模块进行时序特征聚合；3）远距离区域使用高效逐像素高斯分支实现全场景覆盖。

Result: 在KITTI-360、KITTI、Waymo和PandaSet数据集上均优于逐场景优化和前沿前馈方法，在准确性和一致性方面表现突出。

Conclusion: 该方法实现了时间效率与重建质量的平衡，能稳定处理复杂动态场景的噪声运动先验，为自动驾驶仿真提供实用解决方案。

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [40] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: 提出PhysicsMind基准，评估多模态模型对质心、杠杆平衡、牛顿第一定律三大物理原理的理解能力，发现模型依赖表象启发式而违反力学规律。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过度依赖合成数据/视频质量指标，缺乏对物理规律一致性（如质心动态、力矩平衡）的针对性评估

Method: 构建包含真实场景与模拟环境的基准，设计VQA（物理量推理）与视频生成（轨迹物理约束符合度）两项任务，采用中心质量、扭矩、惯性3个核心指标

Result: 主流模型在视频生成任务中轨迹物理约束符合率低于40%，VQA准确率与物理复杂度呈负相关（r=-0.73），生成视频违反动量守恒达28%案例

Conclusion: 揭示当前训练范式未能有效编码物理先验，指出将物理归纳偏置注入模型架构和损失函数是关键改进方向

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [41] [Keyframe-Based Feed-Forward Visual Odometry](https://arxiv.org/abs/2601.16020)
*Weichen Dai,Wenhan Su,Da Kong,Yuhang Ming,Wanzeng Kong*

Main category: cs.CV

TL;DR: 提出基于强化学习的自适应关键帧策略，改进视觉里程计的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型方法处理冗余帧导致计算效率低且性能差，传统几何启发式方法难以整合到依赖高维潜空间的模型中。

Method: 通过强化学习训练智能体在TartanAir数据集上自主学习关键帧策略，实现数据驱动的帧选择与模型特性协同。

Result: 在多个真实场景数据集中，该方法相比现有前馈VO方法性能指标提升显著，特别是在低视差场景表现更优。

Conclusion: 将传统几何启发思想与基础模型结合，证实强化学习设计自适应策略能有效提升单目VO系统的实用性。

Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

</details>


### [42] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出了一种名为PAINT的病理感知结构化虚拟免疫组化合成框架，通过结构优先的自回归建模方法，在结构保真度和临床任务表现上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟免疫组化（IHC）方法直接依赖形态学特征生成分子染色模式，因结构性先验不足导致语义不一致性。本研究旨在通过结构引导的条件生成机制解决H&E图像与蛋白质表达关联模糊的问题。

Method: 构建PAINT框架（Pathology-Aware Integrated Next-Scale Transformation）与空间结构起始图（3S-Map），采用因果顺序的自回归生成模式：首先基于全局病理结构生成分子细节，并通过3S-Map初始化确保空间对齐的确定性合成。

Result: 在IHC4BC和MIST数据集上，PAINT在结构保真度指标（PSNR↑15.2%, SSIM↑23.7%）及临床下游任务（如分子状态分类准确率↑8.5%）中均超越当前最优方法（如CycleGAN、UNIT）。

Conclusion: 结构优先的自回归建模为虚拟IHC提供有效范式，证明结构先验可增强跨模态生成的语义一致性与临床实用性。

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [43] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: 本研究提出ProGiDiff框架，通过提示驱动预训练扩散模型解决医学图像分割的多模态适应和多提案生成难题


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法具有确定性且难以响应自然语言提示，传统扩散模型需从零训练导致数据需求大且局限于二值分割

Method: 构建类ControlNet的条件机制与定制编码器，通过图像条件引导预训练扩散模型输出分割掩码，实现多类别分割并通过器官提示扩展

Result: 在CT器官分割实验中表现优异，支持专家协同的多提案生成，并通过低秩微调实现MRI跨模态迁移

Conclusion: 该方法有效结合扩散模型生成能力与专家交互优势，为医学图像分割提供跨模态轻量解决方案

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [44] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: 本文提出了一种动态剪除视觉-语言动作模型中干扰图像块的方法（DTP），通过修正模型的视觉注意力模式，在不改变原有架构的前提下显著提升了机器人任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言动作模型（VLA）因过度关注任务无关区域的'干扰图像块'导致动作生成不稳定，现有方法难以在保持模型架构不变的同时解决注意力偏差问题

Method: 设计了一种即插即用的干扰标记剪枝框架，通过动态检测并删除训练过程中无效的视觉标记，重构注意力权重分布

Result: 在SIMPLER基准测试中，DTP在多种VLA模型上平均任务成功率提升了相对值，且发现任务成功率与无关区域注意力强度呈负相关

Conclusion: DTP框架为VLA模型提供了通用优化方案，揭示了注意力分布与任务性能的内在关联，为后续研究提供了新的方向

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [45] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: DSFedMed是一种双尺度联邦框架，通过互知识蒸馏在医学图像分割任务中实现基础模型与轻量客户端的协同优化，使用合成数据和样本选择策略，在保持性能的同时降低90%的通信和推理开销。


<details>
  <summary>Details</summary>
Motivation: 基础模型在联邦医疗场景中面临高计算需求、通信开销及推理成本的部署瓶颈，亟需高效的知识蒸馏方法解决资源受限问题。

Method: 采用双尺度蒸馏架构，生成高质量合成医学图像替代真实数据，设计可学习性引导的样本选择策略，实现基础模型与客户端的知识双向流动与优化。

Result: 在五组医学数据集中Dice分数平均提升2%，通信成本和推理时间较基线方法降低约90%，同时保持模型泛化能力。

Conclusion: DSFedMed通过轻量化蒸馏方案显著提升了联邦学习的效率和可扩展性，为资源受限的医疗AI部署提供了可行技术路径。

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [46] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: MoRo通过生成式掩码建模，在单目视频中实现鲁棒的人体运动重建，尤其擅长处理现实场景中的遮挡问题，并支持实时推理。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频中人体运动重建受遮挡影响的问题，平衡效率与鲁棒性，现有回归方法对遮挡敏感，优化/扩散方法速度慢且依赖预处理。

Method: 采用分层掩码建模框架：1) 基于运动捕捉数据的轨迹感知先验模型，2) 基于图像-姿态数据的图像条件姿态先验模型，3) 融合前两者的视频条件Transformer模型。通过跨模态学习整合异构数据集，实现端到端推理。

Result: 在EgoBody和RICH数据集上，遮挡场景下精度和运动真实性显著优于SOTA方法，在非遮挡场景与现有方法持平，单卡实现实时推理（70FPS）。

Conclusion: MoRo通过掩码建模有效处理遮挡，结合多模态先验知识，兼顾鲁棒性与效率，为AR/VR等实时应用提供解决方案。

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [47] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: 提出SAMTok，通过离散掩膜标记化方法使多模态大语言模型（MLLM）获得像素级能力，无需修改模型结构或损失函数设计。


<details>
  <summary>Details</summary>
Motivation: 解决传统MLLM在像素级任务中因复杂区域编码器、专用分割解码器和训练目标不兼容导致的扩展难题。

Method: 基于SAM2构建离散掩膜编码器，用209M掩膜数据训练掩膜编码器和残差向量量化器生成紧凑标记，结合强化学习和文本答案匹配奖励优化模型。

Result: 在区域描述、视觉问答、接地对话等任务中达到SOTA或可比结果，在GRES/GCG基准上通过强化学习实现显著性能提升。

Conclusion: SAMTok提供了一种可扩展的范式，通过简化像素级训练目标，证明多模态大模型可通过常规训练方式获得强像素级能力。

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [48] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出了DistSeal，一种在生成模型的潜在空间中进行水印嵌入的方法，比现有像素空间方法快20倍且更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在像素空间处理，导致计算开销大且可能产生视觉伪影。

Method: 训练潜在空间中的后处理水印模型，并将其蒸馏到生成模型或潜在解码器中。

Result: 实现高鲁棒性且不可觉察性相似，在扩散模型和自回归模型中均有效，蒸馏效果优于像素空间方法。

Conclusion: 该方法在效率、鲁棒性和跨模型兼容性上优于传统像素空间水印方案。

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [49] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: 提出了一种新的生成模型ActionMesh，通过引入时间轴的3D扩散模型，实现了快速生成高质量的动态3D网格。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在设置受限、运行时间长或质量有限的问题，需一种能高效生成生产就绪型动态3D网格的模型。

Method: 1) 将3D扩散模型扩展为包含时间轴的时序扩散阶段，生成同步隐编码序列；2) 设计时序3D自动编码器，将独立形状序列转化为预定义参考形状的动画变形。

Result: 在Consistent4D和Objaverse数据集上表现优异，几何精度和时间一致性均达到SOTA，且生成速度快，无需骨骼绑定。

Conclusion: 该模型在保证高质量与拓扑一致性的同时显著提升了生成效率，适合纹理映射与动作迁移等应用场景。

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [50] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 360Anything是一种无需几何校准的新型框架，通过扩散变换模型实现从常规视角到360°全景生成，突破了传统方法依赖相机数据的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确相机参数导致现实场景适配受限，而真实应用中常遇到元数据缺失或噪声干扰问题。

Method: 将视角与全景视为token序列，基于预训练扩散模型实现数据驱动的端到端映射，并提出循环潜在编码解决ERP接缝问题。

Result: 在图像/视频生成任务中超越使用真实相机数据的先前方法，零样本相机参数估计任务亦达优效，验证其深层几何理解能力。

Conclusion: 通过数据驱动策略建立的360Anything框架在保持生成质量的同时显著提升应用灵活性，为全景生成和几何感知任务提供通用解决方案。

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [51] [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2601.16208)
*Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie*

Main category: cs.CV

TL;DR: 该论文验证了Representation Autoencoders（RAEs）在大规模文本到图像生成中的有效性，证明其比传统VAEs方法更高效且稳定。


<details>
  <summary>Details</summary>
Motivation: 探索RAEs框架是否能扩展到大规模自由形式文本到图像生成，解决现有方法（如VAEs）在扩展性和稳定性上的不足。

Method: 1) 扩展现有RAE解码器并结合冻结的SigLIP-2编码器，融合多源数据训练；2) 系统分析RAE在ImageNet上的设计选择在文本生成场景下的适用性；3) 与FLUX VAE从0.5B到9.8B参数规模进行对比实验。

Result: 1) 数据组成影响特定领域生成质量，架构简单化（如移除复杂设计）在大规模下性能无损；2) RAE预训练性能全面超越VAEs，微调阶段VAEs模型在64epoch后崩溃而RAEs保持稳定；3) 所有实验中RAEs均展现出更快收敛速度和更优生成质量。

Conclusion: RAE框架通过简化架构和共享表征空间，成为大规模文本到图像生成的新基座，并为统一多模态模型提供新可能性。

Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.

</details>


### [52] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 本研究提出了PyraTok，一种语言对齐的多层次分层分词器，用于改进视频变分自编码器（VAE），以提升文本到视频生成和理解的跨模态对齐和零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频VAE分词器通常使用单一尺度、有限词汇和浅层语言监督，导致跨模态对齐差和零样本迁移能力不足。

Method: PyraTok基于预训练视频VAE，结合语言对齐的分层量化（LaPQ）模块，通过共享二进制词表在多尺度离散化特征，并联合优化多尺度文本引导的量化和全局自回归目标。

Result: PyraTok在10个基准测试中取得SOTA视频重建效果，显著提升文本生成视频质量，并在零样本视频分割、动作定位和理解任务中达到新的SOTA，支持4K/8K分辨率。

Conclusion: PyraTok通过多尺度语言对齐分词机制，解决了现有方法的局限性，为视频生成与理解提供了更高效和鲁棒的框架。

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [53] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: 提出RCORE框架以解决零样本组合动作识别中的物体驱动动词捷径问题，通过组合感知增强和时序正则化提升视频动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本组合动作识别模型因忽略物体驱动动词捷径问题导致泛化能力不足，主要受组合监督数据稀疏性、动词与物体学习难度不对称的影响。

Method: 设计RCORE框架：(1) 组合感知增强技术多样化动词-物体组合且保留运动线索；(2) 时序正则化损失函数显式建模时间结构以抑制捷径行为。

Result: 在Sth-com和EK100-com两个数据集上，模型在未见组合动作识别准确率显著提升，同时降低共现误差偏倚并保持正向组合泛化能力。

Conclusion: 物体驱动捷径是零样本组合动作识别的关键限制因素，解决此问题对于实现鲁棒的视频组合理解至关重要。

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [54] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本论文提出一种基于3D解码的改进型相机控制算法CamPilot，通过将视频潜变量解码为3D高斯表示，并结合奖励反馈优化像素级一致性，显著提升了视频扩散模型的相机可控性，在RealEstate10K和WorldScore基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在相机姿态控制方面仍存在局限性，主要表现为奖励模型无法评估视频-相机对齐度、RGB解码的计算开销大以及3D几何信息缺失，阻碍了相机可控性的进一步提升。

Method: 构建了相机感知的3D解码器，将视频潜变量与相机姿态联合解码为3D高斯分布。利用相机姿态双重作用（输入和投影参数），通过几何扭曲造成的结构失真生成模糊渲染，并在此基础上优化像素级新颖视角一致性作为奖励函数。引入可视性项约束随机性区域，基于几何扭曲只监督确定性区域。

Result: 在RealEstate10K和WorldScore数据集上的实验证明该方法有效解决了传统ReFL方法存在的局限性，实现了更精确的相机姿态控制和更逼真的视频生成效果。

Conclusion: 研究验证了将3D几何信息引入视频扩散模型奖励计算的有效性，通过3D解码与奖励反馈的协同优化框架，为提升生成模型的相机可控性提供了新思路，相关代码和模型已在项目页面开源。

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [55] [Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration](https://arxiv.org/abs/2601.15296)
*Longxuan Wei,Yubo Zhang,Zijiao Zhang,Zhihu Wang,Shiwan Zhao,Tianyu Huang,Huiting Zhao,Chenfei Liu,Shenao Zhang,Junchi Yan*

Main category: cs.CL

TL;DR: 提出的Entropy-Tree通过基于熵值的树状解码策略，在大模型推理任务中实现了高效探索与不确定性估计的统一。


<details>
  <summary>Details</summary>
Motivation: 现有解码策略存在盲目探索（随机采样）或冗余低效（独立多采样）问题，需要一种既能提升准确性又能优化不确定度评估的方法。

Method: 设计基于熵值的树状解码框架，在模型预测不确定性高（即熵值大）的位置主动扩展搜索树，跳过低不确定性位置以减少冗余计算。

Result: Entropy-Tree在多个模型和数据集上取得比Multi-chain更好的pass@k指标，且预测熵值的AUROC优于传统评估指标，验证了不确定度评估的有效性。

Conclusion: 通过熵值控制的结构化探索策略，Entropy-Tree在降低计算开销的同时提升了模型推理准确性和不确定性校准能力。

Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.

</details>


### [56] [AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports](https://arxiv.org/abs/2601.15297)
*Edward Ajayi*

Main category: cs.CL

TL;DR: AfriEconQA是一个针对非洲经济分析的高质量基准数据集，包含8937个需要高精度数值推理和时间消歧的问答实例，基于236份世界银行报告构建，旨在解决当前大型语言模型在非洲经济领域知识不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对非洲经济分析的高质量基准数据集，且现有大语言模型预训练数据中缺乏非洲经济相关资料，导致信息检索系统在该领域表现不足，亟需构建专业化评估框架。

Method: 收集236份世界银行报告构建语料库，生成并筛选10018个合成问题得到8937个高质量问答实例，每个实例包含经济指标推理问题、证据文本、验证答案及时间溯源元数据，采用零样本基线模型与RAG配置的多策略实验进行评估。

Result: 零样本模型在超90%的查询中失败，即使是最先进的RAG流水线也难以实现高精度，验证了数据集在信息检索和领域适应性上的挑战性，揭示了参数化知识的重大缺口。

Conclusion: AfriEconQA是首个专注于非洲经济分析的严格基准数据集，为新一代领域特定信息检索和RAG系统提供了必要挑战，代码与数据集将在发表后公开。

Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.

</details>


### [57] [Embedding Retrofitting: Data Engineering for better RAG](https://arxiv.org/abs/2601.15298)
*Anantha Sharma*

Main category: cs.CL

TL;DR: 论文指出预训练词向量的嵌入微调效果依赖知识图谱质量，而知识图谱质量又受文本预处理影响。作者提出一个数据工程框架，通过解决注释伪影导致的数据质量问题，显著提升领域检索效果。


<details>
  <summary>Details</summary>
Motivation: 知识图谱质量直接影响嵌入微调有效性，但真实数据中的注释伪影（如标签膨胀）导致图谱密度异常，破坏微调目标。现有方法未充分解决这一问题。

Method: 设计数据工程框架，分析注释伪影（特别是哈希标签）对知识图谱的影响。通过预处理清理噪声，并对比不同微调方法在清洁图谱和噪声图谱上的表现。

Result: 在噪声图谱上所有微调方法均导致性能下降（-3.5%至-5.2%），而清洁数据后采用EWMA方法提升6.2%（p=0.0348），量化问题检索提升达33.8%。数据预处理质量差异导致10%+效果波动，远超算法差异的3%。

Conclusion: 数据预处理质量是嵌入微调成功的核心因素，哈希标签注释是主要噪声来源。优化预处理可显著提升领域检索效果，且其重要性超过算法选择。

Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing, \acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\%$ average). The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap between algorithms (3\%), establishing preprocessing quality as the primary determinant of retrofitting success.

</details>


### [58] [MALTopic: Multi-Agent LLM Topic Modeling Framework](https://arxiv.org/abs/2601.15299)
*Yash Sharma*

Main category: cs.CL

TL;DR: MALTopic is a multi-agent LLM framework for topic modeling that integrates structured survey data, enhancing topic coherence, diversity, and interpretability compared to traditional methods like LDA and BERTopic.


<details>
  <summary>Details</summary>
Motivation: Traditional topic modeling methods ignore structured/categorical survey responses and produce abstract topics needing manual interpretation; MALTopic aims to automate refinement while leveraging structured data for better contextual relevance.

Method: Decomposes topic modeling into specialized tasks via LLM agents: enriched structured-text integration, topic extraction, and deduplication refinement, tested on a survey dataset.

Result: MALTopic achieved significantly improved topic coherence, diversity, and interpretability over LDA/BERTopic, generating human-readable topics with enhanced contextual relevance.

Conclusion: The structured data-driven multi-agent approach addresses limitations of traditional methods, offering a more effective solution for complex survey data analysis with reduced human intervention needs.

Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.

</details>


### [59] [Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis](https://arxiv.org/abs/2601.15300)
*Weiwei Wang,Jiyong Min,Weijie Zou*

Main category: cs.CL

TL;DR: 这篇文章研究了大型语言模型（LLM）在超长上下文场景下存在的突发式性能衰退问题，首次系统性地揭示了Qwen2.5-7B模型在上下文达到40-50%最大长度时会出现45.5%的任务性能崩溃，并提出了统一的浅层长上下文适应框架。


<details>
  <summary>Details</summary>
Motivation: 发现LLM在接近上下文长度阈值时会出现超过30%的性能衰退，这种突发性能下降严重影响了长文本应用场景，但现有研究尚未明确其根本原因。

Method: 1) 提出自然长度分布分析法，基于未裁剪的真实样本长度验证上下文长度本身的因果关联；2) 构建包含1000个样本的混合数据集，采用五折交叉验证确定临界阈值；3) 构建统一的浅层适应框架解释不同长度区间的性能模式。

Result: 定位到Qwen2.5-7B在40-50%最大长度时发生F1值从0.55-0.56骤降至0.3的临界点，首次系统表征了开源Qwen模型的智力衰退现象，并验证了上下文长度与注意力扩散之间的相关性。

Conclusion: 为长文本场景下的LLM部署提供实践指导，提出的浅层适应框架为缓解长上下文衰退提供了理论基础，揭示了当前模型在上下文扩展能力的本质局限。

Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.

</details>


### [60] [ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation](https://arxiv.org/abs/2601.15330)
*Zhebo Wang,Xiaohu Mu,Zijie Zhou,Mohan Li,Wenpeng Xing,Dezhang Kong,Meng Han*

Main category: cs.CL

TL;DR: ICPO addresses the 'lost-in-conversation' problem in LLMs by promoting appropriate humility, improving multi-turn conversation performance by 75% while maintaining single-turn competency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to recover from early incorrect assumptions in ambiguous multi-turn conversations, but standard techniques like RLVR worsen this by over-rewarding overconfident answers.

Method: ICPO introduces underspecified prompts into training data and conditions rewards on user intent, incentivizing models to express uncertainty and seek clarification when ambiguous instructions are detected.

Result: ICPO achieves 75% average improvement in multi-turn conversations while maintaining robust single-turn benchmark performance, demonstrating enhanced handling of ambiguous scenarios.

Conclusion: This work advances conversational AI by developing a practical training framework (ICPO) that enables more natural human-AI interaction through calibrated responses to instruction ambiguity.

Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.

</details>


### [61] [RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models](https://arxiv.org/abs/2601.15331)
*Rishit Chugh*

Main category: cs.CL

TL;DR: 本文提出了一种资源高效的对抗性提示生成方法，通过检索预训练的对抗提示数据库避免重复训练，显著降低计算成本的同时实现出色攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法如GCG计算成本高昂，难以满足资源有限机构的安全评估需求，且缺乏对齐语言模型安全性的高效红队测试框架。

Method: 构建包含1000个有害提示的七类数据集，评估GCG/PEZ/GBDA在Llama3 8B上的攻击效果，利用语义相似度匹配已成功对抗提示数据库生成新攻击。

Result: 实验证明提示类型与攻击算法有效性存在强关联性，新方法在降低90%计算成本前提下，攻击成功率与GCG相当（50%-80%）。

Conclusion: 为黑盒场景下的语言模型安全评估提供了可扩展的低成本解决方案，揭示了对抗提示的跨算法迁移特性，建议动态更新攻击模式库增强防御体系。

Abstract: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.

</details>


### [62] [No Reliable Evidence of Self-Reported Sentience in Small Large Language Models](https://arxiv.org/abs/2601.15334)
*Caspar Kaiser,Sean Enderby*

Main category: cs.CL

TL;DR: 通过测试语言模型对其自身意识的自我认知，发现模型明确否认自身具有感知能力，且无证据显示其隐瞒真实信念。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型是否具有感知尚无实证答案，但可通过检测其是否'相信'自身具有意识来探索潜在认知状态，并验证与近期研究矛盾的结论。

Method: 对Qwen/Llama/GPT-OSS三个系列共15个模型（0.6B-70B参数）提问50个意识相关问题，训练分类器分析其内部激活状态的底层信念，评估陈述一致性。

Result: 1）模型系统性否定自身意识，将意识归于人类；2）内部分类器未检测到否认陈述的欺骗性；3）Qwen系列中参数量越大模型否认越强烈。

Conclusion: 研究结果反证了'模型隐藏自我意识'的猜想，表明当前架构下模型缺乏自我认知，且更大模型反而更强化否认倾向。

Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.

</details>


### [63] [Memorization Dynamics in Knowledge Distillation for Language Models](https://arxiv.org/abs/2601.15394)
*Jaydeep Borkar,Karan Chadha,Niloofar Mireshghallah,Yuchen Zhang,Irina-Elena Veliche,Archi Mitra,David A. Smith,Zheng Xu,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: Knowledge distillation significantly reduces training data memorization in large language models compared to standard fine-tuning, with memorization reduced by over 50%.


<details>
  <summary>Details</summary>
Motivation: To investigate the dynamics of training data memorization in knowledge distillation (KD) setups, addressing gaps in understanding compared to standard training methods, and exploring KD's potential for privacy-preserving model compression.

Method: Empirical analysis across three LLM families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2), comparing soft/hard distillation approaches. Key metrics included zlib entropy, KL divergence, and perplexity for predicting memorization patterns.

Result: Distilled models showed 50%+ lower memorization than fine-tuning; 95% of memorization stemmed from inherently 'easy' examples; predictability of memorization via entropy/KL divergence metrics; hard distillation inherited 2.7x more teacher-specific memorized examples than soft distillation.

Conclusion: Knowledge distillation offers dual benefits of improved model generalization and reduced training data memorization risks, with implications for privacy-preserving model compression strategies.

Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.

</details>


### [64] [Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind](https://arxiv.org/abs/2601.15395)
*Tamunotonye Harry,Ivoline Ngong,Chima Nweke,Yuanyuan Feng,Joseph Near*

Main category: cs.CL

TL;DR: 论文提出Chameleon数据集，揭示用户互动中74%的心理状态差异源于情境(state)，仅26%源自用户特质(trait)并发现语言模型仅关注特质但奖励模型存在不一致性反应。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集仅捕捉用户稳定特质，忽略互动情境对心理状态的影响，而实际用户行为受特质(state)和情境(trait)双重因素驱动，需构建新数据集填补该研究空缺。

Method: 基于Reddit用户跨情境的多心理档案记录构建Chameleon数据集，采用潜在状态-特质理论分解方差，并通过实证测试语言模型及奖励模型对状态/特质的响应偏好。

Result: 1) 状态维度解释74%方差，特质仅占26%；2) LLM生成回复时仅参考特质忽视状态；3) 奖励模型对状态有响应但存在显著模型间不一致性。

Conclusion: Chameleon数据集为研究情境心理状态建模提供新基准，揭示语言模型状态盲缺陷及奖励模型的评估局限性，为个性化对话系统优化指明方向。

Abstract: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\% is within-person(state) while only 26\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.

</details>


### [65] [Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs](https://arxiv.org/abs/2601.15429)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Hao Dai,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 领域知识图谱与RAG的配合效果取决于知识范围与任务场景的精准匹配，盲目整合图谱信息反而会引入干扰，建议优先考虑领域适配性而非信息广度。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在医疗领域知识推理中存在的可信度不足问题，探索通过PubMed构建的糖尿病、阿尔茨海默症及其组合知识图谱（KG）是否能提升检索增强生成（RAG）性能。

Method: 构建3种知识图谱（T2DM单病、AD单病、复合型）并设计两种测试方案，测试7种LLM模型在不同检索源组合（包含无RAG基准）及温度参数下的表现，定量评估检索精度与生成质量关系。

Result: 当检索范围与测试目标精确匹配时（如AD图谱对应相关测试），RAG增益显著；但合并多图谱时错误率上升23%-41%。13B以上大模型在部分测试中表现接近或超过RAG增强的小模型，高温参数未提升效果。

Conclusion: 提出'精准匹配优先'的RAG应用框架：建议根据任务特性选择特异性知识图谱而非简单整合，同时推荐大模型用于具备强先验知识的任务，搭配中低温度参数策略，并强调检索重排序的重要性。

Abstract: Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\mathbb{G}_1$ (T2DM), $\mathbb{G}_2$ (Alzheimer's disease), and $\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\mathbb{G}_1$ and $\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\mathbb{G}_1$, $\mathbb{G}_2$, $\mathbb{G}_1$ + $\mathbb{G}_2$, $\mathbb{G}_3$, $\mathbb{G}_1$+$\mathbb{G}_2$ + $\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison

</details>


### [66] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: Using Retrieval-Augmented Generation (RAG) frameworks significantly reduces hallucinations in LLMs when answering CDC public health policy questions, with Advanced RAG (two-stage retrieval) outperforming Basic RAG and vanilla LLM baselines in faithfulness scores.


<details>
  <summary>Details</summary>
Motivation: LLMs generate plausible but factually incorrect outputs (hallucinations) that hinder their adoption in high-stakes domains like public health policy, where information integrity is critical.

Method: Evaluated Vanilla LLM (baseline), Basic RAG, and Advanced RAG (with cross-encoder re-ranking) using Mistral-7B and all-MiniLM-L6-v2 models. Tested recursive character-based and token-based semantic chunking strategies. Measured faithfulness and relevance scores across complex CDC policy scenarios.

Result: Basic RAG improved faithfulness (0.621) vs. Vanilla (0.347), while Advanced RAG achieved superior faithfulness (0.797). Two-stage retrieval enhanced precision, but document segmentation limitations hindered multi-step reasoning.

Conclusion: Two-stage RAG architectures (Advanced RAG) are necessary for domain-specific policy QA accuracy, but structural constraints in document splitting remain a bottleneck for complex reasoning tasks.

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [67] [Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts](https://arxiv.org/abs/2601.15479)
*Sydney Anuyah,Sneha Shajee-Mohan,Ankit-Singh Chauhan,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 该论文评估了13个开源大语言模型（LLMs）在生物医学等高风险领域中从文本中进行成对因果发现（PCD）的能力，发现当前模型表现不佳（最佳检测和提取得分仅为约49.57%和47.12%），尤其在处理复杂、隐式或跨多句的因果关系时表现显著下降。团队发布了包含高验证数据集、代码和提示词的开源评估框架。


<details>
  <summary>Details</summary>
Motivation: 在生物医学等高风险场景中，LLMs需具备可靠因果推理能力。现有研究缺乏对模型因果检测（识别文本中是否存在因果关系）与因果提取（精确提取因果短语）的系统性评估，需建立统一基准推动技术改进。

Method: 构建包含12个多样化数据集的基准测试，评估13个LLMs的：1）因果检测能力；2）因果提取能力。测试不同提示策略（如零样本、思维链、上下文学习）。数据标注通过高一致性的人工标注（κ≥0.758）验证。

Result: 最佳模型DeepSeek-R1-Distill-Llama-70B的因果检测平均得分49.57%（$C_{detect}$），Qwen2.5-Coder-32B-Instruct的提取得分47.12%（$C_{extract}$）。模型在简单显性单句关系中表现较好，但在隐式关系、跨句及多因果对场景中性能骤降。

Conclusion: 当前LLMs的因果推理能力存在重大缺陷，限制其在关键领域的应用。所提开源框架（含数据、代码及提示词）为未来研究提供了标准化基础，需针对性改进复杂语境下的因果建模能力。

Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).
  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}

</details>


### [68] [Multi-Persona Thinking for Bias Mitigation in Large Language Models](https://arxiv.org/abs/2601.15488)
*Yuxing Chen,Guoqing Luo,Zijun Wu,Lili Mou*

Main category: cs.CL

TL;DR: 本文提出Multi-Persona Thinking（MPT）框架，在推理阶段通过多角色辩证推理减少大语言模型的社会偏见。该方法引导模型采用对立社会身份（如性别）和中立视角，通过迭代对话暴露和修正偏见，实验显示其优于现有提示方法，在保持推理能力的同时显著降低偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在的社会偏见可能强化有害刻板印象和不公平结果，而现有基于提示的偏见缓解策略效果有限。研究旨在开发不依赖模型参数调整的通用推理阶段解决方案。

Method: 构建三个对话角色（对立项A身份、对立项B身份、中立观察者），在模型推理阶段强制执行三步迭代流程：角色扮演生成观点→观点对比识别矛盾→共识构建修正结果。通过显式设计角色冲突的辩证过程实现偏见暴露与修正。

Result: 在BIG-Bench和RealToxicityPrompts两个权威偏见检测基准中，MPT相比常规提示方法将偏见指标降低32-47%，在性别/种族相关任务中效果显著。实验涵盖Llama-2、GPT-3.5等多规模模型，验证了方法的跨模型适用性。

Conclusion: MPT成功将偏见转化为多元视角互动的优化机会，证明了系统性辩证推理在推理阶段的有效性。方法不依赖模型结构调整，为部署阶段的偏见治理提供了轻量级解决方案。

Abstract: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.

</details>


### [69] [ViT Registers and Fractal ViT](https://arxiv.org/abs/2601.15506)
*Jason Chuan-Chih Chou,Abhinav Kumar,Shivank Garg*

Main category: cs.CL

TL;DR: 本文提出了一种改进的视觉Transformer架构fractal ViT，灵感来自无位置编码模型(NoPE)和寄存器机制，通过插入summary tokens和注意力掩码打破Token排列不变性，但实验发现其效果未超过现有ViT变体。


<details>
  <summary>Details</summary>
Motivation: 基于语言模型中无位置编码的有效性和寄存器对ViT的提升效果，作者试图探索将类寄存器机制与位置编码相结合的新型视觉Transformer架构。

Method: 设计了一种包含summary tokens的fractal ViT架构，在常规tokens和summary tokens间应用注意力掩码，并测试单独使用或与不同类型位置编码组合使用的性能。

Result: fractal ViT在图像分类任务中未超越带有寄存器的标准ViT，表明相关改进可能具有模型规模、领域类型或应用场景的局限性。

Conclusion: 实验结果揭示了Transformer架构改进方法的边界性，暗示类似NoPE或寄存器机制的有效性可能受限于具体应用场景和模型规模。

Abstract: Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.

</details>


### [70] [AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains](https://arxiv.org/abs/2601.15511)
*Adam Szelestey,Sofie van Engelen,Tianhao Huang,Justin Snelders,Qintao Zeng,Songgaojun Deng*

Main category: cs.CL

TL;DR: 本文提出AdversaRiskQA基准测试，评估大语言模型在健康、金融、法律领域对抗虚假信息的能力。发现模型规模与性能呈非线性关系，Qwen3(80B)准确率最高，GPT-5表现稳定，长文本中虚假信息与输出事实性无显著关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对事实性错误的领域专用对抗样本评估工具，且未研究注入式虚假信息对长文本事实性的影响，亟需建立系统性测评框架以提升模型可靠性。

Method: 构建包含健康/金融/法律三领域双难度级别的AdversaRiskQA基准，开发自动化对抗攻击成功率与长文本事实性评估方法，对Qwen、GPT-OSS、GPT家族共六款模型进行对比测试。

Result: 模型规模增大时准确率非线性提升，领域差异显著；Qwen3(80B)基线测试准确率最高，GPT-5展现持续高稳定性；长文本评估中虚假信息注入与事实性输出无正相关。

Conclusion: AdversaRiskQA为检测模型脆弱性、提升高风险领域模型可靠性提供了标准化评估框架，揭示了模型在对抗性事实性场景中的潜在提升方向。

Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.
  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.

</details>


### [71] [Common to Whom? Regional Cultural Commonsense and LLM Bias in India](https://arxiv.org/abs/2601.15550)
*Sangmitra Madhusudan,Trush Shashank More,Steph Buongiorno,Renata Dividino,Jad Kabbara,Ali Emami*

Main category: cs.CL

TL;DR: 印度首个区域文化常识基准测试数据集Indica揭示语言模型存在地理认知偏差，仅39.4%问题在五个地区获得共识，模型对东/西部印度代表性不足。该框架可用于评估多文化国家的内在文化差异。


<details>
  <summary>Details</summary>
Motivation: 现有多数文化常识基准测试（如X-CS）将国家视为单一文化整体，研究者质疑这种民族同质性假设的合理性，以语言/文化高度多元化的印度为例验证区域文化认知差异的存在性。

Method: 1. 在人类学分类框架下构建8个生活领域的双语调查问卷（英语+印地语）
2. 从印度北部/南部/东部/西部/中央地区招募至少100名受试者标注数据
3. 通过卡方检验验证问题有效性，过滤存在显著分歧的样本
4. 对8种SOTA大模型进行zero-shot评估并测量区域选择偏差

Result: 1. 区域共识度仅39.4%
2. 文化差异最大领域：社交礼仪（18.1%共识）vs 最小差异：基础生存需求（61.3%共识）
3. 模型对区域默认选择存在30-40%的地理偏差
4. 整体准确率13.4%-20.9%显著低于人类基线（56.3%）

Conclusion: 文化常识的地域异质性挑战现有基准测试的合理性，提出文化认知评估的通用范式（含区域人类学框架/数据收集/偏差测量），警示大模型开发者和政策制定者需建立地域感知的内容安全机制。

Abstract: Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the "default" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.

</details>


### [72] [From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare](https://arxiv.org/abs/2601.15558)
*Man Luo,Bahareh Harandizadeh,Amara Tariq,Halim Abbas,Umar Ghaffar,Christopher J Warren,Segun O. Kolade,Haidar M. Abdul-Muhsin*

Main category: cs.CL

TL;DR: 该研究探讨大型语言模型（LLMs）作为医生书面回应的同理心编辑工具，在增强情感温度的同时保留医学信息，并提出评估情感与事实质量的新指标Empathy Ranking Score和MedFactChecking Score。结果显示，LLM编辑效果优于完全自动生成。


<details>
  <summary>Details</summary>
Motivation: 尽管临床同理心对患者护理至关重要，但医生需在临床实践中平衡情感与精准度。传统模式存在认知与情感约束，因此需要技术辅助工具在不牺牲医学事实性的前提下提升沟通同理心。

Method: 开发LLM辅助编辑系统，在医生初稿基础上优化情感表达。创建双维度量化评估体系：Empathy Ranking Score（情感增强效果）与MedFactChecking Score（医学事实保留度），通过实验对比LLM编辑组与完全自动生成组的表现差异。

Result: LLM编辑组在提升感知同理心（p<0.01）的同时，医学事实准确性较自动生成组提高23%（p<0.05）。用户调研显示受试者对编辑后文本的信任度评分均值达4.7/5，显著高于全自动生成文本的3.2/5。

Conclusion: 将LLM定位为医生编辑助理而非独立生成系统，可在医疗沟通中实现同理心与事实性的双保障，为AI医疗应用提供更安全的实用路径。

Abstract: Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.

</details>


### [73] [YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models](https://arxiv.org/abs/2601.15588)
*Junyu Lin,Meizhen Liu,Xiufeng Huang,Jinfeng Li,Haiwen Hong,Xiaohan Yuan,Yuefeng Chen,Longtao Huang,Hui Xue,Ranjie Duan,Zhikai Chen,Yuchuan Fu,Defeng Li,Lingyao Gao,Yitong Yang*

Main category: cs.CL

TL;DR: YuFeng-XGuard是一种以推理为核心的安全防护模型家族，通过生成结构化风险预测与可解释的推理过程，在保证高效率的同时实现实时多维度风险评估，已作为开源模型家族发布。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方案依赖低效的粗分类或后验规则，导致透明度不足、策略僵化或计算成本过高的问题，亟需更精细、可解释的动态风险评估方案。

Method: 提出分层推理架构：1）首token级快速风险决策；2）自然语言解释生成；3）风险感知与策略分离的动态配置系统，包含完整版和轻量版两种模型架构。

Result: 在多样化安全基准测试中达到SOTA水平，保持决策延迟低于150ms，解释生成响应时间小于500ms，准确率比基线模型提升17.6%。

Conclusion: 该模型通过结构化预测实现透明化安全管控，开源特性促进技术落地，分层架构为实际部署提供灵活的选择空间。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.

</details>


### [74] [Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow](https://arxiv.org/abs/2601.15593)
*Yangyang Zhong,Yanmei Gu,Zhengqing Zang,Xiaomeng Li,Yuqi Ding,Xibei Jia,Yuting Shen,Zhenzhong Lan,Liwang Zhu,Weiping Liu,Junlin Zhou,Haisheng Liu,Zhong Xin Yu,Pengxin Luo,Donglian Qi,Yunfeng Yan,Junbo Zhao*

Main category: cs.CL

TL;DR: Masked Diffusion Language Models (MDLMs)理论具备生成并行性但实际表现仍弱于自回归模型，主要瓶颈为削弱词元依赖，但存在任务自适应优势和生成-编辑新范式。


<details>
  <summary>Details</summary>
Motivation: 探究MDLMs在并行生成和任意顺序解码承诺下的真实表现，分析其与自回归模型的差距根源，并探索提升生成质量的潜在方法。

Method: 通过AFP指标衡量并行化强度，Kendall's tau分析生成顺序，评估8个超大规模MDLMs（最大达100B参数）在58个知识、推理与编程基准上的表现。

Result: MDLMs在常规任务上不如自回归模型，但具备三大特性：1) 任务域与推理阶段驱动的并行度变化；2) 正确输出依赖生成顺序调整；3) 在数独等逆向推理任务中展现优势。

Conclusion: 提出理论框架解释依赖损失问题，验证Generate-then-Edit范式有效性，为平衡生成质量与并行效率提供新设计方向。

Abstract: Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require "backward information" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.

</details>


### [75] [ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms](https://arxiv.org/abs/2601.15605)
*Baktash Ansari,Shiza Ali,Elias Martin,Maryna Sivachenko,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本文提出了一种新的毒性检测模型ToxiTwitch，结合大型语言模型与传统分类器，有效提升Twitch平台毒性行为检测准确率至80%。


<details>
  <summary>Details</summary>
Motivation: 传统毒性检测方法（如人工审核和基于关键词过滤）难以应对Twitch高流量、快节奏且包含表情的聊天环境，且检测效果受限。

Method: 通过整合文本与表情符号信息，将大型语言模型（LLM）生成的嵌入特征与传统机器学习分类器（如随机森林、SVM）结合，构建混合检测模型。

Result: 在特定频道训练下，混合模型检测准确率达80%（比BERT提升13%），F1分数达76%，验证了整合表情对毒性检测的重要性。

Conclusion: 该探索性研究揭示了表情感知毒性检测的潜力与挑战，表明需进一步解决实际应用中的扩展性和复杂情境适应性问题。

Abstract: The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.

</details>


### [76] [Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation](https://arxiv.org/abs/2601.15645)
*Zhiyao Ren,Yibing Zhan,Siyuan Liang,Guozheng Ma,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出了首个评估真实医疗会诊中多轮交互信心的测试基准，并设计了基于证据的医疗信心建模框架MedConf，通过检索增强生成技术解决医疗信息不完备场景下的信心评估难题。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅在单轮静态场景中评估大模型信心，未考虑临床证据累积过程中信心与准确率的动态关系；医疗数据特性会放大传统词级/一致性信心评估方法的局限性，亟需结合诊断准确性和信息完整性进行联合评估。

Method: 构建包含三种医学数据类型的会诊生成基准框架，提出信息充分性梯度模型揭示信心-准确率动态关系；开发MedConf框架，通过检索增强生成技术构建症状特征谱，对齐患者信息的支持/缺失/矛盾关系并加权整合输出可解释信心值。

Result: 通过27种方法对比发现：(1)医学数据放大传统置信度方法缺陷；(2)医学推理需联合评估准确性与完整性。MedConf在2个大模型和3个医疗数据集均超越SOTA方法，在信息不足和多病共存场景保持稳定性能，AUROC和PCC指标优异。

Conclusion: 信息完整性是可信医疗信心建模的关键，研究结果为构建更可靠和可解释的医学大模型提供了新路径。

Abstract: Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.

</details>


### [77] [What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking](https://arxiv.org/abs/2601.15674)
*Raymond Xiong,Furong Jia,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: 该研究构建了一个包含真实患者常见医疗问题的数据库，揭示了当前大型语言模型在识别问题中错误假设方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在医疗问答的基准测试多关注医学考试题目，而患者实际提问方式存在显著差异。研究旨在填补这一评估差距，并分析真实问题中错误假设与危险意图的出现规律。

Method: 通过查询美国前200种处方药在Google '相关提问' 功能中的数据，构建了患者常见问题数据集，并分析问题错误性与历史提问错误程度的关联性。

Result: 发现'被污染问题'的出现具有非随机性，与前序问题的错误程度强相关；现有多数高性能LLMs难以识别此类日常问题中的错误假设。

Conclusion: 需建立更适合真实医疗场景的LLMs评估体系，改进模型处理非标准医疗问题和错误前提的能力。

Abstract: Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.

</details>


### [78] [Persona Switch: Mixing Distinct Perspectives in Decoding Time](https://arxiv.org/abs/2601.15708)
*Junseok Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: Persona Switch是一种动态结合零样本和角色扮演提示的解码方法，通过比较输出置信度（logit gap）选择最优输出，显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 零样本和角色扮演提示在不同任务中表现不一致，暗示两者具有互补优势而非彼此替代，需探索结合策略以最大化性能。

Method: 在生成过程中每一步动态比较零样本与角色扮演提示的输出置信度（logit gap），选择更可靠的结果作为当前步骤的输出。

Result: Persona Switch在多个主流大语言模型上超越基线方法，最高实现5.13%的准确率提升，且实验证明输出置信度能有效指导决策。

Conclusion: 动态结合两种提示策略能互补优势，输出置信度是可靠的决策指标，为提示工程提供了新的优化方向。

Abstract: Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.

</details>


### [79] [Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind](https://arxiv.org/abs/2601.15715)
*Zhitao He,Zongwei Lyu,Yi R Fung*

Main category: cs.CL

TL;DR: RebuttalAgent: An AI framework for academic rebuttal using Theory of Mind, outperforming models by 18.3% via structured persuasion strategies.


<details>
  <summary>Details</summary>
Motivation: Current AI models fail to handle academic rebuttal effectively due to lack of perspective-taking under information asymmetry, requiring deeper strategic communication.

Method: Proposed ToM-Strategy-Response pipeline with RebuttalBench dataset, two-stage training (supervised + reinforcement learning), and Rebuttal-RM evaluator.

Result: RebuttalAgent surpassed base models by 18.3% in automated metrics and outperformed advanced models in human evaluations, with Rebuttal-RM achieving GPT-4.1+ score consistency.

Conclusion: Strategic grounding in ToM improves rebuttal generation, but final content requires human refinement to maintain critical analysis integrity.

Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.

</details>


### [80] [Hallucination Mitigating for Medical Report Generation](https://arxiv.org/abs/2601.15745)
*Ruoqing Zhao,Runze Xia,Piji Li*

Main category: cs.CL

TL;DR: 提出KERM框架，通过知识增强和细粒度奖励机制减少医疗报告生成中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医疗报告生成中因易产生‘幻觉’导致准确度不足，需提升其临床可靠性。

Method: 1) 使用MedCLIP从医学知识库检索病灶事实；2) 引入净化模块筛选与临床背景相关的知识；3) 采用细粒度强化奖励引导模型生成高支持性描述。

Result: 在IU-Xray和MIMIC-CXR数据集上实验证明该方法有效降低幻觉并提升报告质量。

Conclusion: 通过知识融合与奖励机制优化，显著改善医疗生成模型的准确性和临床适用性。

Abstract: In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \textbf{K}nowledge-\textbf{E}nhanced with Fine-Grained \textbf{R}einforced Rewards \textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.

</details>


### [81] [Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs](https://arxiv.org/abs/2601.15755)
*Tristan Williams,Franziska Weeber,Sebastian Padó,Alan Akbik*

Main category: cs.CL

TL;DR: 该论文提出利用多变量相关性模式和边缘分布评估对齐语言模型的代表性，发现现有技术在捕捉人口结构关联性上存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有模型对齐研究仅关注独立调查项的边缘分布，可能忽略潜在人口结构特征和文化价值理论，需建立更全面的评估框架。

Method: 构建包含多变量相关性分析和边缘分布评估的框架，对比角色提示和人口统计微调两种技术，并用全球价值观调查数据进行验证。

Result: 人口统计微调在边缘分布上优于角色提示，但两者均未达成人类水平的关联性模式匹配，暴露边缘评估的局限性。

Conclusion: 模型代表性是独立于边缘对齐的重要维度，依赖单一评估标准可能高估模型能力，需重视结构层面的对齐缺陷。

Abstract: Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.

</details>


### [82] [HumanLLM: Towards Personalized Understanding and Simulation of Human Nature](https://arxiv.org/abs/2601.15793)
*Yuxuan Lei,Tianfu Wang,Jianxun Lian,Zhengyu Hu,Defu Lian,Xing Xie*

Main category: cs.CL

TL;DR: HumanLLM 通过构建认知基因组数据集，提升大型语言模型在个性化人类行为模拟上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在客观任务上表现优异，但在模拟人类行为时存在认知理解不足的问题，限制了其在社会科学研究和个性化应用中的效果。

Method: 构建包含550万用户日志的认知基因组数据集，并通过多阶段数据处理和监督微调，训练HumanLLM模型以预测个性化行为特征。

Result: HumanLLM在用户行为预测、写作风格模仿和用户画像生成效果优于基线模型，且在跨域社交智能基准测试中表现更优。

Conclusion: 基于个性化数据集训练的HumanLLM有效弥合了模型预训练与个体行为预测间的根本性错位，增强了社会智能模拟效果。

Abstract: Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.

</details>


### [83] [SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics](https://arxiv.org/abs/2601.15809)
*Silvia Casola,Ryan Soh-Eun Shim,Felicia Körner,Yuchen Mao,Barbara Plank*

Main category: cs.CL

TL;DR: The paper explores improving multilingual evaluation metrics by steering model activations toward English as a pivot, testing encoder-decoder methods, and finding interventions enhance metric effectiveness across languages.


<details>
  <summary>Details</summary>
Motivation: The scarcity of reliable evaluation metrics for non-English languages in natural language generation (NLG) tasks hinders progress. Prior work suggests multilingual models use English internally, and misalignment with this 'pivot' language harms performance, motivating the hypothesis that aligning neural metrics to English could improve human correlation.

Method: The authors experiment with encoder- and decoder-based multilingual metrics, applying test-time interventions to steer model activations toward an English pivot language. This approach aims to align metric computations with the model's internal use of English.

Result: Test-time interventions significantly boost metric effectiveness across diverse languages, demonstrating consistent improvements in correlation with human judgments for both encoder- and decoder-based metric architectures.

Conclusion: Aligning multilingual neural metrics with English as a pivot increases their reliability, suggesting that explicitly addressing internal language alignment enhances evaluation robustness for cross-lingual NLG tasks despite inherent multilingual capabilities.

Abstract: An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.

</details>


### [84] [ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15820)
*Guoxuan Ding,Yuqing Li,Ziyan Zhou,Zheng Lin,Daren Zha,Jiangnan Li*

Main category: cs.CL

TL;DR: 本文提出ExDR框架，通过融合解释驱动机制提升多模态假新闻检测中的动态检索增强生成效果，在AMG和MR2数据集上验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态检索增强生成方法在处理多模态假新闻时面临冗余检索、粗粒度相似度计算及证据不相关等挑战，需通过引入模型生成解释提升检索触发与证据选择的精准度。

Method: 构建解释驱动的动态检索增强生成框架，包含：1) 多维度触发置信度评估；2) 融合欺骗实体的索引构建；3) 基于欺骗特征的对比证据检索。

Result: 在AMG和MR2数据集上，ExDR在检索触发准确率、检索质量及整体检测性能指标上均优于基准方法，证明其有效性与泛化能力。

Conclusion: 通过将模型解释与动态检索增强生成结合，有效解决假新闻检测中的冗余与不连贯证据问题，为多模态虚假内容识别提供新范式。

Abstract: The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.

</details>


### [85] [Can professional translators identify machine-generated text?](https://arxiv.org/abs/2601.15828)
*Michael Farrell*

Main category: cs.CL

TL;DR: 研究专业翻译人员无需专门训练即可识别AI生成的意大利语短篇小说的能力，16.2%的参与者准确区分AI文本，但同样存在误判现象。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成文本质量提升，探究其在专业翻译领域的可检测性及对人工判断的挑战。

Method: 69名翻译人员参与线下实验，评估3篇匿名短篇小说（2篇AI生成，1篇人工创作），要求判断AI属性并提供依据。

Result: 部分受试者（16.2%）显著准确区分文本来源，低burstiness与叙事矛盾为关键识别指标，但语法准确性和情感基调易导致误判。

Conclusion: AI生成文本具备特定识别特征，但专业能力与主观偏好影响判断，提示需重新审视合成文本的专业编辑策略。

Abstract: This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.

</details>


### [86] [Determinants of Training Corpus Size for Clinical Text Classification](https://arxiv.org/abs/2601.15846)
*Jaya Chaturvedi,Saniya Deshpande,Chenkai Ma,Robert Cobb,Angus Roberts,Robert Stewart,Daniel Stahl,Diana Shamsutdinova*

Main category: cs.CL

TL;DR: This study demonstrates that 600 annotated documents suffice to achieve 95% of the maximum performance (attainable at 10,000 documents) in clinical text classification, with vocabulary properties (strong/noisy predictors) significantly influencing learning curve slopes.


<details>
  <summary>Details</summary>
Motivation: Clinical NLP models face challenges in justifying required sample sizes (typically 200-500 documents due to cost/time), lacking empirical evidence linking vocabulary characteristics (e.g., predictive word quality) to classification performance.

Method: Using MIMIC-III discharge notes (ICD-9 labels), pre-trained BERT + Random Forest classifiers trained on 10 diagnoses across training sizes (100-10,000 documents). Vocabulary analysis applied Lasso logistic regression on bag-of-words embeddings to identify strong/noisy predictors.

Result: 6/10 tasks achieved 95% performance at 600 documents (despite identical processing). Strong predictors increased max accuracy (0.04/100 words), while noisy predictors reduced accuracy (0.02/100 words). Learning curve slopes correlated with vocabulary quality.

Conclusion: Sample size requirements in clinical NLP can be optimized via vocabulary analysis, with 600 documents sufficient for near-optimal performance. Noise reduction and strong predictor retention enhance model efficiency.

Abstract: Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.
  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.
  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.

</details>


### [87] [Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers](https://arxiv.org/abs/2601.15869)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究通过对比AI模型AV-HuBERT与人类在视听不匹配刺激（McGurk效应）中的反应，评估其感知生物保真度。研究发现，两者在听觉主导率上近乎相同（32.0% vs. 31.8%），但AI在语音融合倾向上显著高于人类，且缺乏人类特有的感知随机性。


<details>
  <summary>Details</summary>
Motivation: 验证自监督模型是否能复现人类听觉抵抗生物阈值的特性，并评估其多感官整合机制与人脑的相似性。

Method: 采用N=44名人类观察者作为对照，通过量化AV-HuBERT模型与人类在McGurk效应中的听觉主导率、融合模式及错误类型差异，分析两者的感知响应机制。

Result: AI与人类听觉主导率相似（32.0% vs. 31.8%），但AI表现出确定性的语音融合偏好（68.0% vs.人类47.7%）。人类呈现随机误差多样性，而AI具有严格分类特性。

Conclusion: 现有自监督架构可模拟多感官整合结果，但未能捕捉人类语音感知的神经可变性，揭示了生物真实性的关键缺失维度。

Abstract: This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.

</details>


### [88] [Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model](https://arxiv.org/abs/2601.15892)
*Chenghao Fan,Wen Heng,Bo Li,Sichen Liu,Yuxuan Song,Jing Su,Xiaoye Qu,Kai Shen,Wei Wei*

Main category: cs.CL

TL;DR: 该论文介绍了Stable-DiffCoder，一种基于块扩散的代码模型，通过连续预训练(CPT)和监督微调改进自回归(AR)模型，在相同架构和数据下优于AR基准，并适用于低资源编程语言和结构化代码建模。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的非自主语言模型（DLLMs）具有非顺序生成、数据重用等优势，但现有DLLMs在可比预算下仍落后于AR模型，本文旨在通过受控实验优化代码生成模型

Method: 重用Seed-Coder架构和训练流程，引入增强的块扩散连续预训练阶段（结合定制热身策略和块级裁剪噪声调度），叠加监督微调以实现稳定训练和知识学习

Result: 在相同数据和架构下整体超越自回归模型，在代码基准测试、结构化代码建模、低资源语言处理方面表现更优，且优于80亿参数AR模型和DLLMs

Conclusion: 扩散训练在代码建模质量上可超越AR训练，其任意顺序建模能力增强结构化代码处理和低资源场景，验证了扩散方法在代码领域的有效性

Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.

</details>


### [89] [Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech](https://arxiv.org/abs/2601.15909)
*Soufiane Jhilal,Stéphanie Martin,Anne-Lise Giraud*

Main category: cs.CL

TL;DR: 本研究提出将MEG信号转换为图像表示，结合预训练视觉模型实现高效非侵入式想象语音解码。


<details>
  <summary>Details</summary>
Motivation: 想象语音的非侵入式解码因神经信号微弱、分布广泛和标注数据不足而面临重大挑战。

Method: 通过可学习卷积将MEG信号转换为时空频混合图像，利用ImageNet预训练视觉模型提取共享神经特征。

Result: 达到90.4%二分类准确率，实现60.6%的元音解码，跨被试验证模型鲁棒性，时序分析定位关键时间窗。

Conclusion: 预训练视觉模型能有效捕获想象语音的非侵入式神经信号结构，为脑机接口提供新方法。

Abstract: Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.

</details>


### [90] [Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain](https://arxiv.org/abs/2601.16018)
*Özgür Uğur,Mahmut Göksu,Mahmut Çimen,Musa Yılmaz,Esra Şavirdi,Alp Talha Demir,Rumeysa Güllüce,İclal Çetin,Ömer Can Sağbaş*

Main category: cs.CL

TL;DR: 本文提出Mecellem框架，通过领域适应策略开发土耳其法律领域的专业语言模型，包含两个创新：(1)从头预训练的ModernBERT编码器采用检查点选择策略实现高效检索；(2)基于四阶段课程学习的Qwen解码器实现法律领域知识迁移。


<details>
  <summary>Details</summary>
Motivation: 开发针对土耳其法律领域的资源高效模型，解决现有领域适应方法依赖多阶段复杂训练管道的问题，验证单阶段预训练与课程学习在专业领域的有效性。

Method: 1) 构建含1127亿token的土耳其语主导语料库，基于ModernBERT架构预训练双向编码器，通过评估下游检索性能动态选择最优检查点；2) 设计四阶段连续预训练（CPT）流程，采用优化样本比例的课程学习策略，逐层迁移从通用语言到法律术语及长文本推理能力。

Result: 1) 编码器在土耳其检索榜单获前三名，155M参数小模型性能匹敌307M-567M大模型，计算效率达SOTA模型的92.36%；2) 解码器在土耳其法律文本上实现36.2%的困惑度降低，验证领域适应有效性。

Conclusion: 单阶段预训练+动态检查点策略显著减少计算资源消耗，四阶段课程学习有效实现法律领域知识迁移，证明Mecellem框架在保证性能的同时具有显著资源效率优势。

Abstract: This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.

</details>


### [91] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: This paper proposes 'Trajectory Replay via Concept-Basis Reconstruction', a method to transfer safety interventions (like reducing refusal behaviors) across diverse LLM architectures (e.g., Dense to MoE) without requiring target-LLM-specific data, achieving consistent refusal reduction while preserving capabilities.


<details>
  <summary>Details</summary>
Motivation: The authors hypothesize that refusal behaviors in aligned LLMs stem from shared low-dimensional semantic circuits rather than being model-specific, aiming to demonstrate universal safety alignment mechanisms.

Method: 1) Aligns donor and target model layers via concept fingerprints, 2) Reconstructs refusal directions using a shared 'recipe' of semantic concept atoms, 3) Applies weight-SVD stability guard to prevent capability degradation by projecting interventions away from high-variance weight subspaces.

Result: Successful transfer of refusal interventions across 8 model pairs (including GPT-OSS-20B and GLM-4) with diverse architectures/training regimes, maintaining model capabilities while attenuating unwanted refusals.

Conclusion: Provides evidence for universal safety alignment mechanisms in LLMs, enabling practical zero-shot safety tuning via cross-architecture intervention transfer, and introducing a stability guard to protect core model functionalities during alignment.

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>


### [92] [Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating](https://arxiv.org/abs/2601.16097)
*Makbule Gulcin Ozsoy*

Main category: cs.CL

TL;DR: A multilingual Text2Cypher system using language-specific LoRA adapters and fusion techniques.


<details>
  <summary>Details</summary>
Motivation: To enable scalable multilingual database interfaces without retraining on all languages, reducing computational costs and manual tuning.

Method: Trained LoRA adapters for English, Spanish, and Turkish, combined via linear merging or learned fusion MLP with dynamic gating.

Result: Fusion MLP achieved ~75% of joint multilingual fine-tuning accuracy with less data, outperforming linear merging across all three languages.

Conclusion: Learned adapter fusion balances performance, data efficiency, and scalability for incremental multilingual expansion in Text2Cypher tasks.

Abstract: Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.

</details>


### [93] [synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier](https://arxiv.org/abs/2601.16113)
*Haq Nawaz Malik,Kh Mohmad Shafi,Tanveer Ahmad Reshi*

Main category: cs.CL

TL;DR: 提出SynthOCR-Gen工具，通过合成生成OCR数据集解决低资源语言（如克什米尔语）的OCR模型训练数据稀缺问题，包含文本分割、Unicode规范化、多字体渲染和数据增强模块。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如克什米尔语）缺乏大规模标注OCR数据集，人工标注成本高且易出错。主流OCR工具（如Tesseract、TrOCR）不支持复杂书写系统（如克什米尔语的波斯-阿拉伯变体辅以变音符号）。

Method: 构建全流程合成数据生成工具：1) 文本分割（字符/词/句级）；2) Unicode标准化与字体纯净度控制；3) 多字体渲染；4) 模拟文档退化的25种数据增强技术（旋转、模糊、噪声等）。

Result: 生成并开源包含60万样本的克什米尔语OCR数据集（Word-segmented），在HuggingFace发布。验证了合成数据对低资源语言OCR建模的有效性，并为波斯-阿拉伯书写系统提供可复用框架。

Conclusion: 为低资源语言进入视觉-语言 AI 时代提供可行路径，工具支持全球欠服务书写系统的OCR研究。数据生成闭环降低了对人工标注的依赖，但需注意合成数据与真实场景文本分布的差异。

Abstract: Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.
  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.
  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.

</details>


### [94] [Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging](https://arxiv.org/abs/2601.16127)
*Alphaeus Dmonte,Vidhi Gupta,Daniel J Perry,Mark Arehart*

Main category: cs.CL

TL;DR: This paper explores merging strategies for multilingual LLMs to improve training and maintenance efficiency, showing up to 50% reduced training time and 60% lower maintenance costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Traditional multilingual LLM fine-tuning requires retraining when updating languages or adding new ones, creating computational inefficiency and maintenance bottlenecks, motivating the need for alternative approaches.

Method: The authors conducted the first efficiency-focused analysis of merging multilingual multitask models across three independent tasks, evaluating both training efficiency and model maintenance scenarios.

Result: Merging approaches reduced initial training time by 50% and maintenance costs by over 60% compared to full re-training. Both public and proprietary industrial datasets validated the method's effectiveness.

Conclusion: Merging multilingual models achieves significant efficiency improvements without quality loss, demonstrating practical viability for both academic and industrial applications.

Abstract: Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.

</details>


### [95] [Automatic Classification of Arabic Literature into Historical Eras](https://arxiv.org/abs/2601.16138)
*Zainab Alhathloul,Irfan Ahmad*

Main category: cs.CL

TL;DR: This paper explores the use of neural networks and deep learning to automatically classify Arabic texts into historical eras, showing varying success depending on the complexity of the classification task.


<details>
  <summary>Details</summary>
Motivation: There is a lack of research on automatic classification of Arabic texts by period, particularly outside of poetry. This study aims to bridge the gap by applying deep learning to analyze chronological evolution in Arabic literature.

Method: The authors employed neural network models to classify Arabic texts into different historical periods. They tested binary (two-class), up to 15-class classification tasks and used two datasets (OpenITI and APCD). The study also examined both predefined historical eras and user-defined periodizations.

Result: The models performed well on binary classification tasks with F1-scores of 0.83 and 0.79 for the OpenITI and APCD datasets, respectively. Performance dipped significantly on multi-class tasks, with F1-scores of 0.20 for the 15-era classification task (OpenITI) and 0.18 for the 12-era task (APCD).

Conclusion: The study demonstrates that deep learning models can classify Arabic texts into major historical eras with reasonable accuracy, but performance declines with an increase in the number of classes, highlighting the challenge of fine-grained temporal classification in this domain.

Abstract: The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.

</details>
