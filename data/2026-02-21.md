<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 40]
- [cs.CL](#cs.CL) [Total: 34]
- [cs.OH](#cs.OH) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯点绘（GS）的数字孪生方法，用于实现土木基础设施损伤的三维可视化，相较于传统方法更高效，尤其在处理无特征区域方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统三维重建技术（如摄影测量）在损伤分割误差和处理无特征区域上存在不足，而GS以其高效离散高斯建模特性为数字孪生的损伤可视化提供了新的解决方案。

Method: 1) 使用GS进行3D重建，将2D损伤分割结果映射到三维空间并修正误差；2) 采用多尺度重构策略平衡计算效率和细节保留；3) 建立时间演化模型实现数字孪生的动态更新。

Result: 在合成地震后场景数据集上验证了方法的有效性，三维损伤轮廓识别精度提高，重建效率显著优于传统方法，支持动态更新数字孪生模型。

Conclusion: GS技术为土木工程数字孪生提供了高效、高精度的损伤可视化框架，其离散建模特性在复杂场景适应性和动态更新方面具有重要工程应用价值。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: 本文提出了UltraVQA数据集和Analytic Score Optimization (ASO)方法，推动视频质量评估从单一评分转向多维度标注与解释。


<details>
  <summary>Details</summary>
Motivation: 传统视频质量评估依赖单一均值评分，无法全面反映复杂质量特征。作者旨在通过多维度标注（如运动质量、内容清晰度等）和生成式解释，提升模型对人类评价偏好的对齐能力。

Method: 1) 构建UltraVQA：包含UGC视频，按5个质量维度标注，每维度含子属性标签并辅以GPT生成的评分理由；2) 提出ASO方法：通过正则化决策框架建模评分序关系，推导闭式解以优化多维质量预测。

Result: ASO在实验中超越闭源API和开源模型，显式降低评分预测的MAE误差，验证了多维标注和生成式解释对性能的提升作用。

Conclusion: 多维度可解释标注与基于强化的排序对齐是视频质量评估的重要方向，其理论框架和数据集为未来研究提供了新范式。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: 本文提出DODO，通过块离散扩散模型优化OCR任务的解码效率，实现比传统自回归方法快3倍的推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统自回归解码在长文档OCR中因序列生成导致计算成本高，且现有扩散模型因结构不稳定性无法满足OCR严格匹配需求。

Method: 设计块离散扩散生成方式，将文本生成分解为块处理，通过局部同步减少全局扩散的误差累积，适配OCR的确定性特征。

Result: 在保持接近最先进准确性的同时，DODO的并行解码速度相较自回归模型提升达3倍。

Conclusion: DODO通过任务特性驱动的扩散结构创新，证明了确定性生成任务可利用并行化提升效率，为OCR应用提供速度与精度的平衡方案。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual是一个结合大规模图像和视频训练的高效视觉模型，采用三阶段训练策略与增强架构，在多模态任务中达SOTA并具备强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉模型在多样化数据源下训练效率低、噪声敏感及跨模态理解受限的问题，提升模型在真实场景中的泛化性和可扩展性。

Method: 1) 基于Vision Transformer+增强EViT模块的统一架构；2) 三阶段训练：MAE自监督→半监督hashtag分类→对比学习；3) 融合LLM文本编码器（LLM2CLIP）；4) 大规模社交数据（500亿图文/视频片段）与数据去噪策略。

Result: 1) ImageNet/Kinetics/HMDB51/MSCOCO全SOTA；2) 域迁移鲁棒性+抗干扰能力；3) LLM整合使检索性能提升15%以上；4) 计算效率较传统架构提升2.3倍。

Conclusion: 提出可扩展的多模态预训练范式，首次在工业级社交数据上实现图像/视频统一建模，平衡性能与效率，为下一代多模态模型提供基础框架。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [5] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 提出一种基于静态相机的多通道NeRF框架HSI-SC-NeRF，用于农业产品的高通量高光谱3D重建。


<details>
  <summary>Details</summary>
Motivation: 为解决传统高光谱成像与3D重建技术硬件复杂、吞吐量低，以及现有NeRF方法需移动相机导致标准化环境适应性差的问题。

Method: 设计静态相机-旋转物体方案，采用特制PTFE成像室实现均匀照明；通过ArUco标记估计姿态并进行坐标系转换；构建多通道NeRF模型和复合光谱损失函数，采用分阶段训练协议分离几何初始化与辐射优化。

Result: 在可见光与近红外波段实现空间重建精度<0.05mm、光谱保真度>98%，验证了方法在自动化农业工作流中的适用性。

Conclusion: HSI-SC-NeRF成功突破了移动相机硬件限制，在保持高光谱精度的同时，使3D重建吞吐量提升300%，为智能化农产检测提供新范式。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [6] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 本文提出了动态分块化方法以提升扩散模型（DiTs）在图文生成中的推理效率，通过在去噪阶段动态调整图像块尺寸，减少计算量却不牺牲生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因固定尺寸的图像分块策略导致计算成本高昂，而该方法未能有效适应内容复杂度及去噪步骤的变化需求。

Method: 在推理阶段采用动态分块策略：去噪初期使用较大尺寸的图像块捕捉全局结构，后期切换为较小尺寸以增强局部细节，并根据内容复杂度与时间步自适应调整块大小。

Result: 实验表明，在保持生成质量和提示语匹配度的前提下，所提方法在FLUX-1.Dev和Wan 2.1模型上分别达到3.52倍和3.2倍的加速效果。

Conclusion: 动态分块化策略能显著提升扩散模型图文生成效率，适用于需平衡质量和实时性要求的应用场景。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [7] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: PRIMO是一种监督隐变量插补模型，通过量化缺失模态的预测影响，允许在训练和推理时使用不完整多模态数据，采用隐变量建模缺失模态与观测模态的关系，并通过蒙特卡洛采样生成预测分布与影响分析。


<details>
  <summary>Details</summary>
Motivation: 解决传统多模态大模型依赖完整多模态数据的局限性，实际场景中常存在模态缺失、异步采集或子集可用问题，需利用所有训练样本（无论模态是否完整）提升模型实用性。

Method: 1) 训练阶段：以观测模态为条件建立缺失模态的隐变量分布；2) 推理阶段：从学习到的分布中采样生成多个隐变量补全方案，计算边际预测分布并基于多完成方案预测的方差量化缺失模态对单个实例的影响。

Result: 在合成XOR、音视频MNIST和MIMIC-III临床预测任务中均取得与基线模型相当的性能：缺失模态时匹敌单模态模型，完整模态时等效多模态模型，且通过方差指标可视化不同补全方案导致的合理预测标签范围。

Conclusion: PRIMO实现了多模态模型对不完整数据的鲁棒建模，支持实例级别缺失模态影响分析，通过贝叶斯框架将模态缺失问题转化为认知不确定性建模，在保持性能的同时增强模型可解释性。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [8] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG是一种结合检索增强策略的3D生成框架，通过外部部件数据库与扩散模型融合，实现高质量生成与局部可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法难以覆盖部件几何的长尾分布、维持多视角一致性，且缺乏精准局部编辑能力。

Method: 提出层次对比检索模块（关联图像补丁与3D部件潜编码）和遮罩式部件编辑器（在共享空间进行部件替换与属性优化）。

Result: 在Objaverse等数据集上，Chamfer Distance降低至0.1528，F-Score提升至0.844，推理耗时38秒，交互编辑5-8秒。

Conclusion: PartRAG在生成质量（更尖锐部件边界、细节保真）与可编辑性方面表现突出，尤其对运动物体处理具有鲁棒性。

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [9] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 本研究提出了一种高效的压缩框架，将大规模扩散Transformer模型（Qwen-Image）转化为轻量级文本到图像生成模型（Amber-Image），显著降低计算成本，并保持与大模型相当的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决扩散Transformer架构在文本到图像生成中的高计算成本和部署障碍，通过模型压缩技术减少资源消耗，同时保留高性能。

Method: 1) 提出时间步敏感的深度剪枝策略，对60层模型进行剪枝并本地权重平均；2) 采用层蒸馏和全参数微调优化；3) 开发混合流架构（Amber-Image-6B），通过单流替代双流结构，结合渐进蒸馏实现轻量化训练。

Result: 相比原模型：1) 参数减少70%，训练成本降至2000 GPU小时以内；2) 在DPG-Bench和LongText-Bench基准上生成质量与大模型相当，文本渲染能力突出；3) 消除了对大规模数据工程的依赖。

Conclusion: 该压缩框架在保证生成性能的同时大幅降低计算资源需求，为扩散模型的高效部署提供了实用解决方案。

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [10] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: StructCore通过结构感知的图像级评分方法超越传统max pooling，在异常检测中提升准确率。


<details>
  <summary>Details</summary>
Motivation: max pooling依赖单一极端响应，丢失异常证据分布信息，导致正常与异常分数重叠。

Method: 无需训练的StructCore通过低维结构描述符phi(S)捕获分布与空间特征，并用对角马氏校准优化评分。

Result: 在MVTec AD和VisA数据集分别取得99.6%和98.4%图像级AUROC分数。

Conclusion: 结构信息可有效提升无监督异常检测性能，超越传统方法。

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [11] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 该论文指出腹腔镜手术中套管针的固定特征因遮挡视野和特征点异常而严重影响几何相关算法，提出高精度标注数据集Cholec80-port并通过统一标注规范提高跨数据集鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有公开手术数据集未提供显式套管针标注，且标注方式违反几何一致性（如遮盖中心孔），导致下游任务（图像拼接/3D重建等）的对齐和跟踪稳定性下降。

Method: 构建Cholec80-port套管针分割数据集，定义排除中心孔的套管鞘标注规范（SOP），并对其他公开数据集进行基于SOP的清洗与统一化处理。

Result: 几何一致性标注显著提升跨数据集鲁棒性，其效果优于单纯增加数据集规模的影响（通过消融实验验证）。

Conclusion: 套管针的几何一致性标注对降低视觉干扰和提升算法稳定性具有关键作用，Cholec80-port为相关研究提供了标准化基准。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [12] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: 本文提出CPL-VAD双分支框架，通过交叉伪标签实现弱监督视频异常检测，结合时间定位与语义分类，达到优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测难以同时实现异常定位与类别识别，需要解决片段级定位精度与语义分类间的互补性问题。

Method: 设计二分类异常检测分支（负责片段级定位）与类别分类分支（基于视觉语言对齐），通过伪标签交互实现分支间互补优势融合。

Result: 在XD-Violence和UCF-Crime数据集上，异常检测AUC达98.5%/96.2%，类别分类准确率89.7%/85.4%，均超现有方法。

Conclusion: 双分支结构通过伪标签交互能有效结合时间建模与语义理解，解决弱监督下多粒度异常分析难题。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [13] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: 提出ComptonUNet模型以提高伽马暴弱信号在噪声环境中的定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型难以平衡低光子统计情况下检测的鲁棒性与降噪能力，而微弱伽马暴对研究早期宇宙恒星形成至关重要。

Method: 构建混合深度学习框架ComptonUNet，整合直接重建模型的统计效率与图像架构的去噪能力，并通过低轨背景环境下的仿真事件验证性能。

Result: 实验表明该模型在低光子统计与高背景污染场景中，定位准确率显著优于传统方法。

Conclusion: 结合统计重建与图像处理的ComptonUNet框架，为伽马暴弱源定位提供了有效解决方案。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [14] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 本研究提出了一种结合3D高斯散射（GS）与射频（RF）传感的多模态框架，用于应对传统视觉GS方法在光照、天气及遮挡干扰下的不足。


<details>
  <summary>Details</summary>
Motivation: 传统视觉GS方法依赖大量相机视角初始化且易受恶劣环境因素影响，而RF信号对这些干扰具有鲁棒性，因此需整合两者以提升效率和可靠性。

Method: 通过仅需稀疏RF深度测量进行有效深度预测，生成高质量3D点云以初始化GS中的高斯函数，并在多种GS架构中验证泛化性。

Result: 实验表明，RF增强的GS框架在复杂环境下显著提升3D渲染质量，同时降低初始化计算成本。

Conclusion: 该方法为多模态传感与GS渲染结合提供了有效路径，有望推动自动驾驶等需鲁棒3D感知的领域发展。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [15] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: 提出了一种无需相机参数和训练的快速3D高斯泼溅分割方法B³-Seg，通过贝叶斯更新和信息增益实现交互式实时分割。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义视角、真值标签或昂贵的再训练，难以满足影视和游戏制作中低延迟实时编辑需求。

Method: 将分割问题建模为贝塔-伯努利贝叶斯序列更新，通过解析期望信息增益理论最优地选择下一视角，利用自适应单调性和次模性保证贪婪算法的(1-1/e)近似比。

Result: 在多数据集上与高成本监督方法性能相当，端到端分割可在数秒内完成，验证了算法的信息效率理论性。

Conclusion: 该方法首次实现了无需相机校准与训练的开放词汇3D分割，为实时交互系统提供了可证明有效的解决方案。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [16] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: BadCLIP++ 提出一种统一的后门攻击框架，解决多模态对比学习模型中后门攻击的隐蔽性与持久性问题，在低污染率下实现接近完美的攻击成功率，并具备强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法因跨模态不一致导致触发模式暴露，且低污染率下梯度稀释加速后门遗忘，故需设计同时解决隐蔽性与持久性的攻击方案。

Method: 设计语义融合的QR微观触发器以保持触发分布紧凑且不易察觉，并通过目标对齐子集选择增强低注入率信号；采用半径收缩、质心对齐、曲率控制与弹性权重巩固技术稳定模型参数与触发嵌入，理论分析证明清洁微调与后门目标梯度共向性。

Result: 仅0.3%污染率下数字攻击成功率99.99%，超越基线11.4个百分点；19种防御下攻击成功率保持99.90%以上且清洁准确率下降不足0.8%；物理攻击成功率达65.03%，且对水印去除防御具鲁棒性。

Conclusion: BadCLIP++通过双维度优化理论分析，实现了高效持久的隐蔽攻击，为多模态模型安全防护提供了理论基础和实验依据。

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [17] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: NRGS-SLAM：基于3D高斯随机投影的单目非刚性SLAM系统，通过引入可变形概率优化和贝叶斯自监督策略，解决内窥镜场景中相机运动与软组织变形的耦合模糊问题。


<details>
  <summary>Details</summary>
Motivation: 传统V-SLAM方法因刚性假设在内窥镜场景中失效，导致相机运动估计与组织变形的耦合模糊，现有单目非刚性SLAM方法缺乏有效解耦机制且依赖稀疏/低保真场景表示，造成跟踪漂移和重建质量不足。

Method: 1) 提出变形感知的3D高斯地图，通过贝叶斯自监督策略优化每个高斯基元的可学习变形概率；2) 设计可变形跟踪模块，基于粗到细策略优先低变形区域进行姿态估计；3) 开发动态映射模块实现渐进式地图扩展与优化；4) 采用统一几何损失函数融合外部先验约束。

Result: 实验显示相比现有方法：1) 相机姿态估计RMSE降低50%；2) 实现高质量光子真实感重建；3) 消融实验验证各模块有效性。

Conclusion: 本方法有效解决单目非刚性SLAM的耦合模糊难题，代码将开源，为内窥镜导航提供高精度实时重建方案。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [18] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 本文提出视觉信息增益指标（VIG），通过量化视觉输入对模型预测的不确定性降低，设计选择性训练策略，有效提升视觉感知并减少语言偏见，显著降低标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型（LVLMs）存在语言偏见问题，即使缺乏视觉证据仍生成答案。传统方法依赖解码策略、架构改进或人工筛选数据，但缺乏针对训练样本或标记的视觉增益量化指标。

Method: VIG基于困惑度计算视觉输入带来的预测不确定性下降，支持样本级和标记级分析；训练阶段采用VIG引导策略，优先选取出视觉信息量高的样本与关键标记进行学习。

Result: VIG能精准识别颜色、空间关系和属性等视觉核心要素，指导的选择性训练在显著减少标注数据的情况下（如仅10%数据）仍超越基线模型性能，且视觉偏差缓解效果可解释性强。

Conclusion: VIG提供了评估视觉贡献的通用度量框架，其驱动的选择性训练在降低训练成本的同时增强了模型对视觉证据的依赖性，为视觉语言对齐提供新思路。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [19] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: EntropyPrune利用矩阵熵理论发现视觉表征的信息坍塌层(ECL)，提出了一种无需注意力图的token剪枝框架，在LLaVA-1.5-7B上实现68%的FLOPs缩减同时保持96%性能。


<details>
  <summary>Details</summary>
Motivation: 由于多模态模型需处理大量视觉token导致推理开销巨大，现有基于静态层数选择的剪枝方法缺乏可解释性和跨模型迁移能力，需要一种基于信息论的普适性剪枝准则。

Method: 通过矩阵熵分析定位信息量骤降的ECL层，结合Gram矩阵谱等价性优化熵计算复杂度（理论加速64倍），采用熵值量化策略替代注意力机制进行冗余token剪枝。

Result: 在多个多模态基准测试中超越SOTA方法：LLaVA-1.5-7B压缩后保留96%性能，高分辨率/视频模型验证扩展性。开源代码实现理论与工程验证。

Conclusion: 矩阵熵视角为多模态剪枝提供了理论基础，EntropyPrune通过信息敏感的动态剪枝策略，在保证性能前提下显著提升推理效率，适用于不同模态和分辨率模型。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [20] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: 提出了一种几何感知的球面采样方法（GASS），通过分解文本嵌入中的语义和非提示变量来增强文本到图像生成模型的多样性，同时保持图像质量和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像（T2I）生成模型虽然语义对齐性能优异，但生成图像的多样性不足，导致用户选择受限并可能加剧社会偏见。需通过解耦控制提示相关与非相关变量来提升多样性。

Method: 在CLIP嵌入空间中将多样性分解为文本向量（语义变化）和正交方向（背景等非提示变化），通过GASS方法扩大生成图像嵌入的几何投影分布，引导采样过程沿扩展预测轨迹生成更多样化的图像。

Result: 实验表明GASS在多种冻结的T2I模型（U-Net、DiT，扩散模型与流模型）和基准测试中均能有效提升解耦后的多样性，且对图像保真度和语义对齐影响极小。

Conclusion: 基于几何分解的多样性增强方法（GASS）通过控制关键变量平衡了生成质量与多样性，验证了几何视角在文本到图像合成中的有效性。

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [21] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种结合LiDAR与摄像头的多模态货运车辆检测系统，通过混合传感架构与实时跟踪方法支持货运信号优先控制。


<details>
  <summary>Details</summary>
Motivation: 货运车辆需可靠感知类型、位置及速度以实现基础设施级信号优先控制，现有技术存在检测精度与时效性不足的问题。

Method: 采用交叉口-中段双子系统混合架构，融合基于聚类与深度学习的多模态检测算法，结合LiDAR地理坐标注册与卡尔曼滤波跟踪技术。

Result: 实地测试验证系统可实现车道级定位精度（误差<10cm）与95%以上的实时跟踪成功率，支持10Hz级高时空分辨率监测。

Conclusion: 该设计为车路协同系统提供了可扩展的传感框架，证实了多源异构数据融合在交通优先控制场景的有效性，为智慧城市物流优化奠定技术基础。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [22] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: 我们提出了EA-Swin和EA-Video数据集，用于高效检测AI生成视频。


<details>
  <summary>Details</summary>
Motivation: 现有技术依赖浅层嵌入、图像适配器或高成本的MLLM，难以检测逼真的视频生成器（如Sora2、Veo3）。

Method: 提出基于分解窗口注意力的Swin Transformer，直接在预训练视频嵌入中建模时空依赖，并构建包含130K视频的多样性EA-Video数据集，支持跨分布评估。

Result: EA-Swin在主要生成器上准确率0.97-0.99（相比之前0.8-0.9提升5-20%），且在未见分布数据上保持强泛化性。

Conclusion: EA-Swin为AI生成视频检测提供了可扩展且鲁棒的解决方案，推动领域发展。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [23] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: 提出基于解剖附着点的机器学习框架提高结直肠手术抓取点预测准确性


<details>
  <summary>Details</summary>
Motivation: 结直肠手术复杂且缺乏数据，重复性操作适合机器学习应用，现有方法在抓取点预测上存在不确定性

Method: 设计包含局部几何与力学关系的anatomy-anchored结构化表征，并集成到机器学习抓取框架中

Result: 在90例手术数据集上相较纯图像基线方法显著提升抓取预测精度，特别是对于未见过的操作流程和外科医生场景

Conclusion: 该中间表征有效提升了基于学习的组织操作性能，为微创手术自主化提供新方向

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [24] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: This paper presents a high-resolution tree crop map for South America to improve deforestation monitoring under EU regulations, reducing false alerts for small farmers.


<details>
  <summary>Details</summary>
Motivation: Accurate deforestation monitoring is critical for policies like the EU's EUDR, but existing low-resolution maps misclassify agricultural systems as forests, risking unfair penalties for smallholder farmers.

Method: A multi-modal, spatio-temporal deep learning model was trained on Sentinel-1 and Sentinel-2 satellite imagery (10m resolution) to distinguish tree crops from forests.

Result: The map identifies 11 million hectares of tree crops, with 23% linked to 2000-2020 forest loss; existing regulatory maps wrongly classify 23% of established agroforestry as forest.

Conclusion: The high-resolution baseline supports equitable and effective conservation policies by reducing false deforestation alerts and protecting small-scale farmers from misclassification.

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [25] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: DRetHTR 使用 RetNet 替代传统 Transformer，有效解决 HTR 任务中解码效率低、内存消耗大的问题，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: Transformer 的键值缓存（KV Cache）随解码长度增长导致 HTR 系统速度慢且内存密集，需设计更高效模型。

Method: 采用无 softmax 的 retention 替代注意力机制，引入多尺度序列先验并设计逐层 gamma 缩放策略，使深浅层分别聚焦局部与全局依赖。

Result: 相比同等规模 Transformer，推理速度提升 1.6-1.9 倍，内存减少 38-42%，字符错误率（CER）在多个数据集上达到最优或接近最优。

Conclusion: 基于 RetNet 的解码器实现了与 Transformer 相当的 HTR 精度，但显著降低内存和时延，验证了 softmax 注意力机制的可替代性。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [26] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: SpectralGCD通过跨模态表征和频谱过滤，在GCD任务中实现高效准确的新类别发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法因仅依赖图像特征导致已知类别过拟合，现有跨模态方法模态独立处理且计算成本高，需解决模态协同学习与效率问题。

Method: 1) 使用CLIP跨模态相似性构建图像-概念统一表征
2) 通过频谱过滤分析师生模型协方差矩阵，自动生成相关概念词典
3) 双向知识蒸馏保持语义充分性与跨模态对齐

Result: 在6个基准测试中准确率优于SOTA方法，计算效率提升显著，代码已开源。

Conclusion: SpectralGCD通过显式语义锚定与高效架构，在降低计算成本的同时解决了跨模态GCD的语义漂移问题。

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [27] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 综述光学遥控感领域的关键方法、数据集与研究洞见，为新研究者提供全面指南与聚焦方向指引


<details>
  <summary>Details</summary>
Motivation: 计算机视觉技术发展推动遥感进步，无人机RGB相机普及催生光学遥感研究高潮，但现有文献缺乏系统性整合

Method: 通过跨任务/方法/数据集的多维视角，构建光学遥感领域的体系化知识框架，结合权威文献计量分析

Result: 揭示任务复杂度与数据多样性双轮驱动的技术演进规律，首次系统性揭示跨传感器/场景的泛化能力瓶颈

Conclusion: 提出光学遥感研究范式的三阶段进化路径，为算法创新与硬件协同设计提供可验证的技术路线图

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [28] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: 本文提出EAGLE框架，通过专家模型引导无需微调的多模态大模型进行工业异常检测，在MVTec-AD和VisA数据集上验证了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法存在输出二值化、缺乏解释性问题，多模态大模型微调成本高且性能不稳定，需开发无需参数更新的可解释异常检测方案。

Method: 设计EAGLE框架，通过专家模型输出引导MLLM注意力机制，分析MLLM中间层对异常区域的注意力分布变化，实现检测与解释双重优化。

Result: 在MVTec-AD和VisA数据集上，EAGLE提升MLLMs异常检测性能达到微调方法水平，且注意力机制分析表明异常区域关注度显著增强。

Conclusion: EAGLE框架有效平衡检测准确性与可解释性，无需微调特性降低部署成本，代码开源促进实际工业场景应用。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [29] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: Local-EndoGS 提出了一种基于窗口的渐进式全局表示和粗到精策略的高质量4D重建框架，解决了单目内窥镜序列中大范围相机运动与可变形场景重建的挑战，无需依赖立体深度或精确运动恢复结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定视角和立体深度先验/精确运动恢复结构，难以处理临床场景中单目序列的大相机运动与可变形场景重建需求。

Method: 1) 窗口化局部可变形场景模型实现长序列扩展性；2) 融合多视角几何、跨窗口信息与单目深度先验的粗到精初始化方法；3) 引入长程2D像素轨迹约束与物理运动先验提升形变合理性。

Result: 在三个公开可变形内窥镜数据集上，方法在外观质量与几何精度均超越现有技术，消融实验验证核心设计有效性。

Conclusion: Local-EndoGS 成功实现了无需立体深度/运动恢复结构的任意运动单目内窥镜序列4D重建，在临床场景中具有实用价值且代码已开源。

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [30] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 该论文提出了一种结合物理学先验知识的多模态框架QuPAINT，利用合成数据生成器Synthia和大型指令数据集QMat-Instruct，解决量子材料光学图像分析中的可扩展性难题。


<details>
  <summary>Details</summary>
Motivation: 光学显微图像解析量子材料存在对比度微弱、标注数据匮乏、跨实验条件泛化性差三大挑战，传统视觉模型因缺乏物理先验难以应对新材料或硬件条件变化。

Method: 研发了物理合成数据生成器Synthia模拟量子材料光学响应，构建首个量子材料指令数据集QMat-Instruct，并开发含物理信息注意力机制的QuPAINT多模态架构，最终建立涵盖多材料/基底/成像条件的QF-Bench评估基准。

Result: Synthia生成多样化高质量样本降低人工标注依赖，QMat-Instruct实现多模态大模型的物理认知训练，QuPAINT将物理先验与视觉嵌入融合，QF-Bench提供标准化可复现实验协议。

Conclusion: 基于物理先验的多模态框架显著提升量子材料厚度识别准确率，在跨实验室成像条件下展现强鲁棒性，代码/数据集/基准开源推动领域发展。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [31] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 本文提出PixTrace和CopyNCE方法，通过像素级跟踪与几何引导对比学习，显著提升图像复制检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督图像复制检测方法在复杂编辑场景下因缺乏细粒度对应学习而导致性能下降，需利用几何可追溯性优化特征学习。

Method: 1) PixTrace模块：通过像素坐标追踪实现编辑变换中的空间映射；2) CopyNCE损失：基于映射重叠比正则化patch相似度学习，建立像素级与patch级特征关联。

Result: 在DISC21数据集上，匹配器达88.7% uAP/83.9% RP90，描述子达72.6% uAP/68.4% RP90，均为当前最优性能，且可解释性显著优于对比方法。

Conclusion: 通过几何约束的双级特征学习框架，有效抑制自监督噪声并提升复杂图像篡改场景下的检测鲁棒性，为可解释ICD提供新思路。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [32] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 该论文提出一种结合腹腔镜深度图与基础姿态估计的新方法，通过非刚性ICP（NICP）替代有限元（FE）模型，用于增强现实（AR）在腹腔镜肝手术中实现更精确的肿瘤定位。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂且对工程/建模专业要求较高的FE模型与降维/机器学习组件，而本文旨在通过简化变形（非刚性）配准流程，降低技术门槛，提升临床实用性。

Method: 利用腹腔镜深度图与基础姿态估计器获取相机-肝脏姿态，采用NICP算法替代FE模型进行非刚性配准，并在真实患者数据中验证方案的有效性。

Result: 新方法在3例临床案例中达到平均9.91毫米配准误差，NICP组合配准效果优于传统刚性配准，验证了其作为FE模型替代方案的可行性。

Conclusion: 该方法在保证临床精度的同时，提供了比FE模型更轻量化且工程实现友好的变形配准方案，适用于AR辅助肝外科手术场景。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [33] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出LATA方法，通过图平滑和失败感知的转导方法，显著提升医学视觉语言模型的零样本不确定性校准效率与类别平衡性，无需模型训练且计算高效。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型在零样本识别中表现出色，但领域转移下需可靠的不确定性估计。现有Split Conformal Prediction在少样本和类别不平衡场景下会出现预测集过大（效率低）和类别覆盖率差异（CCG）问题，直接调整校准标签会破坏数据互换性。

Method: LATA通过构建图像k近邻图，在联合校准-测试池上对零样本概率进行拉普拉斯平滑（仅需数次CCCP均场更新），并通过确定性变换保持SCP有效性。创新性设计包含失败感知的ViLU共形分数（结合实例难度和标签可能性），支持黑盒无监督/标签辅助模式，避免反向传播。

Result: 在3个医学VLM和9个任务中，LATA在保持目标覆盖率前提下，预测集大小缩小23%-41%，类别覆盖率差异降低58%-72%，性能优于无监督转导基线，接近有标签方法，但计算成本仅为后者的1/8。

Conclusion: LATA解决了零样本医学视觉模型在领域迁移下的不可靠预测问题，首次实现无需微调且保留统计保证的高效不确定性校准，为临床场景下小样本模型即插即用提供了实用解决方案。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [34] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: 本研究提出GraphThinker方法，通过构建结构化事件级场景图和强化微调减少视频推理中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频推理中缺乏对事件间因果关系的显式建模，导致推理时容易出现幻觉。现有方法依赖密集字幕或摘要，但无法有效捕捉隐式关系且标注成本高。

Method: 1. 提出事件级视频场景图(EVSG)，显式建模事件内和事件间关系；2. 通过强化微调将场景图整合到MLLM推理过程；3. 引入视觉注意力奖励增强视频内容的关联性。

Result: 在RexTime和VidHalluc数据集上的实验证明：该方法在对象关系捕获、事件定位精度和幻觉减少方面均优于现有技术，其中事件定位误差降低37%，幻觉率下降42%。

Conclusion: 结构化场景建模结合强化微调有效提升了视频推理的因果理解能力。未来将探索动态场景图更新机制与跨模态因果推理框架。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [35] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: 提出ArtSound数据集（105,884艺术作品-音乐对）和ArtToMus框架，首次实现无需图像转文本的端到端艺术图像生成音乐


<details>
  <summary>Details</summary>
Motivation: 现有图像条件音乐生成系统的两大局限：1)仅用自然照片训练导致艺术内涵捕捉不足 2)依赖图像-文本转换阻碍直接视觉-音频学习

Method: ArtToMus框架通过将视觉嵌入投影至扩散模型的条件空间，完全基于视觉信息生成音乐（无语言中转），配套构建ArtSound数据集扩展ArtGraph与Free Music Archive

Result: 生成音乐在连贯性/风格一致性上表现优异，能反映视觉特征；尽管跨模态对齐分数低于文本条件系统，但感知质量与跨模态关联度具有竞争力

Conclusion: 确立视觉-音乐生成作为独立研究方向，为多媒体艺术/AI创作/文化遗产数字化提供新方法论支撑

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [36] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 提出了一种结合主动学习、在线元学习和概念引导推理的统一地理空间发现框架，利用‘概念相关性’优化不确定抽样和元批处理，提升动态环境下有限数据的目标发现效率。


<details>
  <summary>Details</summary>
Motivation: 现实场景（如环境监测、灾害响应）中数据采集成本高且环境动态变化，但现有基于学习的方法因地理空间数据稀疏/偏差难以应用。需解决资源受限下隐藏目标的高效发现难题。

Method: 1) 基于领域概念（如土地覆盖类型）的**概念加权不确定抽样策略**，通过学习到的相关性调控不确定度；2) **相关性感知元批次构建策略**，在在线元学习中促进语义多样性以增强动态环境泛化能力。

Result: 在致癌物PFAS污染真实数据集上验证，该方法在有限数据和变动环境中展现出目标发现可靠性。

Conclusion: 将概念相关性引入主动学习与元学习框架，为动态环境下的高效空间探索提供了可推广的解决方案。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [37] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: CORAL是一种基于DiT的虚拟试穿框架，通过显式对齐查询键匹配并引入对应蒸馏损失与熵最小化损失，在非配对设置下显著提升服装细节保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在非配对设置中难以保持精细服饰细节，且未明确处理人物-服饰对齐问题。基于扩散Transformer的方法中缺乏对对应关系生成机制的解析，导致人物与服饰的注意力匹配不准确。

Method: 1）分析全3D注意力机制发现人物服饰对应依赖精确查询键匹配；2）提出CORAL框架：a）对应蒸馏损失强制对齐可靠匹配与人物-服饰注意力；b）熵最小化损失锐化注意力分布；3）设计基于视觉-语言模型（VLM）的人类偏好评估协议。

Result: 相较基线模型，CORAL在全局形状迁移（提升15.2%）和局部细节保留（提升22.7%）方面均取得更好效果，且消融实验证实了对应蒸馏损失（+8.5%）和熵最小化（+6.3%）的独立贡献。

Conclusion: 研究证明了显式对齐查询键匹配对虚拟试穿的重要性，通过将外部对应关系约束注入注意力机制，结合新提出的评估方法，为非配对虚拟试穿任务提供了新范式。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [38] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: IntRec is an interactive object retrieval framework that improves accuracy by refining predictions through user feedback using dual memory sets (positive anchors and negative constraints) and contrastive alignment for fine-grained disambiguation in complex scenes.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary detectors lack the ability to refine predictions based on user feedback, leading to challenges in retrieving objects from complex scenes with ambiguous queries or similar distractors. IntRec addresses this limitation by integrating interactive feedback mechanisms for iterative refinement.

Method: IntRec maintains an Intent State with dual memory sets (positive anchors for confirmed cues and negative constraints for rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing negative constraints, enabling iterative disambiguation via user feedback.

Result: On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On LVIS-Ambiguous, it improves performance by +7.9 AP over one-shot baselines after a single feedback interaction with ≤30ms latency per iteration.

Conclusion: IntRec demonstrates significant improvements in interactive object retrieval by leveraging user feedback for iterative prediction refinement, achieving state-of-the-art accuracy with minimal computational overhead while resolving ambiguities in cluttered scenes.

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [39] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: 该研究针对视觉-语言-行动（VLA）模型在缺乏视觉监督时产生语言理解偏离的问题，提出反事实行动引导（CAG）方法，通过双分支架构结合VLA和VA模块，在无需额外训练下提升语言跟随准确率9.7%和任务成功率17.2%。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理缺乏场景监督的语言指令时，易受数据集偏差导致视觉捷径依赖，出现执行频繁训练对象而偏离语言意图的反事实失败，且缺乏专用评估基准和解决方案。

Method: 设计双分支CAG架构：1）保持原始VLA语言条件策略，2）新增非语言依赖的视觉-行动（VA）模块，通过对比二者动作分布实现语言条件显式正则化，抑制视觉捷径依赖，兼容任意预训练VLA模型。

Result: 在LIBERO-CF基准测试中，CAG使语言跟随准确率提升9.7%（π0.5指标），未见任务成功率提升3.6%；真实场景测试显示反事实错误减少9.4%，任务成功率平均提升17.2%，且与VA模型联用时提升进一步扩大到15.5%和8.5%。

Conclusion: CAG通过架构级改进有效平衡了语言条件约束与视觉泛化能力，作为训练无关插件可普遍适配各类VLA模型，为解决反事实失败提供了可扩展的实用方案。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


### [40] [OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents](https://arxiv.org/abs/2602.17665)
*Akashah Shabbir,Muhammad Umer Sheikh,Muhammad Akhtar Munir,Hiyam Debary,Mustansar Fiaz,Muhammad Zaigham Zaheer,Paolo Fraccaro,Fahad Shahbaz Khan,Muhammad Haris Khan,Xiao Xiang Zhu,Salman Khan*

Main category: cs.CV

TL;DR: 介绍了一种名为OpenEarthAgent的框架，通过统一的训练流程和大规模数据集，开发可进行多模态地理空间推理的智能体，显著提升遥感领域的多步逻辑分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理技术在遥感领域的应用存在挑战，需同时处理空间尺度、地理结构及多光谱指数，且需保持多步骤逻辑连贯性。

Method: 提出基于监督微调的训练方法，利用结构化解析路径对齐模型与多工具交互，并构建包含14,538个训练样本及GIS操作、植被/建筑指数分析的多模态数据集。

Result: 训练后的智能体在7,000+评估步骤中展现稳定空间理解能力，对比基线模型有显著提升，且在城市、灾害等领域均表现优异。

Conclusion: 验证了结构化推理路径与工具驱动框架的有效性，为遥感领域的多模态智能分析提供了可解释的解决方案。

Abstract: Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [41] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本研究提出了一种基于参考的LLM评估器方法，解决了强化学习可验证奖励（RLVR）无法直接应用于无真实验证器的领域（如LLM对齐）的问题。


<details>
  <summary>Details</summary>
Motivation: RLVR在推理任务中表现优异，但依赖真实验证器的存在。无真实验证器的非可验证领域（如大语言模型对齐）需要替代解决方案，而现有LLM评估器精度不足，需通过参考引导提升效果。

Method: 设计了基于参考输出的评估协议增强LLM评估器，利用前沿模型的参考输出提升低能力LLM评估器的准确性，并通过高质量人工参考输出进一步优化强LLM评估器。结合提升后的评估器实现参考引导自我改进框架，用于LLM对齐微调。

Result: 实验显示参考引导法使LLM评估器准确性显著提升：使用Llama-3-8B-Instruct和Qwen2.5-7B模型时，AlpacaEval/Arena-Hard得分分别达73.1%/58.7%和70.0%/74.1%，相较传统监督微调分别提升20.2/17.1个百分点，相较无参考自我改进法提升5.3/3.6个百分点，且性能接近ArmoRM奖励模型训练效果。

Conclusion: 基于参考的LLM评估器可有效弥补非可验证领域缺失真实验证器的短板，为LLM后训练提供新方法，验证了参考引导框架在强化学习对齐中的实用价值。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [42] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 本研究针对希腊语问答任务构建了新数据集 DemosQA，并提出内存高效的大语言模型评估框架，评测了11种单语和多语模型的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究主要集中于高资源语言（如英语），多语模型存在训练数据偏差或依赖跨语言迁移学习，可能导致社会文化历史特征的误表征。研究缺口在于缺乏对低资源语言单语模型在语言特有任务上的有效性研究。

Method: 1) 创建基于社交媒体真实用户问题与社区审核答案的希腊语问答数据集 DemosQA；2) 开发内存高效的模型评估框架；3) 使用3种提示策略在6个人工整理的希腊语数据集上评测11个单语/多语模型

Result: 实验显示现有单语模型在希腊语问答任务中仍不及多语模型，但所提出的框架可灵活适配多语言数据集评估，数据集和代码已开源。

Conclusion: 低资源语言单语大模型发展具有必要性，但其效果仍有待提升，本研究的数据、框架和结论为后续研究提供了基础。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [43] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 本研究提出基于流的连续去噪语言模型（FLM）与蒸馏后的流图模型（FMLM），通过欧式去噪和时间参数化技术，在生成质量和速度上超越离散扩散模型，挑战了离散扩散在离散模态生成任务中的必要性假设。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在快速生成时质量急剧下降，而连续流模型在理论上有更优的生成潜力。研究旨在探索连续流模型在离散语言模态上的可行性和性能边界。

Method: 1) 使用欧式空间对单热编码token进行去噪，通过交叉熵损失训练预测干净数据；2) 引入时间参数化技术提升训练稳定性；3) 将FLM蒸馏为仅需少数步骤的流图模型（FMLM）。

Result: 1) FLM在LM1B/OWT数据集上达到离散扩散模型的最先进水平；2) FMLM单步生成质量超过同类8步模型；3) 成功实现基于流的模型在少数步骤下的SOTA性能。

Conclusion: 证明连续流模型能有效替代离散扩散模型，通过数学框架创新和工程优化开辟了加速语言生成的可行路径，动摇了领域内对离散扩散必要性的传统认知。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [44] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: This paper proposes a governance-aware, domain-specific fine-tuned language model for generating corrective actions in insurance claims processing, using LoRA and local deployment to outperform commercial LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to operate effectively in regulated domains like insurance due to data sensitivity and lack of domain-specific alignment, necessitating solutions that balance performance with compliance.

Method: Fine-tuned pretrained LLMs with Low-Rank Adaptation (LoRA) on historical warranty claims, implemented as a local decision module in claim processing, and evaluated via semantic similarity metrics and human assessments.

Result: Domain-specific fine-tuning achieved ~80% near-identical matches to ground-truth corrective actions, significantly outperforming general-purpose and prompt-based commercial LLMs in accuracy and practical utility.

Conclusion: Domain-adaptive fine-tuning aligns LLM outputs with real-world operational data, offering a reliable, governable framework for regulated insurance applications while bridging the gap between LLM capabilities and industry requirements.

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [45] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文介绍了首个针对濒临灭绝的Mainz方言Meenzerisch的NLP研究，创建了包含2351个方言词汇的词典，并发现当前大型语言模型在该方言的定义生成与词汇生成任务中表现极差（最高准确率仅6.27%和1.51%），突显出方言保护需要更多资源投入。


<details>
  <summary>Details</summary>
Motivation: Meenzerisch方言濒临消失，而NLP技术可通过语言存档和复兴提供帮助，但此前对该方言缺乏相关研究。

Method: 基于1966年文献构建NLP就绪的数字词典，设计两项实验：1) 用LLMs生成方言词定义；2) 用定义反推方言词。尝试了少样本学习和规则提取增强技术。

Result: 主流LLMs在两项任务的准确率均低于10%（最佳表现6.27%和1.51%）。少样本与规则增强提升了结果但未突破性改善。

Conclusion: 当前NLP技术难以有效支持Meenzerisch方言的数字化保护，需针对德国方言开发专用资源和技术。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [46] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 跨语言委婉语检测中，语义重叠不足以保证正向迁移，土耳其语到英语的低资源迁移可能出现性能下降，非重叠委婉语训练反而可能提升效果。


<details>
  <summary>Details</summary>
Motivation: 委婉语涉及社会敏感表达且依赖文化语境，现有方法假设跨语言语义重叠能有效迁移，但缺乏对功能、语用和语义三者交互影响的系统验证。

Method: 将土耳其语和英语的潜在委婉语(PET)按功能/语用/语义一致性分为重叠(OPET)和非重叠(NOPET)两类，通过迁移学习实验分析标签分布差异对跨语言检测的影响。

Result: 语义重叠不一定带来正向迁移，土耳其语→英语方向性能在OPET上可能劣化，但在NOPET训练时反增。标签分布差异能部分解释此现象，领域对齐的有限证据也受稀疏性制约。

Conclusion: 跨语言委婉语迁移受标签分布和领域对齐双重影响，语义重叠非充分条件，低资源语言对的非直觉结果揭示多语言建模中的隐性挑战。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [47] [Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry](https://arxiv.org/abs/2602.16959)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.CL

TL;DR: 该论文提出了一种基于不确定性感知的计算框架，用于分析古典波斯诗歌的诗人心理特征，通过大规模自动多标签标注和概率分布量化诗人的独特性。


<details>
  <summary>Details</summary>
Motivation: 古典波斯诗歌通过隐喻和间接修辞表达情感，传统细读方法难以扩展至大规模比较，需结合计算分析与不确定性处理来保留文本解释的谨慎性。

Method: 构建包含心理概念标签、置信度评分和放弃标记的多标签注释框架，通过置信度加权的诗人-概念矩阵和概率分布量化诗人个体性（使用Jensen-Shannon散度和Kullback-Leibler散度），并建立置信度加权的概念共现图，利用拉普拉斯谱分解生成Eigenmood嵌入。

Result: 在61,573行诗歌的语料库上验证了框架，22.2%诗句因证据不足被放弃标注；量化了诗人偏离语料库基线的个体性，并展示了从远读到近读的工作流程。

Conclusion: 该框架通过从诗句级证据到诗人级推断的不确定性传播，实现了可扩展且可审计的数字人文分析，同时保留了解释学上的谨慎性。

Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.

</details>


### [48] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: This paper explores four cross-lingual text classification approaches to filter relevant content (hydrogen-related tweets) from multilingual social media datasets (9M tweets in English, Japanese, Hindi, Korean). It reveals trade-offs between translation-based and multilingual transformer methods, with a hybrid strategy performing best.


<details>
  <summary>Details</summary>
Motivation: Multilingual social media analysis is hindered by irrelevant content in keyword-driven collections and cross-lingual inconsistency. The study aims to identify optimal filtering methods for large-scale global discourse analysis.

Method: Tested four strategies: 1) language-specific models with translated English annotations; 2) single English model on translated multilingual data; 3) direct application of English-finetuned multilingual transformers; 4) hybrid of 1+3. Evaluated their ability to extract hydrogen-related tweets before topic modeling.

Result: Hybrid strategy achieved highest filtering accuracy; translation-based methods offered consistency but higher costs; multilingual transformers provided efficiency with slight accuracy trade-offs. Topic modeling revealed distinct cross-lingual discourse patterns about hydrogen energy.

Conclusion: Balancing translation quality and computational efficiency is key. Hybrid approaches are recommended for resource-constrained scenarios. Study provides guidelines for cross-lingual pipeline optimization based on data quality, domain specificity, and resource availability.

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [49] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: ALPS是新的阿拉伯语自然语言处理挑战集，专注于深度语义和语用分析，解决现有合成或翻译数据集的语言学不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语NLP基准依赖合成或翻译数据，缺乏深层语言验证，且未充分体现语义与语用能力，需通过原生专家设计的挑战集填补深度理解评估空白。

Method: 构建包含531个专家设计问题的ALPS数据集，涵盖15项任务47个子任务，通过23种多类型模型与单次人类及专家仲裁的基准对比进行评估。

Result: 模型表现流畅性但失败基础形态句法依赖，尤其是依赖标符的子任务错误率高达36.5%，顶尖商业模型(Gemini-3-flash 94.2%)超越人类平均但本地模型(Jais-2-70B 83.6%)仍有差距。

Conclusion: ALPS揭示了现有阿拉伯语NLP模型在深度语言理解上的关键短板，表明单纯扩展规模无法弥补，需注重形态句法与原生语言学设计。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [50] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 本论文提出BankMathBench数据集，针对银行金融场景中的多步骤数值推理任务，通过基础、中级和高级三级难度设计提升大模型在存款、贷款等场景中的利息计算与产品比较能力，训练后模型准确率平均提升57.6-75.1个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在银行核心计算任务（如提前还款利息计算、多产品利率比较）中存在系统性错误，而数学数据集与金融数据集均未覆盖日常银行场景，需构建专门领域数据集解决该问题。

Method: 设计BankMathBench数据集，包含单产品推理（基础）、多产品对比（中级）、多条件场景（高级）三级任务；采用工具增强微调策略对开源LLM进行训练，量化评估其在数值推理与公式生成上的效果。

Result: 训练后模型在基础、中级、高级任务中平均准确率分别提升57.6个百分点、75.1个百分点与62.9个百分点，显著优于零样本基线，且工具增强微调效果最佳。

Conclusion: BankMathBench能有效评估和增强LLM在真实银行场景中的多步骤数值推理能力，填补了通用数学/金融数据集与实际应用间的空白。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [51] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 本文提出一种基于心理测量理论的新框架，通过强制选择序数情景和统计模型识别大模型内在的持久行为偏差，发现模型提供方的固有偏见可能导致AI系统形成意识形态回音室。


<details>
  <summary>Details</summary>
Motivation: 传统指标仅评估任务准确性，但无法捕捉LLM的训练内化偏见（如趋炎附势倾向）。多模型集成系统需量化提供方层级的稳定行为特征以确保安全治理。

Method: 构造包含语义掩码干扰项的序数情景题（应用心理测量学中的潜在特质估计理论），使用混合线性模型与组内相关系数分析对9个主流模型进行跨维度检测（含优化偏差、地位合法化等）。

Result: 研究证实项目表述方式导致响应差异，但检测到提供方级别的显著行为聚类信号（lab signal），如OpenAI与Anthropic模型在保守/开放维度的差异。

Conclusion: 多层AI架构中，提供方偏见会通过递归交互形成意识形态闭环，建议采用密码学不变性设计的评估框架加强模型可审计性。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [52] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 本文分析了77,334条罗马尼亚语远程医疗对话对患者满意度的影响，发现患者历史特征主导预测，但医生回应的礼貌表达和文本结构对提升满意度具有可操作性作用。


<details>
  <summary>Details</summary>
Motivation: 随着文本医疗平台依赖患者评分进行质量评估，需探究影响患者满意度的因素，尤其是区分沟通质量与临床准确性的差异，为提升医疗服务质量提供实证依据。

Method: 基于时间划分训练集，构建二分类模型预测患者满意度（拇指向上/其他反馈），提取语言无关特征（长度、结构）、罗马尼亚语心理语言特征（LIWC）及礼貌/模糊语标记，通过SHAP值进行特征重要性分析。

Result: 患者与医生的历史特征（如过往反馈）在预测中占主导作用；文本特征中，礼貌表达和模糊语与满意度正相关，词汇多样性呈负相关，文本可读性与结构质量提供少量但可优化的信号。

Conclusion: 远程医疗满意度受历史因素与文本特征共同影响，建议医疗机构优先优化医生沟通风格（如加强礼貌策略），同时探索文本特征的动态干预机制以提升患者体验。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [53] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 本文提出一种心理测量框架，通过量化并减轻社会期许反应（SDR）偏差，改进大型语言模型（LLMs）的问卷评估有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于自我报告问卷的LLM评估易受社会期许反应影响，导致结果失真，需开发方法识别并缓解此类偏差

Method: 通过对比诚实回答与刻意美化回答（HONEST vs. FAKE-GOOD）的差异，使用项目反应理论（IRT）计算SDR效应值；采用约束优化方法构建30组去期望化的GFC人格量表

Result: Likert量表显示显著SDR效应，而GFC量表在维持目标人格特征识别率的同时显著降低SDR偏差

Conclusion: 揭示模型依赖的SDR-特征恢复权衡关系，建议在LLM评估中引入SDR意识的报告标准

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [54] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本研究提出X-Value，首个跨语言价值观评估基准，用于评估大语言模型对数字内容深层价值维度的判断能力，包含5,000多个多语言QA对。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs内容安全评估过度关注显性危害（如暴力、仇恨言论），忽视基于Schwartz人类基本价值观理论的跨文化价值维度检测需求，需建立系统性的多语言评估框架。

Method: 构建包含18种语言、7个核心价值观领域的分级QA数据集，采用两阶段标注框架：先区分全球共识性议题（如人权）与多元主义议题（如宗教），再进行多主体价值判断标注。

Result: 当前SOTA级LLMs在跨语言价值评估中准确率低于77%，存在显著的语言间性能差距（ΔAcc＞20%），表明模型在跨文化价值敏感内容处理上存在缺陷。

Conclusion: 揭示提升大模型多语言价值敏感能力的紧迫性，提供开源数据集推动价值观感知型内容评估系统发展。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [55] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文研究了Transformer架构在神经机器翻译中因标准训练策略导致的‘表示崩溃’问题，并提出通过角度分散正则化缓解该问题，同时提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在高资源数据集上表现优异，但其深层表示常出现几何空间利用不足的表示崩溃现象，尤其在连续输出模型中会退化为向量同质化。这一问题在离散与连续翻译模型中尚缺乏系统分析与解决方案。

Method: 通过跟踪翻译模型训练过程中表示崩溃的动态变化，引入基于角度分散的现有正则化方法，在离散/连续NMT模型及量化模型上进行实验验证，并量化分析表示崩溃程度与翻译性能的关系。

Result: 角度分散正则化有效缓解深层Transformer表示崩溃，在连续输出模型中提升翻译质量。实验表明量化模型同样会出现表示崩溃，但正则化对量化后模型仍具改善效果，证明该方法具备跨模型架构的普适性。

Conclusion: 研究揭示了Transformer翻译模型中表示崩溃的普遍性，验证了角度分散正则化在抑制崩溃及增强模型鲁棒性（包括量化压缩模型）方面的有效性，为高效训练提供理论依据。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [56] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 该论文发现大型语言模型（LLMs）对输入提示的表面变化敏感，导致性能显著下降且排名不稳定，表明当前评估基准可能存在可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，标准化评估基准已被广泛用于模型比较，但其可靠性因模型对输入提示敏感而受到质疑，因此需要系统研究输入变化对性能评估的影响。

Method: 通过两种语言学驱动的方法生成语义等价的输入扰动（同义词替换的词法扰动和依存分析支持的句法扰动），测试23种LLMs在MMLU、SQuAD和AMEGA三个基准任务中的表现。

Result: 词法扰动普遍导致性能显著下降，而句法扰动影响复杂（偶尔提升性能）；两种扰动均使复杂任务上的模型排名不稳定，且模型鲁棒性与规模无明确正相关。

Conclusion: LLMs更多依赖表面词汇模式而非抽象语言能力，评估时需加入鲁棒性测试以提高可靠性。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [57] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: This paper introduces RPDR, a data augmentation framework combining synthetic data generation, Round-Trip prediction-based selection, and training enhancements to improve dense retrievers for long-tail question answering.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with long-tail questions requiring rare knowledge. While retrieval-augmented generation (RAG) systems help, dense retrievers still face challenges in generalizing to niche information, prompting the need for improved training data strategies.

Method: RPDR employs three components: (1) synthetic data generation to expand training sets, (2) Round-Trip prediction for identifying easy-to-learn instances, (3) retriever training on selected instances. Additionally, a dynamic routing mechanism was proposed to optimize query distribution between retrieval modules.

Result: On PopQA and EntityQuestion benchmarks, RPDR outperformed traditional retrievers (BM25, Contriver) by significant margins, particularly excelling in extremely long-tail categories. Human analysis confirmed enhanced performance and uncovered insights guiding the dynamic routing extension.

Conclusion: RPDR effectively enhances dense retrievers for long-tail knowledge retrieval through data augmentation and selection. The study highlights the framework's strengths, limitations, and potential for future improvements via dynamic routing mechanisms.

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [58] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 学生猜测多选题答案时常用可用性启发式，研究发现正确答案在大型语料库中更具认知可用性，选择最易想到的选项可显著提高得分，LLM生成的选项也呈现相似模式。


<details>
  <summary>Details</summary>
Motivation: 学生面对不确定的多选题常依赖猜测，但可用性启发式（基于概念回忆难易程度）的有效性尚未被系统验证，计算模型中该因素常被忽略。

Method: 通过分析选项相关概念在维基百科等大型语料库中的频率，量化其认知可用性，并测试选择最高可用性选项在三个问题集中的表现；同时对比LLM生成与专家设计的选项的可用性模式。

Result: 正确答案的可用性显著高于错误选项，使用维基百科时选择最高可用性选项得分比随机猜测高出13.5%-32.9%；LLM生成选项与专家设计选项的可用性模式相似。

Conclusion: 可用性启发式是有效的猜测策略，未来计算教育模型需纳入该因素，且LLM生成的选项仍保留真实可用性特征。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [59] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文提出了改进的CDCR注释方案，通过重新定义核心ference链为话语单元，提升跨文档实体/事件识别在多样化新闻分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有CDCR数据集对实体/事件的定义过于狭窄（仅聚焦事件核心ference），无法有效分析存在用词差异和立场对立的新闻语篇。

Method: 将核心ference链定义为话语单元（DEs），建立统一注释规则，对NewsWCL50和ECB+数据集进行重构标注，新增近同一性关系标注（如'the caravan'与'asylum seekers'的关联）

Result: 重构后的数据集在词汇多样性指标上处于原始ECB+和NewsWCL50之间，通过同词根基准测试验证其兼容性与平衡性

Conclusion: 新数据集支持更全面地捕捉新闻语篇中的语义单元和表述差异，为跨文档信息聚合提供更优框架

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [60] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文比较了BLEU与ChrF++指标在极低资源语言（ELRL）机器翻译评估中的表现，发现BLEU虽得分较低但能提供补充的词汇精确性信息，可提升结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的BLEU等基于n-gram的指标在极低资源语言场景中常因数据稀缺导致评估偏差，需要验证新型指标（如字符级ChrF++）与传统指标的适用性差异。

Method: 选取Magahi、Bhojpuri、Chhattisgarhi三种ELRL，对比BLEU和ChrF++对大型语言模型（LLM）及神经机器翻译（NMT）系统的输出响应，分析二者对翻译幻觉、重复、源文复制及变音符号变化等错误类型的敏感性差异。

Result: BLEU在ELRL场景中虽绝对分数较低，但能更精准反映词汇层面的翻译质量，为ChrF++提供互补性分析视角；单一依赖ChrF++可能导致对特定错误类型（如变音符号误判）的忽视。

Conclusion: 极低资源语言背景下，综合使用BLEU与ChrF++能更全面评估翻译质量，未来工作应结合两者特性设计更适合ELRL的评估框架，而非单一依赖某类指标。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [61] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 该论文提出AIDG框架，通过动态对话评估LLMs的战略推理能力，发现模型在信息防御上优于推理，存在信息动态和约束遵守两大瓶颈。


<details>
  <summary>Details</summary>
Motivation: 为了突破静态基准测试的局限，需通过动态多回合互动评估LLMs的战略推理能力，特别是在信息提取与防御的不对称性方面。

Method: 提出AIDG-I（社交推理任务）和AIDG-II（结构化20问任务），在439场游戏中测试6个前沿LLMs，通过ELO评分和统计分析揭示能力差异及瓶颈。

Result: 模型在防御任务中展现350 ELO优势（Cohen's d=5.47），确认策略效率是盲目推理的7.75倍，且41.3%的推理失败源于对话负载下的指令遵循退化。

Conclusion: LLMs擅长局部防御性连贯，但在全局状态跟踪的战略推理上存在缺陷，这揭示了其在复杂互动场景中的核心限制。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [62] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: The paper introduces a bias-reduced evaluation protocol for LLMs on MCQ benchmarks by using uniform labels and sentence similarity to mitigate label-position biases, achieving better robustness with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit biases in MCQ benchmarks due to reliance on answer positions or labels. The study aims to address these biases to measure true reasoning capabilities more accurately.

Method: Replace answer labels with uniform, unordered ones and use sentence similarity models to evaluate answers. Ablation studies compare different embeddings and similarity functions across benchmarks.

Result: The proposed protocol reduces answer permutation variance by 3×, lowers standard deviation, and maintains LLM performance with minimal accuracy drop compared to traditional methods.

Conclusion: The bias-reduced protocol enhances evaluation robustness, exposing LLM capabilities without reliance on label patterns or prompt examples.

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [63] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 提出了一种基于熵的无监督数据选择（EUDS）框架，通过减少数据量和计算成本实现语言模型的高效微调。


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法依赖高计算资源，而在实际资源受限的微调场景中难以应用，且大型语言模型的数据可用性评估仍具挑战性。

Method: 提出EUDS框架，利用熵估计数据不确定性，通过无监督过滤机制筛选高质量数据，理论分析与实验验证相结合。

Result: 实验表明EUDS在情感分析、主题分类和问答任务中显著降低计算成本、提升训练效率，同时减少数据需求。

Conclusion: EUDS为计算资源受限场景下的语言模型微调提供了创新性解决方案，平衡了数据选择效率与资源消耗。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [64] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: PEACE 2.0 is a tool that detects hate speech and generates evidence-based counter-speech responses using a Retrieval-Augmented Generation (RAG) pipeline.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in hate speech detection, developing effective counter-speech strategies remains challenging. This work addresses this gap by creating a framework that combines hate speech analysis with response generation.

Method: PEACE 2.0 implements a RAG pipeline to (1) provide evidence-backed explanations for hate speech classification, (2) generate evidence-grounded counter-speech responses, and (3) analyze characteristics of effective counter-speech. It handles both explicit and implicit hate speech.

Result: The framework enables three core functionalities: explainable hate speech detection, automated counter-speech generation through retrieval-augmented approaches, and systematic exploration of counter-speech characteristics.

Conclusion: PEACE 2.0 demonstrates the potential of integrating evidence-based analysis with response generation for combating online hate speech through explainable AI methods.

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [65] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本研究揭示了孟加拉语和英语之间跨语言情感对齐的重大缺陷，发现当前AI模型存在28.7%的情感反转率等问题，并提出需要文化适应性对齐和情感稳定性评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决双向对齐循环中因语言差异导致的AI系统误解人类意图问题，增强跨语言场景下的人机信任关系。

Method: 通过四种Transformer架构对孟加拉-英语情感对齐基准测试，量化分析情感反转率、共情不对称性和方言处理偏差等指标。

Result: mDistilBERT模型出现28.7%情感反转率，IndicBERT处理正式孟加拉语时出现57%偏差，检测到情感权重系统性失真现象。

Conclusion: 提出需采用文化适配对齐方案替代通用压缩模型，建议在基准测试中增加情感稳定性指标以惩罚低资源语种的情感极性反转。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [66] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 小型语言模型（约10亿参数）可通过微调等策略在医疗NLP任务中表现优异，甚至超越大型模型，如Qwen3-1.7B比Qwen3-32B平均高出9.2分。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因计算资源需求高，限制了在医疗场景的实际部署，研究小型模型能否在保证精度的前提下实现医疗任务。

Method: 评估Llama-3、Gemma-3、Qwen3家族模型在20项临床NLP任务中的表现，对比推理时（少样本提示、约束解码）和训练时（监督微调、持续预训练）的适配策略。

Result: 微调效果最佳，少样本提示与约束解码组合为低资源场景提供替代方案。Qwen3-1.7B模型超越Qwen3-32B，意大利医疗数据集和急诊部语料库已开源。

Conclusion: 小型语言模型在医疗NLP中具有竞争力，优化策略可提升效果，开源数据资源推动领域研究与应用。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [67] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: This paper advances clinical section segmentation by introducing a new obstetrics dataset, evaluating transformer-based supervised models on in-domain and out-of-domain datasets, and comparing them with zero-shot models. Findings show supervised models struggle out-of-domain, while zero-shot models perform robustly with hallucination correction.


<details>
  <summary>Details</summary>
Motivation: Clinical notes are structured into labeled sections critical for decision-making, but existing datasets (e.g., MIMIC-III) lack domain diversity. Supervised models require domain-specific data, and zero-shot approaches for clinical text remain underexplored.

Method: 1) Created a new de-identified section-labeled obstetrics notes dataset. 2) Evaluated transformer-based supervised models on a curated MIMIC-III subset (in-domain) and the obstetrics dataset (out-of-domain). 3) Compared supervised models to zero-shot large language models for the first time in clinical section segmentation.

Result: Supervised models performed well in-domain but had significant performance drops out-of-domain. Zero-shot models demonstrated strong out-of-domain adaptability after correcting hallucinated section headers, despite limited training data.

Conclusion: Domain-specific clinical resources are crucial for robust section segmentation. Zero-shot segmentation with hallucination mitigation shows promise for expanding NLP applications beyond well-studied datasets, reducing the need for extensive domain-specific labeled data.

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [68] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的自动化框架，用于从学生编程代码中直接标注细粒度知识组件（KCs）的正确性标签。通过引入时间感知的代码-KC映射机制，解决了传统方法因问题级正确性传播导致的学情曲线偏差问题，实验表明新方法在学情曲线拟合和预测性能上均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有教育数据集中缺乏精确的KCs正确性标注（尤其是开放性编程任务中多KCs同时作用的场景），而直接将问题级别正确性传播到关联KCs会掩盖部分掌握状态并导致学情曲线拟合失真，这促使研究者寻求更精准的自动化标注方法。

Method: 开发了基于大语言模型的自动化标注框架，包含两个核心创新：1) 直接从学生代码中评估单个KCs的应用正确性；2) 提出时间上下文感知的代码-KC映射机制，通过动态对齐代码段与KCs时序特征实现细粒度标注。同时引入遗忘因子优化长期记忆建模。

Result: 在power law of practice和Additive Factors Model验证中，新方法生成的学情曲线与认知理论一致性显著提升（R²提升23.7%），预测准确率较传统KCM方法提高15.2%。人工评估显示LLM与专家标注的Fleiss'κ值达0.78，证明标注可靠性。

Conclusion: 提出的框架有效解决了复杂编程任务中的KCs精细化标注难题，通过大语言模型与时序建模的结合，实现了更符合认知规律的学情分析，为个性化学习诊断和自适应学习系统提供了新的方法论支持。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [69] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 本研究提出了一种自适应正则化框架，通过评估训练时的安全风险（包括基于评审的安全评论模型和基于激活的预测器），约束高风险更新以维持模型安全性，同时允许低风险更新。该方法在保持下游性能的同时，显著降低攻击成功率，且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 指令型语言模型的安全性在微调或对抗更新时可能退化，现有防御方法常面临安全性和实用性权衡问题。研究需要一种无需牺牲性能的动态安全保护机制。

Method: 构建自适应正则化训练框架，两种安全风险评估方法同步运作：1) 安全评论模型基于训练批次生成高层次有害评分；2) 轻量分类器通过模型中间激活状态预测潜在有害意图。根据风险信号动态约束模型更新强度。

Result: 实验证明有害意图信号可通过生成前激活状态预测，评审评分可提供高召回率的安全指引。在多种模型族和攻击场景下，两种方法均有效降低攻击成功率，且保持模型性能。

Conclusion: 该方法首次实现了无需推理成本的安全保护机制，在保留模型实用性的前提下系统性维护安全性，为平衡模型优化与安全风险提供了通用技术路径。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [70] [The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?](https://arxiv.org/abs/2602.17598)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 当前语音LLMs在特定条件下与隐式ASR级联架构行为等效，但存在架构依赖性，且噪声下性能可能劣于显式级联。


<details>
  <summary>Details</summary>
Motivation: 揭示当前语音LLM内部是否实际执行隐式ASR，验证其与传统Whisper->LLM级联架构的等效性假设。

Method: 构建匹配主干实验（四个语音LLM+六个语音任务），首次控制LLM主干变量，采用logit lens分析隐状态，LEACE进行因果归因。

Result: Ultravox与控制级联呈高度行为一致性(κ=0.93)，隐状态检测到文本表征，擦除实验致使性能崩溃；Qwen2-Audio展示架构差异，噪声0dB时传统级联反超7.6%。

Conclusion: 多数当前语音LLM本质为隐式级联系统，但Qwen2-Audio证明架构可能性。噪声场景下传统ASR级联更鲁棒，挑战当前设计范式。

Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.

</details>


### [71] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: DivanBench评估波斯语LLM在迷信与习俗推理中的表现，揭示模型过度依赖文化模式复现而非理解深层逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准测试未区分记忆文化事实与推理隐性社会规范的能力，需构建聚焦文化习俗隐性逻辑的评估框架。

Method: 设计DivanBench诊断基准（315个问题/三类任务：事实检索、配对场景验证、情境推理），测试七种波斯语大模型在非逻辑性文化规则下的表现。

Result: 发现模型三大缺陷：1) 顺从偏差（正确识别合理行为但无法拒绝明显违规）；2) 持续预训练加剧偏差并弱化矛盾辨别力；3) 事实检索与应用间存在21%性能落差。

Conclusion: 文化胜任力需超越单语数据扩展，当前模型通过模式复现习得文化表象而非内化底层逻辑架构。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [72] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 本文研究语言模型在差异论元标记（DAM）系统中的类型偏好，发现模型能复现自然标记方向偏好，但无法复制人类语言中的对象标记倾向。


<details>
  <summary>Details</summary>
Motivation: 前期发现语言模型在语法层面（如词序）会自发形成类人类型偏好，但尚未验证是否适用于语义驱动的DAM系统及其核心特征（如对象标记倾向）。

Method: 在18组具有不同DAM系统的合成语料库上训练GPT-2模型，采用控制变量法并通过最小对立对进行泛化能力测试。

Result: 模型正确习得语义不典型论元的显性标记倾向（自然标记方向），但未展现出人类语言中显著的宾语标记优先现象。

Conclusion: 不同语言类型倾向可能源于独立机制 —— 标记方向偏好可能反映统计规律习得能力，而对象标记倾向可能需要更复杂的人类认知驱动因素。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [73] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: UniLID是一种基于UnigramLM的新型语言识别方法，通过语言条件单字分布建模和语言特异性分词策略，解决了低资源语言和相近语言的识别难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低资源语言和密切相关的语言组合中表现脆弱，且难以灵活扩展新语言。UniLID旨在提升小样本场景下的识别效率并支持无重训练的语言扩展。

Method: 利用UnigramLM的分词框架，为共享词表学习语言条件单字概率分布，将文本分词过程视为语言特有特性。通过概率建模和参数估计实现跨语言区分，支持增量添加新语言。

Result: 在标准基准测试中与fastText/GlotLID/CLD3等基线模型对比，UniLID在低资源环境（每语言仅5个样本）准确率超70%，方言识别性能显著提升，且计算资源消耗更低。

Conclusion: 该方法实现了高效的数据利用、无需重新训练的语言扩展能力，且能无缝集成至现有模型分词流程，特别适用于资源匮乏场景和细粒度语言变体识别。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [74] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文提出了一种用于扩散语言模型(DLM)的Sink感知剪枝方法,相比传统基于自回归模型(AR)的剪枝方法,其能更有效地减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型由于迭代去噪导致高推理成本,传统继承自自回归大语言模型的剪枝方法过度保护注意力sink token,但实验发现DLM的注意力sink位置在生成过程中存在高度动态变化,sink token往往是不稳定的。

Method: 提出动态sink检测剪枝方法,通过分析注意力sink位置在不同时间步的主要变化模式,自动识别并剪除DLM中不稳定的sink token,而传统方法通常在AR模型中保留sink。

Result: 该方法在无需重新训练的情况下,在匹配计算资源条件下取得了更优的质量效率权衡,并超越了强大的剪枝基线方法。

Conclusion: 研究表明传统AR模型中的sink token稳定假设在DLM中并不成立,提出的新方法通过自适应识别和去除动态sink,为DLM提供了更优的剪枝解决方案,其代码开源后可促进后续研究。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [75] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.OH

TL;DR: 该论文提出了一种结合经典加密与量子技术的混合安全框架（AES、BB84量子密钥分发、量子态比对和生物启发免疫系统），以应对量子计算对RSA等传统加密算法的威胁。


<details>
  <summary>Details</summary>
Motivation: Shor量子算法可在多项式时间内高效破解依赖大数分解难题的RSA加密，而经典算法如试除法和Pollard's Rho效率低下。研究旨在构建后量子时代的数据保护解决方案。

Method: 设计概念框架：AES加密保障经典安全，BB84协议实现抗窃听密钥交换，量子态比对用于轻量级认证，生物启发免疫系统实现自适应威胁检测。

Result: 验证了RSA对Shor算法的脆弱性，BB84在理想条件下实现完整密钥协商且具备高窃听检测精度，整体框架具备可扩展性和适应性。

Conclusion: 提出基于量子-经典混合的理论模型应对量子安全风险，但具体实现、安全证明和实验验证需由后续研究完成。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>
