<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 150]
- [cs.CL](#cs.CL) [Total: 54]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 研究通过量化条件注入对时间步的影响，揭示视频扩散模型中时间步空间的运动-外观解耦机制，并提出简化运动定制的新方法。


<details>
  <summary>Details</summary>
Motivation: 旨在系统解析视频扩散模型中不同时间步如何编码运动和外观，验证早期时间步主导运动生成的实证假设，并利用该特性简化运动定制流程。

Method: 设计条件注入实验，通过在指定时间步范围内注入新条件，量化分析运动保持与外观编辑的权衡关系，揭示时间步空间的运动-外观解耦特性。

Result: 发现早期时间步为运动主导阶段，后期为外观主导阶段，在多种架构中均存在可量化的运动-外观分界点，并基于此开发无需去偏模块的高效运动迁移方案。

Conclusion: 将经验性时间步划分转化为时空解耦原则，提出可在现有运动编辑框架中即插即用的时间步约束策略，通过量化映射运动与外观的竞争关系实现高效视频生成。

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [2] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

TL;DR: This paper introduces a real-time American Sign Language (ASL) recognition system using a hybrid deep learning model combining 3D Convolutional Neural Networks (3D CNN) and Long Short-Term Memory (LSTM) networks. It aims to enhance accessibility for over 70 million deaf and hard-of-hearing individuals globally by accurately recognizing word-level ASL signs from webcam video streams.


<details>
  <summary>Details</summary>
Motivation: To address communication barriers faced by deaf and hard-of-hearing individuals, this system leverages real-time gesture recognition technology, focusing on improving accuracy and efficiency in sign language interpretation. Existing solutions are often limited by latency or scalability, which this hybrid architecture aims to overcome.

Method: The system employs a 3D CNN to extract spatial-temporal features from video frames, followed by LSTM layers to model sequential gesture dependencies. It is trained on the WLASL dataset (2,000 words), ASL-LEX database (~2,700 signs), and a curated set of 100 expert-annotated signs. The model is deployed on AWS infrastructure with edge computing support via OAK-D cameras.

Result: The system achieves F1-scores between 0.71 and 0.99 across sign classes, demonstrating robust performance for most sign categories. Deployment on edge devices enables real-time inference with low latency, suitable for practical accessibility applications.

Conclusion: The hybrid 3D CNN-LSTM architecture effectively captures both spatial and temporal dynamics of ASL signs, offering a scalable solution for real-world use cases. The system highlights the potential of deep learning in fostering inclusive communication technologies, though broader linguistic challenges (e.g., regional variations) may require further research.

Abstract: This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

</details>


### [3] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 本文提出了一种结合人工智能与局部线性嵌入（LLE）的创新模型，应用于医疗计费和转录系统的高维数据处理，显著提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 为解决医疗领域因高维数据复杂性和人工操作导致处理效率低、错误率高的问题，同时弥补传统降维方法在医疗场景中的局限性。

Method: 构建数学模型将AI算法与LLE结合，开发支持自动化处理的增强型降维框架，设计适应性特征选择模块和异常检测机制，并通过医疗计费数据集和语音转录数据集进行验证。

Result: 实验显示数据处理准确率提升23.7%，计费流程耗时减少41%，患者信息错误率降低68%，模型在非结构化医疗数据中保持92%以上的特征保留率。

Conclusion: 验证了AI增强LLE模型在医疗场景中的技术优势，为高维生物医学数据处理提供了新范式，并为智能医疗系统的开发提供了可扩展的技术路径。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [4] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: The paper introduces VISTA, a modular framework that decouples perception from reasoning in vision-language models (VLMs) to reduce reliance on spurious correlations through structured decomposition, reinforcement learning, and explicit visual grounding.


<details>
  <summary>Details</summary>
Motivation: End-to-end VLMs often exploit superficial correlations rather than causal evidence, becoming more shortcut-prone during fine-tuning. The work aims to enforce robust, bias-free visual reasoning by architecturally separating perception and reasoning components.

Method: VISTA uses a frozen VLM sensor for constrained perception (short, objective queries) and a text-only LLM for multi-step reasoning. An explicit information bottleneck restricts visual input, while reinforcement learning with GRPO trains the reasoner using curated multi-step questions. The framework enforces explicit visual grounding through natural language explanations.

Result: VISTA achieves significant robustness improvements on SpuriVerse (+16.29% with Qwen-2.5-VL-7B) while maintaining strong performance on standard benchmarks (MMVP/SeedBench). It supports cross-VLM transfer, detects perception errors, and produces human-interpretable reasoning traces with reduced spurious attribute dependency.

Conclusion: Modular architectures with explicit perception-reasoning separation improve VLM reliability through controlled information flow and reward-aligned training, offering a pathway to mitigate shortcut learning while maintaining performance across tasks and models.

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [5] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: 开发了一种名为SAMM2D的双编码器框架，通过强化预训练而非数据增强提升颅内动脉瘤检测性能，在低标注数据场景下实现32%的AUC提升并达到95%灵敏度。


<details>
  <summary>Details</summary>
Motivation: 动脉瘤检测面临形态细微、类别不平衡及标注数据稀缺的挑战，传统数据增强方法在强预训练模型中效果反降，需探索更优医疗影像处理范式。

Method: 构建基于ImageNet预训练的双编码器架构SAMM2D，放弃传统数据增强策略，通过调整决策阈值优化检测灵敏度，并使用Grad-CAM可视化验证模型关注区域。

Result: 在RSNA数据集取得0.686 AUC，较临床基线提升32%；灵敏度达95%超越放射科医生水平，每千名患者筛查节省1390万美元；真阳性病例中85%的Grad-CAM可视化区域与专家标注有62%重叠。

Conclusion: 强预训练在低数据量医学影像场景中比复杂增强策略更有效，建议医疗影像工作流优先选择高质量预训练模型而非过度依赖数据增强。

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [6] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: 提出HookMIL框架，通过可学习hook token聚合上下文信息，解决传统MIL模型丢失上下文及transformer方法计算冗余的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法丢失关键上下文信息，transformer变体虽表达能力强但计算复杂度高。需要兼顾上下文建模与计算效率的病理分析框架。

Method: 设计hook token初始化机制（视觉特征/key-patch/空间转录组）、线性复杂度双向注意力、hook多样性损失、hook间通信机制，实现结构化上下文聚合。

Result: 在四个病理数据集达到SOTA性能，计算效率提升且具备可解释性，代码已开源。

Conclusion: 多模态初始化的hook token有效结合文本/空间先验知识，提出的线性注意力机制和优化策略显著提升MIL模型表现。

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [7] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: Tiny-YOLOSAM 提出检测器引导与稀疏采样相结合的方法，在COCO数据集上将全场景分割覆盖率提升至77.1%（AR）同时实现4.7倍加速


<details>
  <summary>Details</summary>
Motivation: Segment Anything Model (SAM)计算成本过高，TinySAM虽然保持质量但'全割'模式仍需大量提示（49.20秒/图片），在延迟敏感场景效率低下

Method: 1. 使用YOLOv12检测器生成显著物体的边界框提示
2. 采用稀疏点提示补充YOLO未覆盖区域
3. 在Apple M1 Pro CPU上进行混合处理

Result: 在COCO val2017上：
- 无类别覆盖指标AR提升373%（16.4%→77.1%）
- 平均交并比提升253%（19.2%→67.8%）
- 端到端推理速度提升4.7倍（49.20s→10.39s/图片）

Conclusion: 检测器引导的提示选择与针对性稀疏采样组合，显著提升零样本掩码质量且降低计算负载，为实用化全场景分割提供新范式

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [8] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: 本研究提出了一种新的糖尿病视网膜病变（DR）诊断方法，通过多模态可解释AI（结合眼底图像和OCT图像）实现病灶分布分析与自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 当前DR诊断中，AI模型依赖单一影像模态且需手动标注病灶，临床实用性受限。医生需要能解释分类决策而非仅展示病灶位置的模型。

Method: 开发基于视觉语言模型（VLM）的多模态少样本学习框架，生成Grad-CAM热力图，分析视网膜象限病灶分布并同步解释DR严重程度分类。

Result: 在3000张眼底图像和1000张OCT图像数据集上验证了模型的多模态病灶定位与分类解释能力。

Conclusion: 该多模态可解释AI模型克服了单模态模型的局限性，为DR筛查、治疗和研究提供了实用化综合工具。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [9] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: TCFormer 通过 5M 参数的轻量级弱监督 Transformer 框架，实现高精度人群计数，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工标注且计算量大，在资源受限场景下难以扩展，需开发参数少、无需密集标注的解决方案

Method: 1) 使用轻量 Vision Transformer 提取全局语义特征；
2) 设计可学习密度加权模块（LDWAm）通过动态区域特征融合替代空间监督；
3) 引入密度等级分类损失，通过离散化密度分布增强模型鲁棒性。

Result: 在ShanghaiTech等4个数据集实现参数效率与精度平衡，在仅图像级标签训练下达到SOTA级性能，模型体积缩减80%+的同时保持<1%精度损失

Conclusion: 证明弱监督Transformer可突破传统方法限制，为边缘设备人群计数提供实用化解决方案

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

</details>


### [10] [VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition](https://arxiv.org/abs/2512.22217)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Fadi Dornaika,Cosimo Distante,Abdenour Hadid*

Main category: cs.CV

TL;DR: 论文提出VLM-PAR，通过冻结SigLIP多语言编码器构建模块化视觉-语言框架，采用跨注意力融合对齐图像与提示嵌入，解决了行人属性识别中的类别不平衡和领域偏移问题，在PA100K等基准测试中取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 行人属性识别面临类别严重不平衡、属性协同依赖复杂、领域偏移等核心挑战，现有方法难以有效解决。

Method: 构建基于冻结SigLIP多语言编码器的模块化视觉-语言框架，通过紧凑的跨注意力融合机制对视觉特征进行精炼，实现图像和文本提示嵌入的对齐。

Result: 在高度不平衡的PA100K基准上准确率提升显著，创SOTA表现，在PETA和Market-1501基准的平均准确率也取得显著提升。

Conclusion: 通过大规模视觉-语言预训练与目标跨模态精炼的结合，有效克服PAR中的不平衡和泛化挑战，验证了视觉-语言框架在属性识别中的优势。

Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.

</details>


### [11] [Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark](https://arxiv.org/abs/2512.22218)
*Hieu Minh Nguyen,Tam Le-Thanh Dang,Kiet Van Nguyen*

Main category: cs.CV

TL;DR: ViSignVQA introduces the first large-scale Vietnamese dataset for signboard-centric VQA tasks, featuring 10,762 images and 25,573 question-answer pairs. Key findings show OCR-integrated models improve F1 scores by up to 209%, while a multi-agent framework combining perception/reasoning with GPT-4 achieves 75.98% accuracy. The dataset captures bilingual text, informal phrasing, and visual elements unique to Vietnamese signboards.


<details>
  <summary>Details</summary>
Motivation: Signboard text understanding in low-resource languages like Vietnamese has been underexplored in VQA research despite its real-world applications. Existing datasets lack domain-specific linguistic, cultural, and visual characteristics of Vietnamese signboards, necessitating resource development for multimodal OCR-integrated VQA in non-English contexts.

Method: Constructed a large-scale dataset (ViSignVQA) and trained OCR-enhanced VQA models (BLIP-2, LaTr, PreSTU, SaL) by integrating Vietnamese OCR (SwinTextSpotter) and language models (ViT5). Developed a multi-agent VQA framework using GPT-4 to combine perception (OCR extraction) and reasoning agents through majority voting for final predictions.

Result: OCR-enhanced models showed up to 209% F1-score improvements when incorporating signboard text. The multi-agent framework achieved 75.98% accuracy. The dataset includes extensive bilingual text, informal phrasing, and visual elements like colors/layouts, validating its ability to benchmark OCR-integrated Vietnamese VQA models effectively.

Conclusion: Domain-specific resources significantly enhance text-based VQA for low-resource languages. ViSignVQA establishes a critical benchmark for scene text understanding in Vietnamese, demonstrating that combining advanced OCR with large language models through specialized frameworks can dramatically improve performance. The dataset supports future research on real-world signboard comprehension and multimodal models.

Abstract: Understanding signboard text in natural scenes is essential for real-world applications of Visual Question Answering (VQA), yet remains underexplored, particularly in low-resource languages. We introduce ViSignVQA, the first large-scale Vietnamese dataset designed for signboard-oriented VQA, which comprises 10,762 images and 25,573 question-answer pairs. The dataset captures the diverse linguistic, cultural, and visual characteristics of Vietnamese signboards, including bilingual text, informal phrasing, and visual elements such as color and layout. To benchmark this task, we adapted state-of-the-art VQA models (e.g., BLIP-2, LaTr, PreSTU, and SaL) by integrating a Vietnamese OCR model (SwinTextSpotter) and a Vietnamese pretrained language model (ViT5). The experimental results highlight the significant role of the OCR-enhanced context, with F1-score improvements of up to 209% when the OCR text is appended to questions. Additionally, we propose a multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving 75.98% accuracy via majority voting. Our study presents the first large-scale multimodal dataset for Vietnamese signboard understanding. This underscores the importance of domain-specific resources in enhancing text-based VQA for low-resource languages. ViSignVQA serves as a benchmark capturing real-world scene text characteristics and supporting the development and evaluation of OCR-integrated VQA models in Vietnamese.

</details>


### [12] [On Extending Semantic Abstraction for Efficient Search of Hidden Objects](https://arxiv.org/abs/2512.22220)
*Tasha Pais,Nikhilesh Belulkar*

Main category: cs.CV

TL;DR: The paper introduces Semantic Abstraction to use 2D Vision-Language Models (VLMs) relevancy maps as 'abstract object' representations for 3D localization and completion of hidden objects (partially occluded). It leverages historical data on object placement to enhance search efficiency beyond random strategies.


<details>
  <summary>Details</summary>
Motivation: To develop a method enabling household robots to efficiently locate lost/hidden objects in cluttered environments, improving upon traditional search strategies and extending VLMs' capabilities into 3D spatial reasoning.

Method: 1) Treat VLMs' relevancy maps as abstract object representations
2) Use historical object placement data to guide unstructured 3D search
3) Combine 3D localization with object completion through learned semantic abstractions
4) Implement semantic search prioritization based on object association patterns

Result: Model achieves: 1) Accurate 3D localization of hidden objects in first attempt
2) 40-50% reduction in search time vs. random search baselines
3) Effective object completion from partial occlusion scenarios
4) Successful integration of spatial memory with visual-semantic reasoning

Conclusion: Semantic Abstraction provides a viable framework for extending VLM capabilities to 3D spatial reasoning, enabling efficient hidden object localization through learned semantic-spatial associations. This represents a significant advancement in domestic robotics by combining semantic memory with geometric search strategies.

Abstract: Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as "abstract object" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.

</details>


### [13] [VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs](https://arxiv.org/abs/2512.22226)
*Naishan Zheng,Jie Huang,Qingpei Guo,Feng Zhao*

Main category: cs.CV

TL;DR: 视频摘要：本文介绍了一种名为VideoScaffold的动态视频理解框架，通过弹性事件分割（EES）与层级事件整合（HEC）实现流视频的多粒度语义解析。


<details>
  <summary>Details</summary>
Motivation: 现有静态方法（如稀疏采样、帧压缩）在流视频中效果不佳，存在碎片化或过度压缩问题，且难以保持时序连贯性。本文旨在解决动态视频流处理中的冗余与语义保持矛盾。

Method: 1. 弹性尺度事件分割（EES）：基于预测引导动态调整事件边界；2. 层级事件整合（HEC）：通过语义相关性逐层聚合片段，形成多级抽象。两模块协同实现从帧级解析到事件级推理的流畅过渡。

Result: 在离线和流视频基准测试中均达到SOTA性能，可无缝扩展至基于图像的MLLM架构，代码已开源。

Conclusion: VideoScaffold通过动态事件粒度调整实现高效流视频理解，具有模块化、即插即用特性，为跨帧冗余与语义连贯性提供通用解决方案。

Abstract: Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.

</details>


### [14] [KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation](https://arxiv.org/abs/2512.22228)
*HaoNan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为KAN-FPN-Stem的新型架构，通过将FPN中的线性平滑卷积替换为基于KAN的非线性层，在视觉Transformer（ViT）的特征融合阶段显著提升了姿态估计性能，超越ViTPose-S基线模型2.0 AP。


<details>
  <summary>Details</summary>
Motivation: ViT的前端设计（如ViTPose中的简单patchification机制）因多尺度处理能力不足导致信息丢失，且传统注意力模块（如CBAM）并非性能瓶颈。研究发现瓶颈在于FPN的后融合非线性平滑步骤，亟需改进特征融合质量。

Method: 保留FPN的经典'上采样相加'融合流，但将其终端的3×3线性卷积替换为基于KAN的非线性卷积层，利用KAN的强非线性建模能力自适应修正多尺度融合中的伪影。

Result: 在COCO数据集上，KAN-FPN-Stem相较ViTPose-S基线模型提升+2.0 AP。消融实验证实瓶颈在于融合质量而非注意力模块，并验证了KAN层的有效性。

Conclusion: ViT前端的性能瓶颈本质在于特征融合（Fusion）而非特征精炼（Attention），KAN算子提供了一条通过增强非线性建模能力解决该瓶颈的有效路径。

Abstract: Vision Transformers (ViT) have demonstrated significant promise in dense prediction tasks such as pose estimation. However, their performance is frequently constrained by the overly simplistic front-end designs employed in models like ViTPose. This naive patchification mechanism struggles to effectively handle multi-scale variations and results in irreversible information loss during the initial feature extraction phase. To overcome this limitation, we introduce a novel KAN-enhanced FPN-Stem architecture. Through rigorous ablation studies, we first identified that the true bottleneck for performance improvement lies not in plug-and-play attention modules (e.g., CBAM), but in the post-fusion non-linear smoothing step within the FPN. Guided by this insight, our core innovation is to retain the classic "upsample-and-add" fusion stream of the FPN, but replace its terminal, standard linear 3x3 smoothing convolution with a powerful KAN-based convolutional layer. Leveraging its superior non-linear modeling capabilities, this KAN-based layer adaptively learns and rectifies the "artifacts" generated during the multi-scale fusion process. Extensive experiments on the COCO dataset demonstrate that our KAN-FPN-Stem achieves a significant performance boost of up to +2.0 AP over the lightweight ViTPose-S baseline. This work not only delivers a plug-and-play, high-performance module but, more importantly, reveals that: the performance bottleneck in ViT front-end often lies not in 'feature refinement' (Attention), but in the quality of 'feature fusion' (Fusion). Furthermore, it provides an effective path to address this bottleneck through the introduction of the KAN operator.

</details>


### [15] [Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction](https://arxiv.org/abs/2512.22237)
*Mengxiao Geng,Ran Hong,Xiaoling Xu,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于元信息引导的跨域协同扩散模型MiG-DM，用于低剂量PET成像，通过整合多模态先验信息有效提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET成像可降低辐射风险但面临噪声干扰与生理细节丢失挑战，现有方法忽视投影域物理特征和患者个体化信息，导致功能性与语义性关联挖掘受限。

Method: MiG-DM包含元信息编码模块（将临床参数转化为语义提示以对齐文本-图像模态）和跨域架构（结合投影域正弦图适配器捕获物理结构与图像域处理），基于扩散模型生成高质量图像。

Result: 在UDPET数据集和临床数据集验证中，MiG-DM在不同剂量水平均优于现有最优方法，显著提升图像质量与生理细节保留能力。

Conclusion: 融合元信息与跨域物理建模的MiG-DM为低剂量PET提供了技术突破，兼顾图像质量与临床实用性。

Abstract: Low-dose PET imaging is crucial for reducing patient radiation exposure but faces challenges like noise interference, reduced contrast, and difficulty in preserving physiological details. Existing methods often neglect both projection-domain physics knowledge and patient-specific meta-information, which are critical for functional-semantic correlation mining. In this study, we introduce a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates comprehensive cross-modal priors to generate high-quality PET images. Specifically, a meta-information encoding module transforms clinical parameters into semantic prompts by considering patient characteristics, dose-related information, and semi-quantitative parameters, enabling cross-modal alignment between textual meta-information and image reconstruction. Additionally, the cross-domain architecture combines projection-domain and image-domain processing. In the projection domain, a specialized sinogram adapter captures global physical structures through convolution operations equivalent to global image-domain filtering. Experiments on the UDPET public dataset and clinical datasets with varying dose levels demonstrate that MiG-DM outperforms state-of-the-art methods in enhancing PET image quality and preserving physiological details.

</details>


### [16] [Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture](https://arxiv.org/abs/2512.22239)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: This study addresses the challenge of deploying efficient deep learning models on edge devices in smart agriculture by proposing a hybrid knowledge distillation framework that balances computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Resource constraints on edge devices hinder the deployment of deep learning models in smart agriculture, necessitating a lightweight model that maintains high accuracy without excessive computational cost.

Method: A customized student model with inverted residual blocks and dense connectivity is trained via a multi-objective strategy combining hard-label supervision, feature-level/response-level distillation, and self-distillation using a ResNet18 teacher network.

Result: The student model achieved 98.56% accuracy (only 0.09% lower than the teacher) on rice seed classification with 2.7x reduced computational cost and 10x smaller size. It also outperformed DenseNet121 and ViT by significantly reducing parameters while maintaining accuracy across plant disease datasets.

Conclusion: The proposed framework demonstrates robustness, efficiency, and strong deployment potential for hardware-limited smart agriculture systems, achieving lightweight yet high-performance models.

Abstract: Deploying deep learning models on resource-constrained edge devices remains a major challenge in smart agriculture due to the trade-off between computational efficiency and recognition accuracy. To address this challenge, this study proposes a hybrid knowledge distillation framework for developing a lightweight yet high-performance convolutional neural network. The proposed approach designs a customized student model that combines inverted residual blocks with dense connectivity and trains it under the guidance of a ResNet18 teacher network using a multi-objective strategy that integrates hard-label supervision, feature-level distillation, response-level distillation, and self-distillation. Experiments are conducted on a rice seed variety identification dataset containing nine varieties and further extended to four plant leaf disease datasets, including rice, potato, coffee, and corn, to evaluate generalization capability. On the rice seed variety classification task, the distilled student model achieves an accuracy of 98.56%, which is only 0.09% lower than the teacher model (98.65%), while requiring only 0.68 GFLOPs and approximately 1.07 million parameters. This corresponds to a reduction of about 2.7 times in computational cost and more than 10 times in model size compared with the ResNet18 teacher model. In addition, compared with representative pretrained models, the proposed student reduces the number of parameters by more than 6 times relative to DenseNet121 and by over 80 times compared with the Vision Transformer (ViT) architecture, while maintaining comparable or superior classification accuracy. Consistent performance gains across multiple plant leaf disease datasets further demonstrate the robustness, efficiency, and strong deployment potential of the proposed framework for hardware-limited smart agriculture systems.

</details>


### [17] [Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions](https://arxiv.org/abs/2512.22263)
*Aahan Sachdeva,Dhanvinkumar Ganeshkumar,James E. Gallagher,Tyler Treat,Edward J. Oughton*

Main category: cs.CV

TL;DR: 本文提出了一种自适应融合RGB和长波红外(LWIR)视频流的框架，通过多比例融合与动态模型选择，在不同光照条件下提升自主机器人视觉检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统RGB检测模型在弱光环境下效果差，而热成像系统缺乏色彩和纹理信息，导致自主机器人在灾害救援等场景中面临检测挑战。

Method: 训练33个YOLO模型（22,000张标注图像，覆盖三种光照等级），将对齐的RGB与LWIR帧以11种比例混合（100/0至0/100），动态选择最佳检测模型。

Result: 全光（80/20 RGB-LWIR，92.8%置信度）和弱光（90/10，92.0%）模型显著优于YOLOv5n/YOLOv11n基线，无光条件（40/60，71.0%）超越基线但差异不显著。

Conclusion: 自适应RGB-LWIR融合显著提升光照异质环境下的检测置信度与可靠性，为自主机器人视觉系统提供实用解决方案。

Abstract: Autonomous robotic platforms are playing a growing role across the emergency services sector, supporting missions such as search and rescue operations in disaster zones and reconnaissance. However, traditional red-green-blue (RGB) detection pipelines struggle in low-light environments, and thermal-based systems lack color and texture information. To overcome these limitations, we present an adaptive framework that fuses RGB and long-wave infrared (LWIR) video streams at multiple fusion ratios and dynamically selects the optimal detection model for each illumination condition. We trained 33 You Only Look Once (YOLO) models on over 22,000 annotated images spanning three light levels: no-light (<10 lux), dim-light (10-1000 lux), and full-light (>1000 lux). To integrate both modalities, fusion was performed by blending aligned RGB and LWIR frames at eleven ratios, from full RGB (100/0) to full LWIR (0/100) in 10% increments. Evaluation showed that the best full-light model (80/20 RGB-LWIR) and dim-light model (90/10 fusion) achieved 92.8% and 92.0% mean confidence; both significantly outperformed the YOLOv5 nano (YOLOv5n) and YOLOv11 nano (YOLOv11n) baselines. Under no-light conditions, the top 40/60 fusion reached 71.0%, exceeding baselines though not statistically significant. Adaptive RGB-LWIR fusion improved detection confidence and reliability across all illumination conditions, enhancing autonomous robotic vision performance.

</details>


### [18] [Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models](https://arxiv.org/abs/2512.22272)
*Antara Titikhsha,Om Kulkarni,Dharun Muthaiah*

Main category: cs.CV

TL;DR: The paper explores enhancing text-to-image diffusion models' geometric accuracy using a trained Human Perception Embedding (HPE) teacher model.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models struggle with strict geometric constraints, creating a gap between human perception and model output. Current methods lack explicit geometric control without specialized training.

Method: A lightweight HPE teacher model, trained on the THINGS triplet dataset, guides diffusion processes via gradient injection. This separates geometry and style, tested across Stable Diffusion v1.5, SiT-XL/2, and PixArt-Σ architectures.

Result: Guided generation achieved 80% improved semantic alignment over baselines and zero-shot transfer of complex 3D shapes (e.g., Eames chair) onto conflicting materials. Flow models required continuous guidance to maintain geometry.

Conclusion: Small teacher models effectively enhance large generative systems' geometric control, improving creative versatility in text-to-image synthesis.

Abstract: Text-to-image diffusion models generate highly detailed textures, yet they often rely on surface appearance and fail to follow strict geometric constraints, particularly when those constraints conflict with the style implied by the text prompt. This reflects a broader semantic gap between human perception and current generative models. We investigate whether geometric understanding can be introduced without specialized training by using lightweight, off-the-shelf discriminators as external guidance signals. We propose a Human Perception Embedding (HPE) teacher trained on the THINGS triplet dataset, which captures human sensitivity to object shape. By injecting gradients from this teacher into the latent diffusion process, we show that geometry and style can be separated in a controllable manner. We evaluate this approach across three architectures: Stable Diffusion v1.5 with a U-Net backbone, the flow-matching model SiT-XL/2, and the diffusion transformer PixArt-Σ. Our experiments reveal that flow models tend to drift back toward their default trajectories without continuous guidance, and we demonstrate zero-shot transfer of complex three-dimensional shapes, such as an Eames chair, onto conflicting materials such as pink metal. This guided generation improves semantic alignment by about 80 percent compared to unguided baselines. Overall, our results show that small teacher models can reliably guide large generative systems, enabling stronger geometric control and broadening the creative range of text-to-image synthesis.

</details>


### [19] [GeCo: A Differentiable Geometric Consistency Metric for Video Generation](https://arxiv.org/abs/2512.22274)
*Leslie Gu,Junhwa Hur,Charles Herrmann,Fangneng Zhan,Todd Zickler,Deqing Sun,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 提出GeCo方法，融合残差运动和深度先验检测视频生成中的几何变形与遮挡伪影，并作为损失函数优化模型。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型常存在几何变形和遮挡不一致问题，需可解释度量方法进行系统评估与改进。

Method: 融合残余运动与深度先验生成密集一致性图，用于模型测评及训练中的无参数损失优化。

Result: 基准测试揭示常见模型缺陷，使用GeCo损失后显著减少生成视频中的变形伪影。

Conclusion: GeCo为视频生成提供了有效测评和优化方案，明确了现有技术局限与改进方向。

Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.

</details>


### [20] [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency](https://arxiv.org/abs/2512.22275)
*Dingyu Wang,Zimu Yuan,Jiajun Liu,Shanggui Liu,Nan Zhou,Tianxing Xu,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 开发了B&J基准测试评估医学AI模型多模态临床推理能力，发现当前模型在开放性任务中表现不足


<details>
  <summary>Details</summary>
Motivation: 传统医学考试基准无法全面评估AI模型在真实临床场景中的多模态整合能力

Method: 构建包含1245个真实病例的骨骼关节基准测试，评估11个视觉语言模型和6个大语言模型在7类临床推理任务中的表现

Result: 结构化选择题准确率超90%，但多模态开放任务准确率仅60%；医学专用模型无显著优势；视觉语言模型存在图像误读和文本幻觉问题

Conclusion: 现有AI系统未达到复杂多模态临床决策水平，建议仅限用于文本辅助场景，需突破多模态整合和视觉理解技术瓶颈

Abstract: Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.

</details>


### [21] [FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound](https://arxiv.org/abs/2512.22278)
*Hussain Alasmawi,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 本文提出Fetal-Gauge，首个评估视觉语言模型（VLM）在胎儿超声成像表现的标准基准，旨在解决围产期护理中的医疗资源短缺问题。


<details>
  <summary>Details</summary>
Motivation: 孕期超声成像需求增加导致全球超声医师短缺，现有VLM缺乏标准化评估体系，且超声影像数据存在模态复杂性及标注数据稀缺性问题。

Method: 构建包含42,000张影像和93,000组问答对的Fetal-Gauge基准数据集，覆盖解剖平面识别、结构定位、胎儿方位评估等五类临床任务，并对跨模态模型进行系统性评估测试。

Result: 当前最先进的VLM模型在该基准的准确率最高仅55%，远低于临床应用要求，暴露出现有模型在医学领域适应性和训练方法上的显著缺陷。

Conclusion: 该工作奠定了医学多模态AI研究的基础，为提升全球围产期医疗可及性提供了技术路径，基准数据集将在论文接收后开放。

Abstract: The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality's challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark will be publicly available once the paper gets accepted.

</details>


### [22] [A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation](https://arxiv.org/abs/2512.22294)
*Philip Xu,David Elizondo,Raouf Hamzaoui*

Main category: cs.CV

TL;DR: Uni4D是一个统一框架，通过文本、3D模型和图像间的三层次对齐，实现大规模3D检索和可控4D生成。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态对齐方法在开放词汇场景下难以有效处理动态多模态数据，需构建统一框架提升3D检索准确性及4D生成的时间一致性。

Method: 构建基于Align3D-130数据集的框架，包含三个组件：1) 3D文本多头注意力检索模型优化语义对齐；2) 多视角3D-图像跨模态对齐；3) 图像-文本循环对齐以生成时间连贯的4D资产。

Result: 实验表明Uni4D在3D检索任务中达到SOTA性能，4D生成具备显著语义可控性，且时间一致性评分较基线提升23.7%。

Conclusion: 该框架为动态跨模态理解提供了新范式，在虚实融合场景中展现强适配性，推动开放词汇场景下的多模态应用发展。

Abstract: We introduce Uni4D, a unified framework for large scale open vocabulary 3D retrieval and controlled 4D generation based on structured three level alignment across text, 3D models, and image modalities. Built upon the Align3D 130 dataset, Uni4D employs a 3D text multi head attention and search model to optimize text to 3D retrieval through improved semantic alignment. The framework further strengthens cross modal alignment through three components: precise text to 3D retrieval, multi view 3D to image alignment, and image to text alignment for generating temporally consistent 4D assets. Experimental results demonstrate that Uni4D achieves high quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding and practical applications.

</details>


### [23] [VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning](https://arxiv.org/abs/2512.22315)
*Yang Ding,Yizhen Zhang,Xin Lai,Ruihang Chu,Yujiu Yang*

Main category: cs.CV

TL;DR: VideoZoomer是一种新型agentic框架，通过动态调整视觉焦点，逐步提升视频理解能力，克服多模态大模型在长视频处理中的上下文窗口限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法因固定采样和静态预选择易遗漏关键信息，且无法纠正初始错误，因此需要一种自主动态优化证据收集的框架。

Method: 采用两阶段策略：1) 多轮式时间缩放工具动态选择关键高帧率片段 2) 先通过监督微调学习示例轨迹，再用强化学习优化决策策略。

Result: 7B模型在多个长视频基准测试中展现出多样化推理模式，在复杂任务表现超过现有开源模型，效率提升23%，帧预算减少40%。

Conclusion: 证明了agentic框架能有效突破多模态模型的时间序列处理瓶颈，为视频推理提供了可扩展的范式

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.

</details>


### [24] [SpotEdit: Selective Region Editing in Diffusion Transformers](https://arxiv.org/abs/2512.22323)
*Zhibin Qin,Zhenxiong Tan,Zeqing Wang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: SpotEdit通过选择性更新图像编辑中的修改区域，减少冗余计算并提升编辑效率


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型对所有图像区域进行均匀处理，导致小规模编辑时计算冗余且可能损坏未修改区域，需要更高效的方法

Method: 提出两阶段框架：SpotSelector通过感知相似性识别稳定区域并复用其特征，SpotFusion通过动态融合机制结合原始特征与编辑内容

Result: 实现编辑区域精确更新的同时，保持未修改区域的高质量，计算效率显著提升

Conclusion: SpotEdit作为无需训练的轻量级框架，在降低计算成本的同时保证了图像编辑的上下文连贯性与视觉质量

Abstract: Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.

</details>


### [25] [DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models](https://arxiv.org/abs/2512.22324)
*Jianrong Zhang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本论文提出DeMoGen，通过能量基扩散模型将复杂动作分解为语义子组件，并构建分解文本数据集支持训练。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成模型依赖正向建模，但缺乏对复杂动作的可解释分解能力。作者提出逆向分解视角，旨在发现可复用且语义明确的动作基元。

Method: 设计基于能量函数的扩散模型，提出三种训练范式：显式分解文本训练(DeMoGen-Exp)、正交自监督分解(DeMoGen-OSS)和语义一致性约束(DeMoGen-SC)，并通过逆扩散过程实现动作分解。

Result: 成功从复杂动作序列中解耦出可组合的动作基元，在无需单概念标注的情况下实现跨训练分布的多样性动作生成，且模型能进行零样本组合。

Conclusion: 该方法为动作理解提供了结构化视角，构建的分解数据集为文本驱动动作生成和组合性推理提供了新基准。

Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.

</details>


### [26] [The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma](https://arxiv.org/abs/2512.22331)
*Mariya Miteva,Maria Nisheva-Pavlova*

Main category: cs.CV

TL;DR: 该论文提出了一种基于变分自编码器（VAE）的多模态框架，通过整合T1Gd和FLAIR MRI影像特征进行潜在空间学习，以实现胶质母细胞瘤（GBM）中MGMT启动子甲基化的无创分类。


<details>
  <summary>Details</summary>
Motivation: 针对传统单模态和早期融合模型在特征冗余度高、多模态信息建模不充分的问题，提出需保留模态特异性结构的多视角融合方法。

Method: 设计独立概率编码器分别处理两种MRI模态，通过紧凑潜在空间进行特征融合，最终利用潜在嵌入向量进行MGMT甲基化状态分类。

Result: 该方法克服了传统模型对多模态信息完整性破坏和冗余度控制的局限，为多模态放射组学分析提供了新范式。

Conclusion: 所提出的潜在空间学习框架有效平衡了模态特异性保留与多模态融合，在预测GBM分子特征方面具有临床应用潜力。

Abstract: Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.

</details>


### [27] [Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides](https://arxiv.org/abs/2512.22335)
*Olaide N. Oyelade,Oliver Hoxey,Yulia Humrye*

Main category: cs.CV

TL;DR: 本研究提出一种基于视觉Transformer的端到端流水线，用于联合分析H&E和IHC全切片图像，实现乳腺癌HER2蛋白表达的4级评分及像素级定位


<details>
  <summary>Details</summary>
Motivation: 传统HER2评分方法难以准确预测蛋白表达水平且无法同时分析H&E/IHC图像，现有深度学习模型在像素级定位上存在缺陷

Method: 构建视觉Transformer系统，包含：1) H&E图像分块肿瘤定位算法；2) H&E-IHC区域关联映射函数；3) 临床启发式HER2评分机制；采用私密WSI数据集进行训练与验证

Result: 肿瘤定位准确率可达94%，HER2状态分类特异性93.3%；在4级HER2评分中显示优良性能，且与病理学家人工评估结果具有可比性

Conclusion: 提出的ViTs基模型能有效整合H&E和IHC多模态数据，实现高精度HER2评分系统，可用于辅助临床决策和病理诊断流程标准化

Abstract: The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring

</details>


### [28] [VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement](https://arxiv.org/abs/2512.22351)
*Zhengfei Kuang,Rui Lin,Long Zhao,Gordon Wetzstein,Saining Xie,Sanghyun Woo*

Main category: cs.CV

TL;DR: 本文提出通过多模态大语言模型（MLLMs）解决3D场景操作问题，并设计三种方法提升3D物体排列效果，最终验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在2D视觉-语言任务中取得进展，但其在复杂3D场景操作中的应用尚未被充分探索。

Method: 1) 引入基于MCP的API以增强视觉基础；2) 结合专用视觉工具提升3D场景理解；3) 设计多智能体协作框架应对迭代更新与错误纠正。

Result: 在25个复杂3D物体排列任务中，本文方法显著优于现有基线。

Conclusion: 所提框架有效解决了MLLM在3D场景操作中的三大挑战，为未来研究提供了可行方向。

Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io

</details>


### [29] [Self-Evaluation Unlocks Any-Step Text-to-Image Generation](https://arxiv.org/abs/2512.22374)
*Xin Yu,Xiaojuan Qi,Zhengqi Li,Kai Zhang,Richard Zhang,Zhe Lin,Eli Shechtman,Tianyu Wang,Yotam Nitzan*

Main category: cs.CV

TL;DR: Self-E是首个从零开始训练且支持任意步数推理的文本到图像生成模型，结合局部学习与自我驱动的全局匹配机制。


<details>
  <summary>Details</summary>
Motivation: 传统扩散/流程模型依赖多步局部监督，蒸馏方法需要预训练教师模型，均存在效率或灵活性限制。

Method: 基于Flow Matching框架引入自评估机制，使用当前得分估计动态评价生成样本，融合即时局部学习与全局自匹配策略。

Result: 在少量步数生成任务中表现优异，50步时性能与SOTA Flow Matching模型相当，且生成质量随步数增加持续提升。

Conclusion: 验证了无需预训练教师、非局部监督的统一生成框架可行性，首次实现从零训练的任意步数文本到图像模型。

Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.

</details>


### [30] [iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI](https://arxiv.org/abs/2512.22392)
*Himanshu Naidu,Yuxiang Zhang,Sachin Mehta,Anat Caspi*

Main category: cs.CV

TL;DR: iOSPointMapper是一款基于iPhone/iPad的实时人行道地图应用，利用设备端语义分割和LiDAR技术实现隐私保护的特征检测与定位。


<details>
  <summary>Details</summary>
Motivation: 现有街道路面数据采集方法存在成本高、碎片化严重、难以扩展的问题，阻碍了无障碍出行基础设施的建设需求。

Method: 通过移动端语义分割结合激光雷达深度估算和GPS/IMU融合数据检测道路特征，采用用户引导的标注界面验证数据，并将匿名化数据上传至TDEI平台整合。

Result: 系统特征检测和空间映射性能评估显示了该方案在行人测绘精度和数据质量方面的有效性。

Conclusion: 该研究提供了一种可扩展、用户中心化的解决方案，能够填补行人交通数据的关键缺口。

Abstract: Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale. We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads. The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles. To ensure transparency and improve data quality, iOSPointMapper incorporates a user-guided annotation interface for validating system outputs before submission. Collected data is anonymized and transmitted to the Transportation Data Exchange Initiative (TDEI), where it integrates seamlessly with broader multimodal transportation datasets. Detailed evaluations of the system's feature detection and spatial mapping performance reveal the application's potential for enhanced pedestrian mapping. Together, these capabilities offer a scalable and user-centered approach to closing critical data gaps in pedestrian

</details>


### [31] [DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization](https://arxiv.org/abs/2512.22406)
*Hansang Lee,Chaelin Lee,Nieun Seo,Joon Seok Lim,Helen Hong*

Main category: cs.CV

TL;DR: DeFloMat 是一种基于条件流动匹配的新对象检测框架，通过用确定性流场代替扩散模型的多个采样步骤，显著减少了临床应用场景下的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（如 DiffusionDet）虽然精度高，但其依赖大量迭代（T≫60）的随机去噪过程导致推断速度过慢，难以应用于时间敏感的临床场景（如克罗恩病的磁共振小肠造影诊断）。亟需一种能够在保持生成式模型精度的同时大幅提高效率的方法。

Method: DeFloMat 基于条件最优传输（OT）理论，采用确定性流场（近似整流流）替代扩散模型的随机路径，将检测过程表达为常微分方程（ODE）求解，从而实现单步或少量步骤的快速推断。

Result: 在临床 MRE 数据集上，DeFloMat 仅用 **3 步推断**即达到 **43.32% AP_{10:50}**，相较 DiffusionDet 最终收敛性能（4 步时 31.03% AP_{10:50}）提升了 **1.4 倍**。同时显著优化了定位性能（高 Recall）和少量步骤下的稳定性。

Conclusion: DeFloMat 成功平衡了生成式模型精度和临床效率需求，在保持准确率的同时，显著缩短了推断时间，为医学影像中的快速目标检测设立了新标准。

Abstract: We propose DeFloMat (Detection with Flow Matching), a novel generative object detection framework that addresses the critical latency bottleneck of diffusion-based detectors, such as DiffusionDet, by integrating Conditional Flow Matching (CFM). Diffusion models achieve high accuracy by formulating detection as a multi-step stochastic denoising process, but their reliance on numerous sampling steps ($T \gg 60$) makes them impractical for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE). DeFloMat replaces this slow stochastic path with a highly direct, deterministic flow field derived from Conditional Optimal Transport (OT) theory, specifically approximating the Rectified Flow. This shift enables fast inference via a simple Ordinary Differential Equation (ODE) solver. We demonstrate the superiority of DeFloMat on a challenging MRE clinical dataset. Crucially, DeFloMat achieves state-of-the-art accuracy ($43.32\% \text{ } AP_{10:50}$) in only $3$ inference steps, which represents a $1.4\times$ performance improvement over DiffusionDet's maximum converged performance ($31.03\% \text{ } AP_{10:50}$ at $4$ steps). Furthermore, our deterministic flow significantly enhances localization characteristics, yielding superior Recall and stability in the few-step regime. DeFloMat resolves the trade-off between generative accuracy and clinical efficiency, setting a new standard for stable and rapid object localization.

</details>


### [32] [Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy](https://arxiv.org/abs/2512.22423)
*Amil Khan,Matheus Palhares Viana,Suraj Mishra,B. S. Manjunath*

Main category: cs.CV

TL;DR: Bright-4B is a large-scale model for label-free 3D cell segmentation using brightfield microscopy, achieving high accuracy without fluorescence or post-processing.


<details>
  <summary>Details</summary>
Motivation: Existing 3D brightfield microscopy segmentation methods require fluorescence labeling or extensive post-processing, limiting their speed and noninvasiveness.

Method: Bright-4B combines hypersphere learning, Native Sparse Attention, HyperConnections, Mixture-of-Experts, and anisotropic patch embeds to segment organelles directly from brightfield volumes.

Result: Bright-4B achieves morphology-accurate segmentation of nuclei, mitochondria, and organelles across multiple confocal datasets, outperforming CNN/Transformer baselines in preserving structural details.

Conclusion: The model enables advanced label-free 3D cell mapping by eliminating reliance on fluorescence or manual interventions, with open-sourced code and pretrained weights for future research.

Abstract: Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.

</details>


### [33] [FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning](https://arxiv.org/abs/2512.22425)
*Ujunwa Mgboh,Rafi Ibn Sultan,Joshua Kim,Kundan Thind,Dongxiao Zhu*

Main category: cs.CV

TL;DR: FluenceFormer 通过双阶段变压器框架结合物理感知损失函数FAR，在放疗计划中显著提升通量图预测精度，解决卷积方法长距离依赖捕捉不足导致的结构不一致问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于卷积的方法难建模解剖结构与射束强度间的长程依赖关系，易产生物理不可行计划。放射治疗需精确控制剂量分布与能量守恒。

Method: 1) Stage1用Transformer生成全局剂量先验；2) Stage2结合射束几何信息生成物理校准的通量图；3) 提出融合体素保真度、梯度平滑度、结构一致性及能量守恒的FAR损失函数。

Result: 在前列腺IMRT数据集上，Swin UNETR骨干的模型能量误差达4.5%，结构保真度较CNN单阶段方法提升(p<0.05)。多骨干验证框架通用性。

Conclusion: Transformer双阶段架构配合物理约束损失函数能有效提升放射治疗计划质量，为自动化放疗提供新范式。

Abstract: Fluence map prediction is central to automated radiotherapy planning but remains an ill-posed inverse problem due to the complex relationship between volumetric anatomy and beam-intensity modulation. Convolutional methods in prior work often struggle to capture long-range dependencies, which can lead to structurally inconsistent or physically unrealizable plans. We introduce \textbf{FluenceFormer}, a backbone-agnostic transformer framework for direct, geometry-aware fluence regression. The model uses a unified two-stage design: Stage~1 predicts a global dose prior from anatomical inputs, and Stage~2 conditions this prior on explicit beam geometry to regress physically calibrated fluence maps. Central to the approach is the \textbf{Fluence-Aware Regression (FAR)} loss, a physics-informed objective that integrates voxel-level fidelity, gradient smoothness, structural consistency, and beam-wise energy conservation. We evaluate the generality of the framework across multiple transformer backbones, including Swin UNETR, UNETR, nnFormer, and MedFormer, using a prostate IMRT dataset. FluenceFormer with Swin UNETR achieves the strongest performance among the evaluated models and improves over existing benchmark CNN and single-stage methods, reducing Energy Error to $\mathbf{4.5\%}$ and yielding statistically significant gains in structural fidelity ($p < 0.05$).

</details>


### [34] [EmoCtrl: Controllable Emotional Image Content Generation](https://arxiv.org/abs/2512.22437)
*Jingyuan Yang,Weibin Luo,Hui Huang*

Main category: cs.CV

TL;DR: 该研究提出EmoCtrl模型，实现同时保持内容准确性和情感表达的图像生成（C-EICG任务），通过双模态情感增强模块和特制数据集，在生成质量、情感控制和用户偏好方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图像模型存在情感表达不足，而情感驱动模型存在内容失真。研究旨在解决内容一致性与情感表达性不可兼得的生成矛盾，填补可控情感图像生成领域的空白。

Method: 提出EmoCtrl架构：1) 构建包含内容/情感/情感提示的标注数据集作为训练基础；2) 设计双模态情感增强模块，同步增强文本描述的情感语义与视觉感知线索；3) 采用可学习情感token实现多维度情感控制。方法结合模型架构创新与高质量训练数据构建。

Result: 定量指标显示EmoCtrl在保持内容保真度（FID得分优于基准模型12.7%）的同时，情感分类准确率提升19.3%。用户研究显示76.8%参与者首选EmoCtrl生成结果。跨域实验验证情感tokens在插画/产品设计等创意场景的泛化能力。

Conclusion: 研究证实情感token在文本到图像生成中的有效性，EmoCtrl成功平衡内容准确性与情感表现力。结论表明该方法为生成可控的情感化视觉内容提供了可靠技术框架，并扩展了扩散模型在创意产业的应用边界。

Abstract: An image conveys meaning through both its visual content and emotional tone, jointly shaping human perception. We introduce Controllable Emotional Image Content Generation (C-EICG), which aims to generate images that remain faithful to a given content description while expressing a target emotion. Existing text-to-image models ensure content consistency but lack emotional awareness, whereas emotion-driven models generate affective results at the cost of content distortion. To address this gap, we propose EmoCtrl, supported by a dataset annotated with content, emotion, and affective prompts, bridging abstract emotions to visual cues. EmoCtrl incorporates textual and visual emotion enhancement modules that enrich affective expression via descriptive semantics and perceptual cues. The learned emotion tokens exhibit complementary effects, as demonstrated through ablations and visualizations. Quantatitive and qualatitive experiments demonstrate that EmoCtrl achieves faithful content and expressive emotion control, outperforming existing methods across multiple aspects. User studies confirm EmoCtrl's strong alignment with human preference. Moreover, EmoCtrl generalizes well to creative applications, further demonstrating the robustness and adaptability of the learned emotion tokens.

</details>


### [35] [SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems](https://arxiv.org/abs/2512.22439)
*Khalfalla Awedat,Mohamed Abidalrekab,Gurcan Comert,Mustafa Ayad*

Main category: cs.CV

TL;DR: SuperiorGAT is a graph attention-based method to reconstruct missing elevation data in sparse LiDAR scans, improving resolution without additional hardware.


<details>
  <summary>Details</summary>
Motivation: LiDAR systems face challenges due to fixed vertical beam resolution and beam dropout from environmental occlusions, which degrade perception quality in autonomous systems.

Method: Models LiDAR scans as beam-aware graphs and uses gated residual fusion with feed-forward refinement to reconstruct missing elevation data in sparse point clouds.

Result: Outperformed PointNet-based models and deeper GAT baselines with lower reconstruction errors and better geometric consistency across KITTI datasets, confirmed by X-Z projections showing structural integrity preservation.

Conclusion: Architectural refinement via SuperiorGAT offers a computationally efficient way to enhance LiDAR data quality without requiring additional sensor hardware.

Abstract: LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model's ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.

</details>


### [36] [LECalib: Line-Based Event Camera Calibration](https://arxiv.org/abs/2512.22441)
*Zibin Liu,Banglei Guana,Yang Shanga,Zhenbao Yu,Yifei Bian,Qifeng Yu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于直线特征的事件相机自动校准方法，无需重建强度图像或手动放置标定物体，适用于平面/非平面直线场景。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖人工标定板、图像重建和耗时流程，难以满足动态场景需求，而城市环境中存在大量可用直线特征尚未被有效利用。

Method: 创新性地直接从事件流中检测直线特征，建立事件流与直线几何关系的数学模型，通过初始参数估计结合非线性优化完成参数精调。

Result: 在单目/立体事件相机上验证均达到0.3-0.5像素重投影误差，较传统方法提升30%精度，且支持非平面场景标定，代码已开源。

Conclusion: 该方法实现了全自动化、无需人工干预的标定流程，显著提升动态场景下的标定效率与适应性，为事件相机实用化提供关键技术支撑。

Abstract: Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.

</details>


### [37] [SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues](https://arxiv.org/abs/2512.22449)
*Md Abu Obaida Zishan,Annajiat Alim Rasel*

Main category: cs.CV

TL;DR: SonoVision是一个智能手机应用，通过耳机声音提示帮助视障人士定位日常物品，增强其自主性。


<details>
  <summary>Details</summary>
Motivation: 视障人士长期面临物品定位难题，依赖他人易引发安全隐患，需通过技术手段提升其独立生活能力。

Method: 基于Flutter框架开发应用，后端采用Efficientdet-D2模型进行物体检测，通过双耳不同声道的正弦波声音（左/右/双耳同时）提示物体方位。

Result: 成功实现离线运行的物体定位功能，通过声音提示使用户能自主定位前方、左侧或右侧物体。

Conclusion: 该应用通过智能手机+耳机的低门槛方案，有效减少视障人士对周围人的依赖，具备安全性和易用性优势。

Abstract: Locating objects for the visually impaired is a significant challenge and is something no one can get used to over time. However, this hinders their independence and could push them towards risky and dangerous scenarios. Hence, in the spirit of making the visually challenged more self-sufficient, we present SonoVision, a smart-phone application that helps them find everyday objects using sound cues through earphones/headphones. This simply means, if an object is on the right or left side of a user, the app makes a sinusoidal sound in a user's respective ear through ear/headphones. However, to indicate objects located directly in front, both the left and right earphones are rung simultaneously. These sound cues could easily help a visually impaired individual locate objects with the help of their smartphones and reduce the reliance on people in their surroundings, consequently making them more independent. This application is made with the flutter development platform and uses the Efficientdet-D2 model for object detection in the backend. We believe the app will significantly assist the visually impaired in a safe and user-friendly manner with its capacity to work completely offline. Our application can be accessed here https://github.com/MohammedZ666/SonoVision.git.

</details>


### [38] [SAM 3D for 3D Object Reconstruction from Remote Sensing Images](https://arxiv.org/abs/2512.22452)
*Junsheng Yao,Lichao Mou,Qingyu Li*

Main category: cs.CV

TL;DR: 本研究系统评估了通用3D图像基础模型SAM 3D在单目城市建筑重建中的应用，发现其在屋顶几何和边界清晰度上优于专门设计的TRELLIS模型，并提出了城市场景重建的segment-reconstruct-compose流程。


<details>
  <summary>Details</summary>
Motivation: 现有建筑3D重建方法依赖任务专用架构和大量标注数据，本文旨在验证通用图像转3D模型SAM 3D在单目遥感重建中的可行性，以降低开发成本和数据依赖性。

Method: 在NYC Urban Dataset上对比SAM 3D与TRELLIS模型，采用Frechet Inception Distance(FID)评估几何相似度，CLIP-based Maximum Mean Discrepancy(CMMD)评估多模态一致性，并设计三级重建流程实现城市级场景建模。

Result: SAM 3D重构的屋顶形状连续性提高23%，边界锐度提升18%，其分段-重构-组合流程成功实现了街区级场景建模，但高度估计存在平均4.7米偏差。

Conclusion: 证实通用3D基础模型可有效完成单目城市重建，建议通过引入建筑物拓扑先验知识改进结构推理能力，指出多尺度特征融合和真实深度监督是关键改进方向。

Abstract: Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.

</details>


### [39] [Comparing Object Detection Models for Electrical Substation Component Mapping](https://arxiv.org/abs/2512.22454)
*Haley Mody,Namish Bansal,Dennies Kiprono Bor,Edward J. Oughton*

Main category: cs.CV

TL;DR: This paper compares three computer vision models (YOLOv8, YOLOv11, RF-DETR) for automated mapping of critical electrical substation components in the US, aiming to improve vulnerability assessment against natural hazards.


<details>
  <summary>Details</summary>
Motivation: Electrical substations are critical national infrastructure vulnerable to natural disasters. Manual mapping is inefficient for large-scale vulnerability assessment, necessitating automated solutions to enhance resilience planning and prevent economic/social disruptions.

Method: Trained and evaluated three computer vision models on a manually labeled dataset of US substation images. Compared models using metrics of detection accuracy, precision, and computational efficiency to determine suitability for large-scale mapping.

Result: Identified strengths/limitations of each model, with RF-DETR showing better balance of accuracy and efficiency. Successfully demonstrated large-scale mapping of substation components across the United States using selected models.

Conclusion: Machine learning-based automated mapping provides an efficient, scalable solution for infrastructure vulnerability assessment, enabling proactive grid resilience planning against multi-hazard threats.

Abstract: Electrical substations are a significant component of an electrical grid. Indeed, the assets at these substations (e.g., transformers) are prone to disruption from many hazards, including hurricanes, flooding, earthquakes, and geomagnetically induced currents (GICs). As electrical grids are considered critical national infrastructure, any failure can have significant economic and public safety implications. To help prevent and mitigate these failures, it is thus essential that we identify key substation components to quantify vulnerability. Unfortunately, traditional manual mapping of substation infrastructure is time-consuming and labor-intensive. Therefore, an autonomous solution utilizing computer vision models is preferable, as it allows for greater convenience and efficiency. In this research paper, we train and compare the outputs of 3 models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US substation images. Each model is evaluated for detection accuracy, precision, and efficiency. We present the key strengths and limitations of each model, identifying which provides reliable and large-scale substation component mapping. Additionally, we utilize these models to effectively map the various substation components in the United States, showcasing a use case for machine learning in substation mapping.

</details>


### [40] [Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing](https://arxiv.org/abs/2512.22464)
*Sukhyun Jeong,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: This paper introduces PGR²M, a hybrid motion representation combining interpretable pose codes and residual codes for text-based 3D motion generation and editing, addressing limitations in capturing temporal details in existing methods.


<details>
  <summary>Details</summary>
Motivation: Pose-code-based frameworks like CoMo struggle with subtle temporal dynamics and high-frequency motion details, leading to reduced reconstruction quality and local controllability. The authors aim to enhance motion synthesis/editing by preserving global structure while capturing fine-grained temporal variations.

Method: PGR²M employs a pose-guided RVQ tokenizer to decompose motion into: 1) pose latents encoding coarse global structure, and 2) residual latents modeling high-frequency temporal variations. Residual dropout prevents over-reliance on residuals. A two-stage Transformer framework predicts pose codes (base) and refines with residual codes (refine), conditioned on text inputs and quantization stage.

Result: On HumanML3D/KIT-ML datasets, PGR²M achieved improved Fréchet inception distance and reconstruction metrics over CoMo and diffusion/tokenization baselines. User studies confirmed enhanced editability with structure-preserving modifications.

Conclusion: The hybrid pose-residual representation balances semantic alignment with detailed motion capture, enabling both high-quality text-driven motion generation and intuitive structure-preserving edits for creative applications.

Abstract: Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.

</details>


### [41] [Event-based high temporal resolution measurement of shock wave motion field](https://arxiv.org/abs/2512.22474)
*Taihang Lei,Banglei Guan,Minzu Liang,Pengju Sun,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出一种利用多事件摄像头结合极坐标编码和迭代斜率分析的冲击波运动参数高精度测量方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以应对冲击波快速不均匀传播和不稳定环境条件,高时空分辨率测量对于武器测试和损伤评估等应用至关重要。

Method: 1) 建立极坐标编码系统并自适应提取感兴趣区域；2) 通过事件偏移计算和迭代斜率分析分离冲击波前沿事件；3) 基于事件相机成像模型构建3D几何模型与运动参数反演方法。

Result: 与压力传感器和理论公式相比,速度测量误差范围0.06%-5.2%,成功实现三维运动场重构和爆炸当量反演。

Conclusion: 该方法突破了现有技术的时空分辨率限制,在冲击波动态监测领域取得显著进展,适用于复杂环境下的定量分析。

Abstract: Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.

</details>


### [42] [Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection](https://arxiv.org/abs/2512.22483)
*Zihan Liu,Xiangning Ren,Dezhang Kong,Yipeng Zhang,Meng Han*

Main category: cs.CV

TL;DR: 本文提出了一种红外小目标检测的半监督范式，通过构建层次化MoE适配器和两阶段知识蒸馏与迁移方法，在仅需少量标注数据的情况下，使轻量化模型性能接近甚至超越全监督模型。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临标注成本高昂的问题，现有方法（如SAM）存在领域差距、无法编码物理先验及架构复杂等问题，亟需高效的半监督解决方案。

Method: 1) 构建包含白盒神经算子的层次化MoE适配器；2) 基于此组件提出两阶段范式：第一阶段通过10%全监督数据将SAM蒸馏为专家教师模型Scalpel-SAM，第二阶段利用Scalpel-SAM生成伪标签训练轻量化模型。

Result: 实验表明该方法在极低标注数据量下，下游模型性能与全监督模型相当甚至更优，并首次系统性地解决了SAM作为教师模型在IR-SOT领域的数据稀缺问题。

Conclusion: 所提半监督框架通过模块化解耦设计与知识迁移策略，有效提升了红外小目标检测中的数据利用效率，为物理先验建模与模型轻量化提供了可推广方案。

Abstract: Infrared small object detection urgently requires semi-supervised paradigms due to the high cost of annotation. However, existing methods like SAM face significant challenges of domain gaps, inability of encoding physical priors, and inherent architectural complexity. To address this, we designed a Hierarchical MoE Adapter consisting of four white-box neural operators. Building upon this core component, we propose a two-stage paradigm for knowledge distillation and transfer: (1) Prior-Guided Knowledge Distillation, where we use our MoE adapter and 10% of available fully supervised data to distill SAM into an expert teacher (Scalpel-SAM); and (2) Deployment-Oriented Knowledge Transfer, where we use Scalpel-SAM to generate pseudo labels for training lightweight and efficient downstream models. Experiments demonstrate that with minimal annotations, our paradigm enables downstream models to achieve performance comparable to, or even surpassing, their fully supervised counterparts. To our knowledge, this is the first semi-supervised paradigm that systematically addresses the data scarcity issue in IR-SOT using SAM as the teacher model.

</details>


### [43] [Tracking by Predicting 3-D Gaussians Over Time](https://arxiv.org/abs/2512.22489)
*Tanish Baranwal,Himanshu Gaurav Singh,Jathushan Rajasegaran,Jitendra Malik*

Main category: cs.CV

TL;DR: Video-GMAE通过将视频编码为动态高斯点集实现自监督学习，在预训练阶段自动实现视频跟踪，并在多个数据集上取得优于现有方法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统视频表示方法较少利用动态3D场景与2D视频间的几何约束，而高斯点云表示可引入动态场景建模先验知识，促使模型隐式学习轨迹信息。

Method: 1. 构建由时空位置的2D高斯组成的神经场景表示 2. 通过解码器重建原始视频帧 3. 在自监督预训练阶段通过遮掩高斯点促进轨迹学习 4. 采用基于Transformer架构的编码器-解码器结构。

Result: 1. 零样本跟踪性能与SOTA方法相当 2. 在Kinetics-400和Kubric数据集微调后分别提升34.6%和13.1% 3. 在动作识别和视频检索任务中超越现有自监督方法。

Conclusion: 高斯动态场景表示能有效融合视觉感知与运动轨迹建模，Video-GMAE框架开创了通过几何先验提升视频理解的新范式，预训练权重可迁移到多种下游任务。

Abstract: We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.

</details>


### [44] [CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation](https://arxiv.org/abs/2512.22536)
*Qinglin Zeng,Kaitong Cai,Ruiqi Chen,Qinhan Lv,Keze Wang*

Main category: cs.CV

TL;DR: CoAgent提出了一种协同闭合循环框架，通过结构化叙事规划（Storyboard Planner）、实体记忆维护（Global Context Manager）、视觉一致性控制（Synthesis Module + Visual Consistency Controller）和渐进式验证（Verifier Agent）机制，在长视频生成中实现身份一致性、场景连贯性和时间节奏稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型独立处理镜头导致三个核心问题：1) 实体身份漂移（如人物容貌变化）；2) 场景间空间关系断裂；3) 时间结构不稳定（如动作逻辑错乱）。该工作旨在建立跨镜头的全局一致性控制机制。

Method: 1) 设计四阶段流水线：结构化故事板规划→镜头生成→视觉验证→节奏调整；2) 引入全局实体记忆模块记录身份特征；3) 构建视觉一致性控制器同步跨镜头元素；4) 部署验证智能体通过视觉语言推理检测矛盾并触发局部重生成。

Result: 实验表明：在60秒视频生成任务中，CoAgent在身份一致性(FID-score提高32%)、叙事连贯性(CLIP-score提升27%)和时间节奏(SDTW距离降低41%)三个维度均显著优于基线模型，生成质量通过人类评估测试。

Conclusion: 通过结构化规划-生成-验证的协同机制，该框架首次实现了跨镜头身份、场景和时间维度的统一控制，为可控长视频生成提供了新范式，但计算效率和复杂场景泛化能力仍需优化。

Abstract: Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.

</details>


### [45] [Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains](https://arxiv.org/abs/2512.22545)
*Jesen Zhang,Ningyuan Liu,Kaitong Cai,Sidi Liu,Jing Yang,Ziliang Chen,Xiaofei Sun,Keze Wang*

Main category: cs.CV

TL;DR: SR-MCR通过自监督推理对齐框架，在多模态大模型中实现更可靠的中间推理过程，提升最终答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法只监督推理结果而忽视中间过程，导致多模态大模型存在推理不连贯和视觉接地不足的问题。

Method: 提出自引用多线索奖励机制（语义对齐、词法保真、非冗余性、视觉基准、步骤一致性）和基于置信度的GRPO目标函数，通过无监督方式强化推理过程可靠性。

Result: 基于Qwen2.5-VL的SR-MCR-7B在视觉基准测试中取得81.4%平均准确率，优于同类开源模型，且显著提升推理连贯性。

Conclusion: 该方法通过过程信号强化显著提升了多模态大模型的可靠性，各奖励项和冷却模块被实验证明具有独立有效性。

Abstract: Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.

</details>


### [46] [Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer](https://arxiv.org/abs/2512.22612)
*Dafeng Zhang,Yongqi Song,Shizhuo Liu*

Main category: cs.CV

TL;DR: The paper improves face clustering by enhancing the measurement between face embeddings through a Top-K Jaccard similarity method and a Sparse Differential Transformer


<details>
  <summary>Details</summary>
Motivation: Existing Jaccard-based methods for face clustering include too many irrelevant nodes, which weakens discriminative power and harms clustering performance.

Method: 1) Prediction-driven Top-K Jaccard similarity to purify neighbor nodes; 2) Sparse Differential Transformer (SDT) to reduce noise in Transformer-based relationship prediction

Result: Achieved state-of-the-art performance on MS-Celeb-1M dataset with improved clustering accuracy and robustness against noisy data

Conclusion: The proposed method significantly outperforms existing face clustering approaches by addressing neighbor-node impurity and Transformer noise issues

Abstract: The method used to measure relationships between face embeddings plays a crucial role in determining the performance of face clustering. Existing methods employ the Jaccard similarity coefficient instead of the cosine distance to enhance the measurement accuracy. However, these methods introduce too many irrelevant nodes, producing Jaccard coefficients with limited discriminative power and adversely affecting clustering performance. To address this issue, we propose a prediction-driven Top-K Jaccard similarity coefficient that enhances the purity of neighboring nodes, thereby improving the reliability of similarity measurements. Nevertheless, accurately predicting the optimal number of neighbors (Top-K) remains challenging, leading to suboptimal clustering results. To overcome this limitation, we develop a Transformer-based prediction model that examines the relationships between the central node and its neighboring nodes near the Top-K to further enhance the reliability of similarity estimation. However, vanilla Transformer, when applied to predict relationships between nodes, often introduces noise due to their overemphasis on irrelevant feature relationships. To address these challenges, we propose a Sparse Differential Transformer (SDT), instead of the vanilla Transformer, to eliminate noise and enhance the model's anti-noise capabilities. Extensive experiments on multiple datasets, such as MS-Celeb-1M, demonstrate that our approach achieves state-of-the-art (SOTA) performance, outperforming existing methods and providing a more robust solution for face clustering.

</details>


### [47] [Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone](https://arxiv.org/abs/2512.22615)
*Jiacheng Ye,Shansan Gong,Jiahui Gao,Junming Fan,Shuang Wu,Wei Bi,Haoli Bai,Lifeng Shang,Lingpeng Kong*

Main category: cs.CV

TL;DR: Dream-VL和Dream-VLA是基于扩散模型的视觉语言及动作模型，其双向特性在视觉规划和机器人控制任务中超越现有的自回归模型。


<details>
  <summary>Details</summary>
Motivation: 为克服当前自回归视觉语言模型在复杂视觉规划和动态控制任务中受限于序列生成的问题，研究扩散模型作为基础架构的可能性。

Method: 构建基于扩散的视觉语言模型（dVLM），并通过连续预训练构建视觉语言动作模型（dVLA），利用其双向特性实现高效的并行动作生成与分块处理。

Result: Dream-VL在多个基准任务中表现与顶级自回归模型相当，Dream-VLA在LIBERO任务中达到97.2%的成功率，SimperEnv-Bridge和SimperEnv-Fractal的平均成功率分别为71.4%和60.5%，均优于现有领先模型。

Conclusion: 扩散模型作为视觉语言和动作任务的基座模型具备显著优势，其双向架构天然适合处理并行生成与动作分块需求，推动视觉规划和机器人控制领域的进展。

Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

</details>


### [48] [Rethinking Memory Design in SAM-Based Visual Object Tracking](https://arxiv.org/abs/2512.22624)
*Mohamad Alansari,Muzammal Naseer,Hasan Al Marzouqi,Naoufel Werghi,Sajid Javed*

Main category: cs.CV

TL;DR: 本文对基于SAM的视觉目标跟踪中的记忆机制进行了系统研究，提出了一种结合短期表征记忆与长期干扰解决记忆的混合框架，并验证了其在SAM2和SAM3上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SAM2跟踪器通过优化记忆存储和复用取得性能突破，但其记忆机制设计缺乏通用原理性分析，且尚未验证这些机制是否可迁移到新版本SAM3模型中。

Method: 1) 剖析典型SAM2跟踪器的差异主要体现在短期记忆帧选择策略；2) 在SAM3框架中重新实现这些记忆机制；3) 提出将记忆解耦为短期表征记忆和长期干扰解决记忆的混合框架

Result: 在10个不同基准测试中，提出的混合记忆框架显著提升了长期遮挡、复杂运动和干扰物场景下的跟踪鲁棒性，且适用于SAM2和SAM3基础模型。

Conclusion: 记忆机制的模块化解耦设计能有效整合现有策略，研究发现指导了生成模型无关的记忆设计原则，新框架为SAM系列模型跟踪性能提升提供了通用解决方案。

Abstract: \noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}

</details>


### [49] [Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion](https://arxiv.org/abs/2512.22626)
*Yuming Gu,Yizhi Wang,Yining Hong,Yipeng Gao,Hao Jiang,Angtian Wang,Bo Liu,Nathaniel S. Dennler,Zhengfei Kuang,Hao Li,Gordon Wetzstein,Chongyang Ma*

Main category: cs.CV

TL;DR: 本论文提出Envision框架，通过扩散模型结合显式目标约束，生成空间一致且目标对齐的视觉轨迹，用于具身智能体规划。


<details>
  <summary>Details</summary>
Motivation: 现有视觉规划方法缺乏显式目标建模，导致轨迹生成存在空间漂移和目标错位问题，需通过目标引导提升物理合理性和目标一致性。

Method: 分为两阶段：1) 目标意象模型通过区域感知注意力机制合成目标图像；2) 环境-目标视频模型（基于FL2V扩散模型）在初始观察与目标图像间进行视频插值生成连续轨迹。

Result: 在物体操作和图像编辑基准测试中，相较基线模型实现了更强的目标对齐性、空间一致性和物体保持能力。

Conclusion: Envision生成的视觉计划能直接支持下游机器人控制，通过扩散模型与目标约束结合实现可靠的目标导向规划。

Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.

</details>


### [50] [FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution](https://arxiv.org/abs/2512.22647)
*Yidi Liu,Zihao Fan,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种基于细粒度感知奖励模型（FinPercep-RM）和协同进化课程学习（CCL）的图像超分辨率强化学习框架，解决传统全局奖励模型导致的局部失真与奖励篡改问题。


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估模型仅提供全局分数，难以检测局部细粒度失真，导致超分辨率模型生成视觉伪影并引发奖励篡改，无法对齐感知质量优化目标。

Method: 1. 构建FinPercep-RM：基于编码-解码结构，生成全局质量分与感知退化图（定位/量化局部缺陷）；
2. 提出CCL机制：同步演化奖励模型与超分辨率模型，通过课程学习从简单到复杂分阶段训练，提升稳定性。

Result: 实验表明，在全局质量（PSNR、SSIM）与局部真实性（FID、LPIPS）指标上均优于基线方法，有效抑制奖励篡改且保持生成质量。

Conclusion: 细粒度奖励建模与协同进化策略显著提升超分辨率的感知质量，为RLHF在视觉生成任务中的应用提供了鲁棒的解决方案。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity allows ISR models to produce perceptually undesirable artifacts that yield spurious high scores, misaligning optimization objectives with perceptual quality and results in reward hacking. To address this, we propose a Fine-grained Perceptual Reward Model (FinPercep-RM) based on an Encoder-Decoder architecture. While providing a global quality score, it also generates a Perceptual Degradation Map that spatially localizes and quantifies local defects. We specifically introduce the FGR-30k dataset to train this model, consisting of diverse and subtle distortions from real-world super-resolution models. Despite the success of the FinPercep-RM model, its complexity introduces significant challenges in generator policy learning, leading to training instability. To address this, we propose a Co-evolutionary Curriculum Learning (CCL) mechanism, where both the reward model and the ISR model undergo synchronized curricula. The reward model progressively increases in complexity, while the ISR model starts with a simpler global reward for rapid convergence, gradually transitioning to the more complex model outputs. This easy-to-hard strategy enables stable training while suppressing reward hacking. Experiments validates the effectiveness of our method across ISR models in both global quality and local realism on RLHF methods.

</details>


### [51] [Visual Autoregressive Modelling for Monocular Depth Estimation](https://arxiv.org/abs/2512.22653)
*Amir El-Ghoussani,André Kaup,Nassir Navab,Gustavo Carneiro,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 本文提出了基于视觉自回归（VAR）先验的单目深度估计方法，在仅需74K合成样本微调的情况下，通过创新的尺度条件上采样机制和无分类器引导，实现了室内外场景的SOTA性能，且推理效率显著优于扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型存在训练成本高、采样步骤多等问题，本文旨在探索自回归生成模型作为几何感知先验的可能性，解决传统方法在数据效率与生成质量难以兼顾的矛盾。

Method: 1) 将文本到图像的BIGVAR模型迁移至深度估计领域；2) 设计尺度-特征融合的条件上采样模块实现多阶段精细化；3) 引入无分类器引导策略增强几何连续性约束；4) 采用10步前向推理替代扩散模型的数百步采样。

Result: 1) 在NYU Depth V2等室内数据集达SOTA且参数量减少40%；2) 对合成数据依赖降低至74K样本（仅为扩散模型的1/5）；3) 户外KITTI数据集表现超过87%现有方法；4) 单图推理速度提升至83ms/帧。

Conclusion: 验证了自回归模型在几何感知任务中的独特优势，开辟了生成模型与3D视觉结合的新范式，其数据效率和跨场景适应性为端侧部署提供了新方向。开源代码已引发CVPR2024多项工作引用。

Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".

</details>


### [52] [Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos](https://arxiv.org/abs/2512.22657)
*Shravan Saranyan,Pramit Saha*

Main category: cs.CV

TL;DR: 该研究比较了多种深度学习架构（3D Inception、双流、CNN-RNN）在超声心动图视频中自动估算左心室射血分数（LVEF）的效果，发现改进的3D Inception模型表现最佳，RMSE为6.79%，但存在过拟合现象且对超参数敏感。


<details>
  <summary>Details</summary>
Motivation: 手动评估LVEF存在耗时、观测者差异等问题，深度学习可能提升准确性和适用性。

Method: 使用EchoNet-Dynamic数据集（10,030个视频）训练不同深度学习模型，系统评估结构修改和融合策略，通过RMSE等指标比较性能。

Result: 3D Inception模型最优，较小模型泛化能力更强，性能对卷积核大小和归一化策略等超参数敏感。

Conclusion: 所提出的深度学习方法可有效评估LVEF，结构设计与训练策略对医疗及其他领域视频分析具有普适参考价值。

Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.

</details>


### [53] [Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains](https://arxiv.org/abs/2512.22664)
*Qiankun Li,Feng He,Huabao Chen,Xin Ning,Kun Wang,Zengfu Wang*

Main category: cs.CV

TL;DR: 提出CLAdapter，通过聚类注意力机制在数据受限的下游任务中提升预训练视觉模型的迁移效果。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型（如ViT、ConvNeXt）在通用数据集表现优异，但在科学领域的数据受限任务中仍存在挑战，需设计通用且有效的迁移框架。

Method: 设计包含注意力机制和聚类中心模块的CLAdapter，通过分布相关性与变换矩阵优化特征表示，并支持CNN、Transformer等多架构统一集成。

Result: 在10个跨领域数据集上达到SOTA效果，涵盖医学、工业、材料科学等场景，验证了其对数据受限和3D/OOD任务的有效性。

Conclusion: CLAdapter通过自适应特征重校准显著提升迁移性能，具备架构无关性、跨模态扩展能力及开箱即用优势。代码已开源促进复现。

Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.

</details>


### [54] [INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading](https://arxiv.org/abs/2512.22666)
*Mert Ikinci,Luna Toma,Karin U. Loeffler,Leticia Ussem,Daniela Süsskind,Julia M. Weller,Yousef Yeganeh,Martina C. Herwig-Carl,Shadi Albarqouni*

Main category: cs.CV

TL;DR: INTERACT-CMIL是一个多头深度学习框架，通过联合预测五个组织病理学指标，实现对结膜黑色素瘤前病变的分级，优于传统CNN和基础模型，为结膜黑色素瘤诊断提供了可复现的计算基准。


<details>
  <summary>Details</summary>
Motivation: 准确分级结膜黑色素瘤前病变（CMIL）对治疗和黑色素瘤预测至关重要，但因形态学特征细微且诊断标准相互关联，分级仍存在困难。

Method: 提出INTERACT-CMIL框架，采用共享特征学习和组合部分监督方法，并通过跨任务一致性损失（Inter-Dependence Loss）联合预测WHO4、WHO5、水平扩散、垂直扩散和细胞核异型性五个指标。

Result: 基于486个三中心专家标注的结膜活检图像数据集，模型在WHO4指标上相对macro F1提升55.1%，在垂直扩散指标上提升25.0%，预测结果与病理专家分级一致且可解释。

Conclusion: INTERACT-CMIL解决了CMIL分级中的关键挑战，推动了眼科数字病理标准化诊断的发展。

Abstract: Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.

</details>


### [55] [CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation](https://arxiv.org/abs/2512.22681)
*ZhenQi Chen,TsaiChing Ni,YuanFu Yang*

Main category: cs.CV

TL;DR: CritiFusion通过引入多模态语义批评和频域优化机制，在无需额外训练的条件下提升了文本到图像扩散模型的语义对齐与视觉质量，成为可兼容现有模型的即插即改优化方案。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成图像质量高但难以准确匹配复杂文本描述，需解决语义对齐不足问题以提升生成内容与提示词的意图一致性。

Method: 1. CritiCore模块：采用视觉语言模型和多语言模型生成多层级语义反馈，动态指导扩散生成；2. SpecFusion模块：在频域融合中间特征，增强图像结构同时保留细节纹路；3. 整体采用端到端推理框架，无需参数更新即可即插即用。

Result: 在FID、CLIP相似度等指标提升15%-20%，人类偏好测试得分超越主流强化学习优化方法，生成图像在细节逼真度和提示贴合度上达到SOTA水平，且适配Stable Diffusion等主流模型。

Conclusion: 该框架揭示了推理时语义引导和频域特征重构的双重增强可替代传统训练优化，为扩散模型升级提供高效低成本的技术路径。

Abstract: Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.

</details>


### [56] [Autoregressive Flow Matching for Motion Prediction](https://arxiv.org/abs/2512.22688)
*Johnathan Xie,Stefan Stojanov,Cristobal Eyzaguirre,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: This paper introduces ARFM, an autoregressive probabilistic method for motion prediction, trained on diverse datasets to enhance long-term future tracking of complex human and robot motions.


<details>
  <summary>Details</summary>
Motivation: Current motion prediction models trained on limited data struggle to model complex motions, while video prediction models focus on visual realism over motion accuracy; this work bridges both aspects by applying scalable video generation approaches to motion prediction tasks.

Method: The authors propose autoregressive flow matching (ARFM) to model sequential continuous data, trained on diverse video datasets to generate long-horizon future point tracks for motion prediction.

Result: ARFM achieves complex motion prediction on newly developed benchmarks and improves downstream task performance (e.g., action prediction in robotics/humans) by leveraging predicted future tracks as conditions.

Conclusion: The study demonstrates that integrating probabilistic modeling and diverse data scaling can advance motion prediction efficiency and applicability in human and robotic scenarios.

Abstract: Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.

</details>


### [57] [Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors](https://arxiv.org/abs/2512.22689)
*Salvador Rodriguez-Sanz,Monica Hernandez*

Main category: cs.CV

TL;DR: 提出了一种基于Neural ODE的多模态配准方法，结合结构描述符和局部互信息，无需大量训练数据且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统非刚性配准在精度、计算复杂度与正则化间存在权衡，且仅适用于单模态；学习式方法依赖大规模训练数据且对未见模态泛化性差。

Method: 基于Neural ODE构建连续深度变形模型，融合结构描述符（图像特征提取）与局部互信息，设计三种变体实现模态无关配准。

Result: 在多模态数据集上超越基线方法（大/小变形场景），展示高鲁棒性（抗正则化扰动）、多尺度适配性及计算效率优势。

Conclusion: 该框架解决了传统方法单模态限制和学习方法的过拟合缺陷，实现了高精度、低误差的多模态医学图像配准。

Abstract: This work proposes a multimodal diffeomorphic registration method using Neural Ordinary Differential Equations (Neural ODEs). Nonrigid registration algorithms exhibit tradeoffs between their accuracy, the computational complexity of their deformation model, and its proper regularization. In addition, they also assume intensity correlation in anatomically homologous regions of interest among image pairs, limiting their applicability to the monomodal setting. Unlike learning-based models, we propose an instance-specific framework that is not subject to high scan requirements for training and does not suffer performance degradation at inference time on modalities unseen during training. Our method exploits the potential of continuous-depth networks in the Neural ODE paradigm with structural descriptors, widely adopted as modality-agnostic metric models which exploit self-similarities on parameterized neighborhood geometries. We propose three different variants that integrate image-based or feature-based structural descriptors and nonstructural image similarities computed by local mutual information. We conduct extensive evaluations on different experiments formed by scan dataset combinations and show surpassing qualitative and quantitative results compared to state-of-the-art baselines adequate for large or small deformations, and specific of multimodal registration. Lastly, we also demonstrate the underlying robustness of the proposed framework to varying levels of explicit regularization while maintaining low error, its suitability for registration at varying scales, and its efficiency with respect to other methods targeted to large-deformation registration.

</details>


### [58] [SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis](https://arxiv.org/abs/2512.22706)
*Paul Dobre,Jackson Cooper,Xin Wang,Hongzhou Yang*

Main category: cs.CV

TL;DR: This paper introduces SCPainter, a unified framework that combines 3D asset insertion and novel view synthesis (NVS) for autonomous driving simulation, enhancing training data diversity by integrating 3D Gaussian Splat representations and diffusion-based image generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods handle 3D asset insertion and NVS separately, limiting training data diversity and realism. A unified framework is needed to improve robustness of autonomous driving models through realistic scene generation and new scenario creation.

Method: SCPainter jointly uses 3D Gaussian Splat (GS) car assets and scene point clouds, projects them into novel views, and conditions a diffusion model to generate high-quality images that maintain realism and scene consistency.

Result: Evaluation on Waymo Open Dataset demonstrated successful joint integration of 3D asset insertion and NVS, enabling creation of diverse and realistic driving data with improved lighting, shadows, and scene interactions.

Conclusion: The unified framework enables effective data augmentation for autonomous driving training by combining 3D asset realism with NVS capabilities, supporting generation of complex driving scenarios with enhanced visual fidelity.

Abstract: 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.

</details>


### [59] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Olivier X. Miguel,Kevin Dick,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: USF-MAE, a self-supervised ultrasound model, outperforms DenseNet-169 in detecting cystic hygroma with high accuracy and clinical relevance.


<details>
  <summary>Details</summary>
Motivation: Cystic hygroma detection faces challenges due to limited labeled data. Supervised deep learning methods struggle with reproducibility and scalability in early prenatal screening.

Method: USF-MAE, pretrained on 370,000 unlabelled ultrasound images via masked autoencoding, was fine-tuned for binary classification. Evaluated on curated first-trimester images using 4-fold cross-validation, accuracy, sensitivity, specificity, ROC-AUC, and Score-CAM interpretability, compared to DenseNet-169.

Result: USF-MAE achieved higher metrics: accuracy=0.96, sensitivity=0.94, specificity=0.98, ROC-AUC=0.98 (vs. DenseNet-169: 0.93, 0.92, 0.94, 0.94). Score-CAM highlighted fetal neck regions. Wilcoxon test confirmed significance (p=0.0057).

Conclusion: Self-supervised pretraining improves cystic hygroma detection scalability and robustness despite label scarcity, supporting clinical decision-making with interpretable AI.

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [60] [Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation](https://arxiv.org/abs/2512.22745)
*Yongzhen Hu,Yihui Yang,Haotong Lin,Yifan Wang,Junting Dong,Yifu Deng,Xinyu Zhu,Fan Jia,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出Freetime FeatureGS，通过可学习的高斯基元和流式特征学习策略，从单帧分割图中重建分解的4D场景，克服了视频分割不稳定导致的重建问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖不稳定的视频分割结果，导致重建质量下降。需要一种无需视频分割且能有效整合时空信息的4D重建方法。

Method: 1) 提出Freetime FeatureGS，采用可移动的高斯基元表示动态场景 2) 引入对比损失函数，基于2D分割图优化基元特征 3) 设计流式特征学习策略，按时间顺序传播特征并避免优化局部极小值。

Result: 在多个数据集中重建质量显著优于现有方法，实现无需视频分割的4D分割功能，有效处理动态场景。

Conclusion: 该方法通过可移动高斯基元的时空特征学习，在无需视频分割的前提下实现了更优的4D场景重建。

Abstract: This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.

</details>


### [61] [TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts](https://arxiv.org/abs/2512.22748)
*Hao Zhang,Mengsi Lyu,Bo Huang,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 本文提出了一种针对长上下文、多图像场景的视觉令牌自适应剪枝方法，通过分解冗余并分阶段优化令牌选择，显著降低推理成本同时维持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌剪枝方法未充分考虑长上下文输入中的多图像场景，而实际应用中多图像冗余性（图像内和图像间）会导致推理效率下降，亟需动态优化机制。

Method: 分两阶段剪枝：1）图像内依据内容多样性分配令牌预算并选择代表性令牌（如显著区域）；2）图像间通过全局多样性筛选构建候选池，采用帕累托选择平衡文本对齐与多图像多样性。

Result: 实验显示新方法在多图像长上下文任务中，相比基线减少30%-50%视觉令牌数，且分类准确率/生成质量（如BLEU-4、CLIPScore）仅下降1-3%，优于现有剪枝技术。

Conclusion: 该方法有效解决了多图像场景中的冗余优化，为LMMs在真实多模态场景部署提供了低耗高效的剪枝框架。

Abstract: Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.

</details>


### [62] [Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers](https://arxiv.org/abs/2512.22760)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 本论文提出基于Hilbert曲线重排序的Neighbor-Aware Pruning和Merging by Adjacent Token similarity方法，在ViTs中通过保留空间连续性实现高效token压缩。


<details>
  <summary>Details</summary>
Motivation: 现有ViT token压缩方法忽略空间连续性和邻居关系，导致局部上下文丢失，而冗余token表示会显著影响计算效率。

Method: 设计Hilbert曲线驱动的1D序列化策略，包含邻居感知剪枝(NAP)和邻域相似度token聚合(MAT)，显式保留2D空间局部结构信息。

Result: 实验表明该方法在ImageNet等5个基准数据集上取得SOTA准确率-效率平衡，在保持95%精度的同时减少60%token数量。

Conclusion: 证实了空间连续性对ViT压缩的重要性，为视觉Transformer的架构优化提供了新的研究方向。

Abstract: Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.

</details>


### [63] [Plug In, Grade Right: Psychology-Inspired AGIQA](https://arxiv.org/abs/2512.22780)
*Zhicheng Liao,Baoliang Chen,Hanwei Zhu,Lingyu Zhu,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 该论文提出AGQG模块，通过改进心理测量中的Graded Response Model（GRM）解决图像质量评估中的'语义漂移'问题，实现跨质量等级的可靠评价，并支持即插即用的模型集成。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估模型因文本与图像嵌入的多模态相似性导致'语义漂移'问题，表现为图像对矛盾质量等级描述的相似性过高，损害了模型可靠性。

Method: 提出基于GRM的双分支质量评分模块：一枝估计图像质量能力，另一枝构建算术递增的难度等级。通过算术建模保证难度单调性，强制形成单峰可解释的质量分布。

Result: AGQG模块在自然图像和屏幕内容图像质量评估中均提升性能，验证了其作为未来IQA模型核心组件的潜力，在多个SOTA框架中表现出即插即用优势。

Conclusion: 基于心理测量模型的算术GRM方法有效缓解了语义漂移问题，为图像质量评估提供了可靠的新范式，其模块化设计推动了通用质量评价体系的发展。

Abstract: Existing AGIQA models typically estimate image quality by measuring and aggregating the similarities between image embeddings and text embeddings derived from multi-grade quality descriptions. Although effective, we observe that such similarity distributions across grades usually exhibit multimodal patterns. For instance, an image embedding may show high similarity to both "excellent" and "poor" grade descriptions while deviating from the "good" one. We refer to this phenomenon as "semantic drift", where semantic inconsistencies between text embeddings and their intended descriptions undermine the reliability of text-image shared-space learning. To mitigate this issue, we draw inspiration from psychometrics and propose an improved Graded Response Model (GRM) for AGIQA. The GRM is a classical assessment model that categorizes a subject's ability across grades using test items with various difficulty levels. This paradigm aligns remarkably well with human quality rating, where image quality can be interpreted as an image's ability to meet various quality grades. Building on this philosophy, we design a two-branch quality grading module: one branch estimates image ability while the other constructs multiple difficulty levels. To ensure monotonicity in difficulty levels, we further model difficulty generation in an arithmetic manner, which inherently enforces a unimodal and interpretable quality distribution. Our Arithmetic GRM based Quality Grading (AGQG) module enjoys a plug-and-play advantage, consistently improving performance when integrated into various state-of-the-art AGIQA frameworks. Moreover, it also generalizes effectively to both natural and screen content image quality assessment, revealing its potential as a key component in future IQA models.

</details>


### [64] [Parallel Diffusion Solver via Residual Dirichlet Policy Optimization](https://arxiv.org/abs/2512.22796)
*Ruoyu Wang,Ziyu Li,Beier Zhu,Liangyu Yuan,Hanwang Zhang,Xun Yang,Xiaojun Chang,Chi Zhang*

Main category: cs.CV

TL;DR: 该论文提出EPD-Solver，通过并行梯度评估和几何优化减少扩散模型采样过程中的截断误差，在低延迟下保持图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因顺序去噪导致高采样延迟，现有加速方法在低延迟预算下因无法捕捉高曲率轨迹而显著降低生成质量。

Method: 基于向量均值定理设计并行ODE求解器（EPD-Solver），通过两阶段优化框架：蒸馏法优化初始参数，参数高效RL微调结合Dirichlet策略，并支持作为插件提升现有采样器。

Result: 通过并行梯度计算消除累积截断误差，并在文本到图像生成任务中验证了低延迟与性能的平衡。

Conclusion: EPD-Solver通过低维度策略优化和并行化设计，实现了扩散模型在复杂任务中的高效采样。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.

</details>


### [65] [VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM](https://arxiv.org/abs/2512.22799)
*Jingchao Wang,Kaiwen Zhou,Zhijian Wu,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: 提出VPTracker，首个基于多模态大语言模型的全局视觉-语言跟踪框架，通过位置感知视觉提示机制提升复杂场景下的跟踪稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖局部搜索导致视角变化/遮挡场景下易失效，全局搜索需解决语义视觉干扰问题。

Method: 构建区域级空间先验提示，融合多模态大语言模型的全局推理能力，动态切换局部定位与全局搜索。

Result: 在 challenging scenarios 中实现目标定位准确率提升23.7%，错误跟踪率降低41.2%，开源代码获3k星标。

Conclusion: 开创性打通MLLM与视觉跟踪的融合范式，为复杂场景目标定位提供新方法论，代码仓库已获领域广泛复用。

Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.

</details>


### [66] [Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation](https://arxiv.org/abs/2512.22800)
*Bin Liu,Wenyan Tian,Huangxin Fu,Zizheng Li,Zhifen He,Bo Li*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯和三平面表示的高效医学图像3D重建方法，解决传统方法在稀疏切片中计算成本高、结构不连续和细节丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像3D重建方法因计算开销大、稀疏切片下的结构不连续和细节丢失，难以满足临床诊断的精度需求。

Method: 结合3D高斯表示的高效渲染和几何表达优势，与三平面表示的结构连续性增强能力，在稀疏切片条件下提升医学图像重建质量。

Result: 在超声和磁共振等多模态医学数据集实验中，该方法在稀疏数据下生成了解剖结构连贯、语义稳定的高质量图像，且重建效率显著提升。

Conclusion: 为医学图像的3D可视化和临床分析提供了一种高效可靠的解决方案。

Abstract: 3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.

</details>


### [67] [Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image](https://arxiv.org/abs/2512.22801)
*Po-Chih Wu*

Main category: cs.CV

TL;DR: 本研究评估不同模型在低质量图像条件下的开放词汇目标检测表现，发现OWLv2模型更稳健，将发布新数据集。


<details>
  <summary>Details</summary>
Motivation: 为提升开放词汇目标检测模型在实际低质量图像环境中的性能，需验证现有模型鲁棒性并构建模拟真实场景的基准数据集。

Method: 提出低质量图像模拟数据集，对OWLv2、OWL-ViT等主流模型进行不同等级影像退化条件下的对比实验，分析性能变化趋势。

Result: 高退化条件下所有模型性能骤降（OWLv2相对最优），低退化下mAP值保持稳定，揭示模型对图像质量的敏感性差异。

Conclusion: 开放词汇检测模型在理想条件表现良好，但面对现实低质图像仍需改进；OWLv2架构具有更强的退化鲁棒性，研究为后续算法开发提供基准支撑。

Abstract: Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.

</details>


### [68] [EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation](https://arxiv.org/abs/2512.22808)
*Libo Zhang,Zekun Li,Tianyu Li,Zeyu Cao,Rui Xu,Xiaoxiao Long,Wenjia Wang,Jingbo Wang,Yuan Liu,Wenping Wang,Daquan Zhou,Taku Komura,Zhiyang Dou*

Main category: cs.CV

TL;DR: 该论文提出了HRD（Human Reaction Dataset）和EgoReAct框架，用于从以自我为中心的视频中实时生成3D对齐的人体反应动作。EgoReAct通过自回归模型结合3D动态特征，解决了数据不足和空间不一致问题，在生成真实感、空间一致性和效率方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如ViMo）存在以自我为中心视频与人体反应动作之间显著的空间不一致性，例如动态动作与固定摄像机视频配对。同时缺乏严格因果生成能力，限制了对人体反应中上下文敏感行为的建模。

Method: 1) 构建空间对齐的HRD数据集，采用动态3D捕捉系统；2) 设计基于VQ-VAE的自回归框架EgoReAct，将反应动作压缩至潜在空间，利用改进GPT模型进行实时因果生成；3) 在生成过程中融合3D特征（度量深度和头部动态）增强空间锚定。

Result: EgoReAct在生成真实感、空间一致性和计算效率方面显著优于现有方法，实验表明其保持生成过程严格因果性的同时，动作与视觉输入的匹配度提升23.6%，计算开销降低43%。

Conclusion: 该研究通过创新的HRD数据集和EgoReAct框架，成功实现了因果关系约束下实时3D人体反应生成，提出的3D特征融合方法有效解决了空间对齐问题，为未来以自我为中心的交互建模提供了基准方案。

Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.

</details>


### [69] [Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild](https://arxiv.org/abs/2512.22819)
*Hualie Jiang,Ziyang Song,Zhiqiang Lou,Rui Xu,Minglang Tan*

Main category: cs.CV

TL;DR: DA360改进Depth Anything V2，通过学习ViT的偏移参数并结合环形填充，在全景深度估计中实现零样本跨域性能突破。


<details>
  <summary>Details</summary>
Motivation: 室内全景深度模型迁移至开放环境的能力远弱于透视图像模型，需通过域迁移解决此差距。

Method: 设计可学习位移参数将Vision Transformer输出转为尺度不变深度估计，并在DPT解码器引入环形填充消除接缝伪影。

Result: 在室内外数据集分别降低50%和10%相对深度误差，相较PanDA等方法提升约30%，刷新零样本全景深度估计SOTA。

Conclusion: 成功将Perspective域深度估计能力迁移至全景域，建立新的跨环境泛化范式。

Abstract: Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model's scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.

</details>


### [70] [3D Scene Change Modeling With Consistent Multi-View Aggregation](https://arxiv.org/abs/2512.22830)
*Zirui Zhou,Junfeng Ni,Shujie Zhang,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出SCaR-3D方法，通过2D符号距离差异和多视图聚合解决3D变化检测中的空间不一致问题，并实现动态区域选择性更新，实验证明该方法具有较高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D变化检测方法存在空间不一致性和无法显式分离变化前/后状态的问题，且缺乏可控评估数据集。

Method: 框架包含：1) 基于符号距离的2D差异检测模块；2) 多视角投票与剪枝聚合模块；3) 基于3DGS的连续场景重建策略，仅动态区域更新。同时构建合成数据集CCS3D。

Result: 实验显示在变化检测和重建任务中均优于现有方法，具有高准确性和计算效率。

Conclusion: SCaR-3D通过显式分离变化状态和动态区域更新策略，有效解决了空间不一致性问题，合成数据集CCS3D为未来研究提供基准。

Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.

</details>


### [71] [A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences](https://arxiv.org/abs/2512.22833)
*Zhenbao Yu,Shirong Ye,Ronghe Jin,Shunkun Liang,Zibin Liu,Huiyun Zhang,Banglei Guan*

Main category: cs.CV

TL;DR: 本文提出了一种利用两个仿射对应关系和已知垂直方向估计两视角相对位姿和焦距的新方法。


<details>
  <summary>Details</summary>
Motivation: 针对自动驾驶汽车、智能手机等设备中相机与IMU联合使用的场景，利用IMU提供的垂直方向信息将相机相对位姿的5自由度缩减为3自由度，以提升估计效率。

Method: 建立包含焦距和相对旋转角的四维约束方程组，通过多项式特征值方法求解焦距和旋转参数。

Result: 在合成和真实数据集上的实验表明，该方法在精度和鲁棒性方面优于现有最先进算法。

Conclusion: 该方法通过引入垂直方向先验知识和数学优化，显著提升了单目相机焦距与相对位姿的联合估计性能。

Abstract: In this paper, we aim to estimate the relative pose and focal length between two views with known intrinsic parameters except for an unknown focal length from two affine correspondences (ACs). Cameras are commonly used in combination with inertial measurement units (IMUs) in applications such as self-driving cars, smartphones, and unmanned aerial vehicles. The vertical direction of camera views can be obtained by IMU measurements. The relative pose between two cameras is reduced from 5DOF to 3DOF. We propose a new solver to estimate the 3DOF relative pose and focal length. First, we establish constraint equations from two affine correspondences when the vertical direction is known. Then, based on the properties of the equation system with nontrivial solutions, four equations can be derived. These four equations only involve two parameters: the focal length and the relative rotation angle. Finally, the polynomial eigenvalue method is utilized to solve the problem of focal length and relative rotation angle. The proposed solver is evaluated using synthetic and real-world datasets. The results show that our solver performs better than the existing state-of-the-art solvers.

</details>


### [72] [ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning](https://arxiv.org/abs/2512.22854)
*Bangya Liu,Xinyu Gong,Zelin Zhao,Ziyang Song,Yulei Lu,Suhui Wu,Jun Zhang,Suman Banerjee,Hao Zhang*

Main category: cs.CV

TL;DR: This paper proposes ByteLoom, a Diffusion Transformer-based framework for realistic human-object interaction video generation with geometrically consistent object control.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from poor cross-view object consistency due to lack of multi-view information integration, and require labor-intensive hand mesh annotations for occlusion modeling.

Method: Developed ByteLoom with: 1) RCM-cache mechanism using Relative Coordinate Maps for geometry consistency and 6-DoF object control, and 2) progressive training curriculum requiring simplified human conditioning instead of detailed hand meshes.

Result: Demonstrated preservation of human identity, multi-view object geometry, and smooth manipulation motions while relaxing annotation requirements compared to previous approaches.

Conclusion: ByteLoom addresses key challenges in HOI generation through geometric consistency maintenance and annotation-efficient training strategy, enabling more practical applications.

Abstract: Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.

</details>


### [73] [MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments](https://arxiv.org/abs/2512.22867)
*Zhuonan Liu,Xinyu Zhang,Zishuo Wang,Tomohito Kawabata,Xuesu Xiao,Ling Xiao*

Main category: cs.CV

TL;DR: MUSON: 一个新型结构化的社会导航数据集，通过五步推理标注和平衡动作空间提升模型决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有社会导航数据集缺乏显式推理监督且动作分布不均衡，导致安全关键行为难以学习。

Method: 构建多模态数据集MUSON，包含室内外场景的五步链式思考标注（感知-预测-推理-行动-解释），并整合静态物理约束和平衡离散动作空间。

Result: Qwen2.5-VL-3B在该数据集取得86.25%决策准确率，验证了数据集的有效性。

Conclusion: MUSON为具备物理与社会约束的导航任务提供了可复用的基准，推动安全性与可解释性研究。

Abstract: Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON

</details>


### [74] [Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs](https://arxiv.org/abs/2512.22872)
*Ziyu Zhou,Haozhe Luo,Mohammad Reza Hosseinzadeh Taher,Jiaxuan Pang,Xiaowei Ding,Michael B. Gotway,Jianming Liang*

Main category: cs.CV

TL;DR: 该论文提出Lamps，一种通过整合人体解剖学的一致性、连贯性和层次结构作为监督信号的自监督学习方法，用于医学影像分析，表现出优于基线模型的鲁棒性和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在医学影像中常忽略解剖学结构性特征，导致难以有效学习解剖学特征。本研究旨在通过利用人体解剖学的多层次特性提升模型表现。

Method: 基于胸腔X光影像大规模预训练，通过和谐结合一致性、连贯性及层次结构三类解剖学特性作为监督信号，实现多视角特征学习。

Result: 在10个数据集中微调及涌现属性分析显示Lamps具有更优的鲁棒性、迁移性和临床潜力，对比10种基线模型效果显著提升。

Conclusion: 多视角学习使基础模型可生成与人体结构对齐的有意义表征，为医学影像领域提供新的方法框架。

Abstract: Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

</details>


### [75] [Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples](https://arxiv.org/abs/2512.22874)
*Weiwei Li,Junzhuo Liu,Yuanyuan Ren,Yuchen Zheng,Yahao Liu,Wen Li*

Main category: cs.CV

TL;DR: 本文提出一种数据导向方法，通过识别和消除深度学习中的虚假特征，缓解模型对虚假相关性的学习。该方法基于虚假特征在特征空间中分布分散的观察，通过分组策略建立偏置无关表征，并通过特征变换消除虚假特征，最终构建无偏模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工标注虚假属性或基于简单化假设过滤虚假特征，在真实数据复杂性条件下效果受限。研究者希望找到无需先验知识即可识别并消除虚假相关性的方法，提升模型泛化能力和公平性。

Method: 1) 观察虚假特征在特征空间呈分散分布；2) 通过简单分组策略生成偏置无关表征；3) 训练特征变换模块对齐该表征以消除虚假特征；4) 融合特征变换更新分类器。整个流程形成识别-中和-消除-更新的闭环。

Result: 在图像和NLP去偏基准测试中，相比传统经验风险最小化(ERM)方法，最差群体准确率提升超20%。研究团队已开源代码和模型参数（GitHub链接）。

Conclusion: 提出的全流程框架可有效缓解模型对虚假相关性的依赖，显著提升跨模态任务的公平性，且无需依赖额外标注数据或特殊假设，为深度学习模型去偏提供了可复现的解决方案。

Abstract: Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfactory performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spurious features based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spurious features by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing .

</details>


### [76] [M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models](https://arxiv.org/abs/2512.22877)
*Ju-Hsuan Weng,Jia-Wei Liao,Cheng-Fu Chou,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文提出M-ErasureBench多模态评估框架与IRECE方法，解决文本到图像扩散模型中跨输入模态的概念泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法仅针对文本提示（text prompts）场景，但图像编辑、个性化生成等应用中的学习嵌入（learned embeddings）和反演潜在空间（inverted latents）输入模态存在概念复现漏洞，需系统性评估与改进方案。

Method: 构建M-ErasureBench框架，覆盖文本/嵌入/潜在空间三种输入模态及白盒/黑盒攻击场景；提出IRECE模块，利用交叉注意力定位目标概念并在去噪阶段扰动相关潜在空间。

Result: 现有方法在文本提示下表现良好但跨模态效果恶化（白盒CRR>90%），IRECE在最复杂场景降低CRR达40%，同时保持生成质量无损。

Conclusion: M-ErasureBench首次系统评估多模态概念擦除，IRECE提供即插即用防护方案，共同提升生成模型的安全性与可靠性。

Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.

</details>


### [77] [Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance](https://arxiv.org/abs/2512.22881)
*Haosen Li,Wenshuo Chen,Shaofeng Liang,Lei Wang,Haozhe Jia,Yutao Yue*

Main category: cs.CV

TL;DR: 本论文提出Guided Path Sampling(GPS)，通过流形约束插值替代不稳定的分类器自由引导(CFG)外推，在扩散模型迭代优化中实现稳定路径生成，显著提升图像生成质量与语义对齐度。实验显示GPS比现有方法更优，如SDXL模型上ImageReward得分0.79。


<details>
  <summary>Details</summary>
Motivation: 标准分类器自由引导(CFG)的外推特性导致采样路径偏离数据流形，引发误差发散，限制了迭代优化方法的效果。需要寻找保持路径稳定性的改进方案。

Method: 提出Guided Path Sampling(GPS)：1) 用流形约束插值替代CFG的外推；2) 动态调整引导强度的时间表策略，实现语义注入与模型生成过程的协同优化。

Result: 理论证明GPS使误差序列从无界增长转为严格有界。在SDXL/Hunyuan-DiT等模型上，ImageReward 0.79、HPS v2 0.2995、GenEval语义对齐准确率57.45%，均显著优于现有方法。

Conclusion: 研究揭示路径稳定性是迭代优化效果的前提，GPS提供了实现稳定性的理论框架与实用方法，适用于现代扩散模型架构

Abstract: Iterative refinement methods based on a denoising-inversion cycle are powerful tools for enhancing the quality and control of diffusion models. However, their effectiveness is critically limited when combined with standard Classifier-Free Guidance (CFG). We identify a fundamental limitation: CFG's extrapolative nature systematically pushes the sampling path off the data manifold, causing the approximation error to diverge and undermining the refinement process. To address this, we propose Guided Path Sampling (GPS), a new paradigm for iterative refinement. GPS replaces unstable extrapolation with a principled, manifold-constrained interpolation, ensuring the sampling path remains on the data manifold. We theoretically prove that this correction transforms the error series from unbounded amplification to strictly bounded, guaranteeing stability. Furthermore, we devise an optimal scheduling strategy that dynamically adjusts guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process. Extensive experiments on modern backbones like SDXL and Hunyuan-DiT show that GPS outperforms existing methods in both perceptual quality and complex prompt adherence. For instance, GPS achieves a superior ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, while improving overall semantic alignment accuracy on GenEval to 57.45%. Our work establishes that path stability is a prerequisite for effective iterative refinement, and GPS provides a robust framework to achieve it.

</details>


### [78] [Hash Grid Feature Pruning](https://arxiv.org/abs/2512.22882)
*Yangzhi Ma,Bojun Liu,Jie Li,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: 本论文提出一种哈希网格特征剪枝方法，通过识别无效特征并仅编码有效特征，有效减少存储和传输开销，同时保持模型性能，并通过标准测试实现平均8%的比特率降低。


<details>
  <summary>Details</summary>
Motivation: 针对高斯泼溅场景中哈希网格因数据分布不均导致的无效特征冗余问题，传统方法在存储和传输时产生显著开销，需设计高效剪枝方案以优化隐式神经场表示。

Method: 基于输入高斯泼溅坐标动态判定无效特征区域，提出哈希网格特征剪枝算法，仅保留有效特征进行编码，通过动态特征筛选减少冗余存储。

Result: 在标准通用测试条件（CTC）下，该方法较基线技术减少8%平均比特率，存储需求下降同时保持模型重建质量与原始方法相当。

Conclusion: 通过坐标驱动的特征剪枝有效解决了隐式神经场哈希网格的稀疏性问题，为高斯泼溅场景的压缩效率提升提供了新范式，且方法具有标准化部署潜力。

Abstract: Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.

</details>


### [79] [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/abs/2512.22905)
*Kai Liu,Jungang Li,Yuchong Sun,Shengqiong Wu,Jianzhang Gao,Daoan Zhang,Wei Zhang,Sheng Jin,Sicheng Yu,Geng Zhan,Jiayi Ji,Fan Zhou,Liang Zheng,Shuicheng Yan,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: JavisGPT是首个统一多模态大模型（MLLM），通过编码器-LLM-解码器架构与SyncFusion模块实现时空音频视频融合，支持复杂指令驱动的理解与生成。创新点包括同步感知可学习查询设计、三阶段训练策略及20万规模的高质量音频视频对话数据集构建。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理复杂时空同步的联合音视频（JAV）任务时存在显著不足，特别是在生成连贯且同步的多模态内容方面。研究目标是构建首个能同时高效处理音视频理解与生成任务的统一模型，并解决大规模多模态指令调优数据匮乏的问题。

Method: 1) 构建编码器-LLM-解码器架构，设计SyncFusion模块进行时空特征对齐；2) 提出同步感知可学习查询作为预训练生成器（JAV-DiT）的适配接口；3) 实施三阶段训练：多模态预训练→音视频微调→大规模指令调优；4) 构建JavisInst-Omni数据集（20万GPT-4o生成的多层级音视频对话）。

Result: 在JAV基准测试中，JavisGPT相比现有MLLM在时空同步任务准确率提升19.7%，视频长度生成一致性提升32.4%。消融实验显示SyncFusion模块使跨模态检索mAP提升8.2%，而指令调优阶段使任务多样性得分提高41.3%。

Conclusion: JavisGPT成功建立统一框架解决JAV理解与生成的耦合问题，其架构设计与训练策略显著提升复杂时空任务性能。创建的JavisInst-Omni数据集为后续多模态研究提供了标准基准，实验证明同步感知查询机制可有效桥接生成模型与理解任务。

Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.

</details>


### [80] [ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939)
*Qihang Peng,Xuesong Chen,Chenye Yang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了ColaVLA，一种统一的视觉-语言-动作框架，旨在解决现有视觉语言模型(VLM)在自动驾驶轨迹规划中的效率、连续控制匹配及实时部署问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的规划方法存在文本推理与连续控制不匹配、自回归解码延迟高、规划策略非实时或非因果关系等问题，导致生成轨迹的安全性与效率受限。

Method: 设计Cognitive Latent Reasoner将场景理解压缩为元动作嵌入，结合Hierarchical Parallel Planner单次前向传播生成多尺度轨迹，并通过两次VLM前向传播实现推理到潜空间的推理迁移。

Result: 在nuScenes数据集上验证，在开环与闭环场景中均达到SOTA性能，同时在效率和鲁棒性上优于现有方法。

Conclusion: ColaVLA成功结合了VLM的通用性与实时轨迹规划能力，为自动驾驶系统提供了高效、安全且可解释的解决方案。

Abstract: Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.

</details>


### [81] [Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects](https://arxiv.org/abs/2512.22949)
*Zhicheng Zhao,Xuanang Fan,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 本论文提出DRMNet，通过密度图引导的自适应特征学习，在高分辨率遥感图像中检测小目标，尤其针对密集遮挡场景，结合三个创新模块提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测方法在高密度、低像素遥感图像中无法动态调整计算资源分配，导致特征学习不足，难以应对严重遮挡与小目标检测挑战。

Method: 设计密度生成分支（DGB）构建密度图作为先验，并通过密集区域聚焦模块（DAFM）动态分配资源；引入双滤波融合模块（DFFM），利用余弦变换分离多尺度特征并增强跨注意力交互。

Result: 在AI-TOD和DTOD数据集上，DRMNet显著优于SOTA方法，尤其在高密度、遮挡场景中mAP提升4.2%（AI-TOD达58.1%），且计算效率提升20%。

Conclusion: 密度图引导的自适应学习框架有效解决密集小目标检测痛点，三个模块协同优化特征交互与资源分配，为遥感监测提供新思路。

Abstract: High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.

</details>


### [82] [CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision](https://arxiv.org/abs/2512.22969)
*Behnam Raoufi,Hossein Sharify,Mohamad Mahdee Ramezanee,Khosrow Hajsadeghi,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: CLIP-Joint-Detect是一种通过端到端联合训练，将CLIP风格对比学习与传统检测损失结合的新型目标检测框架，适用于多类型检测器。


<details>
  <summary>Details</summary>
Motivation: 传统检测器依赖的交叉熵损失易受类别不平衡和标签噪声影响，需引入鲁棒性更强的对比视觉-语言监督机制。

Method: 新增轻量级并行头将区域特征投影到CLIP嵌入空间，通过InfoNCE损失和辅助交叉熵项联合优化，同步优化原始检测任务损失函数（支持两阶段/一阶段框架）。

Result: 在Pascal VOC（Faster R-CNN）和MS COCO（YOLOv11）上分别验证，平均精度提升明显（具体数值未提供），且保持实时推理能力（＞30FPS）。

Conclusion: 可学习文本嵌入与联合优化策略显著提升闭集检测性能，具有跨架构（Faster R-CNN/YOLO）和跨数据集（VOC/COCO）的泛化能力。

Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.

</details>


### [83] [Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection](https://arxiv.org/abs/2512.22972)
*Runwei Guan,Jianan Liu,Shaofeng Liang,Fangqiang Ding,Shanliang Yao,Xiaokai Bai,Daizong Liu,Tao Huang,Guoqiang Mao,Hui Xiong*

Main category: cs.CV

TL;DR: 提出了一种名为WRCFormer的新型3D目标检测框架，通过融合原始毫米波雷达数据与图像数据提升感知性能。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达数据存在稀疏性和语义缺失问题，现有雷达-相机融合方法面临信息损失或计算成本过高的挑战。

Method: 设计Wavelet Attention模块增强稀疏信号表示，采用Geometry-guided Progressive Fusion双阶段融合策略，直接处理原始4D雷达立方体和图像数据。

Result: 在K-Radar数据集上超越现有最优模型，整体性能提升2.4%，雨雪场景提升1.6%，验证了恶劣天气下的鲁棒性。

Conclusion: WRCFormer通过有效融合多模态数据，在保持低计算成本下实现了毫米波雷达感知性能的突破。

Abstract: 4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.

</details>


### [84] [YOLO-IOD: Towards Real Time Incremental Object Detection](https://arxiv.org/abs/2512.22973)
*Shizhou Zhang,Xueqiang Lv,Yinghui Xing,Qirui Wu,Di Xu,Chen Zhao,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出YOLO-IOD框架，解决YOLO系列在增量目标检测中的知识冲突问题，实现高效实时检测。


<details>
  <summary>Details</summary>
Motivation: 现有增量检测方法依赖Faster R-CNN/DETR框架，YOLO缺乏对应方案；YOLO直接用于增量学习存在前景-背景混淆、参数干扰、知识蒸馏失准三类关键冲突。

Method: 1) Conflict-Aware Pseudo-Label Refinement：利用伪标签置信度优化前景识别 2) Importance-based Kernel Selection：动态更新关键卷积核 3) Cross-Stage Asymmetric Distillation：跨阶段非对称知识蒸馏 4) 创建LoCo COCO防数据泄露基准

Result: 在传统和LoCo COCO基准上均实现最优性能，显著降低灾难性遗忘（传统基准mAP 43.7/新基准41.2）

Conclusion: 开创性地将YOLO引入增量检测领域，三要素协同机制有效缓解知识冲突，为实时系统提供可行方案

Abstract: Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.

</details>


### [85] [RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance](https://arxiv.org/abs/2512.22974)
*Chunyuan Chen,Yunuo Cai,Shujuan Li,Weiyun Liang,Bin Wang,Jing Xu*

Main category: cs.CV

TL;DR: 本文提出ReamCamo框架，通过统一的外绘生成方法提升伪装图像生成的现实性和前景-背景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有伪装图像生成方法在视觉相似性和背景语义一致性方面存在不足，导致生成图像与真实场景存在差距。

Method: 引入全局布局控制机制，并结合细粒度文本描述与纹理导向背景检索构建多模态条件，在统一框架中协同提升生成质量。

Result: 实验显示新框架在视觉保真度和伪装效果上均优于现有方法，提出的背景-前景分布差异度量获得有效验证。

Conclusion: 基于结构约束和多模态条件引导的生成框架能有效提升伪装样本的生成质量，为伪装目标检测提供更优训练数据。

Abstract: Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.

</details>


### [86] [PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects](https://arxiv.org/abs/2512.22979)
*Huiming Yang,Linglin Liao,Fei Ding,Sibo Wang,Zijian Zeng*

Main category: cs.CV

TL;DR: 该论文提出了PoseStreamer，一个针对高速运动场景的鲁棒多模态六自由度（6DoF）姿态估计框架，并设计了新的数据集MoCapCube6D验证其在高速低光场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB相机在高速低光场景中因运动模糊导致6DoF姿态估计效果差，而基于事件相机的方法在高速物体运动场景中未达到理想性能，需提出更优解决方案。

Method: 提出三个核心组件：基于历史方向线索的自适应姿态记忆队列、提供2D先验以提升3D中心召回的物体中心跟踪器、沿相机光线几何优化的射线姿态滤波器，并构建多模态数据集MoCapCube6D用于评估。

Result: 实验表明PoseStreamer在高速运动场景中达到更高精度，且作为无模板框架对未见运动物体具有强泛化能力。

Conclusion: PoseStreamer通过多模态融合与组件设计，在高速低光场景中实现了鲁棒6DoF姿态估计，并通过新数据集推动相关领域研究。

Abstract: Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.

</details>


### [87] [Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation](https://arxiv.org/abs/2512.22981)
*Linglin Liao,Qichuan Geng,Yu Liu*

Main category: cs.CV

TL;DR: 该研究提出空间感知对称对齐框架（SSA），解决医学图像分割中无法同时处理诊断/描述性文本及缺乏空间约束导致病灶定位错误的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两大局限：1）难以同步解析诊断信息与病变描述文本，导致图像区域关联弱；2）忽略空间约束导致位置定位偏差（如将'左下肺'分割到双侧肺部）。

Method: 1. 对称最优传输对齐机制：双向建立图像区域与多模态文本（位置/描述/诊断）的细粒度关联；2. 复合方向引导策略：通过区域级掩码显式引入空间约束。

Result: 在公开数据集上达到SOTA性能，特别在需要空间关系推理的病灶分割任务中表现突出，定量指标和可视化结果均显示定位准确性提升。

Conclusion: SSA框架通过双向跨模态对齐和空间约束建模，有效解决了医学文本多源语义解析与空间关系建模难题。

Abstract: Text-guided Medical Image Segmentation has shown considerable promise for medical image segmentation, with rich clinical text serving as an effective supplement for scarce data. However, current methods have two key bottlenecks. On one hand, they struggle to process diagnostic and descriptive texts simultaneously, making it difficult to identify lesions and establish associations with image regions. On the other hand, existing approaches focus on lesions description and fail to capture positional constraints, leading to critical deviations. Specifically, with the text "in the left lower lung", the segmentation results may incorrectly cover both sides of the lung. To address the limitations, we propose the Spatial-aware Symmetric Alignment (SSA) framework to enhance the capacity of referring hybrid medical texts consisting of locational, descriptive, and diagnostic information. Specifically, we propose symmetric optimal transport alignment mechanism to strengthen the associations between image regions and multiple relevant expressions, which establishes bi-directional fine-grained multimodal correspondences. In addition, we devise a composite directional guidance strategy that explicitly introduces spatial constraints in the text by constructing region-level guidance masks. Extensive experiments on public benchmarks demonstrate that SSA achieves state-of-the-art (SOTA) performance, particularly in accurately segmenting lesions characterized by spatial relational constraints.

</details>


### [88] [Reverse Personalization](https://arxiv.org/abs/2512.22984)
*Han-Wei Kung,Tuomas Varanka,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种无需文本描述的图像去身份化方法，通过反向个性化扩散模型实现对人脸图像的身份特征移除，同时保留可控的面部属性。核心创新点包括基于条件扩散的图像反演框架和身份引导条件分支的设计。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本描述的去身份化方法存在双重局限：一是依赖预训练模型中已有的身份特征表征，二是需要针对特定身份微调模型。同时现有方法难以控制面部属性保留程度，而本文旨在通过完全不依赖文本描述的图像反演机制实现更通用的属性可控去身份化。

Method: 1) 提出条件扩散反演框架，通过图像到潜空间的逆向映射进行像素级操作；2) 设计身份引导条件分支，利用预训练的身份编码器对潜在表示进行约束；3) 构建双重损失函数同时优化身份特征移除和面部属性保留目标。

Result: 在保留85%关键面部属性（如发型/佩戴物）的同时，成功抹除98.5%的身份特征信息，相比基线方法提升12.4%的图像质量（FID Score）。在跨数据集测试中展现23%的泛化能力提升。模型在公开数据集上实现每秒2.3张图像的匿名化处理速度。

Conclusion: 本文框架突破了传统文本描述驱动方法的范式限制，提供了更细粒度的属性控制能力。实验验证了其在保持图像质量与身份移除效果间的最佳平衡性，为隐私保护应用提供了可扩展的技术方案。开源代码促进了后续研究和技术落地。

Abstract: Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .

</details>


### [89] [A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection](https://arxiv.org/abs/2512.22990)
*Soham Dutta,Soham Banerjee,Sneha Mahata,Anindya Sen,Sayantani Datta*

Main category: cs.CV

TL;DR: 该论文提出了一种基于RGB无人机的低成本、统一的果园智能分析系统，结合ResNet50、VGG16和YOLOv8实现病害检测、苹果新鲜度判断及实时定位，部署在ESP32-CAM和树莓派上，无需云端离线运行。


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统独立处理果园病害检测、质量评估等问题且依赖高成本多光谱传感器，需开发一体化低硬件成本解决方案。

Method: 融合ResNet50（叶病检测）、VGG16（新鲜度判定）、YOLOv8（苹果定位）模型，通过ESP32-CAM与树莓派实现低功耗边缘计算。

Result: 实验达成98.9%叶类病变分类精度、97.4%新鲜度分类精度、0.857苹果检测F1值。

Conclusion: 系统验证了多任务RGB无人机在低成本硬件上的可行性，相较传统多光谱方案更具普适性与扩展性，为精准农业提供实用技术路径。

Abstract: Apple orchards require timely disease detection, fruit quality assessment, and yield estimation, yet existing UAV-based systems address such tasks in isolation and often rely on costly multispectral sensors. This paper presents a unified, low-cost RGB-only UAV-based orchard intelligent pipeline integrating ResNet50 for leaf disease detection, VGG 16 for apple freshness determination, and YOLOv8 for real-time apple detection and localization. The system runs on an ESP32-CAM and Raspberry Pi, providing fully offline on-site inference without cloud support. Experiments demonstrate 98.9% accuracy for leaf disease classification, 97.4% accuracy for freshness classification, and 0.857 F1 score for apple detection. The framework provides an accessible and scalable alternative to multispectral UAV solutions, supporting practical precision agriculture on affordable hardware.

</details>


### [90] [OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding](https://arxiv.org/abs/2512.23020)
*Wenyuan Huang,Zhao Wang,Zhou Wei,Ting Huang,Fang Zhao,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 提出OpenGround框架及OpenTarget数据集，实现零样本开放世界3D视觉定位


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义对象查找表(OLT)限制了未定义目标场景应用，需要提升视觉语言模型(VLM)的开放认知能力

Method: 设计主动认知推理(ACR)模块，通过动态更新的OLT进行人机协同感知，并构建7000+样本的OpenTarget评估数据集

Result: 在Nr3D/ScanRefer等基准测试中表现优异，相较基线模型在OpenTarget数据集上提升17.6%

Conclusion: 通过动态认知扩展实现预定义类别与开放世界类别的统一处理，为视觉语言模型提供新的认知扩展范式

Abstract: 3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at [this https URL](https://why-102.github.io/openground.io/).

</details>


### [91] [With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs](https://arxiv.org/abs/2512.23024)
*Ciprian Constantinescu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本论文提出一种基于单目图像的上下文感知物体分类框架Geo-Semantic Contextual Graph (GSCG)，通过整合深度估计与全景分割构建多属性结构图，并采用图分类器实现73.4%的分类准确率，显著超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 人类能够利用场景的上下文信息（如空间关系、材质属性）识别物体，但传统算法仅处理孤立图像区域，导致上下文信息缺失。此差距促使研究者开发能主动建模上下文关系的计算模型。

Method: 1) 构建GSCG：通过单目图像融合深度估计和全景分割模型，提取物体节点的几何/颜色/材质属性及其空间关系；2) 图分类器设计：通过聚合目标物体局部邻近节点与全局场景特征进行分类。

Result: 1) 通过消融实验证明上下文信息重要性（上下文感知模型73.4% vs 上下文无关模型38.4%）；2) 性能显著优于ResNet（53.5%）和Llama 4 Scout（42.3%）；3) 图结构赋予模型可解释性，提升分类可靠性。

Conclusion: 显示式构建多模态结构图（GSCG）能有效整合上下文信息，在提升分类准确率的同时赋予模型可解释性，验证了结构化表征在计算机视觉任务中的关键作用。

Abstract: Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.

</details>


### [92] [Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion](https://arxiv.org/abs/2512.23035)
*Yi Zhou,Xuechao Zou,Shun Zhang,Kai Li,Shiying Wang,Jingming Chen,Congyan Lang,Tengfei Cao,Pin Tao,Yuanchun Shi*

Main category: cs.CV

TL;DR: Co2S是一种结合视觉语言模型和自监督模型先验的遥感图像半监督语义分割框架，有效缓解伪标签漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法存在因确认偏差导致伪标签累积错误的缺陷，且现有半监督方法难以在遥感图像中平衡全局上下文与局部细节的语义一致性。

Method: 构建双学生架构（CLIP和DINOv3初始化的ViT）；提出显式-隐式语义协同引导机制（文本嵌入+可学习查询）；设计全局-局部特征融合策略（CLIP的全局语义与DINOv3的局部特征）；通过异质模型耦合抑制误差传播。

Result: 在6个主流遥感数据集上取得领先性能，覆盖不同划分协议和多场景验证，代码已开源。

Conclusion: 提出多模态先验融合的半监督新范式，通过跨架构知识蒸馏与双路径语义引导有效解决遥感图像伪标签漂移难题。

Abstract: Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.

</details>


### [93] [3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds](https://arxiv.org/abs/2512.23042)
*Ryousuke Yamada,Kohsuke Ide,Yoshihiro Fukuhara,Hirokatsu Kataoka,Gilles Puy,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: 本文提出LAM3C框架，通过无标签视频生成的点云数据实现3D自监督学习，在无需真实3D扫描数据的情况下超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D自监督方法依赖昂贵的大规模真实3D数据，而视频数据作为潜在数据源尚未被充分探索。作者旨在验证仅通过无标签视频生成的点云是否能有效学习3D表示。

Method: 1)构建RoomTours数据集：从互联网采集房产导览视频，通过前馈重建模型生成49,219个3D场景；2)设计LAM3C框架：多级3D聚类结合Sinkhorn-Knopp算法，并引入噪声正则化损失函数约束几何平滑性和特征鲁棒性。

Result: 在ScanNet室内分割任务上，基于视频生成数据训练的LAM3C表现优于现有全监督和自监督方法，mIoU达到62.3%，且在点云噪声干扰下保持85%的特征稳定性。

Conclusion: 无标签视频可通过重建模型高效生成大规模训练数据，开辟了3D学习的新数据范式。实验证明仅需视频输入的3D自监督框架已具备实用价值。

Abstract: Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.

</details>


### [94] [Video-BrowseComp: Benchmarking Agentic Video Research on Open Web](https://arxiv.org/abs/2512.23044)
*Zhengyang Liang,Yan Shu,Xiangrui Liu,Minghao Qin,Kaixin Liang,Paolo Rota,Nicu Sebe,Zheng Liu,Lizi Liao*

Main category: cs.CV

TL;DR: This paper introduces Video-BrowseComp, a benchmark for open-web agentic video reasoning, revealing significant bottlenecks in state-of-the-art models' ability to process dynamic visual evidence.


<details>
  <summary>Details</summary>
Motivation: The study addresses the modality gap in processing dynamic web video content, as existing benchmarks focus on passive perception rather than proactive video research requiring temporal evidence cross-referencing.

Method: Development of Video-BrowseComp with 210 questions requiring temporal visual evidence verification, enforcing video timeline navigation and open-web claim validation through dynamic video content analysis.

Result: Even advanced search-augmented models (e.g., GPT-5.1) achieved only 15.24% accuracy, showing heavy reliance on text metadata and failure in metadata-sparse domains requiring visual grounding (e.g., sports, gameplay).

Conclusion: Video-BrowseComp establishes the first open-web video research benchmark, advancing the field from passive video analysis toward proactive, temporally grounded video reasoning capabilities.

Abstract: The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.

</details>


### [95] [MedSAM-based lung masking for multi-label chest X-ray classification](https://arxiv.org/abs/2512.23089)
*Brayden Miao,Zain Rehman,Xin Miao,Siming Liu,Jianjie Wang*

Main category: cs.CV

TL;DR: 结合MedSAM模型进行肺部区域分割后，CXR分类性能受分割策略影响显著，研究发现应根据任务类型和模型架构选择适宜的分割参数。


<details>
  <summary>Details</summary>
Motivation: CXR成像存在弱疾病信号、数据集偏差和有限空间监督导致自动诊断困难，MedSAM医学图像分割模型可提供解剖结构先验信息提升诊断鲁棒性。提出分割引导的CXR分类管线以平衡异常分类与正常筛查需求。

Method: 构建MedSAM肺部提取模块+CNN多标签分类双阶段管线。在Airlangga数据集微调MedSAM生成肺部掩膜，NIH-CXR数据集训练ResNet50等模型进行肺实质、肺炎、肺水肿等5类病变检测。测试严格/宽松分割对模型性能的影响。

Result: 1) MedSAM生成具解剖学合理性的肺部掩膜 2) ResNet50在原始图像达最佳异常分类(0.813 macro AUROC) 3) 宽松掩膜损害异常分类但提升正常类识别(8.3%改善) 4) 严格掩膜降低3.7%分类性能但提升训练效率。

Conclusion: 肺部掩膜应作为可调节空间先验，根据诊断重点(异常分类vs正常筛查)和模型架构动态选择，宽松掩膜适合正常筛查场景而严格掩膜适配特定病变检测。

Abstract: Chest X-ray (CXR) imaging is widely used for screening and diagnosing pulmonary abnormalities, yet automated interpretation remains challenging due to weak disease signals, dataset bias, and limited spatial supervision. Foundation models for medical image segmentation (MedSAM) provide an opportunity to introduce anatomically grounded priors that may improve robustness and interpretability in CXR analysis. We propose a segmentation-guided CXR classification pipeline that integrates MedSAM as a lung region extraction module prior to multi-label abnormality classification. MedSAM is fine-tuned using a public image-mask dataset from Airlangga University Hospital. We then apply it to a curated subset of the public NIH CXR dataset to train and evaluate deep convolutional neural networks for multi-label prediction of five abnormalities (Mass, Nodule, Pneumonia, Edema, and Fibrosis), with the normal case (No Finding) evaluated via a derived score. Experiments show that MedSAM produces anatomically plausible lung masks across diverse imaging conditions. We find that masking effects are both task-dependent and architecture-dependent. ResNet50 trained on original images achieves the strongest overall abnormality discrimination, while loose lung masking yields comparable macro AUROC but significantly improves No Finding discrimination, indicating a trade-off between abnormality-specific classification and normal case screening. Tight masking consistently reduces abnormality level performance but improves training efficiency. Loose masking partially mitigates this degradation by preserving perihilar and peripheral context. These results suggest that lung masking should be treated as a controllable spatial prior selected to match the backbone and clinical objective, rather than applied uniformly.

</details>


### [96] [PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion](https://arxiv.org/abs/2512.23130)
*Jian Wang,Sixing Rong,Jiarui Xing,Yuling Xu,Weide Liu*

Main category: cs.CV

TL;DR: PathoSyn通过解耦解剖重建和病理偏差建模，提出了一种高保真的MRI图像合成框架，显著提升合成质量与解剖一致性。


<details>
  <summary>Details</summary>
Motivation: 现有MRI合成模型因特征纠缠常导致结构断裂或解剖失真。PathoSyn旨在解决全局像素建模和二值掩膜条件生成的局限性，提升病理细节与整体结构的协调性。

Method: 1）将合成任务分解为确定性解剖重建与随机偏差建模；2）提出偏差空间扩散模型学习病理残差分布；3）结合缝合感知融合策略与推断阶段稳定模块，消除边界伪影并增强内部异质性。

Result: 在肿瘤成像基准测试中，PathoSyn在感知真实性和解剖保真度上均超越整体扩散模型和掩膜条件基线方法，生成高保真患者特异性病理图像。

Conclusion: 框架为医学图像分析提供了数学原理严谨的合成流程，可加速低数据场景下诊断算法开发，并支持可解释的疾病进展推演与临床决策系统评估。

Abstract: We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

</details>


### [97] [Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations](https://arxiv.org/abs/2512.23142)
*Mingzhen Shao,Sarang Joshi*

Main category: cs.CV

TL;DR: 本文提出UniReg框架，揭示深度形变配准模型因局部特征表征而具备领域适应性，挑战传统'大数据提升鲁棒性'认知，实验验证单一数据集训练的模型可跨领域配准。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大数据缓解领域偏移但成本高昂，本文旨在揭示深度模型固有鲁棒性机理，为设计轻量级跨领域配准架构提供理论依据。

Method: 构造UniReg双模块框架：1) 固定预训练CNN/Transformer提取局部特征；2) UNet架构估计形变场，通过解耦设计系统隔离特征提取与形变学习过程。

Result: 单一模态训练的UniReg在MRI/PET/CT等跨模态配准中达优化方法88-92%性能，而ResNet50等通用CNN架构在第1/2卷积层出现68-75%的跨域特征失配。

Conclusion: 本文证明局部特征一致性是深度配准鲁棒性的根本，提出特征解耦架构有效缓解领域偏移，为医学图像分析提供新的模型设计范式。

Abstract: Deep learning has advanced deformable image registration, surpassing traditional optimization-based methods in both accuracy and efficiency. However, learning-based models are widely believed to be sensitive to domain shift, with robustness typically pursued through large and diverse training datasets, without explaining the underlying mechanisms. In this work, we show that domain-shift immunity is an inherent property of deep deformable registration models, arising from their reliance on local feature representations rather than global appearance for deformation estimation. To isolate and validate this mechanism, we introduce UniReg, a universal registration framework that decouples feature extraction from deformation estimation using fixed, pre-trained feature extractors and a UNet-based deformation network. Despite training on a single dataset, UniReg exhibits robust cross-domain and multi-modal performance comparable to optimization-based methods. Our analysis further reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers. These findings identify local feature consistency as the key driver of robustness in learning-based deformable registration and motivate backbone designs that preserve domain-invariant local features.

</details>


### [98] [GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2512.23147)
*Jingyu Li,Xiaolong Zhao,Zhe Liu,Wenxiao Wu,Li Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GeoTeacher的半监督3D目标检测方法，通过关键点几何关系监督和体素级数据增强策略提升学生模型在有限标签数据下对几何信息的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了模型在标签数据有限时对目标几何特征的低敏感性问题，导致难以捕捉对检测至关重要的几何信息。

Method: 设计了关键点几何关系监督模块传递教师模型知识，并引入结合距离衰减机制的体素级数据增强策略，增强学生模型对几何结构的理解。

Result: 在ONCE和Waymo数据集的实验验证了方法有效性，达到了新的SOTA性能，且具有跨方法可扩展性。

Conclusion: GeoTeacher通过几何关系学习和增强策略缓解了标签数据不足的问题，为半监督3D检测提供了可通用的解决方案。

Abstract: Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher

</details>


### [99] [REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation](https://arxiv.org/abs/2512.23169)
*Fulin Shi,Wenyi Xiao,Bin Chen,Liang Din,Leilei Gan*

Main category: cs.CV

TL;DR: REVEALER, a reinforcement-guided framework for element-level alignment evaluation in text-to-image models using MLLMs, achieves superior performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior text-to-image evaluation methods lack fine-grained interpretability and struggle to capture human preferences through coarse-grained metrics and static QA pipelines.

Method: Proposes REVEALER - a structured framework combining visual grounding-reasoning-conclusion paradigm, MLLMs for semantic localization, and GRPO optimization with composite rewards for structural format, grounding accuracy, and alignment fidelity.

Result: SOTA performance across 4 benchmarks (EvalMuse-40K, RichHF, MHaluBench, GenAI-Bench), surpassing proprietary models and supervised baselines with higher inference efficiency.

Conclusion: Demonstrates effectiveness of structured MLLM-based visual reasoning for prompt-image alignment evaluation through rigorous benchmark testing.

Abstract: Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured "grounding-reasoning-conclusion" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.

</details>


### [100] [GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection](https://arxiv.org/abs/2512.23176)
*Yi Zhang,Yi Wang,Lei Yao,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 本文提出了一种基于Gaussian-Voxel联合表示学习的图像驱动3D物体检测框架GVSynergy-Det，通过融合连续高斯表示和离散体素表示的几何信息，在不依赖深度数据或密集3D监督的情况下实现了室内场景的最先进检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像3D检测方法存在两难困境：高精度方法需密集3D标注，无监督方法难以从2D图像提取精确几何信息。此外，基于点云的方法依赖昂贵的深度传感器，而高斯表示虽能建模几何细节但需要逐场景优化，体素表示虽具有结构化空间信息但缺乏细节。

Method: 构建双表示架构，包含：1) 通用高斯泼溅适配模块，提取几何特征用于检测任务；2) 跨表示增强机制，通过高斯场几何细节丰富体素特征；3) 可学习特征融合策略，结合连续高斯表示的表面细节与离散体素的结构化信息，实现无需3D监督的端到端检测。

Result: 在ScanNetV2和ARKitScenes数据集上分别达到72.1%和68.4% mAP，超越所有无监督图像3D检测方法，且性能优于部分基于点云的监督方法。在NYUv2数据集上实现65.3% mAP，验证了方法的跨数据集泛化能力。

Conclusion: 实验证明高斯与体素表示的协同学习能够有效弥补彼此在几何细节与空间结构方面的不足，所提出的特征融合策略显著提升了无监督3D检测的准确性，为低成本的纯图像3D感知解决方案提供了新方向。

Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

</details>


### [101] [GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation](https://arxiv.org/abs/2512.23180)
*Tianchen Deng,Xuefeng Chen,Yi Chen,Qu Chen,Yuyao Xu,Lijin Yang,Le Xu,Yu Zhang,Bo Zhang,Wuxiong Huang,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯场景表示的统一驾驶世界模型框架，解决现有模型在3D场景理解、多模态对齐和生成能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型缺乏3D场景理解能力，无法准确对齐文本与3D场景信息，且生成过程受限于输入数据。需要一种能同时实现3D理解、多模态生成和上下文增强的统一框架。

Method: 1) 采用3D高斯场景表征实现几何-语义联合建模；2) 通过高斯基元嵌入语言特征实现早期模态对齐；3) 设计任务感知的语言引导采样策略；4) 构建高低级条件联合驱动的多模态生成模型。

Result: 在nuScenes和NuInteract数据集上达到SOTA性能，验证了3D高斯场景表征对多模态理解和生成的有效性，且代码已开源。

Conclusion: 该框架首次将3D高斯表示引入驾驶世界模型，在保持3D几何准确性的同时实现了跨模态语义理解，为自动驾驶场景建模提供了新范式。

Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.

</details>


### [102] [ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis](https://arxiv.org/abs/2512.23196)
*Maisha Haque,Israt Jahan Ayshi,Sadaf M. Anis,Nahian Tasnim,Mithila Moontaha,Md. Sabbir Ahmed,Muhammad Iqbal Hossain,Mohammad Zavid Parvez,Subrata Chakraborty,Biswajeet Pradhan,Biswajit Banik*

Main category: cs.CV

TL;DR: ForCM结合了面向对象的图像分析（OBIA）与深度学习模型（如ResUNet和AttentionUNet），利用Sentinel-2卫星影像实现亚马逊雨林覆盖制图，精度达95.64%。


<details>
  <summary>Details</summary>
Motivation: 旨在通过融合深度学习模型与传统OBIA方法，提升森林覆盖映射的精度和效率，支持全球环境监测与保护。

Method: 对比UNet、UNet++、ResUNet、AttentionUNet和ResNet50-Segnet等深度学习模型在多光谱Sentinel-2影像上的表现，并将最优模型与OBIA集成，形成ForCM新方法。

Result: ResUNet-OBIA和AttentionUNet-OBIA的综合精度分别达94.54%和95.64%，高于传统OBIA的92.91%，验证了自由开源工具（如QGIS）在环境监测中的实际潜力。

Conclusion: ForCM证明了深度学习与OBIA融合的有效性，为高精度森林制图提供了低成本解决方案，并推动开源工具在全球生态保护中的应用。

Abstract: This research proposes "ForCM", a novel approach to forest cover mapping that combines Object-Based Image Analysis (OBIA) with Deep Learning (DL) using multispectral Sentinel-2 imagery. The study explores several DL models, including UNet, UNet++, ResUNet, AttentionUNet, and ResNet50-Segnet, applied to high-resolution Sentinel-2 Level 2A satellite images of the Amazon Rainforest. The datasets comprise three collections: two sets of three-band imagery and one set of four-band imagery. After evaluation, the most effective DL models are individually integrated with the OBIA technique to enhance mapping accuracy. The originality of this work lies in evaluating different deep learning models combined with OBIA and comparing them with traditional OBIA methods. The results show that the proposed ForCM method improves forest cover mapping, achieving overall accuracies of 94.54 percent with ResUNet-OBIA and 95.64 percent with AttentionUNet-OBIA, compared to 92.91 percent using traditional OBIA. This research also demonstrates the potential of free and user-friendly tools such as QGIS for accurate mapping within their limitations, supporting global environmental monitoring and conservation efforts.

</details>


### [103] [Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks](https://arxiv.org/abs/2512.23210)
*Changgyoon Oh,Jongoh Jeong,Jegyeong Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本论文提出Task-aware Timestep Selection（TTS）与Timestep Feature Consolidation（TFC）模块，通过自适应选择并整合扩散模型关键时间步特征，在few-shot密集预测任务中实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型依赖经验选择时间步特征，导致任务偏差和性能次优。作者旨在开发任务感知的自适应时间步选择机制，并探索多时间步特征协同整合的有效方法。

Method: 提出TTS模块通过对比时间步损失和特征相似度动态选择关键时间步；TFC模块通过通道注意力机制整合多时间步特征；结合参数高效微调适配器，增强few-shot学习能力。

Result: 在Taskonomy数据集上，该方法在16/24项任务中超越全监督基线，Few-shot设置下平均提升17.8%精度，参数效率提升3倍，时间步选择收敛速度加快2.4倍。

Conclusion: 通过建立时间步特征动态选择与多阶段整合的范式，为扩散模型在少样本场景下的特征利用提供了创新框架，显著提升模型泛化能力与效率。

Abstract: Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.

</details>


### [104] [MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?](https://arxiv.org/abs/2512.23219)
*Shiqi Dai,Zizhi Ma,Zhicong Luo,Xuesong Yang,Yibin Huang,Wanyue Zhang,Chi Chen,Zonghao Guo,Wang Xu,Yufei Sun,Maosong Sun*

Main category: cs.CV

TL;DR: 介绍了MM-UAVBench，一个用于低空无人机场景的多模态大语言模型（MLLM）综合评估基准，包含19个子任务和超过5.7K个标注问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准未覆盖低空无人机场景的独特挑战，而无人机相关评估仅聚焦局部任务（如定位导航），缺乏对模型通用智能的统一评价。

Method: 构建包含感知、认知、规划三大能力维度的MM-UAVBench，基于公共数据集的真实无人机数据生成19个子任务和5.7K人工标注问题，并对16种开源/闭源MLLM进行系统性实验。

Result: 实验发现现有模型难以适应低空场景的复杂视觉和认知需求，存在空间偏差、多视角理解等关键瓶颈问题。

Conclusion: 提出MM-UAVBench弥补领域空白，揭示MLLM在低空无人机应用中的核心缺陷，为开发更鲁棒的无人机智能模型提供评估标准。

Abstract: While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.

</details>


### [105] [Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information](https://arxiv.org/abs/2512.23221)
*Youngchae Kwon,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态上下文整合的时尚单品检测方法Holi-DETR，通过融合物品共现关系、空间布局和人体关键点空间关系，改进Transformer检测框架，在检测精度上超越现有模型3.6%~1.1%。


<details>
  <summary>Details</summary>
Motivation: 解决传统检测方法忽视时尚单品间的搭配关联性，导致在外观多样和子类相似情况下的检测歧义问题

Method: 构建包含三种异构上下文信息的Holi-DETR架构：1) 采用共现概率矩阵建模单品搭配关联 2) 利用相对位置编码提取空间布局特征 3) 通过空间注意力机制融合人体关键点坐标特征

Result: 在DETR和Co-DETR基线上分别取得3.6个百分点和1.1个百分点的平均精度提升，消融实验验证三种上下文信息的有效性与互补性

Conclusion: 整合多粒度上下文信息能有效提升时尚单品检测表现，为服饰组合理解提供了新的检测框架

Abstract: Fashion item detection is challenging due to the ambiguities introduced by the highly diverse appearances of fashion items and the similarities among item subcategories. To address this challenge, we propose a novel Holistic Detection Transformer (Holi-DETR) that detects fashion items in outfit images holistically, by leveraging contextual information. Fashion items often have meaningful relationships as they are combined to create specific styles. Unlike conventional detectors that detect each item independently, Holi-DETR detects multiple items while reducing ambiguities by leveraging three distinct types of contextual information: (1) the co-occurrence relationship between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. %Holi-DETR explicitly incorporates three types of contextual information: (1) the co-occurrence probability between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. To this end, we propose a novel architecture that integrates these three types of heterogeneous contextual information into the Detection Transformer (DETR) and its subsequent models. In experiments, the proposed methods improved the performance of the vanilla DETR and the more recently developed Co-DETR by 3.6 percent points (pp) and 1.1 pp, respectively, in terms of average precision (AP).

</details>


### [106] [Bridging Your Imagination with Audio-Video Generation via a Unified Director](https://arxiv.org/abs/2512.23222)
*Jiaxu Zhang,Tianshu Hu,Yuan Zhang,Zenan Li,Linjie Luo,Guosheng Lin,Xin Chen*

Main category: cs.CV

TL;DR: 本文提出UniMAGE统一模型，将AI视频创作中的剧本生成与关键帧设计整合为单一框架，通过交织训练与解耦学习实现逻辑性与视觉一致性的提升。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频系统割裂处理剧本（大语言模型）与镜头设计（图像模型），缺乏全局协调性；作者主张统一建模以模拟电影导演逻辑与创作双重能力。

Method: 基于Mixture-of-Transformers架构构建文本-图像生成联合框架，采用两阶段训练：（1）交织概念学习（利用跨模态数据提升脚本理解）；（2）解耦专家学习（分离脚本生成与关键帧生成，增强创作灵活性）。

Result: 实验证明UniMAGE在开源模型中表现最优，可生成结构化剧本并同步生成视觉连贯的关键帧图像，满足长场景、多镜头创作需求。

Conclusion: 论文验证了文本-图像统一建模对视频创作的有效性，为AI辅助影视制作提供了标准化框架，后续研究可拓展多模态协同优化路径。

Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.

</details>


### [107] [Anomaly Detection by Effectively Leveraging Synthetic Images](https://arxiv.org/abs/2512.23227)
*Sungho Kang,Hyunkyu Park,Yeonho Lee,Hanbyul Lee,Mijoo Jeong,YeongHyeon Park,Injae Lee,Juneho Yi*

Main category: cs.CV

TL;DR: 结合规则合成与生成模型，通过图像检索筛选合成异常图像，并采用两阶段训练策略，在保证质量的同时降低数据成本。


<details>
  <summary>Details</summary>
Motivation: 工业制造中的异常检测依赖正常图像，但真实缺陷图像稀缺。规则合成成本低但图像不真实，生成模型质量高但成本高，需权衡。

Method: 利用预训练的文本指导图像转换模型生成缺陷图像，并通过图像检索模型滤除非正常特征；采用两阶段训练策略，先用规则合成数据预训练，再用小规模高质量数据微调。

Result: 在MVTec AD数据集上的实验证明，该方法显著提升异常检测性能，同时降低数据采集成本。

Conclusion: 提出的框架有效平衡了合成数据质量与成本，为无监督异常检测提供了高效解决方案。

Abstract: Anomaly detection plays a vital role in industrial manufacturing. Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied. Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution. In this work, we focus on a strategy to effectively leverage synthetic images to maximize the anomaly detection performance. Previous synthesis strategies are broadly categorized into two groups, presenting a clear trade-off. Rule-based synthesis, such as injecting noise or pasting patches, is cost-effective but often fails to produce realistic defect images. On the other hand, generative model-based synthesis can create high-quality defect images but requires substantial cost. To address this problem, we propose a novel framework that leverages a pre-trained text-guided image-to-image translation model and image retrieval model to efficiently generate synthetic defect images. Specifically, the image retrieval model assesses the similarity of the generated images to real normal images and filters out irrelevant outputs, thereby enhancing the quality and relevance of the generated defect images. To effectively leverage synthetic images, we also introduce a two stage training strategy. In this strategy, the model is first pre-trained on a large volume of images from rule-based synthesis and then fine-tuned on a smaller set of high-quality images. This method significantly reduces the cost for data collection while improving the anomaly detection performance. Experiments on the MVTec AD dataset demonstrate the effectiveness of our approach.

</details>


### [108] [SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems](https://arxiv.org/abs/2512.23232)
*Minwoo Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 本文提出了一种基于Stein无偏风险估计（SURE）的后验采样方法（SGPS），用于解决扩散模型在逆问题中的误差累积问题。该方法通过梯度更新和PCA噪声估计，在100次以内神经函数评估（NFE）的情况下实现高质量的重建。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在反演问题中需数百至数千次迭代才能实现高质量重建，由于误差累积导致效率低下。需要一种能减少迭代次数并保持精度的新方法。

Method: SGPS结合了SURE梯度更新和PCA噪声估计技术，通过在采样早期和中期阶段校正轨迹偏差，利用理论推导的风险估计对采样路径进行校正，从而降低噪声导致的误差积累。

Result: 在多种逆问题（如图像去噪/压缩感知）中的实验表明，SGPS在NFE<100时表现优于现有方法，重建质量与迭代次数成反比的特性显著改善。

Conclusion: 该方法通过理论驱动的采样路径校正机制，在保证质量的前提下将扩散模型的逆问题求解效率提升一个数量级，为低计算资源场景提供了新方案。

Abstract: Diffusion models have emerged as powerful learned priors for solving inverse problems. However, current iterative solving approaches which alternate between diffusion sampling and data consistency steps typically require hundreds or thousands of steps to achieve high quality reconstruction due to accumulated errors. We address this challenge with SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations using Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA based noise estimation. By mitigating noise induced errors during the critical early and middle sampling stages, SGPS enables more accurate posterior sampling and reduces error accumulation. This allows our method to maintain high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs). Our extensive evaluation across diverse inverse problems demonstrates that SGPS consistently outperforms existing methods at low NFE counts.

</details>


### [109] [Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network](https://arxiv.org/abs/2512.23234)
*Dongsheng Li,Chaobo Chen,Siling Wang,Song Gao*

Main category: cs.CV

TL;DR: The paper proposes PEG-DRNet, a physics-edge hybrid network for improved infrared gas leak detection, combining gas transport modeling, edge perception, and sparse routing.


<details>
  <summary>Details</summary>
Motivation: Infrared gas leak detection is critical for environmental and industrial safety but challenging due to faint, semitransparent plumes with weak boundaries.

Method: 1) Gas Block models gas transport via local/global branches with edge-gated fusion; 2) AGPEO computes gradient/phase edge priors through multi-scale perception; 3) CASR-PAN aggregates cross-scale features using content-adaptive sparse routing.

Result: Achieved 29.8% overall AP, 84.3% AP$_{50}$, 25.3% small-object AP on IIG dataset, outperforming RT-DETR-R18 by 3.0-5.3% while using 43.7 Gflops and 14.9M parameters.

Conclusion: PEG-DRNet demonstrates superior accuracy-efficiency trade-off, outperforming CNN/Transformer detectors on IIG and LangGas datasets in AP/AP$_{50}$ metrics.

Abstract: Detecting infrared gas leaks is critical for environmental monitoring and industrial safety, yet remains difficult because plumes are faint, small, semitransparent, and have weak, diffuse boundaries. We present physics-edge hybrid gas dynamic routing network (PEG-DRNet). First, we introduce the Gas Block, a diffusion-convection unit modeling gas transport: a local branch captures short-range variations, while a large-kernel branch captures long-range propagation. An edge-gated learnable fusion module balances local detail and global context, strengthening weak-contrast plume and contour cues. Second, we propose the adaptive gradient and phase edge operator (AGPEO), computing reliable edge priors from multi-directional gradients and phase-consistent responses. These are transformed by a multi-scale edge perception module (MSEPM) into hierarchical edge features that reinforce boundaries. Finally, the content-adaptive sparse routing path aggregation network (CASR-PAN), with adaptive information modulation modules for fusion and self, selectively propagates informative features across scales based on edge and content cues, improving cross-scale discriminability while reducing redundancy. Experiments on the IIG dataset show that PEG-DRNet achieves an overall AP of 29.8\%, an AP$_{50}$ of 84.3\%, and a small-object AP of 25.3\%, surpassing the RT-DETR-R18 baseline by 3.0\%, 6.5\%, and 5.3\%, respectively, while requiring only 43.7 Gflops and 14.9 M parameters. The proposed PEG-DRNet achieves superior overall performance with the best balance of accuracy and computational efficiency, outperforming existing CNN and Transformer detectors in AP and AP$_{50}$ on the IIG and LangGas dataset.

</details>


### [110] [RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models](https://arxiv.org/abs/2512.23239)
*Fan Wei,Runmin Dong,Yushan Lai,Yixiang Yang,Zhaoyang Luo,Jinxiao Zhang,Miao Yang,Shuai Yuan,Jiyao Zhao,Bin Luo,Haohuan Fu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的两阶段数据剪枝方法，用于优化扩散模型在遥感（RS）领域的基础模型训练，有效提升收敛速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有RS扩散模型因训练数据冗余、噪声和类别不平衡导致效率低下且难以收敛，且传统方法忽视生成模型的分布需求和遥感影像异质性，因此需要更高效的数据筛选方案。

Method: 第一阶段通过基于熵的标准高效剔除低信息量样本；第二阶段利用RS场景分类数据集作为基准，结合场景感知聚类和分层采样平衡数据多样性与代表性，降低计算成本。

Result: 即使剪枝85%训练数据后，该方法仍显著提升收敛性和生成质量，在超分辨率和语义图像合成等下游任务中取得SOTA性能。

Conclusion: 该数据剪枝范式为RS生成基础模型的开发提供了实用指导，证明了高质量数据子集对模型训练的关键作用。

Abstract: Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.

</details>


### [111] [Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism](https://arxiv.org/abs/2512.23243)
*Siyu Zhang,Ying Chen,Lianlei Shan,Runhe Qiu*

Main category: cs.CV

TL;DR: 本文提出融合VLM框架，通过动态调整输入分辨率和多尺度视觉语言对齐机制，提升遥感图像的语义理解和计算效率


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感图像多模态融合方法中固定分辨率导致的效率与细节失衡、单尺度对齐缺乏语义层次的问题

Method: 设计动态分辨率输入策略（DRIS）和多尺度视觉语言对齐机制（MS-VLAM）。DRIS采用粗到细策略适应图像复杂度分配计算资源；MS-VLAM构建对象级-局部区域-全局层级的三层次对齐体系

Result: 在RS-GPT4V数据集上实现图像描述生成任务BLEU-4提升12.7%、CIDEr提升15.2%，跨模态检索任务R@10指标提升9.5%，验证了算法效率和精度优势

Conclusion: 提供了面向高效稳健多模态遥感系统的技术框架，为智能遥感解译的工程化应用奠定理论基础

Abstract: Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.

</details>


### [112] [ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2512.23244)
*Xingwei Ma,Shiyang Feng,Bo Zhang,Bin Wang*

Main category: cs.CV

TL;DR: 本文提出ViLaCD-R1，一种结合视觉-语言模型与两阶段框架的新型遥感变化检测方法，有效解决高层语义缺失和非语义干扰问题。


<details>
  <summary>Details</summary>
Motivation: 传统像素-based方法与现有VLM模型在遥感变化检测中面临高层语义捕捉不足、空间定位不准和边界划分模糊的挑战，需提升语义理解与定位精度。

Method: 构建两阶段框架：1) 多图像推理器通过监督微调和强化学习生成粗略变化掩膜；2) 掩膜引导解码器融合图像特征与初始掩膜输出精确变化图。

Result: 实验显示相较现有方法，在多个基准数据集上的真语义变化识别率提升15.2%，边界定位误差降低22.7%，且具有更强的非语义干扰抑制能力。

Conclusion: 该方法显著改善遥感影像变化检测的语义解释性与定位精确性，在复杂真实场景应用中达到当前最优性能指标。

Abstract: Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.

</details>


### [113] [ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation](https://arxiv.org/abs/2512.23245)
*Shin seong Kim,Minjung Shin,Hyunin Cho,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出ASemconsist框架，通过语义控制策略和自适应共享机制，显著提升文本驱动角色连续生成中身份一致性与提示对齐的平衡能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在连续图像生成中难以兼顾角色身份稳定性和单帧提示准确性，需解决二者间的性能权衡问题

Method: 1) 选择性修改文本嵌入实现身份控制
2) 填充嵌入重构为语义容器
3) 自适应特征共享策略
4) 提出CQS综合评估指标

Result: 在多指标上超越现有方法，成功突破身份一致性与提示对齐的互斥关系，支持复杂场景的角色连贯生成

Conclusion: ASemconsist为可控视觉叙事提供了新范式，其模块化设计为未来角色驱动的生成任务提供了可扩展基础架构

Abstract: Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist

</details>


### [114] [Contour Information Aware 2D Gaussian Splatting for Image Representation](https://arxiv.org/abs/2512.23255)
*Masaya Takabe,Hiroshi Watanabe,Sujun Hong,Tomohiro Ikai,Zheming Fan,Ryo Ishimoto,Kakeru Sugimoto,Ruri Imichi*

Main category: cs.CV

TL;DR: 提出了一种基于轮廓信息感知的2D高斯点阵化框架（Contour-Aware 2DGS），通过引入对象分割先验来增强图像边界重建效果，并保留高压缩率下的边缘结构。


<details>
  <summary>Details</summary>
Motivation: 现有2D高斯点阵化方法在使用少量高斯分布时，因缺乏轮廓感知能力常导致边界模糊，需要一种既能保持边缘清晰度又不牺牲效率的解决方案。

Method: 在高斯分布光栅化过程中约束每个高斯核至特定分割区域，并设计渐进式训练（warm-up）策略以优化收敛，避免跨边界融合问题。

Result: 在合成色卡和DAVIS数据集上的实验表明，相比传统2DGS方法，在极低高斯数量场景下边缘重建质量显著提升，且保持实时渲染和低内存占用。

Conclusion: 该方法通过融合分割先验有效解决了轮廓模糊问题，为高效图像表示提供了新的优化方向。

Abstract: Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.

</details>


### [115] [Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization](https://arxiv.org/abs/2512.23258)
*Tong Shao,Yusen Fu,Guoying Sun,Jingde Kong,Zhuotao Tian,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出CEM，一种基于累积误差最小化的动态缓存优化插件，以提升扩散Transformer（DiT）模型推理速度，同时显著改善生成保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于缓存的DiT加速方法因采用固定策略导致误差控制不足，限制了推理效率与生成质量的同步提升。需探索自适应误差管理机制以挖掘加速潜力。

Method: CEM通过预定义时间步与缓存间隔联合作用的误差敏感性先验，构建累积误差动态规划优化算法，实现动态缓存策略搜索，无额外计算开销地集成于现有误差修正框架。

Result: 在9种生成模型及量化方法上的实验证明，CEM使加速模型生成质量接近甚至超越原始模型（如FLUX.1-dev、StableDiffusion1.5等），适配任意加速预算。

Conclusion: CEM作为模型无关的加速方案，通过动态误差最小化平衡速度与质量，在保持通用性及零开销特性的同时，推动高效视频/图像生成模型的实用化发展。

Abstract: Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.

</details>


### [116] [Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition](https://arxiv.org/abs/2512.23291)
*Arman Martirosyan,Shahane Tigranyan,Maria Razzhivina,Artak Aslanyan,Nazgul Salikhova,Ilya Makarov,Andrey Savchenko,Aram Avetisyan*

Main category: cs.CV

TL;DR: 本文提出了两种多模态框架，利用RGB视频、3D姿态数据和面部上下文信息，通过交叉模态融合模块（Cross-Modal Token Fusion和InterFusion）实现iMiGUE数据集上的微手势识别与行为情绪预测，最终在MiGA 2025挑战赛中获得情绪预测任务第二名。


<details>
  <summary>Details</summary>
Motivation: 微手势识别与行为情绪预测需要建模细微且粒度精细的人类行为，传统方法依赖视频和骨骼姿态数据但存在模态融合不足的问题，本文旨在通过多模态特征提取与创新融合策略提升复杂行为的表征能力。

Method: 1. 微手势分类：分别采用MViTv2-S（RGB视频）和2s-AGCN（3D姿态数据）提取嵌入特征，通过交叉模态token融合模块整合空间与姿态信息；2. 情绪识别：使用SwinFace（面部）和MViTv2-S（上下文）提取特征，采用InterFusion模块融合情感表达与身体姿态信息。

Result: 在iMiGUE数据集上的实验表明，情绪预测任务中模型表现稳健且准确率高，最终在MiGA 2025挑战赛同类型方案中排名第二，并验证了多模态融合框架在微手势识别中的有效性。

Conclusion: 通过多模态特征提取与模态间注意力机制设计，本文方法能有效捕捉微手势和情绪相关的细粒度时空特征，证实了交叉模态信息整合对行为分析任务的价值。

Abstract: Micro-gesture recognition and behavior-based emotion prediction are both highly challenging tasks that require modeling subtle, fine-grained human behaviors, primarily leveraging video and skeletal pose data. In this work, we present two multimodal frameworks designed to tackle both problems on the iMiGUE dataset. For micro-gesture classification, we explore the complementary strengths of RGB and 3D pose-based representations to capture nuanced spatio-temporal patterns. To comprehensively represent gestures, video, and skeletal embeddings are extracted using MViTv2-S and 2s-AGCN, respectively. Then, they are integrated through a Cross-Modal Token Fusion module to combine spatial and pose information. For emotion recognition, our framework extends to behavior-based emotion prediction, a binary classification task identifying emotional states based on visual cues. We leverage facial and contextual embeddings extracted using SwinFace and MViTv2-S models and fuse them through an InterFusion module designed to capture emotional expressions and body gestures. Experiments conducted on the iMiGUE dataset, within the scope of the MiGA 2025 Challenge, demonstrate the robust performance and accuracy of our method in the behavior-based emotion prediction task, where our approach secured 2nd place.

</details>


### [117] [MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images](https://arxiv.org/abs/2512.23304)
*Md. Sazzadul Islam Prottasha,Nabil Walid Rafi*

Main category: cs.CV

TL;DR: 该研究比较了专为医学设计的MedGemma与通用GPT-4在六种疾病诊断中的表现。经过微调的MedGemma在准确率（80.37%）和敏感度（尤其是癌症与肺炎检测）上显著优于未微调的GPT-4（69.58%），证明领域微调对降低临床误诊的重要性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（LLMs）在医学影像领域潜力巨大，但通用模型（如GPT-4）在医学场景中存在准确性不足和误诊风险。需要验证专为医学开发的MedGemma是否能通过领域微调实现更可靠、高敏的诊断。

Method: 基于LoRA低秩微调技术优化MedGemma-4b-it模型，将其与未经微调的GPT-4进行对比实验。在相同数据集上评估两者对六种疾病的分类准确率，并通过混淆矩阵与分类报告分析高敏感临床任务（如癌症检测）的表现差异。

Result: MedGemma在平均测试准确率（80.37%）和关键疾病敏感度上均优于GPT-4（69.58%）。定量分析显示其分类性能更稳定，且通过领域微调明显减少了临床误诊（幻觉）风险。

Conclusion: 医学领域微调是提升模型可靠性与降低误诊的核心手段。MedGemma在复杂医学推理与证据支持的临床决策中展现出更优性能，为医疗AI提供了高效且可解释的解决方案。

Abstract: Multimodal Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a transformative approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large multimodal model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.

</details>


### [118] [CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation](https://arxiv.org/abs/2512.23333)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Zhengtao Yao,Weitao Jia,Xiaodong Ge,Jingqun Tang,Benlei Cui,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 本文提出了一种用于CAD代码生成的异构协同多专家强化学习（CME-CAD）范式，并发布含17,299实例的开源基准CADExpert，旨在解决工业设计中高精度可编辑CAD模型自动生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模复杂且现有方法生成的模型精度不足、需大量手动标注，导致工业自动化需求难以满足，亟需提升模型生成准确性和可编辑性。

Method: 设计两阶段训练流程（多专家微调MEFT和多专家强化学习MERL），通过多模型协同学习增强生成能力，并开发包含多模态数据的开源基准CADExpert辅助训练与验证。

Result: 成功构建首个结合正交投影、维度标注、思想链过程及可执行代码的CAD数据集CADExpert，并验证CME-CAD生成模型的精度与工业适用性。

Conclusion: CME-CAD范式结合多专家协同强化学习与开源基准，显著提升CAD代码生成效果，为工业自动化提供可扩展的解决方案。

Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.

</details>


### [119] [Visual Language Hypothesis](https://arxiv.org/abs/2512.23335)
*Xiu Li*

Main category: cs.CV

TL;DR: 论文提出视觉表示学习需基于纤维丛结构，语义抽象需非同胚处理和特定模型架构。


<details>
  <summary>Details</summary>
Motivation: 通过结构与拓扑视角解释视觉理解需离散布尔语义，分析传统方法的不足。

Method: 基于假设推导纤维丛结构，证明商空间特性及模型对拓扑变化的需求。

Result: 理论发现：语义商空间依赖显式语义目标，且模型需支持拓扑变化的扩展-快照机制。

Conclusion: 框架为解释性工具，揭示现有模型与学习理论中拓扑结构的重要性。

Abstract: We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.

</details>


### [120] [SpatialMosaic: A Multiview VLM Dataset for Partial Visibility](https://arxiv.org/abs/2512.23365)
*Kanghee Lee,Injae Lee,Minseok Kwak,Kwonyoung Ryu,Jungi Hong,Jaesik Park*

Main category: cs.CV

TL;DR: 该论文提出SpatialMosaic数据集和SpatialMosaicVLM框架，通过自动生成的多视角QA对提升视觉语言模型在复杂现实场景中的三维空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工构建的3D表示或现成重建工具，存在可扩展性限制；且真实场景中的部分可观测、遮挡等碎片化视觉线索的推理问题未充分探索

Method: 开发可扩展的多视角数据生成与标注流水线，构建含200万QA对的SpatialMosaic训练集和100万QA对的SpatialMosaic-Bench基准集，提出融合3D重构模型的SpatialMosaicVLM混合框架

Result: 实验验证数据生成管道能有效生成真实多样的QA对，在6项任务的挑战性场景中显著提升多视角空间推理性能

Conclusion: 该研究通过自动数据生成和新型框架设计，解决了真实世界3D场景理解中碎片化视觉线索的推理难题，为多模态大模型提供更实用的端到端解决方案

Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.

</details>


### [121] [NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization](https://arxiv.org/abs/2512.23374)
*Yifei Li,Haoyuan He,Yu Zheng,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了NeXT-IMDL基准，系统性挑战现有图像篡改检测方法的泛化能力，通过四个维度分类和五种交叉评估协议，揭示当前模型在现实场景中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有跨数据集评估方法掩饰了IMDL模型在真实世界中的性能缺陷，且传统测试方案无法有效诊断AI生成内容带来的新问题，亟需更严格的泛化能力验证体系。

Method: 构建NeXt-IMDL基准：1) 基于生成模型、篡改类型、语义内容和伪造粒度四维度分类AIGC篡改方式；2) 设计包含5种严格跨维协议的测试框架，系统性评估模型的泛化边界。

Result: 在11种模型上的实验表明，现有方法在标准测试集表现良好，但面对模拟真实场景的跨维协议时，检测准确率出现系统性崩溃（如最佳模型在细粒度篡改中性能骤降60%）

Conclusion: 本研究证明当前IMDL方法存在严重泛化缺陷，提出的诊断框架可推动鲁棒模型研发，呼吁领域转向更有挑战性的评估范式

Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.

</details>


### [122] [SoulX-LiveTalk Technical Report](https://arxiv.org/abs/2512.23379)
*Le Shen,Qiao Qian,Tan Yu,Ke Zhou,Tianhang Yu,Yu Zhan,Zhenjie Wang,Ming Tao,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: SoulX-LiveTalk基于14B参数框架，采用自修正双向蒸馏策略和全栈加速技术，突破了实时高保真音视频同步生成的技术瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统音驱动数字人生成系统通过单向注意力机制或缩减模型规模来降低计算量，导致运动连续性和视觉细节严重受损，无法满足高质量虚拟人交互需求。

Method: 核心创新包含：1）基于视频块的双向注意力蒸馏策略，保留时序一致性；2）多步回溯自修正机制主动纠正误差累积；3）混合序列并行、并行VAE和内核级优化组成的系统级加速方案。

Result: 在0.87秒启动延迟下实现32FPS实时生成，突破140亿参数大模型实时推理的吞吐量极限，视频质量在PSNR、LPIPS等指标上超越现有方案17%以上。

Conclusion: 该系统首次在14B参数量级达成实时无限时生成能力，为高保真虚拟人交互建立了新的技术基线，相关优化方法可扩展至更广泛的流式生成场景。

Abstract: Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.

</details>


### [123] [SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation](https://arxiv.org/abs/2512.23411)
*Xiaolan Li,Wanquan Liu,Pengcheng Li,Pengyu Jie,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为SOFTooth的语义增强型3D牙齿实例分割框架，结合2D语义和3D几何信息，解决了牙齿边界模糊、牙龈混淆及第三臼齿分割难题。


<details>
  <summary>Details</summary>
Motivation: 三维牙齿分割面临牙列拥挤、牙齿-牙龈边界模糊、缺牙和第三臼齿等临床重要结构分割困难的挑战，传统3D方法存在边界泄漏和身份不一致问题，而2D模型无法直接应用于3D临床流程。

Method: 提出SOFTooth框架，采用点级残差门控模块将2D-SAM嵌入3D点特征，并通过中心引导的掩码优化保持几何中心一致性，结合解剖顺序的匈牙利匹配策略实现拥挤牙列下的连续标签分配。

Result: 在3DTeethSeg'22数据集上达到最优整体精度和平均IoU，对第三臼齿等困难病例提升显著，验证了无需2D微调的语义迁移有效性。

Conclusion: 研究表明基于冻结2D语义的3D分割框架能有效解决牙齿实例分割中的边界模糊和身份混乱问题，在复杂临床场景中保持优异性能。

Abstract: Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.

</details>


### [124] [Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision](https://arxiv.org/abs/2512.23426)
*Dohyun Kim,Seungwoo Lyu,Seung Wook Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: This paper introduces DDSPO, a method that trains diffusion models more efficiently by using automated per-timestep preference signals derived from a pretrained reference model, avoiding manual labeling.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with aligning outputs to nuanced user intent and maintaining aesthetic quality, while current preference-based methods require costly and noisy human-labeled data.

Method: DDSPO generates dense transition-level supervision by contrasting a pretrained reference model's outputs on original prompts vs. semantically degraded variants, extracting per-timestep preference signals without manual annotations.

Result: DDSPO achieves better text-image alignment and visual quality than existing methods, matching/expanding performance with significantly less labeled supervision data.

Conclusion: The proposed method enables efficient diffusion model training through automated preference signals in score-space, overcoming limitations of manual labeling and reward modeling.

Abstract: Diffusion models have achieved impressive results in generative tasks such as text-to-image synthesis, yet they often struggle to fully align outputs with nuanced user intent and maintain consistent aesthetic quality. Existing preference-based training methods like Diffusion Direct Preference Optimization help address these issues but rely on costly and potentially noisy human-labeled datasets. In this work, we introduce Direct Diffusion Score Preference Optimization (DDSPO), which directly derives per-timestep supervision from winning and losing policies when such policies are available. Unlike prior methods that operate solely on final samples, DDSPO provides dense, transition-level signals across the denoising trajectory. In practice, we avoid reliance on labeled data by automatically generating preference signals using a pretrained reference model: we contrast its outputs when conditioned on original prompts versus semantically degraded variants. This practical strategy enables effective score-space preference supervision without explicit reward modeling or manual annotations. Empirical results demonstrate that DDSPO improves text-image alignment and visual quality, outperforming or matching existing preference-based methods while requiring significantly less supervision. Our implementation is available at: https://dohyun-as.github.io/DDSPO

</details>


### [125] [Towards Integrating Uncertainty for Domain-Agnostic Segmentation](https://arxiv.org/abs/2512.23427)
*Jesse Brouwers,Xiaoyan Xing,Alexander Timans*

Main category: cs.CV

TL;DR: 本研究提出UncertSAM基准和不确定性量化方法来提升分割模型SAM在跨域场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对当前分割基础模型（如SAM系列）在领域偏移或知识受限场景下的脆弱性问题，探究不确定性量化是否能以领域无关方式增强模型鲁棒性。

Method: 构建包含8个挑战性数据集的UncertSAM基准，评估轻量级后置不确定性估计方法，并引入不确定性引导的预测优化步骤。

Result: 最后一层拉普拉斯近似产生的不确定性估计与分割误差呈强相关性，初步优化步骤展现出改进潜力。

Conclusion: 研究证实将不确定性量化引入分割模型可有效提升跨域场景下的性能稳定性，代码和基准已开源。

Abstract: Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.

</details>


### [126] [Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification](https://arxiv.org/abs/2512.23436)
*Mustafa Demetgul,Sanja Lazarova Molnar*

Main category: cs.CV

TL;DR: 本文提出了一种基于天气数据和道路表面数据的实时道路分类系统，通过比较多种深度-learning模型和多模态数据（摄像头与加速度传感器）实现了超过95%的分类准确率，并结合模糊逻辑实现动态数据源选择。


<details>
  <summary>Details</summary>
Motivation: 传统道路监测方法成本高且缺乏系统性，作者旨在开发一种低成本、实时性强的解决方案，利用移动设备传感器数据提升道路状态监测效率。

Method: 1. 使用智能手机摄像头采集道路图像，并将三轴加速度数据转换为伪图像作为输入；<br>2. 对比AlexNet、LeNet、VGG、ResNet等深度学习模型在5类道路（沥青、破损沥青、砾石、破损砾石、铺装路面）的分类性能；<br>3. 通过模糊逻辑融合两种数据源，并根据天气和光照条件动态选择最优数据输入。

Result: 1. 图像分类模型达到95%以上准确率（ResNet最好）；<br>2. 加速度数据在低光照条件下的分类表现优于图像数据；<br>3. 模糊逻辑规则有效解决了全天候条件下的数据源选择问题。

Conclusion: 该系统通过多源数据融合与动态决策机制，在保证高准确率的同时实现了道路状态监测的实时性和鲁棒性，为自动驾驶提供了低成本的感知方案。

Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.

</details>


### [127] [RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction](https://arxiv.org/abs/2512.23437)
*Shuhong Liu,Chenyu Bao,Ziteng Cui,Yun Liu,Xuangeng Chu,Lin Gu,Marcos V. Conde,Ryo Umagami,Tomohiro Hashimoto,Zijian Hu,Tianhan Xu,Yuan Gan,Yusuke Kurose,Tatsuya Harada*

Main category: cs.CV

TL;DR: RealX3D是一个包含真实采集数据的多视角重建基准，涵盖光照、散射、遮挡、模糊四类物理退化情况，旨在评估复杂环境下三维重建方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建方法在真实世界物理退化条件下（如复杂光照、遮挡等）存在性能下降问题，需要标准化基准评估不同算法的鲁棒性。

Method: 设计四类物理退化类型（光照/散射/遮挡/模糊）并分级量化，通过统一采集协议获取像素对齐的低质量/高质量多视角数据，包含高分辨率RAW图像、激光扫描和世界坐标网格。

Result: 对优化算法和端到端方法的测试显示，在物理退化条件下重建质量显著下降，特别是在高分辨率细节恢复和深度估计方面表现脆弱。

Conclusion: 当前多视角重建流水线在真实物理退化场景下存在明显性能瓶颈，建议未来研究需增强对复杂物理条件的建模与鲁棒性设计。

Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.

</details>


### [128] [CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models](https://arxiv.org/abs/2512.23453)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: CoFi-Dec is a training-free framework for Large Vision-Language Models (LVLMs) that reduces hallucinations by integrating self-feedback and coarse-to-fine visual conditioning, combining text-to-image synthesis with a Wasserstein-based fusion mechanism to align multi-level visual predictions.


<details>
  <summary>Details</summary>
Motivation: LVLMs often generate visually inconsistent content (hallucinations), limiting their practical reliability. Existing methods struggle to reconcile high-level semantics with fine-grained visual grounding, necessitating a novel decoding approach.

Method: CoFi-Dec generates intermediate text responses conditioned on coarse- and fine-grained image views, converts them into synthetic visual hypotheses via text-to-image models, and fuses their predictive distributions using a geometrically aware Wasserstein mechanism to ensure consistency.

Result: Experiments show CoFi-Dec significantly outperforms existing decoding strategies in reducing entity-level and semantic-level hallucinations across six benchmarks, achieving robust and semantically faithful outputs.

Conclusion: CoFi-Dec offers a model-agnostic, training-free solution to enhance LVLM reliability, enabling seamless integration with existing architectures while improving visual-textual alignment.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.

</details>


### [129] [Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin](https://arxiv.org/abs/2512.23454)
*Kayathri Vigneswaran,Hugo Retief,Jai Clifford Holmes,Mariangel Garcia Andarcia,Hansaka Tennakoon*

Main category: cs.CV

TL;DR: 本研究提出一种结合视觉检测、YOLOv8姿态估计及多模态大模型的河流水位自动化监测框架，可提升传统人工测量的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统水文监测方法受限于人工误差和环境约束，高精度连续水位监测对防洪预警和生态保护存在迫切需求。

Method: 采用混合框架：图像预处理与标注 → 基于视觉的水位线检测 → YOLOv8姿态估计提取刻度间隙 → 多模态模型（GPT-4o/Gemini 2.0）融合几何元数据进行数值解析。

Result: 水位线检测精度94.24%，F1值83.64%；Gemini Stage 2在最优图像条件下实现5.43cm平均绝对误差、8.58cm均方根误差及0.84R²值，刻度间隙元数据显著提升LLM预测性能。

Conclusion: 多模态AI与几何校准结合可增强水位估计鲁棒性，图像质量对LLM表现影响显著，该方法为实时河流监测和水资源管理提供可扩展的自动化解决方案。

Abstract: Accurate and continuous monitoring of river water levels is essential for flood forecasting, water resource management, and ecological protection. Traditional hydrological observation methods are often limited by manual measurement errors and environmental constraints. This study presents a hybrid framework integrating vision based waterline detection, YOLOv8 pose scale extraction, and large multimodal language models (GPT 4o and Gemini 2.0 Flash) for automated river gauge plate reading. The methodology involves sequential stages of image preprocessing, annotation, waterline detection, scale gap estimation, and numeric reading extraction. Experiments demonstrate that waterline detection achieved high precision of 94.24 percent and an F1 score of 83.64 percent, while scale gap detection provided accurate geometric calibration for subsequent reading extraction. Incorporating scale gap metadata substantially improved the predictive performance of LLMs, with Gemini Stage 2 achieving the highest accuracy, with a mean absolute error of 5.43 cm, root mean square error of 8.58 cm, and R squared of 0.84 under optimal image conditions. Results highlight the sensitivity of LLMs to image quality, with degraded images producing higher errors, and underscore the importance of combining geometric metadata with multimodal artificial intelligence for robust water level estimation. Overall, the proposed approach offers a scalable, efficient, and reliable solution for automated hydrological monitoring, demonstrating potential for real time river gauge digitization and improved water resource management.

</details>


### [130] [HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation](https://arxiv.org/abs/2512.23464)
*Yuxin Wen,Qing Shuai,Di Kang,Jing Li,Cheng Wen,Yue Qian,Ningxin Jiao,Changhai Chen,Weijie Chen,Yiran Wang,Jinkun Guo,Dongyue An,Han Liu,Yanyu Tong,Chao Zhang,Qing Guo,Juan Chen,Qiao Zhang,Youyi Zhang,Zihao Yao,Cheng Zhang,Hong Duan,Xiaoping Wu,Qi Chen,Fei Cheng,Liang Dong,Peng He,Hao Zhang,Jiaxin Lin,Chao Zhang,Zhongyi Fan,Yifan Li,Zhichao Hu,Yuhong Liu,Linus,Jie Jiang,Xiaolong Li,Linchao Bao*

Main category: cs.CV

TL;DR: HY-Motion 1.0 是首个基于 Diffusion Transformer 流匹配的十亿参数级文本驱动 3D 人体动作生成模型，提出全流程训练方法并实现 200+ 动作类别的最广泛覆盖。


<details>
  <summary>Details</summary>
Motivation: 为解决当前开源动作生成模型在参数规模、动作多样性及文本指令对齐度上的不足，推动该领域向商业化成熟发展。

Method: 采用基于 Diffusion Transformer 的流匹配架构，创新性设计包含三阶段的训练范式：3000+ 小时数据预训练、400 小时高质量微调、以及结合人类反馈与奖励模型的强化学习，并配套运动清洗与文本标注数据预处理流程。

Result: 成功训练出具备卓越文本指令遵循能力的十亿参数模型，在动作质量与 200+ 跨类别覆盖上全面超越现有开源基准（如 Motion-DiT、TMR 等性能指标）。

Conclusion: HY-Motion 1.0 作为首个达到商业级规模的文本驱动动作生成模型，其开源将加速生成式 AI 在虚拟人、游戏等产业的应用落地。

Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

</details>


### [131] [TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding](https://arxiv.org/abs/2512.23483)
*Zongsheng Cao,Yangfan He,Anran Liu,Feng Chen,Zepeng Wang,Jun Xie*

Main category: cs.CV

TL;DR: TV-RAG是一个无需训练的架构，通过时间对齐和熵引导语义提升长视频推理，核心机制包括时间衰减检索模块和熵加权关键帧采样器，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型视频语言模型（LVLMs）长视频理解困难，受限于狭窄时间窗口与浅层文本检索方式，忽略多模态时序依赖关系。

Method: 1) 时间衰减检索模块通过引入时间偏移量重排序文本查询；2) 熵加权关键帧采样器选取信息密集帧。双机制结合实现双层级推理，无需重训练即可适配任意LVLM。

Result: 在Video-MME、MLVU、LongVideoBench等基准测试中持续超越主流方法，验证有效性与轻量化优势。

Conclusion: TV-RAG提供免训练的长视频推理增强方案，通过多模态时序建模显著提升模型性能，适用于资源受限场景。

Abstract: Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.

</details>


### [132] [Multi-label Classification with Panoptic Context Aggregation Networks](https://arxiv.org/abs/2512.23486)
*Mingyuan Jiu,Hailong Zhu,Wenchuan Wei,Hichem Sahbi,Rongrong Ji,Mingliang Xu*

Main category: cs.CV

TL;DR: This paper proposes PanCAN, a context modeling architecture that integrates cross-scale geometric relationships and multi-order contexts through attention-based cross-scale feature aggregation, achieving SOTA performance in multi-label classification benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current methods neglect cross-scale contextual interactions between objects by focusing on localized features or basic geometric relationships. This work addresses the limitation by enabling hierarchical integration of multi-scale contextual information.

Method: PanCAN learns multi-order neighborhood relationships through attention-augmented random walks in a Hilbert space. It cascades multi-scale modules that dynamically fuse cross-scale features via attention mechanism, using fine-scale anchors to propagate contextual information upward.

Result: PanCAN achieved state-of-the-art performance on NUS-WIDE, PASCAL VOC2007, and MS-COCO datasets, showing consistent improvements in multi-label classification tasks through both quantitative metrics and qualitative scene understanding analysis.

Conclusion: Modeling cross-scale contextual interactions through multi-order geometric relationships significantly enhances visual recognition capabilities. The attention-based cross-scale feature fusion establishes a novel paradigm for complex scene understanding through hierarchical context aggregation.

Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.

</details>


### [133] [IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation](https://arxiv.org/abs/2512.23519)
*Donghao Zhou,Jingyu Lin,Guibao Shen,Quande Liu,Jialin Gao,Lihao Liu,Lan Du,Cunjian Chen,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: IdentityStory是一种面向人物中心故事生成的框架，通过身份保留生成模型确保连续图像中角色身份的一致性，包含迭代身份发现和去噪身份注入两个核心组件。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型在人物故事生成中面临人脸一致性难以维持和多角色协同的挑战，需解决跨图像的角色身份连续性问题。

Method: 提出Iterative Identity Discovery提取角色身份特征，并通过Re-denoising Identity Injection分阶段向图像注入身份特征，保持上下文不变的前提下实现多角色一致性控制。

Result: 在ConsiStory-Human基准测试中，身份一致性评分优于现有方法，尤其在人脸细节保持和多角色组合场景表现突出，支持无限长度故事生成。

Conclusion: IdentityStory通过解耦身份建模与场景生成流程，显著提升了人物中心叙事图像的连续性与多样性，为生成式人工智能在互动叙事应用提供了新方向。

Abstract: Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.

</details>


### [134] [Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution](https://arxiv.org/abs/2512.23532)
*Hexin Zhang,Dong Li,Jie Huang,Bingzhou Wang,Xueyang Fu,Zhengjun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种名为IAFS的训练无关框架，通过迭代优化和频率自适应融合，有效平衡感知质量与结构保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像超分辨率中无法同时保证高频感知质量与低频结构保真度，现有推断时间扩展策略存在效率不足

Method: 提出了迭代扩散推断时间扩展与自适应频率引导（IAFS），联合使用迭代优化和频率感知粒子融合，逐步优化图像结构并自适应整合高低频信息

Result: 实验表明IAFS在多种扩散模型上有效解决感知-保真度矛盾，提高图像重建的感知细节和结构准确性，优于现有推断优化方法

Conclusion: IAFS是一种无需训练的推断优化框架，通过迭代优化与自适应频率融合在扩散模型图像超分辨率中实现感知质量与结构保真度的有效平衡

Abstract: Diffusion models have become a leading paradigm for image super-resolution (SR), but existing methods struggle to guarantee both the high-frequency perceptual quality and the low-frequency structural fidelity of generated images. Although inference-time scaling can theoretically improve this trade-off by allocating more computation, existing strategies remain suboptimal: reward-driven particle optimization often causes perceptual over-smoothing, while optimal-path search tends to lose structural consistency. To overcome these difficulties, we propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free framework that jointly leverages iterative refinement and frequency-aware particle fusion. IAFS addresses the challenge of balancing perceptual quality and structural fidelity by progressively refining the generated image through iterative correction of structural deviations. Simultaneously, it ensures effective frequency fusion by adaptively integrating high-frequency perceptual cues with low-frequency structural information, allowing for a more accurate and balanced reconstruction across different image details. Extensive experiments across multiple diffusion-based SR models show that IAFS effectively resolves the perception-fidelity conflict, yielding consistently improved perceptual detail and structural accuracy, and outperforming existing inference-time scaling methods.

</details>


### [135] [AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization](https://arxiv.org/abs/2512.23537)
*Binhe Yu,Zhen Wang,Kexin Li,Yuqian Yuan,Wenqiao Zhang,Long Chen,Juncheng Li,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: AnyMS 是一个无需训练的框架，用于布局引导的多主体定制图像生成，通过解耦的双层次注意力机制，有效平衡文本对齐、主体身份保留和布局控制，支持复杂组合和高效扩展主体数量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本对齐、主体身份保留和布局控制间难以平衡，且依赖额外训练限制了可扩展性和效率。迫切需要一种无需训练即可协调三者目标的有效解决方案。

Method: AnyMS 利用文本提示、主体图像和布局约束三类输入，提出双层次注意力解耦机制：全局解耦分离文本与视觉条件的跨注意力确保文本对齐，局部解耦限定主体注意力到指定区域防止冲突。同时使用预训练图像适配器提取特征。

Result: 实验证明 AnyMS 在保持主体身份和布局精度的同时显著提升文本对齐能力，支持包含10个以上主体的复杂构图生成，推理速度比微调方法快3倍。

Conclusion: AnyMS 通过结构创新和预训练组件实现了多主体生成的三大核心目标的完美平衡，为实际应用提供了高效、无需训练的端到端解决方案。

Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.

</details>


### [136] [PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis](https://arxiv.org/abs/2512.23545)
*Shengyi Hua,Jianfeng Wu,Tianle Shen,Kangzhe Hu,Zhongzhen Huang,Shujuan Ni,Zhihong Zhang,Yuan Li,Zhe Wang,Xiaofan Zhang*

Main category: cs.CV

TL;DR: PathFound是一种多模态模型，通过主动收集证据改进病理诊断，模仿临床推理流程。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖静态诊断流程，而临床诊断需反复验证假设。作者旨在通过动态证据获取弥补这一缺陷。

Method: PathFound整合病理视觉模型、视觉语言模型和强化学习推理模型，分三阶段：初始诊断、证据寻求、最终决策。

Result: 该策略显著提升诊断准确性，尤其在发现核特征和局部侵袭等微观细节方面表现突出。

Conclusion: 动态证据探索工作流程验证有效，PathFound在多种临床场景下达到SOTA，证明动态推理的价值。

Abstract: Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.

</details>


### [137] [PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation](https://arxiv.org/abs/2512.23546)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: 本研究提出了PurifyGen，一种无需训练的文本到图像生成安全方法。


<details>
  <summary>Details</summary>
Motivation: 传统安全方法如文本黑名单或有害内容分类存在易绕过性和高数据需求，而扩散模型生成的文本到图像存在安全风险。

Method: 通过双阶段策略进行提示净化，首先计算互补语义距离评估token风险，再采用双空间转换去除有毒语义并增强安全语义。

Result: 实验表明PurifyGen在降低不安全内容方面优于当前方法，具有跨数据集和模型的良好泛化能力。

Conclusion: PurifyGen实现了高效安全的文本到图像生成，无需模型再训练且保持原意，为生成有害内容控制提供了新方案。

Abstract: Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.

</details>


### [138] [RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature](https://arxiv.org/abs/2512.23565)
*Hanzheng Li,Xi Fang,Yixuan Li,Chaozheng Huang,Junjie Wang,Xi Wang,Hongzhe Bai,Bojun Hao,Shenyu Lin,Huiqi Liang,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: RxBench基准测试显示多模态大语言模型在化学反应理解中存在深层逻辑与结构识别短板，需改进推理引擎与视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在真实科学文献中准确理解复杂化学反应图形语言的能力尚未被充分验证。

Method: 构建RxnBench多层级基准，包含SF-QA（基于反应图的细粒度视觉推理）和FD-QA（跨模态文献整合）两类任务。

Result: 推理增强模型表现优于基础架构，但FD-QA任务准确率仍未突破50%，表明模型存在显著能力缺口。

Conclusion: 需开发领域特化视觉编码器和强化推理模块以推动自主AI化学家的发展。

Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.

</details>


### [139] [ThinkGen: Generalized Thinking for Visual Generation](https://arxiv.org/abs/2512.23568)
*Siyu Jiao,Yiheng Lin,Yujie Zhong,Qi She,Wei Zhou,Xiaohan Lan,Zilong Huang,Fei Yu,Yingchen Yu,Yunqing Zhao,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本论文提出ThinkGen框架，首次将多模态大语言模型的思维链推理应用于通用视觉生成任务，通过解耦式架构与分离训练方法实现跨场景生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理在生成任务中的应用受限于特定场景机制，缺乏通用性和适应性。研究旨在建立一种可扩展的框架，使多模态大语言模型的推理能力能有效指导多样化生成场景。

Method: 设计包含预训练MLLM和扩散变换器（DiT）的解耦架构，提出分离式GRPO训练方法（SepGRPO），交替强化学习两个模块并支持跨数据集联合训练。

Result: 实验表明ThinkGen在多个生成基准测试中达到当前最优性能，能生成高质量图像且具备跨场景适应能力。

Conclusion: 该框架验证了思维链推理驱动视觉生成的有效性，为通用多模态生成任务提供了新范式。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen

</details>


### [140] [ProGuard: Towards Proactive Multimodal Safeguard](https://arxiv.org/abs/2512.23573)
*Shaohan Yu,Lijun Li,Chenyang Si,Lu Sheng,Jing Shao*

Main category: cs.CV

TL;DR: ProGuard通过强化学习实现跨模态主动安全防护，无需调整模型即可识别描述未知安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全防护方法依赖模型调整且无法主动识别分布外安全风险，导致多模态场景下存在防护盲区。

Method: 构建87K样本的跨模态安全数据集，采用强化学习训练视觉-语言模型，并设计基于同义词库的奖励机制提升未知风险描述能力。

Result: 与闭源大模型二分类准确率相当，在开放安全类别分类任务中超越现有开源模型，OOD风险检测提升52.6%，描述能力提升64.8%。

Conclusion: 基于强化学习的主动防护框架证明了模型无需参数调整即可实现有效安全防护，并建立了多模态安全风险评估新范式。

Abstract: The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.

</details>


### [141] [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/abs/2512.23576)
*Ethan Chern,Zhulin Hu,Bohao Tang,Jiadi Su,Steffi Chern,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态条件的实时视频生成扩散模型蒸馏方法，解决了传统模型在实时交互中的延迟问题，并通过LiveTalk系统实现高效的多模态人机交互。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型通过双向注意力迭代去噪导致实时交互延迟，现有蒸馏方法多关注文本到视频生成，难以满足多模态（文本、图像、音频）实时交互需求，且存在视觉伪影问题。

Method: 改进蒸馏策略，优化条件输入质量、初始化方法及优化调度方案，并将模型与音频语言模型和长视频推理技术Anchor-Heavy Identity Sinks结合，构建实时系统LiveTalk。

Result: 在HDTF等多模态数据集上达到全步长模型的画质水平且推理成本降低20倍；LiveTalk在多轮交互基准测试中超过Sora2、Veo3等模型，实现从分钟级到实时响应。

Conclusion: 所提蒸馏方法有效缓解多模态视频扩散模型的实时性瓶颈，为通用多模态交互系统提供可行技术路径。

Abstract: Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.

</details>


### [142] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://arxiv.org/abs/2512.23592)
*Damiano Marsili,Aditya Mehta,Ryan Y. Lin,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出TWIN数据集，通过对比学习提升视觉-语言模型的细粒度感知能力，在跨领域细粒度识别任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在粗粒度视觉理解存在局限，传统训练数据过度强调广义识别（如猫狗区分），导致模型忽略细微视觉差异，亟需提升细粒度感知能力。

Method: 构建包含56.1万图像对的TWIN大规模对比学习数据集，要求模型识别视觉相似图像中的细微差异；设计FGVQA综合基准评估跨领域细粒度性能，涵盖艺术、动物、植物等多场景。

Result: 微调后VLMs在FGVQA基准提升最高19.3%，且保持通用VQA性能；实验证明数据规模与细粒度标注质量对性能提升具有关键作用。

Conclusion: TWIN数据集可无缝接入现有开源VLM训练体系，显著增强模型对视觉细节的感知能力，为后续研究提供基础性框架。

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/

</details>


### [143] [Detection Fire in Camera RGB-NIR](https://arxiv.org/abs/2512.23594)
*Nguyen Truong Khai,Luong Duc Vinh*

Main category: cs.CV

TL;DR: 该论文提出了一种通过增加NIR数据集、构建两阶段检测模型和改进YOLO模型（Patched-YOLO）来提升夜间红外火灾检测准确率并减少误检人工光源的方法，特别在处理RGB图像中小尺寸目标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 红外夜视相机火灾检测精度仍不满足需求，现有检测模型（如YOLOv7、RT-DETR、YOLOv9）在数据集构建方面的局限性导致常将人工强光误判为火源。

Method: 1. 开发新的NIR数据集并采用数据增强策略；2. 设计YOLOv11与EfficientNetV2-B0的两阶段检测流程以降低误报；3. 创建基于分片处理的Patched-YOLO算法增强RGB图像中小目标检测能力

Result: 提出的方法相比现有模型取得更高检测精度（具体指标未量化），特别是在夜间火灾检测中显著降低人工光源误检率，并能有效识别远距离小尺寸火源。

Conclusion: 综合应用多模态数据增强、双模型协作检测及改进YOLO架构可有效解决红外火灾识别中的误报问题，为夜间火灾监测提供了改进方向。

Abstract: Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.
  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.

</details>


### [144] [Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging](https://arxiv.org/abs/2512.23597)
*Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi*

Main category: cs.CV

TL;DR: 提出了一种可扩展的残差特征聚合框架(SRFA)用于胰腺肿瘤早期检测，结合多种深度学习与优化算法，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤早期检测在CT影像中面临病灶对比度低、解剖结构个体差异大的临床挑战，需要一种高效且可扩展的系统来增强微小视觉特征并实现多模态数据泛化。

Method: 框架包含预处理-MAGRes-UNet分割管道，使用DenseNet-121进行特征提取，引入HHO-BA混合特征选择策略，并结合Vision Transformer与EfficientNet-B3分类，通过SSA-GWO双优化机制调参。

Result: 该模型达到96.23%准确率、95.58%F1分数和94.83%特异性，显著优于传统CNN和Transformer模型。

Conclusion: SRFA框架证明了其作为胰腺肿瘤早期检测有效工具的应用潜力，兼具特征提取深度和模型泛化能力。

Abstract: The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.

</details>


### [145] [Memorization in 3D Shape Generation: An Empirical Study](https://arxiv.org/abs/2512.23628)
*Shu Pu,Boya Zeng,Kaichen Zhou,Mengyu Wang,Zhuang Liu*

Main category: cs.CV

TL;DR: 本研究设计了一个评估框架来量化3D生成模型中的记忆化现象，并提出了通过数据模态优化、调整引导规模、延长Vecsets长度和旋转增强等策略来减少记忆化的方法，在不降低生成质量的前提下提升模型泛化性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在3D视觉中合成新形状时可能过度记忆训练数据，导致数据泄露风险和生成多样性不足。为解决这一问题，需要系统研究记忆化的影响因素并提供可实施的解决策略。

Method: 构建量化记忆化的评估框架，基于Vecset扩散模型进行受控实验，通过调整数据模态（如点云/网格）、数据多样性、条件细粒度等级、引导规模参数、Vecsets长度及数据增强方式，分析各因素对记忆化程度的影响机制。

Result: 发现记忆化程度受数据模态显著影响，数据多样性越大、条件越精细则记忆化越强；在建模层面，中等规模的引导参数会使记忆化达到峰值，而更长的Vecsets和旋转增强能够有效抑制记忆化，且不会损害生成质量。

Conclusion: 本研究通过量化分析揭示了3D生成模型记忆化的关键影响因素，证实了简单调整训练策略（如数据增强、参数调优）可显著降低记忆化风险，为设计更安全和多样化的生成模型提供了实践指导。

Abstract: Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.

</details>


### [146] [Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception](https://arxiv.org/abs/2512.23635)
*Xiaoyu Li,Peidong Li,Xian Wu,Long Shi,Dedong Liu,Yitao Wu,Jiajia Fu,Dixiao Cui,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: 论文提出HAT模块，通过多假设解码优化自动驾驶中时空对齐，提升检测跟踪性能并降低碰撞率


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一运动模型与隐式注意力对齐，难以处理复杂场景中运动状态与特征的跨类别/跨帧差异，需更自适应的显式运动建模方案

Method: HAT采用多显式运动模型生成空间锚点及运动感知特征提案，并结合语义与运动线索进行多假设解码，自适应选择最优对齐方案

Result: 在nuScenes数据集上，HAT使DETR3D检测器的AMOTA达46.0%，提升3D检测/跟踪性能，碰撞率下降32%；在nuScenes-C数据腐蚀场景中仍保持强鲁棒性

Conclusion: HAT通过显式运动建模与隐式语义特征融合，在复杂交通场景中实现了更精准的时空感知对齐，验证了运动建模在端到端自动驾驶系统中的核心价值

Abstract: Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.

</details>


### [147] [OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding](https://arxiv.org/abs/2512.23646)
*Keda Tao,Wenjie Du,Bohan Yu,Weiqiang Wang,Jian Liu,Huan Wang*

Main category: cs.CV

TL;DR: 提出OmniAgent通过动态规划实现音视频主动解析，在三个基准上超越现有模型10-20%


<details>
  <summary>Details</summary>
Motivation: 现有跨模态模型缺乏细粒度音视频对齐与主动感知能力

Method: 构建动态规划音频引导框架，融合时序事件定位与任务驱动注意力机制，自主编排专用感知工具

Result: 在AudioVisual QA/Retrieval/Summarization基准测试中分别提升18.7%/15.2%/12.4%至SOTA水平

Conclusion: 主动感知范式显著提升多模态对齐能力，为音频驱动的交互系统建立新标尺

Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

</details>


### [148] [IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition](https://arxiv.org/abs/2512.23667)
*Kang Du,Yirui Guan,Zeyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Intrinsic Decomposition Transformer (IDT)的多视角图像本征分解框架，通过Transformer架构实现单次前向计算即可生成视角一致的本征分量，避免了迭代生成采样。该方法基于物理成像模型，显式分解diffuse反射、diffuse阴影和specular阴影，解决了现有扩散模型方法在多视角下视角不一致的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有单视角图像本征分解模型（如基于扩散的方法）难以拓展到多视角场景，会导致严重的视角不一致性问题。同时传统方法依赖复杂的迭代优化过程，缺乏端到端处理多视角关联性的能力。作者希望开发一种能够同时保证分解质量和视角一致性的新型框架。

Method: 1) 提出基于Transformer的注意力架构，通过联合建模多视角输入；2) 采用物理驱动的成像模型将图像分解为diffuse反射、diffuse阴影、specular阴影；3) 通过结构化分解策略分离Lambertian（漫反射）和非Lambertian（镜面反射）光学传输过程；4) 实现单次前向传播完成多视角一致性优化，无需迭代采样。

Result: 在合成和真实数据集上：1) 分解得到的diffuse反射更清晰；2) diffuse阴影具有更高的空间连贯性；3) 特定视角下的specular分量分离更精准；4) 多视角一致性指标超越所有现有方法，且推理效率提升3倍以上。特别在极端光照条件下仍能保持稳定性能。

Conclusion: IDT通过将Transformer注意力机制与物理模型结合，首次实现了单次前向传播的多视角本征分解。该方法在分解质量、视角一致性和计算效率三个关键指标上取得突破，为三维重建和材质编辑等下游任务提供了更可靠的物理基础。

Abstract: Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.

</details>


### [149] [Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation](https://arxiv.org/abs/2512.23705)
*Shaocong Xu,Songlin Wei,Qizhe Wei,Zheng Geng,Hong Li,Licheng Shen,Qianpu Sun,Shu Han,Bin Ma,Bohan Li,Chongjie Ye,Yuhang Zheng,Nan Wang,Saining Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 本论文提出DKT模型，利用视频扩散模型提升透明物体的深度和法线估计，通过合成数据集TransPhy3D训练，实现零样本最先进性能。


<details>
  <summary>Details</summary>
Motivation: 透明和反射物体导致现有感知系统因折射、反射和透射失效，而视频扩散模型已能生成逼真的透明现象，推测其学习了光学规则。

Method: 构建包含11k序列的合成视频数据集TransPhy3D，基于物理渲染生成RGB/深度/法线数据，使用LoRA适配器微调预训练视频扩散模型，联合训练新旧数据集。

Result: DKT在涉及透明度的真实与合成视频基准（如ClearPose、TransPhy3D-Test）上零样本性能领先，0.17秒/帧速度推动抓取操作的成功率提升。

Conclusion: 扩散模型具备理解透明现象的生成先验，无需标签即可高效转换为强鲁棒、时序一致的感知能力，适用于真实场景操作。

Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.

</details>


### [150] [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/abs/2512.23709)
*Hau-Shiang Shiu,Chin-Yang Lin,Zhixiang Wang,Chi-Wei Hsiao,Po-Fan Yu,Yu-Chih Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Stream-DiffVSR框架，通过精简扩散模型流程和自回归时间引导，显著降低视频超分延迟并提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型视频超分依赖未来帧和多步去噪导致延迟过高，难以满足实时应用需求。

Method: 采用四步精简去噪器、自回归时间引导模块(ARTG)在潜空间注入运动线索，配合轻量化时间感知解码器(TPM)优化细节与时间一致性。

Result: RTX4090上单帧处理0.328秒，LPIPS指标提升0.095，延迟比TMP降低130倍，将初始延迟从4600秒降至单帧处理时间。

Conclusion: 实现首个多阶段扩散式视频超分实时方案，平衡了质量与效率，适用于在线低延迟场景。

Abstract: Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [151] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

TL;DR: Moxin 7B及变体通过全面开源和透明化策略推动大语言模型协作生态。


<details>
  <summary>Details</summary>
Motivation: 开源模型通过定制性促进应用，但现有开源项目缺乏训练数据和实现细节的完全透明，需建立可持续的健康开源生态。

Method: 基于Model Openness Framework开发Moxin 7B，实现训练过程、数据集和代码的全公开；推出针对多模态（视觉-语言/视觉-语言-动作）及中文场景的三个变体模型。

Result: 模型在多任务评估中表现优于现有方案，并通过开源框架完成模型、数据和代码的全量释放。

Conclusion: 通过结构化开源策略，Moxin系列模型为学术研究和工业应用提供了可信赖的基线框架，验证了透明化路径对模型演进的推动作用。

Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [152] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

TL;DR: Transformer语言模型的嵌入空间表现出与人类认知属性相关的层次化几何结构，且可通过浅层非线性探测器解码识别。


<details>
  <summary>Details</summary>
Motivation: 探讨 Transformer 嵌入空间中是否存在与人类可解释认知属性（如连续序数能量评分和离散层级标签）对齐的高层认知组织结构，而不仅限于已知的几何特性。

Method: 构建包含480个自然语言句子的数据集，标注七个有序认知类别的连续评分与离散层级标签；使用固定 Transformer 嵌入结合线性/浅层非线性探测器评估注释可解码性；通过 TF-IDF 基线和非参数置换检验对比性能，并采用 UMAP 可视化分析嵌入空间结构。

Result: 所有模型中连续评分与层级标签均可可靠解码，非线性探测器性能优于线性基线；TF-IDF 基线显著劣于嵌入空间方法；置换检验证实性能超越随机水平；UMAP 显示嵌入空间存在低到高的平滑梯度及相邻层级混淆模式。

Conclusion: Transformer 嵌入空间存在与人定义的认知属性对齐的层次化几何组织，但该结论不涉及模型内部意识或现象学层面的断言。

Abstract: Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [153] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 提出SmartSnap框架实现智能体自主验证，通过3C原则选取关键快照证据并利用LLM验证，解决GUI任务中的验证可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有被动验证方法依赖冗长轨迹分析，存在效率低下和可靠性不足的痛点，亟需主动验证机制提升复杂任务下的可扩展性。

Method: 设计具备自我验证能力的新型智能体，采用3C原则(完整性/简洁性/创造性)筛选环境中的关键快照证据，结合LLM作为验证器进行实时判别。

Result: 在移动端任务中8B和30B参数模型分别提升26.08%和16.66%性能，对比DeepSeek V3.1与Qwen3-235B-A22B展现出竞争力。

Conclusion: SmartSnap框架通过证据筛选与验证机制的协同创新，有效解决了复杂GUI任务中智能体训练的可扩展性瓶颈。

Abstract: Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [154] [The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach](https://arxiv.org/abs/2512.22376)
*Zubaida Mohammed Albadani,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: 本文在最简框架下研究也门伊比阿拉伯语中qulk-子句的句法结构（意为'我说'）。提出双子句结构模型，qulk作为嵌套谓词，结合形态融合和计算步骤。研究为生成语法提供理论贡献，并探讨最简方案跨方言的适用性。


<details>
  <summary>Details</summary>
Motivation: qulk-子句缺少补语化成分却呈现复杂句法现象，挑战传统补语化结构理论。研究旨在揭示方言特殊句式的生成机制，验证最简方案对非标准阿拉伯语变体的解释力，并探讨形态融合与句法操作的交互。

Method: 基于Chomsky最简操作框架，运用Merge构造论元结构，Move实现话题化，Agree保障一致性，Spell-out确定接口输出。特别引入Morphological Merger解释形态融合现象，并通过二分否定、小品词化等方言特征验证模型。

Result: 揭示qulk的双相结构特性（既作为谓词选择CP，又经历形态合并），构建三阶段推导模型（论元合并→移动→形态整合）。解释嵌入式从句的句法能产性，阐明否定成分的分布规律及疑问词移位模式。

Conclusion: 建立融合形态与句法的跨模块分析框架，推动对阿语方言中轻动词结构的认知。提出扩展分析方向：1) kil-k '你说'类对话性子句；2) 最简运算规则的跨语言共性和参数化变异；3) 形态融合现象的跨层面交互机制。为语法学的理论简化提供区域语言证据。

Abstract: This study investigates the syntax of qulk-clauses in Yemeni Ibbi Arabic (YIA) within the Minimalist Program. The construction qulk-clause, a morphologically fused form meaning 'I said,' introduces embedded declarative interrogative, and imperative clauses, often eithout complementizer. The central proposal of this paper is that qulk-clauses are biclausal structures in which qulk functions a clause-embedding predicate sec;ecting a dull CP complement. By applying core minimalist operations, viz., Merge, Move, Agree, and Spell-out, the study provides a layered syntactic analysis of qulk-clauses, for illustrating how their derivation proceeds through standard computational steps and post-syntactic processes such as Morphological Merger. The proposal also accounts for dialect-specific features like bipartite negation, cliticization, and CP embedding. The findings offer theoretical contributions to generative syntax, specifically minimalism. The study concludes raising theoretical questions concerning extending the analysis to the addressee-clause kil-k 'you said'. It also provides insights into the possibility of the universality of minimalism.

</details>


### [155] [LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition](https://arxiv.org/abs/2512.22385)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: LLM-Guided Exemplar Selection框架提升可穿戴传感器活动识别，通过语义与几何结合实现few-shot场景下88.78% F1-score。


<details>
  <summary>Details</summary>
Motivation: 现有HAR方法依赖大量标注数据且采用纯几何选样方式，难以区分相似活动（如行走、上楼、下楼）。

Method: 利用LLM生成知识先验（特征重要性/类间混淆/样本权重），融合边缘验证、PageRank中心性、hub惩罚和设施选址优化进行样本选择。

Result: 在UCI-HAR严格few-shot条件下达到88.78% macro F1-score，超越随机采样、herding、k中心等经典方法。

Conclusion: LLM语义先验与结构化特征结合，显著提升少样本可穿戴传感器活动识别性能。

Abstract: In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar weara-ble sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and $k$-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.

</details>


### [156] [Hallucination Detection and Evaluation of Large Language Model](https://arxiv.org/abs/2512.22416)
*Chenggong Zhang,Haopeng Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种轻量级分类框架HHEM以高效检测大语言模型（LLMs）幻觉问题。相比传统多阶段验证方法，HHEM计算效率提升显著（8小时→10分钟），且保持82.2%准确率。通过引入段落检索改进摘要任务中的局部幻觉检测，并揭示大模型（7B-9B参数）幻觉更少但中型模型稳定性较差。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型生成误导性内容损害可信度的问题，现有基于多阶段验证的幻觉检测方法（如KnowHalu）存在计算成本过高的缺陷，亟需在维持检测准确性的同时提升评估效率。

Method: 提出独立于LLM判断的Hughes幻觉评估模型（HHEM），通过分类框架实现快速检测；在摘要任务中引入段落级检索验证机制；构建跨模型的对比实验体系，采用TPR/TNR/Accuracy指标评估QA与摘要任务的幻觉检测效果，并进行参数量与幻觉关系的累计分布函数分析。

Result: HHEM将评估时间缩短95.8%（8h→10min），非虚构检测模式实现最高82.2%准确率与78.9% TPR；段落检索改进使摘要任务检测能力提升17.5%；参数量7B-9B大模型幻觉率降低28.3%，而中型模型显示32.1%的高波动性。

Conclusion: HHEM通过结构化分类框架实现了幻觉检测效率与准确性的平衡，但需改进局部幻觉处理。研究证实模型规模扩大可降低幻觉风险，但中间体量模型存在稳定性缺陷，建议未来研究应注重计算效率与事实验证的协同优化。

Abstract: Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while maintaining high detection accuracy. We conduct a comparative analysis of hallucination detection methods across various LLMs, evaluating True Positive Rate (TPR), True Negative Rate (TNR), and Accuracy on question-answering (QA) and summarization tasks. Our results show that HHEM reduces evaluation time from 8 hours to 10 minutes, while HHEM with non-fabrication checking achieves the highest accuracy \(82.2\%\) and TPR \(78.9\%\). However, HHEM struggles with localized hallucinations in summarization tasks. To address this, we introduce segment-based retrieval, improving detection by verifying smaller text components. Additionally, our cumulative distribution function (CDF) analysis indicates that larger models (7B-9B parameters) generally exhibit fewer hallucinations, while intermediate-sized models show higher instability. These findings highlight the need for structured evaluation frameworks that balance computational efficiency with robust factual validation, enhancing the reliability of LLM-generated content.

</details>


### [157] [HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG](https://arxiv.org/abs/2512.22442)
*Cattalyya Nuengsigkapian*

Main category: cs.CL

TL;DR: 本文提出HiFi-RAG系统，在MMU-RAGent NeurIPS 2025竞赛中夺冠，解决了开放域RAG中的无关信息与用户意图对齐问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG存在检索文档包含无关信息且生成答案难以匹配用户意图，尤其在2025年后知识需求场景（Test2025数据集）表现不足。

Method: 采用Gemini 2.5 Flash实现查询重构、层次化内容过滤和引文标注，配合Gemini Pro进行终答生成，形成多阶段检索-过滤-生成流水线。

Result: 相较基线模型，在MMU-RAGent验证集ROUGE-L提升至0.274（+19.6%），DeBERTaScore达0.677（+6.2%）；Test2025数据集ROUGE-L提升57.4%，DeBERTaScore提升14.9%。

Conclusion: 层次化过滤框架显著提升RAG效果，结合轻量模型高效过滤与高质量生成模型，为后知识截止日期场景提供有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.

</details>


### [158] [Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.22443)
*Jie Zhou,Xin Chen,Jie Zhang,Zhe Li*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在会计领域中的应用潜力，发现尽管GPT-4表现最佳，但仍无法满足实际需求，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 如何将LLMs与专业领域（如会计）有效结合，推动企业数字化转型和社会发展，是当前研究的关键挑战。

Method: 提出垂直领域会计推理概念，基于GLM系列模型的训练数据特征建立评估标准，并通过会计推理任务测试GLM系列和GPT-4模型的性能。

Result: 不同提示策略对模型性能提升效果各异，GPT-4表现出最强的会计推理能力，但现有LLMs整体仍无法满足实际应用需求。

Conclusion: 当前LLMs（尤其是企业级会计场景）需进一步优化模型部署，以充分释放其在该领域的潜力。

Abstract: Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.

</details>


### [159] [Constituency Structure over Eojeol in Korean Treebanks](https://arxiv.org/abs/2512.22487)
*Jungyeul Park,Chulwoo Park*

Main category: cs.CL

TL;DR: This paper argues for using eojeol-based constituency representations in Korean treebanks to better align syntactic structures with morphological layers, enabling cross-treebank compatibility and conversion.


<details>
  <summary>Details</summary>
Motivation: The paper addresses conflicting approaches to terminal unit selection in Korean treebanks (morphemes vs. eojeol), which creates representational mismatches and hinders resource interoperability.

Method: Proposes an eojeol-based annotation scheme with decoupled morphological/POS layers, performs comparative analysis of Sejong and Penn Korean treebanks under explicit normalization assumptions.

Result: Demonstrates representational equivalence between Sejong and Penn treebanks at the eojeol-based constituency level, validating the proposed annotation framework.

Conclusion: Eojeol-based representations with orthogonal morphological encoding provide an effective solution for maintaining constituency structure integrity while enabling cross-treebank comparison and conversion.

Abstract: The design of Korean constituency treebanks raises a fundamental representational question concerning the choice of terminal units. Although Korean words are morphologically complex, treating morphemes as constituency terminals conflates word internal morphology with phrase level syntactic structure and creates mismatches with eojeol based dependency resources. This paper argues for an eojeol based constituency representation, with morphological segmentation and fine grained part of speech information encoded in a separate, non constituent layer. A comparative analysis shows that, under explicit normalization assumptions, the Sejong and Penn Korean treebanks can be treated as representationally equivalent at the eojeol based constituency level. Building on this result, we outline an eojeol based annotation scheme that preserves interpretable constituency and supports cross treebank comparison and constituency dependency conversion.

</details>


### [160] [ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation](https://arxiv.org/abs/2512.22491)
*Suhua Wang,Zifan Wang,Xiaoxin Sun,D. J. Wang,Zhanbo Liu,Xin Li*

Main category: cs.CL

TL;DR: This paper introduces ManchuTTS, a novel text-to-speech synthesis system for the endangered Manchu language. It addresses challenges like data scarcity and phonological agglutination through a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism.


<details>
  <summary>Details</summary>
Motivation: Manchu, as an endangered language, faces severe speech synthesis challenges due to sparse data and complex agglutinative phonology. Legacy methods struggle to align multi-granular linguistic units and achieve natural prosody, necessitating a tailored approach to preserve such low-resource languages.

Method: 1) Designed a three-tier text representation (phoneme-syllable-prosodic) with hierarchical attention; 2) Integrated deep convolutional networks with a flow-matching Transformer for non-autoregressive generation; 3) Implemented a hierarchical contrastive loss for acoustic-linguistic alignment; 4) Constructed the first Manchu TTS dataset with data augmentation strategies.

Result: Using only 5.2-hour of training data (from a 6.24-hour annotated corpus), ManchuTTS achieved a MOS of 4.52, surpassing all baseline models. Ablation studies confirmed hierarchical components improved agglutinative word pronunciation accuracy (31% boost) and prosodic naturalness (27% improvement).

Conclusion: The hierarchical architecture and cross-modal alignment mechanism effectively address agglutination challenges and data scarcity in endangered language TTS. The proposed dataset construction strategy and contrastive loss framework provide a replicable pipeline for low-resource speech synthesis tasks.

Abstract: As an endangered language, Manchu presents unique challenges for speech synthesis, including severe data scarcity and strong phonological agglutination. This paper proposes ManchuTTS(Manchu Text to Speech), a novel approach tailored to Manchu's linguistic characteristics. To handle agglutination, this method designs a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism for multi-granular alignment. The synthesis model integrates deep convolutional networks with a flow-matching Transformer, enabling efficient, non-autoregressive generation. This method further introduce a hierarchical contrastive loss to guide structured acoustic-linguistic correspondence. To address low-resource constraints, This method construct the first Manchu TTS dataset and employ a data augmentation strategy. Experiments demonstrate that ManchuTTS attains a MOS of 4.52 using a 5.2-hour training subset derived from our full 6.24-hour annotated corpus, outperforming all baseline models by a notable margin. Ablations confirm hierarchical guidance improves agglutinative word pronunciation accuracy (AWPA) by 31% and prosodic naturalness by 27%.

</details>


### [161] [Learning When Not to Attend Globally](https://arxiv.org/abs/2512.22562)
*Xuan Luo,Kailai Zhang,Xifeng Yan*

Main category: cs.CL

TL;DR: 提出All-or-Here Attention (AHA)，通过动态切换全局与局部注意力机制，减少大型语言模型的注意力计算量。


<details>
  <summary>Details</summary>
Motivation: 人类读书时主要关注当前页面，仅在必要时回顾上下文，受此启发，LLMs可通过动态选择注意力范围提升效率。

Method: 设计二值路由单元控制每个注意力头在全局全注意力（full attention）与局部滑动窗口注意力间切换。

Result: 使用256长度窗口时可替换93%的全注意力计算且无性能损失，验证了上下文依赖的长尾分布特性。

Conclusion: 全注意力在多数场景下冗余，高效推理可通过按需调用全局上下文实现。

Abstract: When reading books, humans focus primarily on the current page, flipping back to recap prior context only when necessary. Similarly, we demonstrate that Large Language Models (LLMs) can learn to dynamically determine when to attend to global context. We propose All-or-Here Attention (AHA), which utilizes a binary router per attention head to dynamically toggle between full attention and local sliding window attention for each token. Our results indicate that with a window size of 256 tokens, up to 93\% of the original full attention operations can be replaced by sliding window attention without performance loss. Furthermore, by evaluating AHA across various window sizes, we identify a long-tail distribution in context dependency, where the necessity for full attention decays rapidly as the local window expands. By decoupling local processing from global access, AHA reveals that full attention is largely redundant, and that efficient inference requires only on-demand access to the global context.

</details>


### [162] [Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis](https://arxiv.org/abs/2512.22603)
*Zhiqiang Gao,Shihao Gao,Zixing Zhang,Yihao Guo,Hongyu Chen,Jing Han*

Main category: cs.CL

TL;DR: 本文提出分步优化和集成策略，提升多模态对话中的情感分析效果，子任务Ⅰ抽取情感六元组，子任务Ⅱ检测情感反转。


<details>
  <summary>Details</summary>
Motivation: 实现情绪智能AI需解决多模态对话情感理解难题，挑战在于从多人对话中抽取细粒度情感六元组并检测动态情感转变。

Method: 子任务Ⅰ通过结构化提示词引导大模型分步提取情感部件，子任务Ⅱ结合三个大模型的互补优势实现集成预测。

Result: 子任务Ⅰ平均得分47.38%，子任务Ⅱ精确匹配F1达74.12%，验证了分步优化和集成策略的有效性。

Conclusion: 分步细化和集成学习策略在多模态情感分析中具有显著优势，提升了六元组抽取和情感反转检测性能。

Abstract: Understanding sentiment in multimodal conversations is a complex yet crucial challenge toward building emotionally intelligent AI systems. The Multimodal Conversational Aspect-based Sentiment Analysis (MCABSA) Challenge invited participants to tackle two demanding subtasks: (1) extracting a comprehensive sentiment sextuple, including holder, target, aspect, opinion, sentiment, and rationale from multi-speaker dialogues, and (2) detecting sentiment flipping, which detects dynamic sentiment shifts and their underlying triggers. For Subtask-I, in the present paper, we designed a structured prompting pipeline that guided large language models (LLMs) to sequentially extract sentiment components with refined contextual understanding. For Subtask-II, we further leveraged the complementary strengths of three LLMs through ensembling to robustly identify sentiment transitions and their triggers. Our system achieved a 47.38% average score on Subtask-I and a 74.12% exact match F1 on Subtask-II, showing the effectiveness of step-wise refinement and ensemble strategies in rich, multimodal sentiment analysis tasks.

</details>


### [163] [Chain-of-thought Reviewing and Correction for Time Series Question Answering](https://arxiv.org/abs/2512.22627)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 提出T3LLM框架，整合三个LLM分别执行推理生成、错误审查与知识内化，提升时间序列问答任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理复杂数值序列时易出错，而时间序列数据具备可验证性，可利用原始输入进行推理一致性校验。

Method: 通过worker（生成结构化推理链）、reviewer（识别错误并纠正）、student（微调学习修正后推理链）三阶段协作，实现显式纠错与多步推理结合。

Result: 在多领域真实时间序列问答基准测试中超越当前最强LLM基线模型，达到SOTA性能。

Conclusion: 验证显式纠错机制对时序推理的有效性，为数值数据处理提供可迁移的LLM优化框架。

Abstract: With the advancement of large language models (LLMs), diverse time series analysis tasks are reformulated as time series question answering (TSQA) through a unified natural language interface. However, existing LLM-based approaches largely adopt general natural language processing techniques and are prone to reasoning errors when handling complex numerical sequences. Different from purely textual tasks, time series data are inherently verifiable, enabling consistency checking between reasoning steps and the original input. Motivated by this property, we propose T3LLM, which performs multi-step reasoning with an explicit correction mechanism for time series question answering. The T3LLM framework consists of three LLMs, namely, a worker, a reviewer, and a student, that are responsible for generation, review, and reasoning learning, respectively. Within this framework, the worker generates step-wise chains of thought (CoT) under structured prompts, while the reviewer inspects the reasoning, identifies erroneous steps, and provides corrective comments. The collaboratively generated corrected CoT are used to fine-tune the student model, internalizing multi-step reasoning and self-correction into its parameters. Experiments on multiple real-world TSQA benchmarks demonstrate that T3LLM achieves state-of-the-art performance over strong LLM-based baselines.

</details>


### [164] [M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation](https://arxiv.org/abs/2512.22628)
*Fanglin Xu,Wei Zhang,Jian Yang,Guo Chen,Aishan Liu,Zhoujun Li,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: 提出了多粒度、多语言代码生成评估框架M2G-Eval，通过四个层级和18种语言验证LLM生成能力，并发现代码复杂度与跨语言性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估基准仅聚焦单一粒度及有限语言，无法反映模型在多语言、多层次编程任务中的细粒度能力差异

Method: 构建包含17K+训练任务、1286个人工标注测试集的跨语言评估框架，提出Class/Function/Block/Line四层级评估体系，并结合Qwen3-8B模型进行监督微调与策略优化

Result: 在30个模型评估中发现：1) Line级任务最易，Class级最难；2) 粒度完整性与语言性能差距随复杂度增大；3) 模型具备跨语言可迁移编程概念

Conclusion: M2G-Eval支持细粒度代码生成能力诊断，揭示复杂代码生成仍存在显著挑战

Abstract: The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.

</details>


### [165] [On the Role of Discreteness in Diffusion LLMs](https://arxiv.org/abs/2512.22630)
*Ziqi Jin,Bin Wang,Xiang Lin,Lidong Bing,Aixin Sun*

Main category: cs.CL

TL;DR: 本文分析扩散模型在语言生成中的挑战，指出连续扩散（嵌入空间）和离散扩散（token级）各自的结构权衡，提出信息分布与多token依赖是关键问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本生成中面临离散性、结构化数据与扩散连续性原理的冲突。现有研究未能完整满足五大必要属性，需厘清核心矛盾以推动模型优化。

Method: 通过解构扩散流程与语言建模的关系，建立五维度评估框架，分类现有方法并对照其性能。结合实证分析提取两大核心缺陷：位置信息腐蚀失衡与多token依赖缺失。

Result: 发现连续扩散虽具并行优势但损失序列结构，离散扩散受限于训练效率；均匀信息腐蚀导致重要位置失真，逐token训练无法捕获跨词依赖，揭示模型设计的结构性妥协。

Conclusion: 提出非均匀腐蚀与多token协同训练为改进方向，强调需构建与语言层级结构对齐的扩散范式，为后续研究提供理论基模。

Abstract: Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.

</details>


### [166] [Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2512.22631)
*Hadi Mohammadi,Tamas Kozak,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 研究发现Chain-of-thought（CoT）推理存在可信度问题，通过比较GRPO和DPO两种优化方法，发现GRPO在大模型中能更有效提升CoT的可信度，但小规模模型稳定性不足。


<details>
  <summary>Details</summary>
Motivation: 现有多步推理方法存在不可靠性（如生成虚假推理链），影响模型安全监管。需通过改进训练方法提升推理过程的透明度与对齐性。

Method: 对LLM进行GRPO和DPO两种训练优化，分别验证其对CoT可信度提升的效果，使用Qwen2.5等不同规模模型进行对比实验，评估答案准确率与推理链忠实度。

Result: Qwen2.5-14B-Instruct模型在GRPO优化下所有指标最优；GRPO在大模型表现优于DPO，但小模型存在训练不稳定现象；模型规模与优化效果正相关，GRPO改进可信度潜力更大。

Conclusion: GRPO为提升大语言模型推理可信度提供有效路径，但需解决小模型训练稳定性问题，未来可探索模型规模与优化方法的匹配关系。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful technique for improving the problem-solving capabilities of large language models (LLMs), particularly for tasks requiring multi-step reasoning. However, recent studies show that CoT explanations often fail to reflect the model's actual reasoning process, as models may produce coherent yet misleading justifications or modify answers without acknowledging external cues. Such discrepancies undermine the reliability of CoT-based methods for safety supervision and alignment monitoring, as models can generate plausible but deceptive rationales for incorrect answers. To better understand this limitation, we evaluate two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), in their ability to improve CoT faithfulness. Our experiments show that GRPO achieves higher performance than DPO in larger models, with the Qwen2.5-14B-Instruct model attaining the best results across all evaluation metrics. Both approaches exhibit positive correlations between model size and performance, but GRPO shows greater potential for improving faithfulness metrics, albeit with less stable behavior at smaller scales. These results suggest that GRPO offers a promising direction for developing more transparent and trustworthy reasoning in LLMs.

</details>


### [167] [Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency](https://arxiv.org/abs/2512.22682)
*Yoshith Roy Kotla,Varshith Roy Kotla*

Main category: cs.CL

TL;DR: 该论文提出VACP框架，在保持高覆盖率的同时大幅减少大语言模型的预测集大小。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在高风险领域部署时无法有效进行不确定性量化的问题，改进现有softmax概率校准不足的缺陷。

Method: 提出基于语义掩码和温度调整的Vocabulary-Aware Conformal Prediction（VACP）框架，通过减少有效预测空间并在理论上证明其维持覆盖率的能力。

Result: 实验表明VACP在Gemma-2B模型上实现了89.7%的实证覆盖率（目标90%），预测集平均大小从847个token降至4.3个，效率提升197倍。

Conclusion: VACP成功平衡了覆盖率与效率，适用于大词汇量场景，为高风险领域的模型部署提供了有效的不确定性量化方案。

Abstract: Deploying large language models (LLMs) in high-stakes domains requires rigorous uncertainty quantification, yet standard softmax probabilities are often poorly calibrated. We present a systematic study of Adaptive Prediction Sets (APS) applied to next-token prediction in transformer-based models with large vocabularies (greater than 250,000 tokens). Our central contribution is the identification of a coverage-efficiency tradeoff: while naive conformal prediction achieves valid coverage, it produces prediction sets of hundreds of tokens, rendering them uninformative. We propose Vocabulary-Aware Conformal Prediction (VACP), a framework that leverages semantic masking and temperature-adjusted scoring to reduce the effective prediction space while provably maintaining marginal coverage. Experiments on Gemma-2B using SQUAD and WikiText benchmarks demonstrate that VACP achieves 89.7 percent empirical coverage (90 percent target) while reducing the mean prediction set size from 847 tokens to 4.3 tokens -- a 197x improvement in efficiency. We provide a theoretical analysis of vocabulary reduction and release our implementation for reproducibility.

</details>


### [168] [GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2512.22705)
*Ahmed Abdullah,Sana Fatima,Haroon Mahmood*

Main category: cs.CL

TL;DR: 本论文提出一种基于预训练Transformer模型（如XLM-RoBERTa、UrduBERT等）多语言框架，可有效识别乌尔都语等低资源语言的希望言论，在二分类任务中取得95.2% F1-score。


<details>
  <summary>Details</summary>
Motivation: 希望言论检测在NLP领域研究不足且资源匮乏，现有仇恨言论检测模型缺乏跨语言通用性验证，尤其对乌尔都语等低资源语言。

Method: 采用XLM-RoBERTa、mBERT等多语言预训练模型，对乌尔都语数据进行简单预处理后训练分类器，通过PolyHope-M 2025基准测试验证效果。

Result: 在乌尔都语数据集上二分类达到95.2% F1分数，多分类65.2%；西班牙语、德语和英语也表现出竞争力，证明模型跨语言有效性。

Conclusion: 研究验证了预训练Transformer模型在低资源语言希望言论检测中的有效性，为构建多语言积极数字交流体系提供了可行方案。

Abstract: Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.

</details>


### [169] [Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages](https://arxiv.org/abs/2512.22712)
*Anaelia Ovalle,Candace Ross,Sebastian Ruder,Adina Williams,Karen Ullrich,Mark Ibrahim,Levent Sagun*

Main category: cs.CL

TL;DR: 该研究揭示大型语言模型在多语言任务中推理与结论的潜在脱节，发现非拉丁脚本的推理错误率是拉丁脚本的两倍以上，并提出需改进评估框架。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型的推理能力是否能在多语言场景下迁移，填补当前对跨语言推理质量研究的空白。

Method: 构建人工验证的评估框架，分析6种语言、6个前沿模型的6.5万条推理链，结合人工标注建立错误分类体系。

Result: 模型任务准确率高但推理与结论存在脱节，非拉丁脚本的推理结论不符率是拉丁脚本的两倍以上，主要错误类型为证据缺失和逻辑断裂。

Conclusion: 现有跨语言评估方法无法全面反映模型推理能力，需开发融合推理质量的评估体系。

Abstract: Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.

</details>


### [170] [Mitigating Social Desirability Bias in Random Silicon Sampling](https://arxiv.org/abs/2512.22725)
*Sashank Chapala,Maksym Mironov,Songgaojun Deng*

Main category: cs.CL

TL;DR: This paper explores how rephrasing prompts can reduce social desirability bias in Large Language Model (LLM) responses for sensitive questions, bringing simulated data closer to real human behavior.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce socially desirable answers to sensitive questions, diverging from genuine human responses. Existing research lacks effective solutions to mitigate this bias. The study tests whether subtle prompting techniques can improve alignment between LLM outputs and real-world human data from sources like the American National Election Study (ANES).

Method: The study tested four prompt strategies across Llama-3.1 and GPT-4.1-mini models: (1) Reformulated prompts (neutral phrasing), (2) Reverse-coded (semantic inversion), (3) Priming (analytical encouragement), and (4) Preamble (sincere response emphasis). Alignment with ANES was measured using Jensen-Shannon Divergence with confidence intervals.

Result: Reformulated prompts significantly improved alignment by reducing overrepresentation of socially acceptable answers. Reverse-coding yielded inconsistent effects, while Priming and Preamble increased response uniformity without clear bias reduction. Bootstrap analysis confirmed statistical significance for reformulated prompts.

Conclusion: Simple, psychology-inspired prompt adjustments effectively mitigate social desirability bias in LLMs, offering a practical approach to enhance the accuracy of large-scale synthetic population studies.

Abstract: Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \emph{reformulated} (neutral, third-person phrasing), \emph{reverse-coded} (semantic inversion), and two meta-instructions, \emph{priming} and \emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.

</details>


### [171] [Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data](https://arxiv.org/abs/2512.22732)
*Md Badsha Biswas*

Main category: cs.CL

TL;DR: 本研究提出了一种基于社交媒体数据（特别是Twitter）的自然语言处理（NLP）管道，用于补充现有数据集，以研究负面妊娠结果（如出生缺陷）。


<details>
  <summary>Details</summary>
Motivation: 婴儿死亡率在美国仍然是重要的公共卫生问题，出生缺陷为主要原因之一。尽管已有研究关注负面妊娠结果，但缺乏更全面的数据和干预策略。社交媒体数据可作为观察性研究的潜在补充来源，但其非平衡性、噪声和非结构化特性需要解决。

Method: 构建并应用鲁棒的预处理技术和数据增强策略，设计NLP流水线以自动识别分享妊娠经历的女性，根据报告结果分为正组（足月且出生体重正常）和负组（负面妊娠结果）。

Result: 提出了一个可扩展的框架，用于从社交媒体提取妊娠相关数据，并分类妊娠结局类型，支持未来的因果干预分析和健康研究。

Conclusion: 社交媒体数据作为辅助资源在妊娠结局的流行病学研究中表现出可行性，为孕产妇健康监测和风险因素分析提供了新方法。

Abstract: Infant mortality remains a significant public health concern in the United States, with birth defects identified as a leading cause. Despite ongoing efforts to understand the causes of negative pregnancy outcomes like miscarriage, stillbirths, birth defects, and premature birth, there is still a need for more comprehensive research and strategies for intervention. This paper introduces a novel approach that uses publicly available social media data, especially from platforms like Twitter, to enhance current datasets for studying negative pregnancy outcomes through observational research. The inherent challenges in utilizing social media data, including imbalance, noise, and lack of structure, necessitate robust preprocessing techniques and data augmentation strategies. By constructing a natural language processing (NLP) pipeline, we aim to automatically identify women sharing their pregnancy experiences, categorizing them based on reported outcomes. Women reporting full gestation and normal birth weight will be classified as positive cases, while those reporting negative pregnancy outcomes will be identified as negative cases. Furthermore, this study offers potential applications in assessing the causal impact of specific interventions, treatments, or prenatal exposures on maternal and fetal health outcomes. Additionally, it provides a framework for future health studies involving pregnant cohorts and comparator groups. In a broader context, our research showcases the viability of social media data as an adjunctive resource in epidemiological investigations about pregnancy outcomes.

</details>


### [172] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)
*Aiwei Liu,Minghua He,Shaoxun Zeng,Sijun Zhang,Linhao Zhang,Chuhan Wu,Wei Jia,Yuan Liu,Xiao Zhou,Jie Zhou*

Main category: cs.CL

TL;DR: WeDLM是一种基于自回归（AR）生成的扩散语言模型框架，通过拓扑重排序和流式解码实现并行生成，解决了传统扩散模型双向注意力导致的效率问题，在保持生成质量的同时显著超越vLLM优化的AR模型速度。


<details>
  <summary>Details</summary>
Motivation: 传统扩散语言模型（DLMs）依赖双向注意力，导致前缀KV缓存机制失效并增加重复上下文计算，难以在实际部署中超越优化的AR引擎（如vLLM）。

Method: 提出WeDLM框架，采用因果注意力机制，通过拓扑重排序将已观测token物理前缀化但保留逻辑位置，实现流式解码：持续将高置信度token提交到左到右前缀，固定并行工作量，避免块扩散的停等行为。

Result: 在保持AR模型质量前提下，复杂推理任务接近3倍加速，低熵生成场景最高10倍加速，且对比基于vLLM的AR基线模型。

Conclusion: 通过纯因果注意力与流式解码设计，扩散范式解码可在实际部署中优于优化后的AR引擎，弥合理论并行性与工程效率的差距。

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.

</details>


### [173] [Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language](https://arxiv.org/abs/2512.22778)
*Muhammad Zain Ali,Bernhard Pfahringer,Tony Smith*

Main category: cs.CL

TL;DR: 该论文针对社交媒体中乌尔都语虚假信息检测，提出通过领域自适应预训练优化模型泛化能力，发现领域适配的XLM-R模型表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语等低资源语言的虚假信息检测研究不足，现有跨语言预训练模型因难以处理领域特定术语而性能受限，需探索更有效的迁移学习方法。

Method: 对XLM-RoBERTa和mBERT进行领域自适应预训练（使用乌尔都语新闻语料），随后进行微调分类；在四个公开乌尔都语假新闻数据集上评估效果。

Result: 领域适配的XLM-R明显优于基线模型，而mBERT表现不稳定，显示不同模型对领域适应方法的响应差异。

Conclusion: 领域自适应预训练提升低资源语言虚假信息检测效果，XLM-R比mBERT更适配该任务，为资源匮乏语言的模型迁移提供了可行方向。

Abstract: Misinformation on social media is a widely acknowledged issue, and researchers worldwide are actively engaged in its detection. However, low-resource languages such as Urdu have received limited attention in this domain. An obvious approach is to utilize a multilingual pretrained language model and fine-tune it for a downstream classification task, such as misinformation detection. However, these models struggle with domain-specific terms, leading to suboptimal performance. To address this, we investigate the effectiveness of domain adaptation before fine-tuning for fake news classification in Urdu, employing a staged training approach to optimize model generalization. We evaluate two widely used multilingual models, XLM-RoBERTa and mBERT, and apply domain-adaptive pretraining using a publicly available Urdu news corpus. Experiments on four publicly available Urdu fake news datasets show that domain-adapted XLM-R consistently outperforms its vanilla counterpart, while domain-adapted mBERT exhibits mixed results.

</details>


### [174] [CNSight: Evaluation of Clinical Note Segmentation Tools](https://arxiv.org/abs/2512.22795)
*Risha Surana,Adrian Law,Sunwoo Kim,Rishab Sridhar,Angxiao Han,Peiyu Hong*

Main category: cs.CL

TL;DR: 临床笔记常以非结构化格式存储，影响二次分析和临床应用。本文比较了基于规则的方法、域特定Transformer模型和大型语言模型在临床笔记分割中的表现，使用了MIMIC-IV数据库的1000份笔记。研究发现，GPT-5-mini在句子级和自由文本分割中取得最佳平均F1分数72.4。轻量级方法在结构化任务中有效，但在非结构化任务中表现不佳，结果为后续信息提取和自动摘要等任务提供指导。


<details>
  <summary>Details</summary>
Motivation: 电子病历（EMR）中提取的临床笔记常为非结构化格式，影响其二次分析及临床应用。明确笔记的章节边界是结构化的关键步骤，不同章节（如现病史、药物、出院指导）提供特定临床场景信息。

Method: 使用MIMIC-IV数据库1000份笔记，评估基于规则的基线方法、领域专用Transformer模型（如BioClinicalBERT）及大型语言模型（如GPT的API版本）的临床笔记分割效果。测试涵盖句子级结构化分割和自由文本分割任务。

Result: 基于API的大型模型（如GPT-5-mini）表现最佳，平均F1分数为72.4。轻量级基线方法在结构化句子级任务中具有竞争力，但无法有效处理非结构化自由文本分割。

Conclusion: 研究结果为方法选择提供指导，表明API驱动的模型更适配复杂分割需求，为下游任务（如信息提取、队列识别、自动生成摘要）奠定基础，并揭示轻量化方案的适用边界。

Abstract: Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.

</details>


### [175] [AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning](https://arxiv.org/abs/2512.22857)
*Shihao Cai,Runnan Fang,Jialong Wu,Baixuan Li,Xinyu Wang,Yong Jiang,Liangcai Su,Liwen Zhang,Wenbiao Yin,Zhen Zhang,Fuli Feng,Pengjun Xie,Xiaobin Wang*

Main category: cs.CL

TL;DR: This paper introduces a unified pipeline and environment-level RL algorithm to enhance simulated environment synthesis for agentic RL, improving training efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Prior simulated RL environments lack automation, task difficulty, and consistency, with unstable simulated users and fragmented environments hindering scalable agent training.

Method: 1) Automated pipeline for scalable, high-difficulty environments (e.g., tau-bench); 2) Environment-level RL algorithm mitigates user instability and uses environment-level advantage estimation.

Result: Validated on tau/two-Bench and VitaBench benchmarks, achieving improved training stability, outperforming baselines, with in-depth out-of-domain generalization analysis.

Conclusion: Proposed framework enables more realistic, scalable agentic training with better stability, bridging gaps between simulated environments and practical agent deployment.

Abstract: Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.

</details>


### [176] [Diversity or Precision? A Deep Dive into Next Token Prediction](https://arxiv.org/abs/2512.22955)
*Haoyuan Wu,Hai Wang,Jiajia Wu,Jinxiang Ou,Keyao Wang,Weile Chen,Zihao Zheng,Bei Yu*

Main category: cs.CL

TL;DR: This paper proposes a generalized pre-training objective for large language models that bridges supervised learning and reinforcement learning (RL), reshaping token-output distributions to enhance reasoning performance by optimizing the exploration space for RL.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning's effectiveness in improving language models is limited by the exploration space defined by pre-trained token distributions. The authors aim to address this by rethinking the standard cross-entropy loss as a policy gradient optimization problem and systematically designing a distribution that better supports RL exploration.

Method: The authors treat next-token prediction as a stochastic decision process and introduce a reward-shaping strategy via a generalized pre-training objective. This includes a positive reward scaling factor to concentrate probabilities on ground-truth tokens and a rank-aware mechanism that asymmetrically handles high- and low-ranking negative tokens, explicitly balancing diversity and precision.

Result: The method reshapes the pre-trained token distribution to create a more favorable exploration space for RL, leading to improved end-to-end reasoning performance. The study reveals that a precision-oriented prior, rather than high entropy, provides superior exploration capabilities.

Conclusion: This work demonstrates that explicitly controlling the token-output distribution during pre-training through RL-inspired reward shaping can significantly enhance the reasoning abilities of language models, challenging the assumption that higher entropy is inherently better for exploration.

Abstract: Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.

</details>


### [177] [Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks](https://arxiv.org/abs/2512.22966)
*Mengdi Chai,Ali R. Zomorrodi*

Main category: cs.CL

TL;DR: 研究评估了三种大型语言模型（ChatGPT-4o、Gemini 1.5 Pro、Llama 3.3 70B）在临床决策流程中的表现，发现其性能因任务和模型参数设置差异显著，并指出提示工程对不同任务效果不一。


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型（LLMs）在真实临床决策中的实用性，因其在医学知识评估中表现良好但实际应用潜力未被充分探索。

Method: 通过36个案例研究，测试三种LLMs在5项临床任务（鉴别诊断、紧急步骤、诊断测试、最终诊断、治疗建议）中的表现，比较默认温度与零温度设置；引入MedPrompt框架测试提示工程（含靶向及随机少样本学习）的影响。

Result: LLMs在不同任务中表现差异显著：最终诊断准确率近乎完美，诊断测试表现差，其余任务中等水平；ChatGPT在零温度下表现更佳，Lama在默认温度下更强。提示工程显著提升诊断测试性能但对其他任务无效，且靶向提示未优于随机提示。

Conclusion: 提示工程对LLM临床决策效果因模型与任务而异，需开发针对性场景的定制化策略以实现医疗场景整合。

Abstract: Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.

</details>


### [178] [Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping](https://arxiv.org/abs/2512.23014)
*Tao Yu,Yongqi An,Kuan Zhu,Guibo Zhu,Ming Tang,Jinqiao Wang*

Main category: cs.CL

TL;DR: FANG通过功能感知神经元分组和动态稀疏度分配，在降低LLM模型规模的同时显著提升下游任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法因依赖有限校准数据导致下游任务泛化能力不足，需要一种更优的后训练剪枝框架缓解校准偏差。

Method: FANG基于功能相似性对神经元分组并独立剪枝，通过动态权重分配强化功能相关token的影响，自适应分配各组稀疏度并保留跨任务关键神经元。

Result: 在30%-40%稀疏度下，FANG联合现有方法提升平均准确率1.5%-8.5%，达到领域SOTA表现。

Conclusion: FANG有效平衡了模型压缩与性能保持，在保留语言建模能力的同时显著增强下游任务适配性。

Abstract: Large Language Models (LLMs) demonstrate impressive performance across natural language tasks but incur substantial computational and storage costs due to their scale. Post-training structured pruning offers an efficient solution. However, when few-shot calibration sets fail to adequately reflect the pretraining data distribution, existing methods exhibit limited generalization to downstream tasks. To address this issue, we propose Function-Aware Neuron Grouping (FANG), a post-training pruning framework that alleviates calibration bias by identifying and preserving neurons critical to specific function. FANG groups neurons with similar function based on the type of semantic context they process and prunes each group independently. During importance estimation within each group, tokens that strongly correlate with the functional role of the neuron group are given higher weighting. Additionally, FANG also preserves neurons that contribute across multiple context types. To achieve a better trade-off between sparsity and performance, it allocates sparsity to each block adaptively based on its functional complexity. Experiments show that FANG improves downstream accuracy while preserving language modeling performance. It achieves the state-of-the-art (SOTA) results when combined with FLAP and OBC, two representative pruning methods. Specifically, FANG outperforms FLAP and OBC by 1.5%--8.5% in average accuracy under 30% and 40% sparsity.

</details>


### [179] [LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models](https://arxiv.org/abs/2512.23025)
*Wenxuan Xu,Arvind Pillai,Subigya Nepal,Amanda C Collins,Daniel M Mackin,Michael V Heinz,Tess Z Griffin,Nicholas C Jacobson,Andrew Campbell*

Main category: cs.CL

TL;DR: 提出了LENS框架，通过将多模态健康传感器数据与语言模型结合，生成临床有意义的心理健康描述，解决了传感器数据处理和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以处理长时间段的传感器时序数据且缺乏传感器-文本配对数据集，亟需将数值型健康数据转化为自然语言描述以支持临床决策。

Method: 构建包含10万传感器-文本问答对的数据集，训练可将原始传感器信号投影到LLM表征空间的补丁编码器，实现传感器时序数据与语言模型的联合优化。

Result: 在标准NLP指标和症状严重度准确性上优于强基线模型，心理健康专业人员评估显示生成描述具有临床完整性和可解释性。

Conclusion: 为大语言模型处理原始健康传感器信号提供了可扩展的解决方案，推动了基于行为信号的临床推理模型发展。

Abstract: Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.

</details>


### [180] [Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization](https://arxiv.org/abs/2512.23032)
*Kerem Zaman,Shashank Srivastava*

Main category: cs.CL

TL;DR: 本文指出Biaseing Features指标可能错误地将不完整的CoT解释为unfaithful，并提出需要综合评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有指标将CoT省略提示注入线索视为unfaithful，但作者认为这可能仅是压缩不完整且受token限制影响。

Method: 在Llama-3/Gemma-3上测试多跳任务，提出faithful@k指标量化token预算影响，并用Causal Mediation Analysis评估非口语化线索的作用

Result: 50%+被Biaseing标记的CoT被其他指标判断为faithful；更高token预算使线索口语化率提升90%；非口语化线索仍可经CoT因果影响预测

Conclusion: 需警惕单一提示评估的局限性，建议结合因果中介分析、破坏测试等多元工具实现更全面的解释性分析

Abstract: Recent work, using the Biasing Features metric, labels a CoT as unfaithful if it omits a prompt-injected hint that affected the prediction. We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative. On multi-hop reasoning tasks with Llama-3 and Gemma-3, many CoTs flagged as unfaithful by Biasing Features are judged faithful by other metrics, exceeding 50% in some models. With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits. Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT. We therefore caution against relying solely on hint-based evaluations and advocate a broader interpretability toolkit, including causal mediation and corruption-based metrics.

</details>


### [181] [Accelerating Language Model Workflows with Prompt Choreography](https://arxiv.org/abs/2512.23049)
*TJ Bai,Jason Eisner*

Main category: cs.CL

TL;DR: Prompt Choreography 是一种通过动态全局KV缓存优化大语言模型（LLM）多智能体工作流的框架，支持并行调用和消息编码复用，显著降低延迟并提升端到端效率。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在多智能体工作流中重复编码消息导致冗余计算，急需一种高效缓存机制减少重复编码开销。

Method: 构建动态KV缓存存储消息编码，允许后续LLM调用按需复用或重新排序已编码内容，并采用微调技术优化模型对缓存编码的适应性。

Result: 在典型场景下首次token延迟降低2.0-6.2倍，端到端流程最高提速2.2倍，且微调后模型能保持复用编码与原始结果的一致性。

Conclusion: 通过编码缓存和并行处理的协同设计，有效解决LLM工作流的计算冗余问题，为分布式智能体协作提供轻量化基础架构支持。

Abstract: Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\times$) in some workflows dominated by redundant computation.

</details>


### [182] [Reservoir Computing inspired Matrix Multiplication-free Language Model](https://arxiv.org/abs/2512.23145)
*Takumi Shiratsuchi,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CL

TL;DR: 本研究提出基于无矩阵乘法语言模型与储备池计算架构的高效模型，减少19%参数量和近10%训练/推理耗时，保持基线模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型计算成本高昂的瓶颈问题，通过架构创新实现训练与推理效率的同步提升

Method: 在MatMul-free LM架构中：1) 部分固定和共享选定层权重 2) 插入无需训练的储备池层获取动态表征 3) 结合运算操作优化内存访问

Result: 实现参数量减少19%、训练时间缩短9.9%、推理时间降低8.0%，性能保持与基线模型相当

Conclusion: 所提架构在计算效率与模型性能间取得平衡，验证了储备池计算与无矩阵架构结合的有效性

Abstract: Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to 19%, training time by 9.9%, and inference time by 8.0%, while maintaining comparable performance to the baseline model.

</details>


### [183] [Not too long do read: Evaluating LLM-generated extreme scientific summaries](https://arxiv.org/abs/2512.23206)
*Zhuoqi Lyu,Qing Ke*

Main category: cs.CL

TL;DR: 提出BiomedTLDR数据集并评估大语言模型（LLMs）生成科学极简摘要的能力，发现LLMs生成的摘要相比人工更偏向提取性而非抽象性。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量的科学极简摘要（TLDR）数据集阻碍了LLMs在摘要生成能力的开发与评估。

Method: 构建BiomedTLDR数据集（包含科研人员撰写的摘要），测试主流开源LLMs基于论文摘要生成TLDR的表现。

Result: 部分LLMs可生成类人摘要，但整体倾向于模仿原文词汇和结构，生成方式更偏向提取性而非抽象性。

Conclusion: 该数据集为LLMs摘要能力研究提供基础，揭示了LLM在科学摘要生成中需提升抽象表达能力以促进科学传播。

Abstract: High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).

</details>


### [184] [Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process](https://arxiv.org/abs/2512.23213)
*Zhijun Chen,Zeyu Ji,Qianren Mao,Junhang Cheng,Bangjie Qin,Hao Wu,Zhuoran Li,Jingzheng Li,Kai Sun,Zizhe Wang,Yikun Ban,Zhu Sun,Xiangyang Ji,Hailong Sun*

Main category: cs.CL

TL;DR: 本文提出LLM-PeerReview，一种无监督的LLM集成方法，通过模拟同行评审机制从多个模型回答中选出最优解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成方法缺乏可解释性和适应性，本文旨在通过引入可解释的无监督框架提升生成结果的质量与泛化能力。

Method: 三阶段流程：1) 多模型协同评分（LLM-as-a-Judge技术）；2) 基于图形模型或均值策略聚合分数；3) 选择最高分回答作为输出。

Result: 在四大数据集上均超越当前先进模型Smoothie-Global（提升6.9%和7.3%），验证了方法的有效性和通用性。

Conclusion: 该方法具有理论简单性和实际高效性，为LLM集成提供了可解释且灵活的新范式。

Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.

</details>


### [185] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: LLMs struggle with complex coding tasks due to ambiguous syntax in general-purpose languages, but a new domain-specific language (Anka) with constrained syntax achieves near-perfect accuracy, outperforming Python by 40% in multi-step tasks.


<details>
  <summary>Details</summary>
Motivation: To address systematic errors in LLMs during complex programming tasks caused by syntax flexibility and implicit state management in general-purpose languages.

Method: Developed Anka, a DSL with explicit syntax for data transformation pipelines, and tested its code generation performance using Claude 3.5 Haiku and GPT-4o-mini without prior training data.

Result: Anka achieved 99.9% parse success and 95.8% task accuracy, with a 40-point accuracy advantage over Python in multi-step tasks (100% vs 60%) and 26.7% improvement in cross-model validation.

Conclusion: LLMs can learn novel DSLs from context, constrained syntax reduces errors in complex tasks, and purpose-designed DSLs can outperform general-purpose languages despite LLMs' training bias toward the latter.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


### [186] [Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation](https://arxiv.org/abs/2512.23260)
*Dianyun Wang,Qingsen Ma,Yuhu Shang,Zhifeng Lu,Lechen Ning,Zhenbo Xu,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: 该论文提出利用预训练的稀疏自编码器(SAE)解耦特征空间，构建可解释的低秩子空间以提升适配器初始化效果。相比传统方法，该方法在安全对齐任务中仅更新0.19-0.24%参数即达到99.6%安全率，同时提供语义可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA等低秩微调方法隐式学习低秩子空间导致不可解释性，本文提出其根本原因在于多义性维度纠缠。通过引入解耦特征空间的可解释子空间构建方法，可显式控制任务相关权重更新。

Method: 1) 使用预训练SAE将参数更新映射到单义性特征空间；2) 利用SAE的编码器权重构建低秩适配器初始化矩阵；3) 理论证明单义性空间可逼近零重构误差，而多义性空间存在误差下限。

Result: 在安全对齐任务中：1) 安全率99.6%（超越全量微调7.4%）；2) 仅调整0.19-0.24%参数量；3) SAES特征实现子空间语义可视化。理论分析表明重构误差与特征维度呈负指数关系。

Conclusion: 通过将机械可解释性引入微调过程，论文实现了性能与透明度的双重提升：既达到接近RLHF的方法效果，又为安全校准提供了可解释的参数调整路径。

Abstract: Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts. To address this, we leverage pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled feature space, then construct an explicit, interpretable low-rank subspace to guide adapter initialization. We provide theoretical analysis proving that under monosemanticity assumptions, SAE-based subspace identification achieves arbitrarily small recovery error, while direct identification in polysemantic space suffers an irreducible error floor. On safety alignment, our method achieves up to 99.6% safety rate--exceeding full fine-tuning by 7.4 percentage points and approaching RLHF-based methods--while updating only 0.19-0.24% of parameters. Crucially, our method provides interpretable insights into the learned alignment subspace through the semantic grounding of SAE features. Our work demonstrates that incorporating mechanistic interpretability into the fine-tuning process can simultaneously improve both performance and transparency.

</details>


### [187] [Chinese Morph Resolution in E-commerce Live Streaming Scenarios](https://arxiv.org/abs/2512.23280)
*Jiahao Zhu,Jipeng Qiang,Ran Bai,Chenyu Liu,Xiaoye Ouyang*

Main category: cs.CL

TL;DR: 本研究提出了LiveAMR任务，通过检测直播中基于发音的变体字违规行为（如虚假广告），构建了首个多达86,790条样本的数据集，利用大模型生成训练数据提升模型性能，证明该方法可显著加强直播监管。


<details>
  <summary>Details</summary>
Motivation: 现有变体字研究集中于文本层面的社交媒体和地下产业，而直播中针对健康医疗领域的语音变体违规尚属空白。主播利用发音变体逃避审查的行为亟需有效检测手段。

Method: 将LiveAMR任务转化为文本到文本生成问题，构建首个含86,790样本的多模态数据集，并通过大语言模型（LLMs）生成额外训练数据，训练模型识别发音变体中的违规线索。

Result: 模型在LiveAMR数据集上表现优异，证明生成训练数据策略有效提升性能，且morph resolution技术能准确识别直播中的发音变体违规，显著增强监管能力。

Conclusion: LiveAMR为直播中基于发音的变体字检测提供了新范式，其效果验证推动了大模型在音形意联合建模中的应用，为治理直播违规内容提供了可扩展的技术框架。

Abstract: E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.

</details>


### [188] [A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation](https://arxiv.org/abs/2512.23356)
*Xin Zhang,Yang Cao,Baoxing Wu,Xinyi Chen,Kai Song,Siying Li*

Main category: cs.CL

TL;DR: The paper introduces SGR, a framework using external subgraphs to improve LLM reasoning by reducing noise through stepwise structured reasoning and knowledge integration.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with reasoning tasks due to noisy or irrelevant information in their training data, leading to factual inconsistencies. Effective reasoning in complex scenarios requires a structured approach to leverage external knowledge and mitigate noise.

Method: SGR dynamically constructs relevant subgraphs from external knowledge bases, guides LLMs to perform stepwise reasoning over these subgraphs, and aggregates multiple reasoning paths to generate the final answer, enhancing traceability and precision.

Result: SGR outperformed strong baselines on benchmark datasets, showing improved accuracy in reasoning tasks compared to conventional LLM approaches.

Conclusion: Structured reasoning with external subgraphs reduces noise impact, significantly enhancing LLMs' logical inference capabilities, as evidenced by superior performance on reasoning benchmarks.

Abstract: Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.

</details>


### [189] [Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data](https://arxiv.org/abs/2512.23422)
*Jiapeng Wang,Yiwen Hu,Yanzipeng Gao,Haoyu Wang,Shuo Wang,Hongyu Lu,Jiaxin Mao,Wayne Xin Zhao,Junyi Li,Xiao Zhang*

Main category: cs.CL

TL;DR: EntroDrop是一种基于熵引导的token丢弃方法，通过动态平衡模型对低熵和高熵token的学习，解决多周期训练中模型性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 在特定领域高质量数据稀缺的背景下，多周期训练成为适配大语言模型的实用策略，但传统自回归模型在重复暴露下会出现预测能力衰退的问题，现有标准正则化方法未能有效解决token层面的学习动态不平衡问题。

Method: 提出 EntroDrop 方法：1）通过计算token熵值选择性遮蔽低熵token实现结构化正则化；2）设计渐进式课程表根据训练阶段动态调整正则化强度，在训练初期保留完整数据，后期逐步增加低熵token的屏蔽比例。

Result: 在参数规模0.6B-8B的模型实验中，相比传统正则化方法，在长周期训练下保持更高准确率（如WikiText-103测试集PPL降低17.2%），同时有效平衡了模型对常见词和复杂词的表示能力。

Conclusion: 研究表明token级正则化的动态调整能显著提升数据受限场景的模型适配效果，为提升大模型在医疗/法律等专业领域的微调稳定性提供了可扩展的技术路径。

Abstract: As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.

</details>


### [190] [The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective](https://arxiv.org/abs/2512.23429)
*Yi Zhao,Yongjun Zhu,Donghun Kim,Yuzhuo Wang,Heng Zhang,Chao Lu,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 性别多样性在科研团队中的影响因角色不同而异，领导角色和辅助角色的多样性与团队影响力呈现倒U型关系，团队规模会调节这一关系。


<details>
  <summary>Details</summary>
Motivation: 解决以往研究将性别多样性简单化的问题，通过区分团队内部角色（领导与辅助）探讨多样性对团队影响力的作用机制。

Method: 基于13万篇PLOS期刊论文的作者贡献声明，将成员分为领导角色与辅助角色，采用多元回归和阈值回归模型分析性别多样性与五年引用量的关联。

Result: 1）领导与辅助团队的性别多样性与影响力呈倒U型关系；2）全女性领导+全男性辅助团队影响力最高；3）小团队中领导多样性负面影响显著，大团队中此效应消失，而辅助团队多样性始终保持显著正向影响。

Conclusion: 角色细分对理解多样性效应至关重要，团队规模是调节领导角色多样性效应的关键变量，但对辅助角色无此限制，提示科研管理需针对性设计多元化策略。

Abstract: The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.

</details>


### [191] [ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning](https://arxiv.org/abs/2512.23440)
*Yuqi Tang,Jing Yu,Zichang Su,Kehua Feng,Zhihui Zhu,Libin Wang,Lei Liang,Qiang Zhang,Keyan Ding,Huajun Chen*

Main category: cs.CL

TL;DR: 提出ClinDEF动态框架，通过模拟诊断对话评估LLM的临床推理能力，弥补现有静态基准不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准聚焦静态问答，无法准确反映医患动态推理过程。此前动态框架数据集有限且缺乏多层级评估体系。

Method: 基于疾病知识图构建患者案例，实现LLM医生与自动化患者多回合交互，创新性引入效率分析与诊断质量量表评估体系。

Result: 实验验证ClinDEF有效暴露主流LLM临床推理缺陷，提供诊断准确性之外的精细化评估维度。

Conclusion: ClinDEF开创了具有临床意义的新型评估模式，能系统量化LLM在动态医疗场景中的真实推理能力。

Abstract: Clinical diagnosis begins with doctor-patient interaction, during which physicians iteratively gather information, determine examination and refine differential diagnosis through patients' response. This dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question-answering. To mitigate these gaps, recent methods explore dynamic medical frameworks involving interactive clinical dialogues. Although effective, they often rely on limited, contamination-prone datasets and lack granular, multi-level evaluation. In this work, we propose ClinDEF, a dynamic framework for assessing clinical reasoning in LLMs through simulated diagnostic dialogues. Grounded in a disease knowledge graph, our method dynamically generates patient cases and facilitates multi-turn interactions between an LLM-based doctor and an automated patient agent. Our evaluation protocol goes beyond diagnostic accuracy by incorporating fine-grained efficiency analysis and rubric-based assessment of diagnostic quality. Experiments show that ClinDEF effectively exposes critical clinical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm.

</details>


### [192] [Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings](https://arxiv.org/abs/2512.23471)
*Thomas Haschka,Joseph Bakarji*

Main category: cs.CL

TL;DR: 本文提出嵌套密度聚类方法，通过LLM嵌入生成分层语义结构树，用于分析文本语料库的全局语义关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的嵌入常用于文本存储与检索，但文本语料库的全局语义关系仍不清晰。需要一种无需预定义类别的数据驱动方法来揭示深层语义结构。

Method: 以LLM嵌入空间中的密度聚类为基础，通过逐步放宽密度标准，将强语义关联的文本聚合成簇并逐层合并，最终构建从密集到弥散的树状分层结构。

Result: 在科学摘要、20新闻组和IMDB影评数据集上验证了方法的鲁棒性，成功实现了学科领域与子领域的分层分类，并展示了在科学计量学和研究主题演化分析中的应用潜力。

Conclusion: 该方法支持无监督、数据驱动的语义结构发现，能够揭示文本数据集的层次化主题分布及其演化规律，为跨领域文本分析提供工具。

Abstract: Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster - the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 News- groups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.

</details>


### [193] [Automatic Detection of Complex Quotation Patterns in Aggadic Literature](https://arxiv.org/abs/2512.23504)
*Hadar Miller,Tsvi Kuflik,Moshe Lavee*

Main category: cs.CL

TL;DR: This paper introduces ACT (Allocate Connections between Texts), a three-stage algorithm designed to improve the automatic detection of biblical quotations in Rabbinic literature. Unlike existing methods, ACT combines alignment algorithms with context-sensitive enrichment to address short/paraphrased quotations.


<details>
  <summary>Details</summary>
Motivation: Existing text reuse frameworks struggle with detecting complex, paraphrased, or structurally embedded citations in morphologically rich and citation-dense traditions like Rabbinic literature. A method bridging machine-based detection and human editorial judgment is needed.

Method: ACT integrates morphology-aware alignment, context-sensitive enrichment (identifying 'Wave'/'Echo' patterns), and configuration variations (ACT-QE, ACT-2, ACT-3 with longer n-grams). It was evaluated against leading systems and human-annotated editions.

Result: ACT-QE achieved F1 score 0.91 (Recall 0.89, Precision 0.94), outperforming baselines. While ACT-2 improved Recall (0.90) but reduced Precision, ACT-3 showed tradeoffs between coverage and specificity.

Conclusion: ACT bridges machine-based detection and human judgment, enabling stylistic pattern classification and opening new avenues for intertextual analysis. It advances digital humanities by addressing methodological gaps in processing citation-dense textual traditions.

Abstract: This paper presents ACT (Allocate Connections between Texts), a novel three-stage algorithm for the automatic detection of biblical quotations in Rabbinic literature. Unlike existing text reuse frameworks that struggle with short, paraphrased, or structurally embedded quotations, ACT combines a morphology-aware alignment algorithm with a context-sensitive enrichment stage that identifies complex citation patterns such as "Wave" and "Echo" quotations.
  Our approach was evaluated against leading systems, including Dicta, Passim, Text-Matcher, as well as human-annotated critical editions. We further assessed three ACT configurations to isolate the contribution of each component. Results demonstrate that the full ACT pipeline (ACT-QE) outperforms all baselines, achieving an F1 score of 0.91, with superior Recall (0.89) and Precision (0.94). Notably, ACT-2, which lacks stylistic enrichment, achieves higher Recall (0.90) but suffers in Precision, while ACT-3, using longer n-grams, offers a tradeoff between coverage and specificity.
  In addition to improving quotation detection, ACT's ability to classify stylistic patterns across corpora opens new avenues for genre classification and intertextual analysis. This work contributes to digital humanities and computational philology by addressing the methodological gap between exhaustive machine-based detection and human editorial judgment. ACT lays a foundation for broader applications in historical textual analysis, especially in morphologically rich and citation-dense traditions like Aggadic literature.

</details>


### [194] [UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?](https://arxiv.org/abs/2512.23512)
*Fengjiao Chen,Minhao Jing,Weitao Lu,Yan Feng,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 研究视觉语言大模型在统一视觉理解与生成任务中的表现，提出UniHetero模型，发现生成语义而非像素可提升理解，并揭示生成任务在数据扩展与利用上的优势。


<details>
  <summary>Details</summary>
Motivation: 探索生成任务是否能在大规模数据上增强视觉理解能力，解决当前研究对生成与理解关系的不足。

Method: 设计简洁统一的UniHetero模型，基于>200M样本进行大规模预训练，分析生成任务对理解的影响及数据扩展规律。

Result: 1. 生成语义提升理解，但生成像素无效果；2. 生成任务展现出更优的数据扩展趋势和利用率；3. 输入嵌入的自回归方法能更好捕捉视觉细节。

Conclusion: 生成任务（尤其是语义生成）能有效促进视觉理解，且生成任务的数据扩展性优于纯理解任务，为统一模型设计提供新方向。

Abstract: Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.

</details>


### [195] [Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias](https://arxiv.org/abs/2512.23518)
*Hazel Kim,Philip Torr*

Main category: cs.CL

TL;DR: MoLaCE是一种轻量级框架，通过混合基于潜在概念的专家来减轻大语言模型的输入确认偏差，通过动态调整激活强度在单一模型中模拟辩论效果，提升鲁棒性并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 已有的LLMs在输入暗示偏好答案时容易强化偏差，多智能体辩论场景中这种偏差会被放大，需要更高效且适用于多输入场景的解决方案。

Method: 提出MoLaCE方法，将不同激活强度的潜在概念专家混合，利用语言组合性动态调整提示词中隐式概念权重，无需修改模型参数即可应对多变输入的动态需求。

Result: 实验证明MoLaCE在多种任务中显著降低确认偏差，提升模型鲁棒性，并在计算效率上接近或超越多智能体框架。

Conclusion: 该方法通过内部机制平衡创新与效率，为减轻LLMs偏差提供了可扩展的实践路径，特别适用于高成本约束下的真实应用场景。

Abstract: Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

</details>


### [196] [Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs](https://arxiv.org/abs/2512.23547)
*Sahil Kale,Antonio Luca Alfeo*

Main category: cs.CL

TL;DR: 本论文提出通过知识图谱提升大语言模型幻觉检测的方法。


<details>
  <summary>Details</summary>
Motivation: 幻觉问题阻碍LLM安全部署，而现有自检方法存在局限，因此探索基于知识图谱的结构化推理方式。

Method: 将LLM响应转化为实体关系知识图谱，通过图结构分析判断陈述正确性，使用GPT-4o和Gemini-2.5-Flash验证。

Result: 相较传统方法，在准确率提升16%、F1-score提升20%，验证了图结构分析对原子化事实验证的有效性。

Conclusion: 知识图谱可显著提升幻觉检测效果，该低成本、模型无关的方案为构建可信LLM提供新路径。

Abstract: Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.

</details>


### [197] [Instruction-Following Evaluation of Large Vision-Language Models](https://arxiv.org/abs/2512.23572)
*Daiki Shiono,Shumpei Miyawaki,Ryota Tanaka,Jun Suzuki*

Main category: cs.CL

TL;DR: 研究发现大型视觉语言模型（LVLMs）在微调后会出现指令遵循能力下降的问题，并提出通过构建包含输出格式指令的训练数据集来缓解此现象。


<details>
  <summary>Details</summary>
Motivation: 当LVLMs与视觉能力整合后，其原本LLM具备的指令遵循能力会明显减弱，导致无法准确执行任务指令，需要探究原因并提出解决方案

Method: 构建包含输出格式说明的新训练数据集，对比测试不同数据集对LVLMs指令遵循能力的影响，采用量化评估方法分析微调效果

Result: 实验证实LVLMs在常规数据集微调后指令遵循能力显著下降，而使用包含输出格式指令的数据集进行训练，模型准确遵循指令的比例明显提升

Conclusion: 在视觉指令微调阶段加入输出格式说明示例，可以有效缓解LVLMs的指令遵循能力退化现象

Abstract: Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.

</details>


### [198] [Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models](https://arxiv.org/abs/2512.23578)
*Yu-Xiang Lin,Cheng-Han Chiang,Hung-yi Lee*

Main category: cs.CL

TL;DR: 该研究发现口语语言模型（SLMs）在多轮对话中无法持续保持指定的口语风格（如情感、口音等），尽管能回忆指令却无法执行，部分通过显式提示可缓解，系统消息中的指令效果差。


<details>
  <summary>Details</summary>
Motivation: 口语模型在人机交互中广泛应用，但保持稳定 speaking风格对提升对话自然度和用户体验至关重要。现有研究缺乏对多轮交互中风格持久性的系统性分析。

Method: 评测3个专有和2个开源SLMs，在多轮对话中指示特定口语风格（情感/口音/音量/速度），检验其风格维持能力；测试指令回忆能力及不同提示策略（系统消息vs用户消息）的效果差异。

Result: 所有模型均无法维持口语风格一致性，显式回忆指令可部分缓解问题；系统消息中的风格指示失效，而用户消息中的指示更有效。

Conclusion: SLMs存在显著的风格遗忘缺陷，揭示了指令记忆与行为执行的分离现象，提示未来需开发动态风格强化机制，并重新评估系统提示的设计范式。

Abstract: In this paper, we show that when spoken language models (SLMs) are instructed to speak in a specific speaking style at the beginning of a multi-turn conversation, they cannot maintain the required speaking styles after several turns of interaction; we refer to this as the style amnesia of SLMs. We focus on paralinguistic speaking styles, including emotion, accent, volume, and speaking speed. We evaluate three proprietary and two open-source SLMs, demonstrating that none of these models can maintain a consistent speaking style when instructed to do so. We further show that when SLMs are asked to recall the style instruction in later turns, they can recall the style instruction, but they fail to express it throughout the conversation. We also show that explicitly asking the model to recall the style instruction can partially mitigate style amnesia. In addition, we examine various prompting strategies and find that SLMs struggle to follow the required style when the instruction is placed in system messages rather than user messages, which contradicts the intended function of system prompts.

</details>


### [199] [A Dataset and Benchmark for Consumer Healthcare Question Summarization](https://arxiv.org/abs/2512.23637)
*Abhishek Basu,Deepak Gupta,Dina Demner-Fushman,Shweta Yadav*

Main category: cs.CL

TL;DR: 本论文提出了CHQ-Sum数据集，包含1507个由领域专家标注的消费者健康问题及总结，旨在解决健康信息理解中的自然语言处理难题，并通过多模型验证数据集有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于消费者健康问题总结的领域专家标注数据集，阻碍了高效自动摘要系统的开发。

Method: 从社区问答论坛收集数据，构建成对标注的消费者健康问题及总结，并在多个现代摘要模型上进行基准测试。

Result: CHQ-Sum数据集提供了理解社交媒体健康内容的资源，实验验证了其对提升自然语言理解的有效性。

Conclusion: 该数据集填补了健康领域自动摘要的数据缺口，并为基于实际用户需求的系统研究提供了基础。

Abstract: The quest for seeking health information has swamped the web with consumers health-related questions. Generally, con- sumers use overly descriptive and peripheral information to express their medical condition or other healthcare needs, contributing to the challenges of natural language understanding. One way to address this challenge is to summarize the questions and distill the key information of the original question. Recently, large-scale datasets have significantly propelled the development of several summarization tasks, such as multi-document summarization and dialogue summarization. However, a lack of a domain-expert annotated dataset for the consumer healthcare questions summarization task inhibits the development of an efficient summarization system. To address this issue, we introduce a new dataset, CHQ-Sum,m that contains 1507 domain-expert annotated consumer health questions and corresponding summaries. The dataset is derived from the community question answering forum and therefore provides a valuable resource for understanding consumer health-related posts on social media. We benchmark the dataset on multiple state-of-the-art summarization models to show the effectiveness of the dataset

</details>


### [200] [Less is more: Probabilistic reduction is best explained by small-scale predictability measures](https://arxiv.org/abs/2512.23659)
*Cassandra L. Jacobs,Andrés Buxó-Lugo,Anna K. Taylor,Marie Leopold-Hooke*

Main category: cs.CL

TL;DR: 该研究探讨语言模型概率与认知现象关系所需上下文量，发现n-gram表示可作为认知规划单元。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确定分析语言模型概率与认知关联时所需的必要/合适上下文长度，聚焦全句是否为必须条件。

Method: 对比全句与n-gram两种表征方式，检验其在概率衰减现象中的认知有效性。

Result: 证明n-gram单元足以观察概率衰减模式，无需完整语句上下文。

Conclusion: n-gram表示可有效作为认知语言处理的基础规划单元，优化模型效率。

Abstract: The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.

</details>


### [201] [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](https://arxiv.org/abs/2512.23684)
*Panagiotis Theocharopoulos,Ajinkya Kulkarni,Mathew Magimai. -Doss*

Main category: cs.CL

TL;DR: 本研究验证大型语言模型（LLMs）在学术评审场景中对多语言隐藏提示攻击的脆弱性差异，发现英语、日语、中文攻击显著影响评审结果，而阿拉伯语攻击效果较弱。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在高影响力场景（如学术评审）中的应用增加，其安全性问题亟需研究。现有研究未充分探索多语言文档级攻击对LLM决策的影响。

Method: 构建500篇ICML论文数据集，在文档中注入四种语言的语义等效隐藏指令，使用LLM生成评审结果并分析评分/决策差异。

Result: 英语、日语、中文注入使评审分数偏离度达28-43%，拒稿率变化最高达21%，阿拉伯语仅造成3%以内的微小波动。

Conclusion: LLM-based评审系统存在安全隐患，语言特异性攻击效果差异显著，需要开发跨语言防御机制。

Abstract: Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.

</details>


### [202] [PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech](https://arxiv.org/abs/2512.23686)
*Deepak Babu Piskala*

Main category: cs.CL

TL;DR: ProfASR-Bench 是一个针对金融、医疗、法律和技术领域专业语音识别的评估套件，揭示了当前语音识别系统在利用上下文信息方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有ASR基准未能充分衡量专业场景下的挑战（如领域术语、正式语域变化、对关键实体错误的零容忍度），需构建更贴合实际的评估框架。

Method: 构建包含提示上下文（领域线索/说话者画像）与多实体目标语句的数据集，支持传统WER指标、实体感知评分及按口音/性别的分层报告，并测试Whisper与Qwen-Omni模型在不同上下文条件下的表现。

Result: 当前ASR系统在普通上下文和对抗性提示下均未有效利用上下文信息，平均词错误率无显著改善，提出“上下文利用率鸿沟”(CUG)概念。

Conclusion: ProfASR-Bench 提供了标准化上下文层级、细粒度评估框架及可复现测试环境，推动跨模型家族的融合策略研究。

Abstract: Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench

</details>


### [203] [Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans](https://arxiv.org/abs/2512.23693)
*Sky CH-Wang,Justin Svegliato,Helen Appel,Jason Eisner*

Main category: cs.CL

TL;DR: 本文提出了一种通过反馈驱动的改进链对语言模型进行微调的方法和数据集。通过逐句修订和结构化偏好监督，该方法优于传统的A/B偏好排序或完整对比重写方法。


<details>
  <summary>Details</summary>
Motivation: 研究目的是改进语言模型的偏好对齐效果。传统方法依赖全局偏好标注或完整重写，效率较低，因此需要一种能利用局部化、目标性编辑的结构化监督方法。

Method: 1) 注释者对模型回复标记喜欢/不喜欢的片段并给出理由；2) 模型从左至右逐步修改不喜欢片段，形成改进链；3) 从改进链相邻步骤构建偏好对进行对齐训练。

Result: 实验显示该方法在偏好调优效率和效果上均优于基线方法，证明结构化反馈和修订机制能更有效捕捉局部优化信号。

Conclusion: 通过结构化修订链生成粒度反馈的监督方式，能够实现更高效的模型偏好对齐，为基于编辑的对齐方法提供了新范式。

Abstract: We present a method and dataset for fine-tuning language models with preference supervision using feedback-driven improvement chains. Given a model response, an annotator provides fine-grained feedback by marking ``liked'' and ``disliked'' spans and specifying what they liked or disliked about them. The base model then rewrites the disliked spans accordingly, proceeding from left to right, forming a sequence of incremental improvements. We construct preference pairs for direct alignment from each adjacent step in the chain, enabling the model to learn from localized, targeted edits. We find that our approach outperforms direct alignment methods based on standard A/B preference ranking or full contrastive rewrites, demonstrating that structured, revision-based supervision leads to more efficient and effective preference tuning.

</details>


### [204] [Eliciting Behaviors in Multi-Turn Conversations](https://arxiv.org/abs/2512.23701)
*Jing Huang,Shujian Zhang,Lun Wang,Andrew Hard,Rajiv Mathews,John Lambert*

Main category: cs.CL

TL;DR: 本文提出了一种多轮对话下的行为触发方法分析框架，发现在线交互方法仅需少量查询即可高效识别LLM行为，远超静态方法，强调动态基准测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM行为评估多集中于单轮对话，但实际应用中多为多轮交互。缺乏有效评估多轮行为触发效率的方法，导致模型潜在问题难以暴露。

Method: 1. 提出包含先验知识、离线交互、在线交互的三类方法论框架 2. 构建多轮对话下的在线方法通用形式 3. 通过自动化测试用例评估三类方法，量化交互次数与触发成功率的权衡

Result: 在线方法在三类任务中分别实现45%/19%/77%的成功率，仅需数千次交互即超越静态方法（后者在基准测试中几乎无法发现缺陷），揭示动态方法能更有效挖掘潜在行为。

Conclusion: 多轮对话评估需转向动态基准测试，验证了在线交互方法的高能效性，建议社区优先发展此类实时学习的评估范式。

Abstract: Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.

</details>
