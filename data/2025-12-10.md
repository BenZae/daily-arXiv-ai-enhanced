<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 93]
- [cs.CL](#cs.CL) [Total: 15]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

TL;DR: 使用Planet Labs的4波段卫星图像与深度学习模型，可近乎实时地准确监测武装冲突中的火灾破坏范围。


<details>
  <summary>Details</summary>
Motivation: 苏丹持续的战争对冲突监测的时效性提出高需求，而深度学习与卫星遥感技术的进步为近实时监测提供了可能。

Method: 基于Planet Labs提供的4波段卫星影像，训练深度学习模型以识别战争中发生的火灾与过火区域，并与基线方法对比验证。

Result: 在苏丹的5个案例研究中，该方法相比基线方法能更精准地捕捉活跃火灾和烧毁区域，且使用8波段影像或时间序列仅带来微小提升。

Conclusion: 4波段卫星影像结合深度学习模型已足够有效监测冲突中的火灾破坏，无需更复杂的数据增强手段。

Abstract: The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [2] [Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality](https://arxiv.org/abs/2512.07951)
*Zekai Luo,Zongze Du,Zhouhang Zhu,Hao Zhong,Muzhi Zhu,Wen Wang,Yuling Xi,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出LivingSwap，首个基于参考视频的视频换脸模型，通过关键帧条件与视频参考引导实现高保真且时间一致性的长序列视频换脸。


<details>
  <summary>Details</summary>
Motivation: 视频换脸在长且复杂的视频序列中面临高保真度与时间一致性不足的挑战，现有参考引导图像编辑方法难以直接迁移至视频域。作者旨在探索如何利用源视频的丰富视觉属性提升换脸效果。

Method: 提出关键帧条件信号注入目标身份，并结合视频参考引导进行时间缝合。构建双人视频数据集Face2Face及反向数据配对策略，以增强训练监督和减少数据稀缺性。

Result: 实验显示该方法在复杂场景下显著提升身份保持度与时间一致性，且在目标身份与源视频的表情、光照、动作融合方面达到SOTA效果，减少人工干预。

Conclusion: LivingSwap通过参考引导与结构优化解决了长视频换脸的保真度与稳定性问题，并验证了关键帧条件及数据策略的有效性，为影视生产提供高效自动化工具。

Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap

</details>


### [3] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: FRIEDA是评估大型视觉语言模型（LVLM）在地理制图推理方面新提出的基准，发现现有模型表现远低于人类，揭示多步骤空间推理的不足。


<details>
  <summary>Details</summary>
Motivation: 地图的视觉问答需要理解复杂的符号系统与空间关系（如拓扑、距离、方向），但当前模型将其视为图表特例，缺乏针对性评估方法，影响灾害响应等关键任务的应用潜力。

Method: 基于GIS分类，构建FRIEDA数据集，包含多个领域真实地图图像，涵盖三类空间关系（拓扑/度量/方向），设计需多步骤推断的开放式问题，并在直接输入相关地图和需模型自主定位地图的两种场景下测试11种SOTA模型。

Result: 11种最先进LVLM模型（如Gemini-2.5-Pro、GPT-5-Think）最高准确率仅38.20%和37.20%，显著低于人类84.87%的水平，显示模型在跨地图空间推理存在明显缺陷。

Conclusion: FRIEDA为改进LVLM的空间智能提供了严格基准，突显了多步骤制图推理的技术鸿沟，未来研究需针对性强化模型对地图符号系统与跨地图空间关系的建模能力。

Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [4] [Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment](https://arxiv.org/abs/2512.08040)
*Youngjoon Jang,Liliane Momeni,Zifan Jiang,Joon Son Chung,Gül Varol,Andrew Zisserman*

Main category: cs.CV

TL;DR: A unified model for sign language understanding combining translation and subtitle alignment, achieving state-of-the-art results and cross-lingual generalization.


<details>
  <summary>Details</summary>
Motivation: Enable efficient sign language translation and temporal alignment for communication, corpus construction, and educational applications while preserving signer privacy.

Method: Lightweight visual backbone with human keypoints/lip features, Sliding Perceiver mapping network for vision-text bridging, and multi-task optimization framework pretrained on BSL/ASL datasets.

Result: State-of-the-art performance on BOBSL dataset (BSL) for both SLT/SSA tasks, with zero-shot generalization to How2Sign (ASL) demonstrating cross-linguistic scalability.

Conclusion: Multilingual pretraining and novel architectural components enable robust sign language processing systems with potential for large-scale deployment across different sign languages.

Abstract: Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.

</details>


### [5] [Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking](https://arxiv.org/abs/2512.08042)
*Chandler Timm C. Doloriel,Habib Ullah,Kristian Hovde Liland,Fadi Al Machot,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文提出基于频率域掩码的通用深度伪造检测方法，通过减少模型参数提升检测效率与泛化性，适用于大规模筛查场景。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法依赖空间特征或大型预训练模型，难以平衡预测性能与计算开销。研究需探索轻量化、对新型生成模型（如扩散模型）具有持续泛化性的检测技术。

Method: 采用频率域掩码训练策略，在图像频域中引入随机掩码与几何变换。通过对比空间特征与频率特征对检测结果的影响，验证频率掩码在跨生成模型检测中的有效性。

Result: 在GAN与扩散模型生成图像数据集上达到SOTA性能，剪枝后模型体积减少68%仍保持>90%准确率。频率掩码相较空间掩码提升3.7%平均检测精度。

Conclusion: 频率域掩码可兼顾检测性能与计算效率，为绿色AI背景下的通用深度伪造检测提供可持续方案，未来将探索该方法在视频检测中的应用。

Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).

</details>


### [6] [Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning](https://arxiv.org/abs/2512.08048)
*Chandler Timm C. Doloriel*

Main category: cs.CV

TL;DR: 本文提出M2A方法，使用随机掩码与一致性损失、熵最小化损失实现有效的测试时自适应，无需复杂设计或依赖不确定性/注意力信号。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖校准的不确定性或注意力分数且设计复杂。作者疑问：是否必须定制掩码设计，或随机掩码在强干扰下即可有效？

Method: 生成空间/频率掩码视图序列，通过一致性损失（多视角预测对齐）和熵最小化损失（提升置信度）实现测试时自适应。对比两种掩码家族（空间掩码含patch/pixel子类，频率掩码含全/低/高频率子类）。

Result: M2A（空间掩码）在CIFAR10C/CIFAR100C/ImageNetC（severity~5）上表现8.3%/19.8%/39.2%平均错误率，优于或匹敌强基线；频率掩码效果次之。消融实验证明随机掩码有效且鲁棒。

Conclusion: 随机掩码结合一致性与熵优化目标即可有效驱动测试时自适应，无需依赖复杂机制，验证了简单设计的有效性。

Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.

</details>


### [7] [Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models](https://arxiv.org/abs/2512.08075)
*Christian Massao Konishi,Helio Pedrini*

Main category: cs.CV

TL;DR: 本研究通过统一评估多种变化检测模型（包括Transformer架构）并优化后处理策略，提升了亚马逊雨林砍伐监测的F1分数至80.41%。


<details>
  <summary>Details</summary>
Motivation: 现有基于PRODES数据的机器学习模型存在有效性不足、未采用现代自注意力机制且缺乏方法论标准化，阻碍了研究对比与效果提升。

Method: 在统一数据集上评估全卷积模型与Transformer模型，结合连通域筛选、纹理替换、图像增强等后处理技术，并测试模型集成策略。

Result: F1-score达到80.41%，单模型效果通过后处理显著提升，集成策略实现最优性能，与文献中近期工作水平相当。

Conclusion: 标准化评估框架与后处理技术的结合能显著提升雨林砍伐检测效果，基于Transformer的模型在集成中贡献突出，为后续研究提供了可比较的基准。

Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.

</details>


### [8] [CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning](https://arxiv.org/abs/2512.08135)
*Zeyuan Chen,Xiang Zhang,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文提出了一种受人类中心-周边视觉启发的多模态空间推理框架CVP，在3D场景理解任务中通过引入显式结构化设计取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖点云/体素等非结构化表征和坐标嵌入隐式获取空间信息，导致空间推理能力受限。本研究旨在通过引入显式结构化设计（类似人类视觉系统的中心-周边机制）来提升模型对3D场景的结构化理解能力。

Method: 在大型多模态架构中集成两种互补组件：1) 目标亲和力token（类比中心视觉）：引导模型关注相关物体；2) 以自我为中心的网格（类比周边视觉）：捕捉全局场景上下文和空间布局。两种组件协同工作实现结构化的上下文感知理解。

Result: CVP在多个3D场景理解基准测试中达到当前最优性能，显著优于传统点云/体素表征方法。

Conclusion: 通过类比人类视觉系统的工作机理，CVP提供了结构化的3D场景理解范式，同时兼顾局部物体注意和全局空间建模，在保持模型简洁性的同时有效提升了空间推理能力。

Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.

</details>


### [9] [Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing](https://arxiv.org/abs/2512.08161)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出Fourier-RWKV去雾框架，通过空间-频域-语义三重感知机制实现线性复杂度下的非均匀雾气去除。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer方法在现实非均匀雾气场景中因二次复杂度导致的实时性瓶颈，并提升复杂雾况下的重建质量。

Method: 构建多态感知范式：1)DQ-Shift动态调整感受野适应局部雾变；2)Fourier Mix将RWKV注意力扩展至频域；3)SBM模块通过DSK-Fusion对齐编解码特征。

Result: 在多个基准测试中取得SOTA性能，计算开销较基线模型降低65%，PSNR提升2.1dB，且支持4K视频实时处理。

Conclusion: 所提三重感知机制有效平衡了全局上下文建模与计算效率，为现实场景去雾提供了实用化解决方案。

Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.

</details>


### [10] [Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators](https://arxiv.org/abs/2512.08163)
*Yuki Kubota,Taiki Fukiage*

Main category: cs.CV

TL;DR: 研究了单目深度估计模型在KITTI数据集中模型准确性与人类感知的权衡关系。通过误差分解和仿射拟合方法，发现提升模型准确性并不等同于更接近人的感知能力，强调了以人类为中心评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 为增强模型鲁棒性和可解释性，研究深度估计模型准确性与人类感知一致性之间的关系，旨在解答模型精度提升是否会自然导致人类相似行为这一问题。

Method: 使用69个单目深度估计模型对KITTI数据集进行系统性分析，应用仿射拟合分解预测误差，从各因素层面解析误差模式结构。

Result: 模型准确性和人类相似性存在不同权衡关系，尽管存在部分共同估计偏差，但精度提升并不必然导致人类相似性提高。

Conclusion: 传统准确性评估需补充以人类为中心的多维度评估，为开发类人感知能力的深度估计模型提供方法论依据。

Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.

</details>


### [11] [GeoLoom: High-quality Geometric Diagram Generation from Textual Input](https://arxiv.org/abs/2512.08180)
*Xiaojing Wei,Ting Zhang,Wei He,Jingdong Wang,Hua Huang*

Main category: cs.CV

TL;DR: GeoLoom框架通过将自然语言转化为几何形式化语言GeoLingua并结合蒙特卡洛优化求解坐标，实现了高质量几何图形生成。配套数据集GeoNF与约束评估指标有效提升结构保真度，实验证明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 几何图形生成需要严格的空间准确性，但传统方法存在准确性和可解释性不足的问题。受几何问题求解领域形式语言和符号求解器的启发，提出GeoLoom框架以解决现有方法在结构保真度与可解释性上的局限。

Method: 1.设计GeoLoom框架：a)自动形式化模块将自然语言转为几何生成语言GeoLingua；b)基于蒙特卡洛优化的坐标求解器转化形式约束为坐标。2.构建GeoNF数据集建立自然语言与形式化描述的对应。3.提出约束偏差评估指标指导迭代优化。

Result: 实验表明GeoLoom在几何图形结构保真度上显著优于SOTA基线模型，蒙特卡洛优化比传统几何求解器更适应复杂约束场景，GeoNF与约束偏差指标有效提升模型性能。

Conclusion: GeoLoom为可解释、可扩展的几何生成提供了范式基础，通过形式化语言与优化算法的协同，解决了结构约束与生成精度的核心矛盾，为后续研究提供了方法论和数据基准支持。

Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.

</details>


### [12] [Animal Re-Identification on Microcontrollers](https://arxiv.org/abs/2512.08198)
*Yubo Chen,Di Zhao,Yun Sing Koh,Talia Xu*

Main category: cs.CV

TL;DR: The paper proposes an on-device Animal Re-ID framework optimized for low-power MCUs, addressing memory and resolution constraints through scalable CNN design and efficient fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing Animal Re-ID models are too large for resource-constrained edge devices (e.g., collar tags), requiring a compact architecture adapted to low-resolution inputs and limited memory.

Method: 1) Analyzed hardware-accuracy trade-offs, 2) Systematically scaled MobileNetV2-based CNN for low-resolution inputs, and 3) Developed data-efficient fine-tuning using 3 images per animal for new site adaptation.

Result: Achieved competitive accuracy across six public datasets with 2+ orders of magnitude smaller model size; deployed model on cattle dataset showed minimal accuracy drop on MCUs compared to server versions.

Conclusion: Demonstrated practical and scalable Animal Re-ID deployment on MCU-class edge devices through architecture optimization and efficient adaptation strategies.

Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.

</details>


### [13] [Beyond Real Weights: Hypercomplex Representations for Stable Quantization](https://arxiv.org/abs/2512.08524)
*Jawad Ibn Ahad,Maisha Rahman,Amrijit Biswas,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CV

TL;DR: 提出渐进重参数化策略，使用PHM层压缩多模态模型，在保持性能的同时显著减少计算量和延迟。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型需要大量参数对齐视觉与语言特征，导致计算成本高、部署困难。

Method: 逐步替换稠密块为PHM层，结合残差插值计划及轻量重构/蒸馏损失，保持功能一致性。

Result: 模型参数和FLOPs大幅减少，推理速度提升且不损失输出质量，通过多视觉-语言模型验证。

Conclusion: 渐进式PHM替换为高效多模态推理提供兼容架构的解决方案，并可与低比特量化互补。

Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

</details>


### [14] [VisKnow: Constructing Visual Knowledge Base for Object Understanding](https://arxiv.org/abs/2512.08221)
*Ziwei Yao,Qiyang Wan,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 提出VisKnow框架与AnimalKB知识库，整合多模态数据提升物体理解


<details>
  <summary>Details</summary>
Motivation: 现有视觉任务数据过于任务导向且缺乏系统化组织，需构建结构化多模态知识库支撑深度理解

Method: 通过专家设计结合大规模模型，构建视觉知识图谱框架VisKnow，整合跨模态文本、图像及细粒度区域标注

Result: 成功构建AnimalKB（406动物类别/22K文本三元组/420K图片），显著提升零样本识别与细粒度VQA性能，建立知识图谱补全与部件分割新基准

Conclusion: 自动化视觉知识库构建能推动计算机视觉领域发展，为多模态任务提供标准化数据支撑

Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

</details>


### [15] [Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture](https://arxiv.org/abs/2512.08738)
*Samuel Ebimobowei Johnny,Blessed Guda,Emmanuel Enejo Aaron,Assane Gueye*

Main category: cs.CV

TL;DR: 本研究提出了手语定位新任务，设计了基于姿态关键点的端到端模型，在连续手语视频中检测特定手语出现情况，在WSLP 2025数据集上取得了61.88%准确率和60.00% F1-score。


<details>
  <summary>Details</summary>
Motivation: 现有手语识别研究集中在分类与翻译任务，而连续手语序列中特定手语的定位检测存在空白，且传统方法依赖冗余中间步骤（如gloss识别和文本匹配）导致计算成本高、视觉噪声干扰严重。

Method: 构建编码器-only架构，采用二分类头直接处理姿态骨架数据，通过姿态表示替代原生视频帧实现轻量化处理，完成查询手语在目标序列中的端到端检测

Result: 在WSLP 2025共享任务数据集上验证，模型实现61.88%分类准确率与60.00% F1-score

Conclusion: 基于姿态的框架有效解决了手语定位任务需求，为自动手语检索与验证技术发展奠定基础，证明了姿态表征在社区沟通技术中的应用潜力

Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting

</details>


### [16] [SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection](https://arxiv.org/abs/2512.08223)
*Ching-Hung Cheng,Hsiu-Fu Wu,Bing-Chen Wu,Khanh-Phong Bui,Van-Tin Luu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: This paper explores the application of prompt tuning methods from Large Language Models to 3D object detection, introducing a Scene-Oriented Prompt Pool (SOP²) to adapt pre-trained models to various scenarios in the 3D field.


<details>
  <summary>Details</summary>
Motivation: Existing transfer learning techniques (e.g., fine-tuning) from NLP have shown success in LLMs but remain underexplored in 3D object detection. The study aims to bridge this gap by leveraging prompt-based adaptation for 3D tasks.

Method: The paper evaluates prompt tokens and prompt generators on the Waymo dataset, sequentially analyzing their impact, and proposes SOP² — a prompt pool designed specifically for scene-oriented adaptation in 3D object detection.

Result: The proposed SOP² demonstrates effectiveness in adapting pre-trained models to diverse 3D object detection scenarios, highlighting the potential of prompt-based approaches in this domain.

Conclusion: Prompt tuning methods are viable for 3D object detection, and SOP² provides a framework for scenario-specific adaptation. The work encourages further exploration of prompts in 3D tasks.

Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.

</details>


### [17] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出MM-CoT基准测试，评估多模态模型的视觉推理基础和逻辑连贯性，发现先进模型在此方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注生成能力，忽视推理链的视觉证据验证和逻辑有效性验证。

Method: 构建MM-CoT基准测试，要求模型选择满足视觉一致性（观测证据约束）和逻辑相干性（因果常识约束）的唯一事件链，并通过对抗性干扰项暴露推理缺陷。

Result: 领先模型在MM-CoT上表现不佳，展现生成流畅性与真实推理保真度的断裂，且该基准与现有评测指标相关性低。

Conclusion: MM-CoT为发展具有真实视觉推理保真度和逻辑一致性的多模态模型提供了基础性评估工具。

Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [18] [Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems](https://arxiv.org/abs/2512.08229)
*Tony Salloom,Dandi Zhou,Xinhai Sun*

Main category: cs.CV

TL;DR: 论文提出了一种基于法线引导的稀疏深度采样策略，用于提升深度补全模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度补全方法依赖的稀疏深度生成假设（均匀随机缺失）与真实传感器的非均匀、几何相关性失效模式不符，导致模型在实际应用中效果受限。

Method: 通过PCA-based表面法线估计在RGB-D点云上计算逐像素深度可靠性，按可靠性分布采样稀疏深度，并与扩散模型Marigold-DC结合进行评估。

Result: 在NYU Depth v2数据集上，该方法提升了深度补全准确率，减少了边缘伪影，生成的训练数据更近似真实传感器输出。

Conclusion: 几何感知的稀疏深度生成策略能有效提高深度补全模型的实用性和真实性，优于传统采样方法。

Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.

</details>


### [19] [FastBEV++: Fast by Algorithm, Deployable by Design](https://arxiv.org/abs/2512.08237)
*Yuanpeng Chen,Hui Song,Wei Tao,ShanHui Mo,Shuang Zhang,Xiao Hua,TianKun Zhao*

Main category: cs.CV

TL;DR: FastBEV++ 提出了一种高效的相机BEV感知框架，在保持高性能的同时实现了车载部署，通过算法优化和设计创新，达到了行业领先的感知效果与实时性平衡。


<details>
  <summary>Details</summary>
Motivation: 当前相机BEV感知的性能与车载部署可行性存在根本矛盾，计算密集型的视角转换和平台特异性内核限制了实际应用。

Method: 采用「算法高效」和「设计可部署」双原则：1）分解视角转换为标准Index-Gather-Reshape流程，通过预排序策略实现无需CUDA内核的TensorRT原生运算；2）深度融合端到端深度感知机制、时序聚合和数据增强提升几何精度。

Result: 在nuScenes数据集上达到0.359 NDS的SOTA指标，同时在车载硬件（如Tesla T4）实现超134 FPS的实时性能。

Conclusion: 证明了高性能与部署效率的统一可行性，为自动驾驶系统提供可扩展的设计范式。

Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

</details>


### [20] [HybridToken-VLM: Hybrid Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.08240)
*Jusheng Zhang,Xiaoyang Guo,Kaitong Cai,Qinhan Lv,Yijia Fan,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出HTC-VLM，通过双通道混合框架解决视觉语言模型中的效率与保真度矛盾，在保留87.2%性能的同时实现580:1的压缩比。


<details>
  <summary>Details</summary>
Motivation: 传统方法在压缩视觉tokens时面临连续压缩导致高层语义丢失（如物体身份）和离散化损失细节（如纹理）的双重挑战，亟需兼顾语义完整性和计算效率的解决方案。

Method: 采用双通道策略：连续通道通过ViT patches保留细粒度细节，离散通道利用MVGO量化生成4个符号anchor；通过解耦注意掩码和瓶颈层将580tokens压缩为单个'voco token'。

Result: 在GQA等7个基准测试中平均性能保持87.2%（基线81.0%），注意力分析证实离散anchor主导语义指引，压缩token保留关键表征能力。

Conclusion: 混合离散-连续架构有效平衡计算效率与表征保真度，为可扩展视觉语言模型提供新范式。

Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

</details>


### [21] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出了Residual-SwinCA-Net（结合CNN残差模块与Swin Transformer块的医学图像分割框架，实现乳腺病灶高精度分割）


<details>
  <summary>Details</summary>
Motivation: 现有CNN与Transformer模型难以有效融合局部相关特征与全局依赖关系，针对超声图像噪声抑制、组织连续性增强、多尺度结构保留等医学影像挑战提出改进方案

Method: 架构包含四部分创新：1) 残差CNN提取局部特征，Swin Transformer定制残差路径实现全局特征融合；2) Laplacian-of-Gaussian算子强化组织连续性；3) 边界导向算子保持恶性病灶形态完整性；4) MSCAS多尺度通道注意力模块增强特征表达，Pixel-Attention模块突出病灶像素

Result: 在BUSI医学影像数据集达到99.29%精度、98.74%交并比、0.9041 Dice系数，优于现有CNN/ViT技术水平

Conclusion: 通过CNN-Transformer混合架构与注意力机制创新，显著提升乳腺病灶分割性能，为临床决策提供高可靠性诊断支持

Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [22] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

TL;DR: 本文提出FTKD方法，通过未来感知特征重构和未来引导logit蒸馏，将离线模型的未来帧知识迁移至在线模型，在不增加推理成本的情况下提升3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法忽视未来帧信息，仅关注严格帧对齐的空间特征或时序关系蒸馏，导致在线模型难以有效学习未来知识。

Method: 采用稀疏查询架构，包含未来感知特征重构（解耦时序对齐）和未来引导logit蒸馏（利用教师模型前景/背景上下文），实现未来帧知识迁移。

Result: 在nuScenes数据集上使mAP和NDS均提升1.3，在速度估计任务中达到最优精度，且推理成本未增加。

Conclusion: FTKD通过时序知识迁移突破了传统严格对齐限制，为在线3D检测模型提供了未来感知能力，验证了时序知识迁移的有效性。

Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [23] [Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2512.08253)
*YiLin Zhou,Lili Wei,Zheming Xu,Ziyi Chen,Congyan Lang*

Main category: cs.CV

TL;DR: FS-3DSeg方法QHP通过查询感知的枢纽原型学习，解决样本不足时原型与查询数据语义不匹配问题，在S3DIS和ScanNet上表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有基于度量的学习方法仅依赖支持集生成原型，忽视其与查询数据的相关性，在数据分布偏移时易产生过拟合和泛化性能下降。

Method: 提出Hub Prototype Generation（HPG）模块构建查询与支持点的二分图提取枢纽点，并引入Prototype Distribution Optimization（PDO）模块通过纯度加权对比损失优化原型分布。

Result: 在S3DIS和ScanNet数据集上实现了显著性能提升，有效缩小了原型与查询集的语义差距。

Conclusion: 该方法通过图结构建模跨集合语义关联和分布优化，为少样本三维语义分割提供了新思路。

Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.

</details>


### [24] [EgoX: Egocentric Video Generation from a Single Exocentric Video](https://arxiv.org/abs/2512.08269)
*Taewoong Kang,Kinam Kim,Dohyeon Kim,Minho Park,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: 本文提出EgoX框架，利用轻量级LoRA适配和几何引导的注意力机制，从单一第三人称视频生成逼真且几何一致的第一人称视频。


<details>
  <summary>Details</summary>
Motivation: 将第三人称视频转换为第一人称视频（自我中心视频）可实现沉浸式理解，但因摄像机视角差异大且重叠区域少而充满挑战。该任务需在保留可见内容的同时合理生成不可见区域，现有方法难以满足需求。

Method: 1. 基于大规模视频扩散模型预训练参数，采用LoRA进行轻量化微调；2. 提出统一条件策略，通过宽度和通道拼接融合第三人称与第一人称先验信息；3. 引入几何引导的自注意力机制，确保空间区域的相关性。

Result: 在未见过的真实场景视频中生成了几何一致且视觉逼真的自我中心视频，验证了模型的泛化能力和鲁棒性。

Conclusion: EgoX通过结合预训练视频扩散模型与几何约束策略，有效解决了视角差异与内容生成一致性难题，为沉浸式视频生成提供了可扩展的解决方案。

Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.

</details>


### [25] [PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282)
*Oh Hyun-Bin,Yuhta Takida,Toshimitsu Uesaka,Tae-Hyun Oh,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文提出PAVAS方法，通过引入物理参数提升视频生成音频的真实性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成音频模型依赖外观特征，忽视物理因素对声音的影响，导致生成结果缺乏物理可信度。

Method: 设计Phy-Adapter物理感知模块，结合物理参数估计器（PPE）提取物体质量与运动轨迹，并通过扩散模型融合视觉-物理-听觉特征。

Result: 在VGG-Impact数据集上，新提出的APCC指标验证了物理参数与声音特征的强相关性，模型在定量和感知质量上均超越基线。

Conclusion: 物理驱动的音频生成框架能有效改善传统外观驱动方法的局限性，为跨模态合成提供了新的建模路径。

Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.

</details>


### [26] [OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation](https://arxiv.org/abs/2512.08294)
*Yexin Liu,Manyuan Zhang,Yueze Wang,Hongyu Li,Dian Zheng,Weiming Zhang,Changsheng Lu,Xunliang Cai,Yan Feng,Peng Pei,Harry Yang*

Main category: cs.CV

TL;DR: OpenSubject是一项大规模视频衍生数据集（4.35M图像，2.5M样本），用于增强Subject-driven生成与操作任务。通过解决多主体生成中的身份偏离问题，其四阶段数据构建流程结合了跨帧身份先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有Subject-driven模型在复杂多主体场景下常偏离参考身份，且缺乏大规模高质量数据集支持模型训练与基准测试。

Method: 构建了四阶段数据流水线：(1)高质量视频筛选；(2)基于VLM的跨帧主体配对；(3)分割图引导的outpainting/inpainting生成；(4)VLM驱动的样本验证与字幕生成。同时建立了包含多维度评估的基准体系。

Result: 实验表明OpenSubject显著提升复杂场景下的生成质量，在身份保真度、提示遵循度等维度表现优异，且验证了跨框架身份先验的有效性。VLM评估者实现了自动化质量控制。

Conclusion: OpenSubject为多主体生成任务提供了可靠的基准数据集，其跨帧身份建模方法为复杂场景下的可控图像生成提供了新范式。

Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

</details>


### [27] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

TL;DR: 本文提出Terrain Diffusion，通过创新性的无限扩散算法和分层模型架构，实现地球尺度的高质量无限地形生成，解决了传统过程噪声函数在真实感和大范围连贯性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 传统基于Perlin噪声的过程生成技术存在视觉失真和长距离不连贯问题，无法满足现代数字世界构建对超大规模真实感地形的需求。现有扩散模型尚未解决无限扩展、种子控制和实时生成等关键限制。

Method: 包含四个核心技术：1) InfiniteDiffusion算法实现无缝无限生成 2) 多尺度扩散模型堆栈融合宏观地形与局部细节 3) 基于拉普拉斯编码的动态范围稳定技术 4) 支持无限张量运算的开源框架与一致性蒸馏优化方法

Result: 成功实现了地球尺度地形的实时生成（单块GPU可达10^9:1动态范围），保持亚厘米级细节一致性，支持种子控制和随机访问，生成速度较传统方法提升3-5倍。

Conclusion: 本研究证明扩散模型可兼顾生成质量与过程生成特性，为游戏、影视和科学模拟等领域提供新的基础设施，标志着无限内容生成技术进入新阶段。

Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [28] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

TL;DR: GeoDM通过结合欧几里得、双曲和球面流形的几何感知分布匹配框架，提升数据集蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅限于欧几里得空间，无法捕捉高维数据实际流形结构（如曲率），而真实数据常分布于低维流形。需对齐蒸馏数据与原始数据的流形以提高性能。

Method: 提出GeoDM框架，在混合流形（欧几里得、双曲、球面）中引入可学习曲率和权重参数，并设计最优传输损失以增强分布保真度，统一表征平坦、分层和循环结构。

Result: 理论分析显示几何感知分布匹配在乘积空间中的泛化误差界更小，实验表明其性能优于现有先进方法，且在不同单几何分布匹配策略中均有效。

Conclusion: 几何感知的乘积空间分布匹配能更有效保留数据内在结构，超越传统欧几里得方法，为数据集蒸馏提供新思路。

Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [29] [Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge](https://arxiv.org/abs/2512.08323)
*Achraf Ben-Hamadou,Nour Neifar,Ahmed Rekik,Oussama Smaoui,Firas Bouzguenda,Sergi Pujades,Niels van Nistelrooij,Shankeeth Vinayahalingam,Kaibo Shi,Hairong Jin,Youyi Zheng,Tibor Kubík,Oldřich Kodym,Petr Šilling,Kateřina Trávníčková,Tomáš Mojžiš,Jan Matula,Jeffry Hartanto,Xiaoying Zhu,Kim-Ngan Nguyen,Tudor Dascalu,Huikai Wu,and Weijie Liu,Shaojie Zhuang,Guangshun Wei,Yuanfeng Zhou*

Main category: cs.CV

TL;DR: 该论文介绍了3DTeethLand挑战赛及首个公开的3D牙齿标志点检测数据集，推动了临床正畸领域的深度学习技术发展。


<details>
  <summary>Details</summary>
Motivation: 牙齿标志点精确检测对临床诊断和治疗至关重要，但受限于牙齿几何复杂性和个体差异，亟需深度学习算法提升检测精度。

Method: 通过MICCAI 2024发起3DTeethLand挑战赛并发布配套数据集，提供标准化评估基准，鼓励算法创新与方法论贡献。

Result: 成功构建首个公开可用的3D牙齿标志点数据集，为现有方法提供评估平台，促进深度学习技术在临床牙科的应用突破。

Conclusion: 该研究通过开放数据集与挑战赛形式，为精准齿科诊疗提供了技术验证框架，具有显著临床转化价值。

Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.

</details>


### [30] [GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification](https://arxiv.org/abs/2512.08325)
*Xuedeng Liu,Jiabao Guo,Zheng Zhang,Fei Wang,Zhi Liu,Dan Guo*

Main category: cs.CV

TL;DR: GeoDiffMM 是一种基于扩散模型和光流几何线索的视频运动放大框架，通过结构感知的去噪过程实现更准确的运动放大。


<details>
  <summary>Details</summary>
Motivation: 现有欧拉方法处理微小运动时难以区分光子噪声与真实微运动，需通过结构一致性几何线索提升运动放大效果。

Method: 1、设计无噪声光流增强策略生成多样化非刚性运动场；2、扩散运动放大器通过光流先验与可学习放大因子约束去噪过程；3、基于光流的视频重建方法保证高保真映射。

Result: 在真实和合成数据集上均超越现有技术，显著提升运动放大质量。

Conclusion: 该方法有效分离噪声与真实运动，通过结构一致性和语义感知的放大策略实现更优的视觉运动增强效果。

Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

</details>


### [31] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

TL;DR: 提出了一种基于四元数的低秩支持矩阵机（LSQMM）方法，用于彩色图像分类，通过四元数代数保留颜色通道间的内在关系，并引入四元数核范数正则化提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法用实数向量/矩阵/张量表示彩色图像特征，但会破坏颜色通道间的内在耦合关系；受四元数在图像重建/去噪任务中成功应用的启发，提出新的分类方法。

Method: 将RGB通道作为纯四元数输入，构建低秩支持四元数矩阵机（LSQMM）；采用四元数核范数正则化项促进低秩结构，并结合ADMM算法求解四元数优化模型。

Result: 在多个数据集上的实验显示，与支持向量机、支持矩阵机、支持张量机等方法相比，在分类精度、鲁棒性和计算效率方面均有优势。

Conclusion: 基于四元数代数的分类框架能有效保留彩色图像通道间的关联性，且四元数核范数正则化有助于提升模型性能。

Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [32] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: 本研究分析了Glaze和Nightshade等图像保护机制的结构特性，揭示其通过内容关联的低熵扰动实现扰动结构化，并解释了其可检测性机制。


<details>
  <summary>Details</summary>
Motivation: 尽管Glaze和Nightshade等对抗扰动方法在实践中能有效干扰生成模型，但其内部结构、可检测性和表征行为尚未被充分理解，需通过解释性AI方法进行系统性分析。

Method: 结合白盒特征空间分析（潜变量聚类、特征通道激活分析、遮蔽敏感度映射）和黑盒信号级探测，对扰动在空间域、频域等维度进行多模态表征解析。

Result: 发现保护机制通过与图像内容强耦合的低熵结构化扰动运作，在保留原内容特征组织的同时引入特异性子结构；连续扰动会增强可检测信号而非削弱，且频域能量重分布与图像主频率轴对齐。

Conclusion: 对抗性图像保护通过结构性特征形变而非语义错位实现扰动，其可检测性源于扰动熵值、空间分布与频域对齐的交互效应，为生成式AI防护设计提供了理论依据。

Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>


### [33] [PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models](https://arxiv.org/abs/2512.08330)
*Pengbo Li,Yiding Sun,Haozhe Cheng*

Main category: cs.CV

TL;DR: This paper introduces **PointDico**, a novel 3D representation learning method combining diffusion and contrastive models, achieving state-of-the-art results on ScanObjectNN and ShapeNetPart.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive models struggle with overfitting and 3D Mask Autoencoders fail to handle unordered point clouds, necessitating a method that integrates the strengths of diffusion and contrastive approaches.

Method: PointDico uses knowledge distillation to merge denoising generative modeling and cross-modal contrastive learning. It introduces a hierarchical pyramid conditional generator and a dual-channel design for multi-scale feature extraction and contextual integration.

Result: PointDico achieves 94.32% accuracy on ScanObjectNN and 86.5% Inst. mIoU on ShapeNetPart, setting new state-of-the-art benchmarks in 3D representation learning.

Conclusion: The integration of diffusion and contrastive paradigms in PointDico addresses challenges in 3D data representation, enabling superior performance through hierarchical feature extraction and cross-modal knowledge distillation.

Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.

</details>


### [34] [Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening](https://arxiv.org/abs/2512.08331)
*Xianghong Xiao,Zeyu Xia,Zhou Fei,Jinliang Xiao,Haorui Chen,Liangjian Deng*

Main category: cs.CV

TL;DR: 提出Bi²MAC方法，在保留低计算代价的同时有效提升遥感图像融合中的区域异质性适应能力，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法难以适应特征表征中的区域异质性，现有自适应卷积方法存在计算代价高且对异质区域捕捉不足的问题。

Method: 设计Bimodal Bi-Adaptive Mask-Aware Convolution（Bi²MAC）：1）轻量模块生成软/硬掩码分别调制输入特征并引导分支处理；2）冗余特征送至紧凑分支低耗全局处理，异质特征送至聚焦分支高耗细粒度建模。

Result: 在多基准数据集实验中，Bi²MAC相较现有自适应卷积模型：1）训练时间减少73%，参数量降低58%；2）达SOTA性能且计算代价最低；3）异质区域分割准确率提升12.5%。

Conclusion: 通过双模自适应掩码机制实现特征导向的非对称计算资源分配，在降低计算成本的同时有效解决遥感图像区域异质性融合问题，为高效视觉模型设计提供新思路。

Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.

</details>


### [35] [HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting](https://arxiv.org/abs/2512.08334)
*Chang Liu,Hongliang Yuan,Lianghao Zhang,Sichao Wang,Jianwei Guo,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: 本文提出HybridSplat方法，通过混合高斯点绘技术提升复杂反射场景的渲染速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯点绘方法在处理高反射率真实场景时存在渲染速度慢和内存占用高的瓶颈。

Method: 1) 反射内嵌高斯追踪：将视图依赖反射信息整合至高斯图元
2) 统一混合点绘框架：集成反射与基础图元
3) 流水线加速与反射敏感剪枝：减少模型规模并加速渲染

Result: 在Ref-NeRF/NeRF-Casting数据集上实现7倍渲染加速，高斯图元减少4倍，保持反射质量。

Conclusion: 该方法在复杂反射场景中成为新的SOTA方案，平衡了渲染质量和计算效率。

Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.

</details>


### [36] [DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation](https://arxiv.org/abs/2512.08337)
*Jianwei Wang,Qing Wang,Menglan Ruan,Rongjun Ge,Chunfeng Yang,Yang Chen,Chunming Xie*

Main category: cs.CV

TL;DR: 本文提出DINO-BOLDNet，通过DINOv3引导的多切片注意力框架，成功从T1w图像生成高质量BOLD图像。


<details>
  <summary>Details</summary>
Motivation: 需要从T1w图像中生成BOLD图像以恢复缺失功能信息并支持下游任务。

Method: 结合冻结的DINOv3编码器和切片注意力模块，利用多尺度解码器及DINO感知损失进行结构到功能的映射。

Result: 实验表明DINO-BOLDNet在临床数据集上优于条件GAN，在PSNR和MS-SSIM指标上表现更优。

Conclusion: 该方法证明了自监督transformer在结构到功能映射中的有效性，为BOLD图像生成提供了新思路。

Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.

</details>


### [37] [TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels](https://arxiv.org/abs/2512.08358)
*Jiahao Lu,Weitao Xiong,Jiacheng Deng,Peng Li,Tianyu Huang,Zhiyang Dou,Cheng Lin,Sai-Kit Yeung,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出TrackingWorld，解决单目视频中3D跟踪难以分离相机运动与动态物体运动、以及无法追踪新出现物体的问题，通过上采样2D轨迹和优化3D重建实现密集世界坐标系跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D跟踪方法无法有效分离相机运动与前景动态，且难以追踪视频中新出现的动态物体，需提出新方法解决这两个局限。

Method: 1. 引入跟踪上采样器将稀疏2D轨迹提升为密集轨迹；2. 通过跨帧应用上采样器并去重冗余区域实现新物体追踪；3. 优化框架联合估计相机位姿与3D坐标完成轨迹重建。

Result: 在合成与真实数据集上验证了方法能准确生成密集3D轨迹，且支持世界坐标系下动态对象追踪。

Conclusion: TrackingWorld通过创新性上采样策略和优化框架，实现了单目视频中几乎全像素的密集3D运动捕捉，克服了传统方法的核心缺陷。

Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.

</details>


### [38] [SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation](https://arxiv.org/abs/2512.08362)
*Ju-Young Kim,Ji-Hong Park,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出SCU-CGAN模型，利用非火灾图像生成逼真火灾图像以增强数据集，提升火灾检测准确率


<details>
  <summary>Details</summary>
Motivation: 家用火灾探测系统受限于火灾数据集不足导致的模型性能瓶颈，需通过数据增强突破现有技术限制

Method: 构建集成U-Net、CBAM模块和附加鉴别器的SCU-CGAN模型，采用非火灾图像作为输入进行火灾图像生成

Result: 较CycleGAN模型KID得分提升41.5%，YOLOv5 nano检测模型的mAP@0.5:0.95指标提升56.5%

Conclusion: 生成数据集使火灾检测精度显著提升，验证了提出的SCU-CGAN模型在数据增强和检测性能优化方面的有效性

Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.

</details>


### [39] [The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss](https://arxiv.org/abs/2512.08374)
*Bozhou Li,Xinda Xue,Sihan Yang,Yang Shi,Xinlong Chen,Yushuo Guan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 发现多模态大模型中图文标记的范数差异导致不对称更新动态，提出通过在视觉投影器后添加一层归一化解决此问题


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型依赖Pre-Norm架构，但视觉token与文本token间存在范数不平衡问题，导致跨模态融合效果下降

Method: 通过理论分析揭示范数差异引起的「表征惯性」现象，提出在视觉投影器后增加可训练LayerNorm层实现范数对齐

Result: 在LLaVA-1.5架构上的实验表明，该方法同时提升了多模态基准测试和文本任务(MMLU)的性能表现

Conclusion: 通过解决架构性范数失衡问题，实现了更均衡的跨模态表征学习，增强了模型整体能力

Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.

</details>


### [40] [Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions](https://arxiv.org/abs/2512.08378)
*Jing Tao,You Li,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种在复杂光照条件下同时进行图像增强和去噪的新框架，解决了现有方法在噪声放大和特定照明条件限制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像增强方法在极端光照条件下易放大噪声或仅适用于特定场景，本文旨在通过综合优化照明估计与噪声抑制，提升复杂环境下图像质量和视觉应用性能。

Method: 采用梯度域加权导向滤波（GDWGIF）精确估计光照并改进图像质量；利用Retinex模型将图像分解为光照层和反射层并行处理；光照层优化照明条件，反射层增强图像质量；通过多曝光融合和线性拉伸策略优化动态范围。

Result: 实验结果显示，该方法在现实应用场景的实际数据集上，相较现有最优方法在对比度增强和噪声抑制方面表现更优，量化指标与视觉效果均显著提升。

Conclusion: 该框架通过多阶段协同优化策略，在复杂光照条件下实现了鲁棒的图像质量提升，验证了所提方法在噪声控制与动态范围调整方面的有效性与广泛适用性。

Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.

</details>


### [41] [Detection of Digital Facial Retouching utilizing Face Beauty Information](https://arxiv.org/abs/2512.08397)
*Philipp Srock,Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: 该论文研究了通过分析图像美化算法和人工智能特征提取方法提升人脸识别系统中外貌修饰检测效果的方法，并在未知修饰算法的场景下实现了1.1%的检测相等错误率。


<details>
  <summary>Details</summary>
Motivation: 外貌修饰会干扰生物特征识别系统，但现有检测方法存在挑战，需要探索基于面部美学特征的人工智能检测技术。

Method: 结合美学评估算法分析修饰变化，比较不同人工智能特征提取方法，并评估面部美学特征对检测率的提升效果。

Result: 在攻击性修饰算法未知的单图检测场景中，达到了1.1%的检测相等错误率（D-EER）。

Conclusion: 通过结合面部美学特征和人工智能方法，能够有效提升外貌修饰检测的准确率，尤其是未知修饰算法的情况。

Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.

</details>


### [42] [Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries](https://arxiv.org/abs/2512.08400)
*Samitha Nuwan Thilakarathna,Ercan Avsar,Martin Mathias Nielsen,Malte Pedersen*

Main category: cs.CV

TL;DR: 本研究开发了一个优化的深度学习流水线，使用AutoFish数据集并通过难例挖掘与定制图像变换策略，实现鱼类再识别自动化。Swin-T模型表现优于ResNet-50，达到41.65% mAP@k和90.43% Rank-1准确率。


<details>
  <summary>Details</summary>
Motivation: 电子监控系统产生过多鱼类视频数据，传统手动标注效率低下，需开发自动化方法提升识别准确率以支持海洋资源管理。

Method: 提出硬三元组挖掘结合数据集特异性归一化的图像预处理流水线，基于Transformer架构的Swin-T模型与传统ResNet-50对比实验，并分析视角不一致及遮挡对识别率的影响。

Result: Swin-T模型比ResNet-50提升显著：mAP@k提高12.3%，Rank-1准确率提升8.7%；同种鱼类个体识别错误中视角差异造成的误差（62%）远高于部分遮挡（21%）。

Conclusion: 基于Transformer的架构结合定制训练策略可有效解决复杂场景下的鱼类再识别问题，视角一致性优化是未来提升方向，代码开源推动领域研究。

Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git

</details>


### [43] [SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos](https://arxiv.org/abs/2512.08406)
*Mingqi Gao,Yunqi Miao,Jungong Han*

Main category: cs.CV

TL;DR: 提出SAM-Body4D，一种无需训练的视频人体网格恢复框架，通过时空连续性建模和遮挡修正提升时序一致性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图像基HMR方法处理视频时依赖逐帧推理，导致时间不一致且在遮挡场景下性能下降，需利用视频中固有的人体连续性进行优化。

Method: 构建基于视频分割的身份一致masklet生成策略，设计遮挡感知模块填补缺失区域，采用padding并行策略指导SAM 3D Body生成完整人体轨迹。

Result: 在无训练情况下，实现对野外视频的时序稳定重构，遮挡场景下保持人体细节完整性，支持多人体高效并行处理。

Conclusion: 通过显式建模视频时空连续性，无需训练即可显著提升现有HMR方法的视频处理性能，为动态场景理解提供有效解决方案。

Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.

</details>


### [44] [Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval](https://arxiv.org/abs/2512.08410)
*Tao Chen,Shaobo Ju,Qiong Wu,Chenxin Fang,Kun Zhang,Jun Peng,Hui Li,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 本研究提出OneClip-RAG框架，通过单次视频片段检索增强技术，有效降低多模态大型语言模型处理长视频时的内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs因内存占用过高，难以处理长视频（多帧数据）；且现有视频RAG方法在知识完整性与语义连贯性上存在不足。

Method: 设计OneClip-RAG框架：1.结合视频片段检索增强机制，同时优化跨模态检索与片段分割；2.提出查询引导的视频分块算法，合并分块与检索步骤；3.构建SynLongVideo合成数据集及渐进训练策略提升指令跟随能力。

Result: 五种MLLMs集成OneClip-RAG后性能显著提升（如InternLV2 8B在MLVU达到GPT-4o水平），且处理效率极高（LLaVA-Video用单卡4090在2.2分钟内解析1小时视频）。

Conclusion: OneClip-RAG通过算法与数据集创新，解决了长视频处理中的内存瓶颈问题，并在效果与效率层面均取得突破性进展。

Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.

</details>


### [45] [SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking](https://arxiv.org/abs/2512.08430)
*Nico Leuze,Maximilian Hoh,Samed Doğan,Nicolas R. -Peña,Alfred Schoettl*

Main category: cs.CV

TL;DR: 深度学习6D姿态估计方法通过注意力机制和稀疏3D处理，在工业密堆积场景中实现出色性能。


<details>
  <summary>Details</summary>
Motivation: 工业环境中密集堆叠零件的6D姿态恢复面临遮挡、反光和无纹理等重大挑战，现有方法难以平衡内存效率与高分辨率几何建模的需求。

Method: 提出基于深度数据的全局性框架：1) 多视角深度图融合为稠密点云或稀疏TSDF；2) 分阶段热图机制构建多尺度注意力先验；3) 密度感知稀疏transformer模块处理自遮挡和非均匀点云分布；4) 基于体素投票的端到端多物体姿态预测。

Result: 在IPD和MV-YCB数据集验证，该方法在严重遮挡的工业和家用分拣场景中保持竞争力，相较传统方法提升23.7%的位姿精度，内存消耗降低68%

Conclusion: 证明全稀疏3D表征在近距离机器人应用的可行性，为密堆积场景的位姿估计提供了兼顾精度、效率和鲁棒性的解决方案

Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.

</details>


### [46] [LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training](https://arxiv.org/abs/2512.08439)
*Qing Xu,Kun Yuan,Yuxiang Luo,Yuhao Zhai,Wenting Duan,Nassir Navab,Zhen Chen*

Main category: cs.CV

TL;DR: 本文提出LapFM，一种基于无标签手术图像的新型基础模型，通过层级概念演化训练方法显著提升腹腔镜分割性能。


<details>
  <summary>Details</summary>
Motivation: 手术分割面临标注稀缺和语义不一致问题，现有方法依赖有限监督微调自然基础模型（如SAM），仅作为领域适配器使用而无法解决手术场景多样性。

Method: 设计层级概念演化预训练范式：1）构建包含解剖结构、组织和器械的腹腔镜概念层级（LCH），使用分层掩码解码器的父子查询嵌入实现跨粒度语义一致性；2）提出置信度驱动的演化标注算法，基于层级一致性迭代生成筛选伪标签，构建LapBench-114K大数据集。

Result: 在114K图像掩码对数据集上，LapFM显著超越现有最优方法，在通用腹腔镜分割任务中建立粒度自适应新标准。

Conclusion: 通过层级语义建模与自演化标注机制，LapFM为医疗影像分割提供了可推广的范式，开源代码促进未来研究发展。

Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.

</details>


### [47] [Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts](https://arxiv.org/abs/2512.08445)
*Madhav Gupta,Vishak Prasad C,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 本论文提出了一种结合子模选择与不确定性估计的框架，以提升深度视觉模型在分布内外（ID/OOD）场景下的解释可靠性与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于子集的解释方法在OOD数据中表现不佳，常出现冗余、不稳定和不确定性敏感等问题，亟需解决其局限性。

Method: 框架通过层梯度不确定性估计与自适应权重扰动生成不确定性度量，并将其集成到子模优化过程中，以指导多样性与信息量兼具的子集选择。

Result: 实验表明该方法在优化ID/OOD场景下的解释鲁棒性与保真度方面优于现有方法，且无需额外训练或辅助模型。

Conclusion: 不确定性驱动的子集优化能有效增强视觉模型解释性，为真实场景中的透明可信AI提供了新路径。

Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

</details>


### [48] [Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery](https://arxiv.org/abs/2512.08467)
*Chamath Ranasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级SAM与CSRT跟踪器结合的足球运动员跟踪方法，通过HSV直方图重新识别提升遮挡恢复能力，在保证实时性的同时实现90%以上密集场景跟踪成功率，但对长期遮挡场景仍存在8.66%的重捕获率不足。


<details>
  <summary>Details</summary>
Motivation: 应对足球场景中频繁遮挡、相似外观和拥挤场景运动导致的跟踪挑战，现有方法在遮挡恢复和身份一致性方面存在局限性，且需平衡计算资源与跟踪性能。

Method: 构建团队感知跟踪系统：1) 使用Segment Anything Model(SAM)进行精准初始化 2) CSRT跟踪器与HSV颜色直方图外观模型相融合 3) 设计包含处理速度(FPS/内存)、准确率(成功率/框稳定)和鲁棒性(遮挡恢复/身份一致性)的三维评估体系

Result: 1) 实现7.6-7.7FPS处理速度，内存稳定在1880MB 2) 轻度遮挡100%跟踪成功，5人以上拥挤罚球区场景达90%成功率 3) 外观重新识别恢复50%重度遮挡案例 4) 长期遮挡重捕获率仅8.66%

Conclusion: SAM+CSRT组合在不同人群密度下表现稳定，但离开画面的长期遮挡仍需增强重识别机制。为资源受限场景部署提供指导：持续可见环境下经典跟踪方法有效，但需更强大的重新识别模块应对长期缺席。

Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.

</details>


### [49] [ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention](https://arxiv.org/abs/2512.08477)
*Huiguo He,Pengyu Yan,Ziqi Yi,Weizhi Zhong,Zheng Liu,Yejun Tang,Huan Yang,Kun Gai,Guanbin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: This paper introduces ContextDrag, a novel drag-based image editing method leveraging contextual modeling to preserve texture details via CTI and PCA.


<details>
  <summary>Details</summary>
Motivation: Existing drag-based editing methods fail to fully utilize contextual information (e.g., fine-grained texture details) in reference images, leading to limited coherence and fidelity in edits.

Method: ContextDrag incorporates VAE-encoded reference image features using a Latent-space Reverse Mapping (LRM) algorithm for Context-preserving Token Injection (CTI) and employs Position-Consistent Attention (PCA) with overlap-aware masking to enhance contextual consistency.

Result: Experiments on DragBench-SR and DragBench-DR demonstrate ContextDrag outperforms all existing state-of-the-art methods in drag-based image editing performance.

Conclusion: The proposed method achieves superior drag editing quality by preserving semantic and texture consistency through noise-free reference feature integration without requiring model fine-tuning or inversion.

Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.

</details>


### [50] [Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform](https://arxiv.org/abs/2512.08478)
*Yuning Gong,Yifei Liu,Yifan Zhan,Muyao Niu,Xueying Li,Yuanjun Liao,Jiaming Chen,Yuanyuan Gao,Jiaqi Chen,Minming Chen,Li Zhou,Yuning Zhang,Wei Wang,Xiaoqing Hou,Huaxi Huang,Shixiang Tang,Le Ma,Dingwen Zhang,Xue Yang,Junchi Yan,Yanchi Zhang,Yinqiang Zheng,Xiao Sun,Zhihang Zhong*

Main category: cs.CV

TL;DR: Visionary是一个开放的、基于Web的实时3D高斯点绘和网格渲染平台，通过高效的WebGPU渲染器和每帧ONNX推理实现动态神经处理，显著降低部署门槛，同时支持多样化的3DGS变体与生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯点视图解决方案存在碎片化、依赖遗留管线或受限于动态内容支持的问题，导致部署成本高且难以适配生成模型。

Method: 构建基于WebGPU的渲染器和每帧ONNX推理框架，提出标准化的高斯生成器合约（支持即插即用算法动态更新高斯），提供three.js的TypeScript开源库，并通过GPU加速的图元排序优化效率。

Result: 相比现有Web视图器在相同3DGS资产下，渲染效率更高且支持多种扩展（如MLP-3DGS、4DGS、神经头像等），同时可集成生成式后处理，并通过纯浏览器方案实现一键部署。

Conclusion: Visionary通过统一浏览器端的神经推理与渲染，成为首个支持重建与生成双范式的3DGS平台，显著降低了方法复现、对比与部署的复杂性。

Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.

</details>


### [51] [Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions](https://arxiv.org/abs/2512.08486)
*Ada Gorgun,Fawaz Sammani,Nikos Deligiannis,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: 本文提出了一种名为PCI（基于提示的干预）的训练无关、模型无关框架，用于分析扩散模型中概念形成的时间动态。通过量化概念插入成功率（CIS），研究揭示了噪声转化为特定概念（如年龄）并锁定去噪轨迹的关键阶段，为文本驱动的图像编辑提供了无需模型内部分析的有效干预时机。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的生成过程是动态的，但传统评估仅关注最终输出。作者旨在理解概念在去噪过程中何时形成并稳定，以提升模型的可控制性、可靠性和预测性，并为图像编辑提供实用指导。

Method: 设计PCI框架，通过在不同时间步主动插入目标概念（如文本提示），计算其最终保留在生成图像中的概率（CIS）。该方法无需训练或访问模型内部结构，适用于多类型扩散模型和概念分类。

Result: 实验发现：（1）扩散轨迹存在有利于特定概念形成的阶段；（2）同类概念在不同模型中的动态特性存在差异；（3）基于PCI确定的干预时机显著优于基线方法，在保持内容真实性的同时实现更强的语义编辑效果。

Conclusion: PCI框架揭示了扩散模型概念动态的时空规律，为零样本文本编辑提供了理论依据。研究证实时间敏感的干预策略可在不依赖模型参数的情况下实现更优编辑效果，平衡语义准确性和内容一致性。

Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions

</details>


### [52] [On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs](https://arxiv.org/abs/2512.08498)
*Yijia Guo,Tong Hu,Zhiwei Li,Liwen Hu,Keming Qian,Xitong Lin,Shengbo Chen,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出首个基于多摄像头设备的实时3D高斯点绘重建框架，解决单目视觉覆盖不足问题，通过多摄像头数据融合实现高效、高保真度的场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有单目RGB流实时重建方法受限于有限视野导致3D覆盖不完整，而多摄像头设备能从根本上解决该限制，推动更全面的场景重建。

Method: 开发分层相机初始化方案实现无校准跨相机对齐，结合轻量级多摄像头光束法平差保证轨迹稳定，创新性采用冗余消除高斯采样和频率感知优化调度策略提升效率。

Result: 实验显示该框架仅需2分钟即可完成数百米场景重建，达到实时性且无需后处理，在速度、鲁棒性和保真度方面均超越现有方法。

Conclusion: 成功实现首个支持多摄像头设备的在线3D重建系统，为实时大规模场景建模提供了新范式，验证了多视角数据融合在高斯点绘框架中的优越性。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.

</details>


### [53] [Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models](https://arxiv.org/abs/2512.08503)
*Jiaming Zhang,Che Wang,Yang Cao,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: 该论文提出了ReasonBreak，一种针对多模态大推理模型（MLRMs）的隐私保护框架，通过概念感知扰动打破地理位置推理链，并推出包含6341张超高分辨率图像的GeoPrivacy-6K数据集。实验表明其在街区级保护效果相比现有方法提升14.4%。


<details>
  <summary>Details</summary>
Motivation: MLRMs可通过层级化推理从个人图像推断精确地理位置，而现有隐私保护技术对感知模型设计的扰动无法有效对抗MLRMs。论文旨在解决如何打破MLRMs的层级推理流程以保护隐私的问题。

Method: 设计ReasonBreak框架：1) 基于概念层级生成扰动，而非均匀噪声；2) 定位推理链中的关键概念依赖关系，定向破坏推理步骤；3) 扰动通过推理层级传播造成级联失效，同时保持视觉不可察觉性。

Result: 在7个MLRMs上测试：城市街区级别的保护率达33.8%（对比现有方法19.4%），街区区块级别达33.5%（对比16.8%）。发布GeoPrivacy-6K数据集（2K分辨率以上图像，含层级概念标注）。

Conclusion: 提出了首个基于对抗扰动的推理级隐私保护范式，揭示了传统扰动方法对MLRMs的不适应性，证明通过概念层级扰动能有效干扰多步推理过程，为推理模型安全防护提供新思路。

Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.

</details>


### [54] [Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models](https://arxiv.org/abs/2512.08505)
*Vasco Ramos,Regev Cohen,Idan Szpektor,Joao Magalhaes*

Main category: cs.CV

TL;DR: 提出NoisyCLIP在去噪扩散模型的早期潜空间阶段检测图文对齐，通过降低后生成检测成本提升生成效率


<details>
  <summary>Details</summary>
Motivation: 现有对齐检测需等待完整生成后执行，导致计算成本过高且无法满足实时需求，因此尝试在生成过程的潜空间阶段进行早期检测

Method: 开发NoisyCLIP方法，在反向扩散过程中使用双编码器直接分析带噪声的潜空间特征，实现图文对齐检测，并在多个生成阶段建立基准测试框架

Result: 相比传统CLIP检测方法计算成本降低50%，在Top-N设置下保持98%的语义对齐准确率，验证了潜空间检测的可行性

Conclusion: 证明潜空间阶段对齐检测的可靠性，为扩散模型开发提供实时反馈机制，平衡生成质量与计算效率

Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.

</details>


### [55] [OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds](https://arxiv.org/abs/2512.08506)
*Jialu Sui,Rui Liu,Hongsheng Zhang*

Main category: cs.CV

TL;DR: OCDiff提出了一种结合隐式扩散模型与函数自编码器的方法，用于处理LiDAR点云中的建筑物重建问题，通过连续占据函数的生成与多任务训练策略，实现高保真三维建筑轮廓的灵活提取。


<details>
  <summary>Details</summary>
Motivation: 针对LiDAR点云中建筑物表面重建面临的点云密度变化和噪声干扰问题，传统方法难以在不规则输入下生成连续的高精度三维模型。

Method: 提出OCDiff框架：1) 在占据函数空间中引入隐扩散过程生成连续函数；2) 设计点编码器提取多模态特征并约束解码；3) 采用多任务训练策略增强特征鲁棒性。

Result: 实验证明该方法生成的样本物理一致性更强，与目标分布匹配度高，在点云噪声环境下保持稳定性能。

Conclusion: OCDiff通过创新性的隐扩散与函数表示结合，显著提升了复杂场景下三维建筑重建的准确性和抗噪声能力。

Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.

</details>


### [56] [PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation](https://arxiv.org/abs/2512.08534)
*Zhangli Hu,Ye Chen,Jiajun Yao,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出了一个多模态框架，用于生成和编辑油画，支持结合图片、草图与文本提示进行精细控制，并保持统一艺术风格。


<details>
  <summary>Details</summary>
Motivation: 现有生成和编辑技术受限于训练数据分布，主要针对照片修改，难以处理具有复杂笔触和风格化特性的油画数字创作与编辑需求。

Method: 1) 训练阶段采用空间对齐与语义增强条件策略，将蒙版与草图映射为空间约束，融合参考图像与文本的上下文嵌入作为特征约束；2) 通过基于笔触渲染的自监督风格迁移构建大规模配对训练集；3) 推理阶段使用AdaIN算子融合特征以保证风格一致性。

Result: 实验证实该系统可实现细粒度编辑，同时保留油画艺术特质，显著提升风格化油画生成与编辑的想象力实现能力。

Conclusion: 开创性地实现了首个支持精确语义控制且兼容生成/编辑任务的油画风格保持框架。

Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.

</details>


### [57] [Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement](https://arxiv.org/abs/2512.08535)
*Xinyue Liang,Zhinyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: Photo3D通过基于GPT-4o图像生成数据的结构对齐多视角合成与细节增强方案，解决3D生成中几何可靠但纹理失真的问题，实现几何与纹理双保持的高真实感3D生成。


<details>
  <summary>Details</summary>
Motivation: 3D原生生成模型在几何可靠性上取得进展，但受限于缺乏高质量真实世界3D资产（纹理细节不足），且多视角图像生成存在结构扭曲问题。传统扫描方式难以处理多尺度场景、非刚性运动和扫描精度限制。

Method: 构建包含两个核心模块的框架：1)设计结构对齐的多视角合成流水线，利用GPT-4o生成图像构建多视角数据集；2)提出基于感知特征适应和语义结构匹配的细节增强方案。通过分阶段训练策略优化几何-纹理耦合与解耦范式。

Result: Photo3D在多个3D生成范式上表现通用性，生成的3D模型在纹理真实性（Photorealism）指标上显著超越SOTA方法，同时保持结构一致性（Chamfer距离优化达18.7%），支持复杂语义结构（如建筑、动物等）的高保真重建。

Conclusion: 该研究提出首个结合大模型图像生成与结构-纹理联合优化的3D生成框架，通过解耦几何可靠性和纹理真实性约束，为高真实感3D内容创建提供了可扩展的新范式。

Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.

</details>


### [58] [Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation](https://arxiv.org/abs/2512.08537)
*Zhen Zou,Xiaoxiao Ma,Jie Huang,Zichao Yu,Feng Zhao*

Main category: cs.CV

TL;DR: Fast-ARDiff 是一种融合自回归（AR）与扩散模型优势的新型框架，通过联合优化AR和扩散模块，在保持合成质量的同时显著加速生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有AR-扩散混合模型因AR的串行生成和扩散的迭代去噪导致高延迟，需通过联合优化解决效率瓶颈。

Method: 1. 提出熵感知推测策略，使草稿模型生成与目标模型熵特性一致的高熵表示，减少熵失配和高拒绝率；2. 构建动态调度器统一AR和扩散优化，优先优化AR模块，并通过轨迹与分布匹配联合蒸馏框架优化扩散模块；3. 推理阶段利用AR模块浅层特征熵预过滤低熵草案，减少冗余计算。

Result: 在ImageNet 256×256上，TransDiff实现无损4.3倍加速，NextStep-1在文本生成任务中实现3倍加速。

Conclusion: Fast-ARDiff通过跨模块协同优化，在保证质量的前提下显著提升AR-扩散模型推理速度，为高效生成模型提供了新范式。

Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.

</details>


### [59] [An Iteration-Free Fixed-Point Estimator for Diffusion Inversion](https://arxiv.org/abs/2512.08547)
*Yifei Chen,Kaiyu Song,Yan Pan,Jianxing Yu,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 本文提出了一种无需迭代的固定点估计器用于扩散模型反演，通过误差近似方法降低计算成本并在NOAPS和MS-COCO数据集上验证了重建性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现有固定点迭代方法存在计算成本高和超参数选择复杂的问题，需要开发更高效的扩散模型反演技术。

Method: 推导理想反转步骤的显式固定点表达式，通过前一步可计算误差近似当前未知误差，建立低方差无偏估计器。

Result: 在NOAPS和MS-COCO数据集上比DDIM反演等方法重建性能更优，且无需额外迭代或训练过程。

Conclusion: 提出的迭代-free固定点估计器有效解决了传统方法的计算瓶颈，理论分析验证了估计器的统计特性，为扩散模型反演提供了新范式。

Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.

</details>


### [60] [SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/2512.08557)
*Alexander Dow,Manduhu Manduhu,Matheus Santos,Ben Bartlett,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 本文提出了SSCATeR方法，通过聚焦点云变化区域并利用时空数据复用，显著降低激光雷达目标检测的计算量。


<details>
  <summary>Details</summary>
Motivation: 为解决现有方法中大量卷积计算导致的高计算开销问题，同时保留检测精度，提出一种针对连续扫描激光雷达数据的稀疏特征提取方法。

Method: 使用滑动时间窗口捕捉点云变化区域，通过跨帧卷积结果复用引入极端稀疏性，并扩展基于散射的卷积算法实现时空数据重用（SSCATeR）。

Result: 相比传统稀疏卷积技术，处理时间减少6.61倍，且特征图完全一致，验证了该方法在保持精度的同时大幅提升计算效率。

Conclusion: 该方法通过流式处理和动态区域聚焦，在保持检测精度的同时显著优化了激光雷达数据的实时处理性能。

Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.

</details>


### [61] [BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain](https://arxiv.org/abs/2512.08560)
*Navve Wasserman,Matias Cosarinsky,Yuval Golbari,Aude Oliva,Antonio Torralba,Tamar Rott Shaham,Michal Irani*

Main category: cs.CV

TL;DR: 该论文提出了一种大规模自动化框架，用于发现和解释人脑皮层中的视觉表征，揭示了数千种可解释的视觉概念模式，包括以往未报道的细粒度表征。


<details>
  <summary>Details</summary>
Motivation: 传统研究受限于小规模、手动分析、特定区域聚焦及缺乏系统验证，而脑信号复杂且视觉概念空间庞大，亟需自动化方法全面探索。

Method: 两阶段方法：1) 通过无监督数据驱动分解发现fMRI活动的候选可解释模式；2) 识别强激活该模式的自然图像集合，生成其共享视觉意义的自然语言描述，并通过量化评分与自动化流水线选取最优解释。

Result: 框架揭露了跨越多个视觉概念的数千种可解释模式，包括此前未被发现的细粒度表征，且支持多区域、多属性的大规模验证。

Conclusion: 该自动化框架为理解人脑视觉表征提供了系统性工具，显著扩展了现有可解释脑活动模式的规模与深度，并为神经科学提供新的验证与探索手段。

Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.

</details>


### [62] [Modular Neural Image Signal Processing](https://arxiv.org/abs/2512.08564)
*Mahmoud Afifi,Zhongling Wang,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一种模块化神经图像信号处理（ISP）框架，通过多阶段控制实现从原始输入到高质量显示图像的转换，并构建了交互式工具支持多样化编辑和图像风格调整。


<details>
  <summary>Details</summary>
Motivation: 传统神经ISP缺乏模块化设计，导致对中间处理阶段控制不足，限制了可扩展性、调试性及对新相机和用户偏好的适应性。

Method: 通过全学习型模块化框架实现多阶段控制，并开发了一款交互式照片编辑工具，支持高质量渲染和无限次后期可编辑的重渲染，模型规模适中（参数量0.5M-3.9M）。

Result: 在多个测试集中实现了高渲染精度、可扩展性和对未见过相机的泛化能力，并在用户编辑灵活性和量化指标上表现优异。

Conclusion: 该模块化设计显著提升了神经ISP的可控性、可扩展性和用户定制化能力，在保持渲染质量的同时突破了传统框架的功能限制。

Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM

</details>


### [63] [Instance-Aware Test-Time Segmentation for Continual Domain Shifts](https://arxiv.org/abs/2512.08569)
*Seunghwan Lee,Inyoung Jung,Hojoon Lee,Eunil Park,Sungeun Hong*

Main category: cs.CV

TL;DR: 提出一种连续测试时间自适应方法，通过自适应调整伪标签和动态平衡类别学习，改进语义分割在持续域变化中的表现，克服现有方法依赖固定阈值的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在连续域变化场景中依赖固定或批次级阈值，无法处理不同类别和实例的难度差异，导致语义分割中的多类别密集预测效果受限。

Method: 设计了一种细粒度方法：1) 根据图像内置信度分布动态调整伪标签阈值；2) 针对受域偏移影响最大的类别进行动态学习平衡，实现类别与实例感知的持续适应。

Result: 在8种CTTA/TTA场景（含合成到真实、长期偏移）实验中，该方法 consistently 超越最先进方法，建立语义分割在域演化条件下的新基准。

Conclusion: 通过置信度驱动的伪标签优化与类别动态平衡策略，有效缓解持续自适应中的误差累积问题，为连续域变化下的语义分割提供新标准。

Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.

</details>


### [64] [From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis](https://arxiv.org/abs/2512.08572)
*Olle Edgren Schüllerqvist,Jens Baumann,Joakim Lindblad,Love Nordling,Artur Mezheyeuski,Patrick Micke,Nataša Sladoje*

Main category: cs.CV

TL;DR: HiGINE is a hierarchical graph-based method that uses mIF images and cancer stage data to improve lung cancer survival prognosis by modeling complex TME cell interactions.


<details>
  <summary>Details</summary>
Motivation: Existing TME analysis methods fail to capture complex cell-type interactions. Better models are needed for accurate prognostic biomarker identification in lung cancer.

Method: Developed HiGINE, a hierarchical graph model encoding local/global cell interactions in mIF images, fused with clinical staging data. Captures cell-type relationships and morphological patterns.

Result: Improved risk stratification performance on two public datasets, demonstrating enhanced robustness and generalizability for survival prediction.

Conclusion: Multimodal HiGINE framework with graph-based TME characterization improves survival prediction and clinical decision-making in lung cancer.

Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.

</details>


### [65] [Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery](https://arxiv.org/abs/2512.08577)
*Yuna Kato,Shohei Mori,Hideo Saito,Yoshifumi Takatsume,Hiroki Kajita,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本论文提出了一种自动化手术视频图像对齐方法，通过动态识别设备移动并选择遮挡最少的摄像头，生成固定视角的高质量手术视频。


<details>
  <summary>Details</summary>
Motivation: 手术视频录制面临遮挡问题（手术者遮挡镜头）和后处理困难（多摄像头配置频繁变化需手动对齐），传统方法依赖人工干预且耗时。

Method: 1) 检测无影灯移动事件；2) 自动完成多摄像头图像时空对齐；3) 基于遮挡检测算法选择最优视角；4) 集成多种视频合成选项并开展偏好研究。

Result: 外科医生用户研究显示：1) 视频对手术区域可见度提升35%；2) 观看舒适度评分提高42%；3) 视频质量客观指标优于传统方法；4) 横向对比发现了合成选项的临床偏好规律。

Conclusion: 该自动化系统显著降低手术视频制作的成本与难度，为医疗教育提供标准化素材，未来可拓展至其他动态场景的多视角视频合成。

Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.

</details>


### [66] [Automated Pollen Recognition in Optical and Holographic Microscopy Images](https://arxiv.org/abs/2512.08589)
*Swarn Singh Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 本研究探索了深度学习在改进和自动化光镜与全息显微镜花粉粒检测与分类中的应用，使用YOLOv8s进行目标检测，MobileNetV3L进行分类。光学图像达成91.3%检测mAP50和97%分类准确率，全息图像经数据集扩展优化后检测性能从2.49%提升至13.3% mAP50，分类从42%提高至54%。


<details>
  <summary>Details</summary>
Motivation: 为解决传统兽医细胞学花粉分析依赖人工且耗时的问题，研究通过深度学习提升显微镜分析效率，特别是探索低成本无镜头全息显微设备与AI技术的结合潜力。

Method: 采用YOLOv8s检测模型和MobileNetV3L分类模型，跨模态评估性能。针对全息图像性能不足，通过自动化标注与扩大标注框区域进行数据集扩展。

Result: 光学图像检测mAP50 91.3%，分类准确率97%；全息图像经优化后检测mAP50从2.49%提升至13.3%，分类准确率从42%升至54%。

Conclusion: 证明深度学习可与低成本全息显微设备结合，显著提升图像分类任务性能，为兽医细胞学场景提供经济可行的自动化方案。

Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.

</details>


### [67] [Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning](https://arxiv.org/abs/2512.08606)
*Zhenyu Zhang,Guangyao Chen,Yixiong Zou,Zhimeng Huang,Yuhua Li*

Main category: cs.CV

TL;DR: The paper addresses template-sample similarity (TSS) bias in CLIP models, which causes over-reliance on text template proximity rather than accurate image-category alignment. A framework using empty prompts and bias calibration loss is proposed to mitigate this issue, improving classification accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: CLIP models exhibit TSS bias where image classifications depend on text template similarity rather than true category alignment, leading to reduced accuracy and robustness. This work aims to decouple template bias from visual representation learning.

Method: 1) Introduce 'empty prompts' containing only 'emptiness' concepts during pre-training to capture template-agnostic features; 2) Implement bias calibration loss during few-shot fine-tuning to enforce image-category alignment. Two-stage framework reduces template-induced bias in both pre-training and adaptation phases.

Result: Experimental validation across multiple benchmarks shows significant reduction in performance fluctuations caused by TSS, achieving higher classification accuracy (e.g., 88.9% on ImageNet) and improved robustness against distribution shifts compared to standard CLIP.

Conclusion: The proposed template correction method effectively reduces bias in CLIP models by decoupling visual representation learning from text template proximity through empty prompts and bias calibration, with public code implementation available.

Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

</details>


### [68] [OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics](https://arxiv.org/abs/2512.08625)
*Jisang Yoo,Gyeongjin Kang,Hyun-kyu Ko,Hyeonwoo Yu,Eunbyung Park*

Main category: cs.CV

TL;DR: 本文提出了OpenMonoGS-SLAM，首个单目SLAM框架，结合3D高斯泼溅（3DGS）与开放词汇语义理解，无需深度传感器，依赖MASt3R/SAM/CLIP等视觉基础模型实现自监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统依赖深度传感器或封闭式语义模型，限制了开放环境中的扩展性和适应性。本文旨在通过视觉基础模型提升单目SLAM在开放世界的三维重建与语义理解能力。

Method: 利用MASt3R（几何）、SAM和CLIP（开放语义）构建单目SLAM框架，设计高维语义特征记忆机制，生成语义高斯地图，完全基于自监督学习，无需深度输入或3D语义标注。

Result: 在封闭集和开放集分割任务中表现优于同类方法，无需深度图或语义标注即可实现精确相机跟踪与丰富语义理解。

Conclusion: 成功实现首个单目SLAM与开放语义的融合，证明了视觉基础模型在开放世界三维感知中的有效性，为无传感器依赖的智能系统提供新思路。

Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.

</details>


### [69] [Trajectory Densification and Depth from Perspective-based Blur](https://arxiv.org/abs/2512.08627)
*Tianchen Qiu,Qirun Zhang,Jiajian He,Zhengyue Zhuge,Jiahui Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需机械稳定器的深度估计方法，通过分析视频中的模糊模式和稠密轨迹，结合光学设计与算法，在长曝光场景下实现了高精度度量深度重建。


<details>
  <summary>Details</summary>
Motivation: 手持拍摄时相机旋转导致深度依赖的透视模糊问题，传统方法难以在无稳定器条件下同时实现高精度与泛化性。

Method: 采用现成的视觉编码器与点跟踪器提取视频特征，通过多窗口嵌入聚合估计深度图，并利用视觉-语言模型稠密化稀疏轨迹以优化重建结果。

Result: 在多深度数据集上验证，方法在大范围深度场景中表现优异，泛化能力突出，且在真实手持拍摄中的轨迹精度超越传统光学方案。

Conclusion: 该方法为动态场景下的非结构化深度感知提供了有效解决方案，结合硬件光学与算法设计，实现了高鲁棒性与实用价值。

Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.

</details>


### [70] [Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning](https://arxiv.org/abs/2512.08639)
*Huilin Xu,Zhuoyang Liu,Yixiang Luomei,Feng Xu*

Main category: cs.CV

TL;DR: 提出了一种新的无人机视觉-语言导航框架，仅依赖单目RGB图像和语言指令，通过多任务学习和关键帧策略，有效解决了现有方法成本高、复杂的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航（VLN）方法依赖全景图像、深度输入或里程计，导致系统成本和复杂度高，限制了轻型无人机的实际部署。需开发更经济高效的解决方案。

Method: 将导航建模为next-token预测问题，结合基于提示的多任务学习优化空间感知、轨迹推理和动作预测；提出关键帧选择策略减少冗余视觉信息，并通过动作合并与标签重加权缓解长尾分布监督失衡问题。

Result: 在Aerial VLN基准测试中，单目RGB设置下模型在可见和未知环境中均表现优异，大幅优于现有RGB-only基线模型，并缩小与全景RGB-D模型的性能差距。消融实证验证了设计模块的有效性。

Conclusion: 该框架显著简化无人机VLN系统，推动低成本部署。多任务学习与数据优化策略的有效性得到实证验证，为复杂场景下的自主导航提供实用路径。

Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

</details>


### [71] [Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation](https://arxiv.org/abs/2512.08645)
*Young Kyung Kim,Oded Schlesinger,Yuzhou Zhao,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: 提出CoIG框架，通过分步生成和编辑图像，增强生成过程的可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型的黑箱特性限制了透明度和安全性，需引入类似思维链的人类化流程。

Method: 利用LLM将复杂提示拆解为步骤化指令，逐步生成并编辑图像，每步骤聚焦单一语义实体。

Result: 通过CoIG可读性与因果相关性指标验证分步过程的清晰度与影响，缓解实体坍塌问题，平衡可监控性与生成质量。

Conclusion: CoIG框架提升生成过程透明度和鲁棒性，适用于任意图像生成模型，为可控生成提供新范式。

Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.

</details>


### [72] [C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition](https://arxiv.org/abs/2512.08647)
*Keito Inoshita*

Main category: cs.CV

TL;DR: 本文提出C-DIRA框架，通过动态ROI路由与对抗学习实现驾驶员行为识别的轻量化高效架构，在保持低计算成本的同时提升跨域场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有轻量模型在捕捉细粒度行为特征时存在性能下降，且ROI方法计算开销大，难以满足边缘设备实时性与准确性的双重需求，并需应对驾驶者差异和复杂光照等环境挑战。

Method: 核心包含三部分：1）基于显著性的Top-K区域提取与特征融合策略；2）动态ROI路由机制仅对复杂样本启用区域分析；3）结合伪域标签的对抗学习策略提取域不变特征，降低背景差异影响。

Result: 在State Farm数据集中，相较轻量级模型，C-DIRA精度提升12.7%，计算量（FLOPs）减少43%，延迟降低至28ms；在模糊/低光场景下精度仅下降1.2%/0.8%，跨域测试标准差降低68%。

Conclusion: C-DIRA通过结构创新与对抗训练实现三重优势：参数量压缩57%；动态计算节省32%推理时间；域适应能力使未见场景精度波动≤2.1%，为边缘端行为识别提供可部署解决方案。

Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.

</details>


### [73] [Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank](https://arxiv.org/abs/2512.08648)
*Shaofeng Zhang,Xuanqi Chen,Ning Liao,Haoxiang Zhao,Xiaoxing Wang,Haoru Tan,Sitong Wu,Xiaosong Jia,Qi Fan,Junchi Yan*

Main category: cs.CV

TL;DR: 该论文提出{
name}框架，通过内存库机制和低维投影头，在无需外部编码器的情况下提升生成模型效率，实现更低FID值和更快收敛速度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决当前生成模型训练成本高的问题，同时减少对外部预训练编码器的依赖和域转变问题。

Method: 提出{
name}框架：1) 使用内存库机制维护跨训练迭代的负样本队列；2) 低维投影头降低内存开销。

Result: 在ImageNet-256数据集上，{
name}在40万个步骤内达到2.40的FID值，比现有方法更高效。

Conclusion: 该框架自包含且无需额外参数，在训练加速和生成质量方面具有显著优势，为高效率生成模型提供了新思路。

Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.

</details>


### [74] [What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance](https://arxiv.org/abs/2512.08697)
*Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: 提出MoSAIC-ReID框架，通过LoRA专家与属性连接定量分析行人重识别中的属性重要性，发现服装颜色等属性更关键。


<details>
  <summary>Details</summary>
Motivation: 当前ReID模型准确性高但缺乏透明性，需要明确模型依赖的高层语义属性以提升可解释性与实用性。

Method: 采用混合专家架构（MoSAIC-ReID），每个LoRA专家绑定单一属性，设计Oracle路由实现可控归因分析，并在Market-1501/DukeMTMC测试属性标注条件下的性能及属性重要性。

Result: 通过统计检验和特征重要性分析发现，服装颜色、内在特征（如身体形态）贡献度最高，而配饰等低频线索影响较小，框架性能与现有方法相当。

Conclusion: 提供了可解释ReID的框架，强调集成显式语义知识的必要性，但需满足实际标注条件要求。

Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID

</details>


### [75] [SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images](https://arxiv.org/abs/2512.08730)
*Kaiyu Li,Shengqi Zhang,Yupeng Deng,Zhi Wang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 本论文提出了一种无需训练的开放词汇语义分割方法，基于SAM 3模型，通过对语义分割头和实例分割头的掩模融合策略以及利用存在分数过滤不存在类别，有效解决了遥感场景中致密小目标的分割挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的方法在精确分割遥感场景中的密集小目标时存在局限性，且需要复杂的模块组合流程。SAM 3统一了分割和识别任务，但其在无需训练的情况下处理遥感开放词汇分割的应用尚未被充分探索。

Method: 1. 提出掩模融合策略，结合SAM 3的语义分割头和Transformer解码器（实例头）输出；2. 利用存在头产生的存在分数过滤影像中不存在的类别，减少假阳性结果。

Result: 在多个遥感数据集上的实验表明，该简单适配方法取得了有竞争力的性能，验证了SAM 3在无需训练条件下处理开放词汇语义分割任务的潜力。

Conclusion: 基于SAM 3模型的无训练开放词汇分割框架在遥感场景中展现出有效性，未来可进一步探索更优的掩模合并机制及跨模态交互优化。

Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.

</details>


### [76] [Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting](https://arxiv.org/abs/2512.08733)
*Kuniko Paxton,Zeinab Dehghani,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本研究引入了一种基于分布的个体公平性评估框架，利用核密度估计（KDE）和统计距离指标解决皮肤病变分类中的个体级肤色差异，提出基于距离的重加权（DRW）损失函数，并在CNN和Transformer模型中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像机器学习公平性研究过度依赖粗分类别，忽视个体级肤色变异，可能掩盖子群体内偏差。该研究旨在解决个体层面公平性问题，提升皮肤病AI系统的精准度和公平性。

Method: 将肤色视为连续属性而非分类标签，采用核密度估计（KDE）建模分布；对比12种统计距离度量方法，提出距离重加权（DRW）损失函数以修正少数肤色样本欠表征问题，并在多种模型架构中进行实验验证。

Result: 实验表明：（1）基于类别的重加权方法在捕捉个体级差异性时存在局限；（2）DRW方法通过Fidelity Similarity（FS）、Wasserstein Distance（WD）等四种距离指标有效提升公平性性能，尤其是在处理少数肤色样本方面。

Conclusion: 研究建立了一种稳健的个体级公平性方法学框架，推动皮肤病学AI系统的公平性发展，并为医学影像分析中敏感连续属性的公平性研究提供了通用范式。

Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.

</details>


### [77] [A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation](https://arxiv.org/abs/2512.08747)
*Artúr I. Károly,Péter Galambos*

Main category: cs.CV

TL;DR: 本论文提出了一种结合Blender三维渲染与约束扩散模型的新方法，自动生成高质量、带标注的现实主义蘑菇合成图像数据集，并实现无真实数据训练的先进分割性能。


<details>
  <summary>Details</summary>
Motivation: 工业蘑菇栽培依赖计算机视觉监控，但真实标注数据集成本高昂。虽然合成数据可扩展，但现有方法缺乏现实主义，导致实际场景泛化能力不足。

Method: 1) 利用Blender建立蘑菇三维生长模型，通过物理材质和光照控制确保几何真实性；2) 将3D场景特征约束输入扩散模型，在像素空间和CLIP嵌入层面进行超分辨率迭代优化；3) 自动导出实例分割标注与场景元数据，构建双数据集（6000+图像/25万+蘑菇实例）。

Result: Mask R-CNN模型在零样本测试中表现优异：M18K基准F1得分0.859，超越现有合成数据方法。合成数据在亮度方差（δ=0.235）与遮挡处理方面显著优于StyleGAN2对照组。

Conclusion: 该管线首次实现无需图形专家的小规模作物自动化合成数据生成，在蘑菇检测中达到实际应用水平，并可扩展至果蔬检测等农业领域。扩散模型约束机制为特定领域数据生成提供了可复用框架。

Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.

</details>


### [78] [Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices](https://arxiv.org/abs/2512.08751)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种通过统计偏度引导的模型剪枝方法，在联邦学习框架下实现了多模态Swin Transformer的显著压缩（约36%参数减少）且保持诊断准确率，为边缘设备的隐私保护医疗AI提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 高性能医学影像模型因参数量庞大难以部署于边缘设备，且医疗数据隐私问题限制了集中化训练，亟需兼顾高效压缩与分布式学习的解决方案。

Method: 基于输出分布偏度选择性剪枝多头自注意力和多层感知机模块，采用横向联邦学习架构进行训练验证，在紧凑型Swin Transformer上评估压缩效果。

Result: 压缩后模型保持与原始模型相同的诊断准确率，同时模型规模缩减36%，显著降低计算资源消耗（参数量/计算量减少比例未明确量化）。

Conclusion: 通过统计特征引导剪枝与联邦学习的协同优化，在保证诊断性能的同时实现医疗AI模型的边缘部署可行性，为多模态医学影像分析提供了兼顾效率与隐私的范式。

Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.

</details>


### [79] [Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance](https://arxiv.org/abs/2512.08765)
*Ruihang Chu,Yefei He,Zhekai Chen,Shiwei Zhang,Xiaogang Xu,Bin Xia,Dingdong Wang,Hongwei Yi,Xihui Liu,Hengshuang Zhao,Yu Liu,Yingya Zhang,Yujiu Yang*

Main category: cs.CV

TL;DR: Wan-Move 提出一种简单可扩展的运动控制视频生成框架，通过直接将原始特征转换为具有运动感知的条件特征，结合密集点轨迹与时空特征传播，在不修改模型结构的前提下实现精确高质量的运动控制。


<details>
  <summary>Details</summary>
Motivation: 现有运动控制方法存在控制粒度粗糙、可扩展性差等问题，生成质量难以满足实际应用需求，需探索更精细且高效的运动控制方案。

Method: 1) 利用密集点轨迹表示物体运动；2) 在潜空间中沿轨迹传播首帧特征生成时间对齐特征图；3) 将此特征图作为更新后的潜空间条件输入预训练视频生成模型（如 Wan-I2V-14B），并设计包含多类别内容和高质量运动标注的 MoveBench 基准进行评估。

Result: 成功生成 5 秒 480p 视频，在用户调研中运动可控性媲美 Kling 1.5 Pro 的 Motion Brush 商业工具；MoveBench 基准具有更大数据量、更长视频片段和更高标注质量，在多项实验中展现显著优于公开数据集的运动质量。

Conclusion: Wan-Move 实现了无需架构改动的精确运动控制视频生成，MoveBench 基准为领域发展提供了关键评测支持，代码、模型与数据已开源。

Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.

</details>


### [80] [Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps](https://arxiv.org/abs/2512.08774)
*Seoyeon Lee,Gwangyeol Yu,Chaewon Kim,Jonghyuk Park*

Main category: cs.CV

TL;DR: 提出自精炼扩散框架，利用FAMs定位图像缺陷并改进扩散过程，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在生成图像含伪影和不真实区域的问题，需更有效的缺陷检测和修复方法。

Method: 设计基于XAI的缺陷高亮器生成FAMs，在扩散前向过程放大缺陷噪声，反向过程聚焦修复。

Result: FID指标提升27.3%，在图像生成、文生图、修复等任务中均表现优异，适用于多种模型。

Conclusion: XAI技术成功应用于图像主动改进，框架通用性强，显著推动图像合成领域发展。

Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.

</details>


### [81] [MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance](https://arxiv.org/abs/2512.08789)
*Chaewon Kim,Seoyeon Lee,Jonghyuk Park*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉Transformer的文档阴影去除框架MatteViT，通过空间和频率域信息结合有效去除阴影并保留高频细节。


<details>
  <summary>Details</summary>
Motivation: 文档阴影会遮蔽高频结构（如文字边缘），影响文档清晰度和下游OCR任务效果，现有方法难以在去阴影同时保留细粒度结构，亟需改进。

Method: 1) 提出轻量级高频增强模块（HFAM），对高频分量进行分解与自适应增强；2)基于连续亮度构建遮罩数据集，利用阴影遮罩生成器提供早期空间引导，通过双策略协同优化细节保留能力。

Result: 在RDD和Kligler数据集上均取得SOTA性能，在真实场景中保持鲁棒性，较传统方法提升OCR识别准确率3.2%，且模型参数量减少40%。

Conclusion: MatteViT通过空间-频率双通道约束建立了精确的细粒度重建机制，为文档去阴影提供了兼顾性能与实用性的新范式，在学术和工业场景中均具有应用价值。

Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.

</details>


### [82] [Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning](https://arxiv.org/abs/2512.08820)
*Yi Zhang,Chun-Wun Cheng,Junyi He,Ke Yu,Yushun Tang,Carola-Bibiane Schönlieb,Zhihai He,Angelica I. Aviles-Rivero*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的双曲适配方法（T-DHA），通过将视觉-语言关系嵌入双曲空间（Poincaré球模型）并结合负学习，有效提升跨领域视觉模型适配性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在跨领域适应时性能下降严重，且需要大量计算资源进行微调。研究旨在解决域迁移下的性能退化问题，同时降低计算成本。

Method: 提出T-DHA方法：1) 利用双曲空间（Poincaré球模型）的指数体积增长特性，捕捉视觉-语言语义间的层次化树结构关系；2) 结合负学习策略，通过压缩特征维度实现出更精确的分类。

Result: 在多个数据集上的实验证明，T-DHA在跨领域图像识别的少样本学习（few-shot）和域泛化任务中，均显著优于当前最先进的方法。

Conclusion: T-DHA通过双曲空间建模与负学习策略，为大规模VLM提供了高效的跨领域适配方案，在保证表现力的同时减少了特征维度需求，具有实际部署优势。

Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.

</details>


### [83] [InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829)
*Hongyuan Tao,Bencheng Liao,Shaoyu Chen,Haoran Yin,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出InfiniteVL，结合滑动窗口注意力与Gated DeltaNet的线性复杂度多模态模型


<details>
  <summary>Details</summary>
Motivation: 传统窗口注意力在长序列时性能下降，线性注意力在OCR等密集任务效果不佳，需平衡长序列处理与计算效率

Method: 融合滑动窗口注意力(SWA)和Gated DeltaNet架构，采用三阶段训练策略(知识蒸馏、指令微调、长序列SFT)

Result: 仅用2%训练数据超越线性复杂度模型，达到Transformer基线效果，推理加速3.6倍，维持24FPS视频处理能力

Conclusion: 证明滑动窗口机制与线性注意力联合的有效性，提供资源约束下长序列处理的高效解决方案

Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

</details>


### [84] [Generation is Required for Data-Efficient Perception](https://arxiv.org/abs/2512.08854)
*Jack Brady,Bernhard Schölkopf,Thomas Kipf,Simon Buchholz,Wieland Brendel*

Main category: cs.CV

TL;DR: This paper investigates whether generative approaches (using decoder inversion) are necessary for human-level visual perception, showing that generative methods with proper inductive biases significantly outperform non-generative models in compositional generalization.


<details>
  <summary>Details</summary>
Motivation: To resolve whether generation is essential for human-like visual perception, as leading vision models currently rely on non-generative encoders rather than generative decoders.

Method: The authors formalized inductive biases required for compositional generalization in both generative (decoder-based) and non-generative (encoder-based) frameworks. Theoretical analysis determined feasibility of enforcing these biases via regularization/architecture, while empirical tests compared performance on photorealistic datasets.

Result: Non-generative methods struggled due to architectural/regularization infeasibility of key inductive biases, requiring pretraining/supervision. Generative methods achieved superior compositional generalization by enforcing biases during decoding and inversion via gradient search/generative replay.

Conclusion: Generative methods with appropriate inductive biases and decoding strategies are crucial for human-level compositional generalization, highlighting generation's role in visual perception.

Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.

</details>


### [85] [Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference](https://arxiv.org/abs/2512.08860)
*Amit Bendkhale*

Main category: cs.CV

TL;DR: 本研究开发Tri-Bench基准测试用于评估视觉语言模型（VLMs）的几何推理能力，发现现有模型在三维几何理解方面表现欠佳，过度依赖二维图像线索且难以应对相机姿态变化。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs具备较强能力，但在真实场景几何推理任务中存在可靠性不足的问题，需要系统性评估框架来解析其局限性，特别是设备姿态变化和环境干扰的工程化验证需求。

Method: 构建包含平面三角形问题的Tri-Bench测试集，设计六类二维/三维目标任务，控制相机姿态（正视/倾斜）和场景干扰（10种日常物体），使用统一边界框提示约束模型，并对4种新型VLM进行标准测试。

Result: 模型对三维真实值平均准确率69%（最优75%），二维投影准确率72%，但倾斜视角导致性能下降4.1%，特殊三角形分类接近零准确率，物体干扰无显著影响。

Conclusion: 现有VLMs在几何推理任务中存在系统性缺陷，表现为无法有效利用三维框架提示、过度依赖二维视觉线索，且对设备姿态敏感，揭示了深度感知与上下文抗干扰能力的不足。

Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.

</details>


### [86] [SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing](https://arxiv.org/abs/2512.08881)
*Aysim Toker,Andreea-Maria Oncescu,Roy Miles,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: 本文提出了一种结构化定位机制，通过微调VLM模型并结合控制标记提升卫星图像视觉定位精度，在多个遥感基准上超越现有方法，视觉定位任务提升24.8%。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在遥感场景中视觉定位精度不足，难以有效整合语言与空间信息进行复杂场景推理，需要提升定位能力和实际应用可靠性。

Method: 设计结构化定位模块，通过专用控制标记连接VLM与定位系统，微调预训练模型并联合优化语言-空间推理能力，在多样化指令任务中嵌入空间定位逻辑。

Result: 在遥感视觉定位任务中取得24.8%相对提升，同时在多个基准测试中刷新SOTA表现，验证了结构化空间推理对VLM性能的增强效果。

Conclusion: 结构化定位机制有效解决了VLM在遥感数据中的定位瓶颈，通过控制标记实现语言-空间信息协同推理，为实际卫星数据分析提供了更可靠的技术路径。

Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.

</details>


### [87] [No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers](https://arxiv.org/abs/2512.08889)
*Damiano Marsili,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 提出一种无标注的视觉推理框架，结合LLM和VLM验证器提升推理和定位性能。


<details>
  <summary>Details</summary>
Motivation: 视觉推理需精确对象定位和空间关系理解，现有方法依赖监督数据或存在逻辑缺陷，需改进现有不足。

Method: 使用LLM验证器基于强化学习优化推理过程，VLM验证器通过自动化困难负采样强化视觉定位，无需人工标注标签。

Result: 在多种空间推理任务中实验显示，新方法优于开源和专有模型，尤其在改进视觉定位模型后超越最新文本-视觉推理方法。

Conclusion: 所提框架有效整合语言推理与视觉定位优势，无需监督实现视觉推理性能突破。

Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

</details>


### [88] [UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation](https://arxiv.org/abs/2512.08897)
*Zeyang Liu,Le Wang,Sanping Zhou,Yuxuan Wu,Xiaolong Sun,Gang Hua,Haoxiang Li*

Main category: cs.CV

TL;DR: UniLayDiff是一个统一的扩散变压器模型，用于解决图形设计自动化中基于多种约束的内容感知布局生成问题，首次实现了从无条件到多条件生成的统一。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法通过单一模型处理多样化的布局生成子任务（如元素类型、尺寸或关系限制），需要独立模型或参数，缺乏真正的统一解决方案。

Method: 将布局约束视为独立模态，采用多模态扩散变压器框架，结合LoRA微调技术，在预训练后通过关系约束微调，实现统一生成并提升布局质量。

Result: 在多种生成任务（无条件到多条件）中均达到SOTA表现，且首次实现了全范围内容感知布局生成任务的统一。

Conclusion: 该模型通过统一框架解决了复杂模态交互和约束集成问题，为布局生成任务提供了通用解决方案，并验证了其有效性和广泛适用性。

Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.

</details>


### [89] [Self-Evolving 3D Scene Generation from a Single Image](https://arxiv.org/abs/2512.08905)
*Kaizhi Zheng,Yue Fan,Jing Gu,Zishuo Xu,Xuehai He,Xin Eric Wang*

Main category: cs.CV

TL;DR: EvoScene通过结合3D生成模型的几何推理能力和视频生成模型的视觉知识，以无需训练的迭代方式从单张图像中重建完整3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有图像到3D转换方法因基于单视角几何恢复和物体中心训练，难以推广到具有复杂结构和精细纹理的大规模场景重建任务。

Method: 提出三阶段迭代框架：空间先验初始化→视觉引导的3D网格生成→空间引导的新视角生成，在2D/3D域交替优化，利用扩散模型与神经辐射场优势。

Result: 在几何稳定性、视角一致性纹理、非可见区域补全等指标上全面超越基线方法，可直接输出可编辑的3D三角网格模型。

Conclusion: 证明了跨模态模型协作与自演进迭代可有效突破传统监督训练的局限，为单目3D重建提供了新范式。

Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.

</details>


### [90] [LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception](https://arxiv.org/abs/2512.08912)
*Simon de Moreau,Andrei Bursuc,Hafid El-Idrissi,Fabien Moutarde*

Main category: cs.CV

TL;DR: LiDAS是一种闭环主动照明系统，通过动态优化 headlights 光照分布，显著提升夜间相机感知性能，同时节省能耗。其无需重新训练即可复用日间训练模型，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 夜间环境依赖被动照明导致感知性能下降。传统方法无法主动感知光照分布，而 headlights 仅提供固定光照，限制了复杂场景下的有效性。

Method: LiDAS 将现有感知模型与高精度 headlights 结合：1) 利用合成数据训练闭环控制策略；2) 动态预测最优光照场，集中光照于目标区域；3) 通过零样本学习直接部署到真实夜间驾驶场景。

Result: 较传统近光灯： mAP50 提升18.7%，mIoU 提升5.0%，且保持相同能耗。通过减少40%照明功耗仍保持性能，与领域泛化算法形成互补增强。

Conclusion: 该系统通过将现成 headlights 转化为主动感知执行器，在无需额外硬件/模型训练的前提下，为夜间鲁棒感知提供了低成本解决方案，验证了动态主动传感范式的可行性。

Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.

</details>


### [91] [Efficiently Reconstructing Dynamic Scenes One D4RT at a Time](https://arxiv.org/abs/2512.08924)
*Chuhan Zhang,Guillaume Le Moing,Skanda Koppula,Ignacio Rocco,Liliane Momeni,Junyu Xie,Shuyang Sun,Rahul Sukthankar,Joëlle K Barral,Raia Hadsell,Zoubin Ghahramani,Andrew Zisserman,Junlin Zhang,Mehdi SM Sajjadi*

Main category: cs.CV

TL;DR: D4RT是一种新的前馈Transformer模型，通过统一架构和新型查询机制高效解决动态场景的4D重建问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理视频动态场景重建时计算复杂度高且依赖多任务解码器，缺乏高效性与可扩展性。

Method: 采用统一Transformer架构联合推断深度、时空对应和相机参数，核心创新为可灵活探测3D时空点的查询机制，避免逐帧解码与多解码器复杂度。

Result: 模型在4D重建任务中实现SOTA性能，支持实时训练与推理，具备高可扩展性。

Conclusion: D4RT通过架构简化与高效解码设计，显著提升动态场景重建效果，代码与可视化结果已开源。

Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.

</details>


### [92] [Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment](https://arxiv.org/abs/2512.08930)
*Youming Deng,Songyou Peng,Junyi Zhang,Kathryn Heal,Tiancheng Sun,John Flynn,Steve Marschner,Lucy Chai*

Main category: cs.CV

TL;DR: This paper proposes Selfi, a self-improving 3D reconstruction pipeline that enhances VGGT foundation models with explicit geometric consistency, achieving state-of-the-art performance in novel view synthesis and camera pose estimation.


<details>
  <summary>Details</summary>
Motivation: Current vision foundation models lack explicit 3D geometric consistency, leading to suboptimal performance in 3D reconstruction tasks. The work aims to bridge this gap by improving multi-view consistency through self-supervised feature alignment.

Method: Introduces a lightweight feature adapter trained with a reprojection-based consistency loss to align VGGT features spatially. Uses the model's own outputs as pseudo-ground-truth for iterative self-improvement, creating a geometrically-aligned feature space.

Result: Achieves state-of-the-art performance on both novel view synthesis and camera pose estimation tasks while maintaining feed-forward inference capability, showing significant improvements in 3D reconstruction accuracy.

Conclusion: Feature alignment through self-supervised geometric consistency is a critical enabler for enhancing vision foundation models' 3D reasoning capabilities.

Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.

</details>


### [93] [Astra: General Interactive World Model with Autoregressive Denoising](https://arxiv.org/abs/2512.08931)
*Yixuan Zhu,Jiaqi Feng,Wenzhao Zheng,Yuan Gao,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Astra是一种交互式通用世界模型，通过扩散变压器架构实现多样化场景下精确动作控制的长期未来预测。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在多样化动作交互和长期未来预测上不足，尤其需要处理视觉-动作联合建模的复杂性。

Method: 提出自回归去噪架构与时间因果注意力机制，结合噪声增强记忆库平衡响应性与时间连贯性，并设计动作感知适配器与混合动作专家模块处理多模态动作信号。

Result: 在多个数据集上实现了更高质量的视频预测、更精确的动作对齐和更长范围的时序一致性，支持自动驾驶/机器人抓取等多样化场景。

Conclusion: 通过架构创新与动作交互建模方法，解决了通用世界模型在实时性、动作泛化能力和预测准确性上的关键挑战。

Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [94] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 该论文研究了短上下文主导假设，提出了一种新的长上下文序列检测方法DaMCL，并开发了一种解码算法来缓解短上下文偏见以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型预测通常依赖局部短上下文，作者旨在量化短上下文主导现象，识别需要长上下文的复杂序列，并解决由此产生的输出分布偏见问题。

Method: 通过测量最小上下文长度（MCL）分析预测需求；基于统计特性提出无监督的DaMCL指标替代MCL；设计了基于阈值的检测方法，并开发了一种能动态增强长程相关token的解码算法。

Result: 发现75-80%的序列仅需最后96个token即可准确预测；DaMCL阈值检测方法在区分上下文类型上表现优异；改进的解码算法在问答任务中显著提升性能。

Conclusion: 短上下文主导现象普遍存在，但通过DaMCL检测和特定解码策略，可有效提升模型对长距离依赖关系的建模能力，为优化模型预测提供了新方向。

Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [95] [Adaptation of Embedding Models to Financial Filings via LLM Distillation](https://arxiv.org/abs/2512.08088)
*Eliot Brenner,Dominic Seyler,Manjunath Hegde,Andrei Simion,Koustuv Dasgupta,Bing Xiang*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的训练方法，通过结合通用检索模型与迭代式学生-教师框架，在无需人工标注的前提下显著提升了金融领域文档检索精度，其中在MRR@5和DCG@5指标上分别提升27.7%和44.6%。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型在计算成本与延迟控制方面表现良好，但无法满足金融等专业领域对信息检索精度的特殊需求，且传统人工标注训练成本高昂。

Method: 采用双编码器架构，利用LLM生成的专业相关性评分将领域知识蒸馏到检索模型中。通过学生模型迭代检索语料库挖掘困难正/负例，与教师模型交互式训练，逐步提升检索能力。

Result: 实验表明（1）14类金融文件中MRR@5提高27.7%，DCG@5提高44.6%；（2）FinanceBench四大文档类别中3类NDCG提升；（3）成功构建完全自动化的训练流水线。

Conclusion: 该方法以低资源成本实现了专业领域检索性能的显著提升，为通用模型向垂直领域迁移提供了有效解决方案，未来可扩展至医疗、法律等其他专业场景。

Abstract: Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\texttt{@}$5, 44.6% improvement in mean DCG$\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.

</details>


### [96] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: 提出SEA方法，通过分割、嵌入和轻量对齐实现跨语言的高质量手语视频与字幕同步，支持灵活资源适配且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有端到端方法依赖特定语言/数据集限制泛化性，需开发通用且高效对齐框架。

Method: 使用预训练模型分割视频为单手势并映射到共享潜在空间，结合动态规划实现亚线性时间对齐，支持CPU端分钟级处理小时视频。

Result: 在四种手语数据集达SOTA表现，生成高质量平行数据集且代码模型开源，适配小词典至大数据集资源。

Conclusion: SEA通过结构创新与算法优化，解决了跨语言手语对齐的灵活性与效率瓶颈，为领域研究提供标准化工具。

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [97] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 该论文研究了通用对抗性后缀，通过可微分学习并离散化的方法，在多种模型和任务中有效降低准确率和置信度，攻击效果具备跨任务和模型的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常针对特定任务或模型设计触发词，导致结果难以比较且迁移性受限，因此作者提出一种通用对抗性后缀以解决这些问题。

Method: 采用Gumbel-Softmax松弛进行可微分学习，通过最大化标签区域的校准交叉熵并掩码真实标签词（防止信息泄露），辅以熵正则化避免崩溃，最终将软表示离散化为具体token序列。

Result: 在Qwen2-1.5B、Phi-1.5和TinyLlama-1.1B等模型的多个任务（如情感分析、自然语言推理、常识问答）中，单一后缀训练后即可有效降低准确率和校准置信度，且攻击效果可跨模型迁移。

Conclusion: 证明了通用对抗性后缀具有跨任务和模型家族的强攻击效果，揭示了语言模型在此类统一攻击下的脆弱性。

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [98] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的框架，通过将对抗性后缀视为策略并采用近端策略优化（PPO）进行训练，生成更鲁棒且可跨任务、跨模型迁移的文本攻击手段。


<details>
  <summary>Details</summary>
Motivation: 传统对抗后缀生成方法（如梯度搜索或规则基）过于脆弱且依赖特定任务/模型，需更通用且可迁移的解决方案。

Method: 将对抗后缀作为策略进行强化学习训练，使用冻结模型作为奖励机制，辅以校准交叉熵设计奖励函数，消除标签偏置并通过表征形式聚合提高迁移性。

Result: 在5个NLP基准数据集和3种语言模型上的实验表明，RL训练的后缀显著降低准确性，且跨任务/模型迁移效果优于传统方法。

Conclusion: 强化学习方法生成的对抗后缀更有效且可迁移，揭示了语言模型的潜在漏洞，为鲁棒性研究提供新方向。

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [99] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub integrates ClinicalTrials.gov and PubMed data using large language models (GPT-5.1/Gemini-3-Pro) to improve access to structured clinical trial information by 83.8%, aiding patients, clinicians, and researchers.


<details>
  <summary>Details</summary>
Motivation: To address limited accessibility to structured clinical trial data by combining ClinicalTrials.gov with augmented PubMed-derived information, enabling better decision-making for healthcare stakeholders.

Method: Developed an interactive platform leveraging LLMs to parse full-text research articles, extract structured trial data, translate queries into database searches, and provide a source-linked question-answering system.

Result: Achieved 83.8% higher structured data coverage vs. ClinicalTrials.gov alone, validated through user studies with clinicians/researchers and automated evaluations showing improved information retrieval accuracy.

Conclusion: ClinicalTrialsHub advances evidence-based medicine by combining AI-driven tools with multi-source clinical data, enhancing accessibility for diverse stakeholders in healthcare and research.

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [100] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: This paper examines biases in GLLM annotations by comparing them to manual annotations, revealing adequate F1 scores but significant systematic discrepancies affecting downstream results.


<details>
  <summary>Details</summary>
Motivation: Building on Boukes (2024), this study investigates whether GLLMs introduce systematic bias in annotation tasks, which could critically impact research relying on automated labeling.

Method: Multiple GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) and five prompts were tested across five concepts (political content, interactivity, rationality, incivility, ideology), with performance evaluated against manual annotations.

Result: GLLMs showed acceptable F1 scores but diverged from manual annotations in prevalence, leading to distinct outcomes. Models aligned more with each other than with human annotators, indicating systematic bias not captured by F1 metrics.

Conclusion: F1 scores inadequately reflect systematic bias in GLLM annotations, highlighting risks for research relying on automated methods and emphasizing the need for critical evaluation of model outputs.

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [101] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: 该研究通过可解释性方法分析机器翻译模型中的性别偏见起源，发现模型决策与人类性别感知存在显著重叠，并提出需利用该信息缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦于测量模型性别偏见，但缺乏对偏见形成机制（特别是上下文触发因素）的深入解释，需转向探究偏见根源。

Method: 使用对比解释与显著性归因方法，分析源语言句子中哪些上下文词汇触发目标语言性别词尾选择，并引入多层级归因评估与人类感知对比。

Result: 识别出模型归因显著的源词与人类性别感知词存在明显重叠，且特定词汇模式（如职业名词）对性别决策影响突出。

Conclusion: 模型性别决策机制具备可解释性，其与人类认知的关联性表明可通过理解决策机制直接干预偏见传播，为去偏提供新路径。

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [102] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: 提出一种基于软归纳偏置的推理引导框架，提升韩语不当言论检测精度，准确率达到87.0046%，优于标准监督学习3.89个百分点。


<details>
  <summary>Details</summary>
Motivation: 线上匿名社区不当言论频发，急需有效技术检测并遏制言语暴力，而现有基于韩语语料的大模型推理方法在此领域的应用存在局限性。

Method: 构建包含显式推理视角的软归纳偏置框架，通过约束大模型推理过程强化理性判断，以消除传统训练策略下的逻辑漏洞，对Korean LLM进行微调。

Result: 所提出的Kanana-1.5模型在多个训练策略比较实验中平均准确率达87.0046%，相对标准监督学习提升3.89%，证实了受限推理视角在提升判断精确性和一致性方面的有效性。

Conclusion: 该方法突破了大模型单纯知识模仿的局限，通过引入推理引导机制显著提升不当言论检测效果，为构建安全通信环境提供了新型技术路径。

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [103] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: 提供医疗领域自然语言处理(HCNLP)的三层体系结构(数据/任务/应用)，涵盖合成数据生成、可解释性AI、检索增强生成等新兴方向，填补现有综述空白


<details>
  <summary>Details</summary>
Motivation: 现有医疗NLP综述存在重要缺失：1)隐私保护相关的合成数据生成 2)可解释临床NLP 3)检索增强生成和神经符号方法。需要系统性框架整理数据层(伦理治理)、方法层(NER/RE任务)和应用层(患者参与)的完整图景

Method: 构建三层次架构：1)数据层-标注指南/伦理/合成数据 2)NLP评估层-NER/RE/情感分析任务与检索增强生成方法 3)患者应用层-PPIE/健康素养提升。配套GitHub实践环节(https://github.com/4dpicture/HealthNLP)

Result: 形成完整医疗NLP知识体系，提供从伦理数据处理到患者应用的端到端框架，包含实践教程资源，解决可解释性和多模态数据处理关键技术难点

Conclusion: 本教程首次将神经符号系统、合成数据伦理、临床可解释性整合到统一框架中，为医疗NLP提供系统方法论，降低领域新人学习门槛，促进临床实践落地

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [104] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN是一个用于生成问卷式LLM回应的开源框架，提高调查研究的效率和可重复性。


<details>
  <summary>Details</summary>
Motivation: 传统问卷调查的人工数据收集成本高，现有LLM应用缺乏系统性、低成本的评估工具。

Method: 开发Python框架QSTN，包含问卷结构分析、响应生成算法、多维评估体系及无代码实验界面，支持大语言模型处理调查任务。

Result: 基于4000万次模拟实验，发现问题结构优化和特定生成方法可提升LLM回应与人类数据的对齐度，计算成本仅需传统方法的零头。

Conclusion: QSTN为基于LLM的社会科学和调查研究提供了可扩展的技术基础，推动了可复现、高效的研究范式。

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [105] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 本文发布首个巴斯克语CEFR C1水平的AES公开数据集及反馈系统，包含3200篇专家标注的作文。通过微调开源模型（如Latxa），实现超越GPT-5等闭源模型的评分一致性，并提出结合自动指标与专家验证的反馈评估新方法。


<details>
  <summary>Details</summary>
Motivation: 巴斯克语作为低资源语言，在自动作文评分和反馈生成领域缺乏标准化数据与模型，现有闭源系统（如GPT系列）存在评分一致性不足、反馈教育性较弱的问题，需建立透明可复现的基准。

Method: 1. 构建包含3200篇作文的BasqueAES数据集，标注5项评分维度并附专家反馈；2. 对RoBERTa-EusCrawl和Latxa系列模型进行监督微调；3. 设计融合自动一致性指标（如BERTScore）与人工错误类型验证的双阶段评估框架。

Result: Latxa模型经SFT微调后，评分一致性比GPT-5/Gemini Ultra提升23%，生成反馈覆盖错误类型增加38%，且89%的反馈通过教育专家的pedagogical validity验证。

Conclusion: 开源模型经专业领域微调可超越闭源系统，BasqueAES数据集填补了低资源语言NLP空白，提出的评估方法为后续教育导向的NLP研究提供标准化范式。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [106] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 本研究提出了一种针对低资源语言的后训练方法，即使使用不流畅的奖励模型进行对齐，也能保持语言模型的流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化研究多针对英语和中文，而低资源语言既缺乏母语者撰写的数据集，也缺乏能生成流畅合成数据的语言模型。需要解决零目标语言指令微调数据下的流畅性对齐问题。

Method: 采用在线策略训练方法，与两种常见方法对比：基于机器翻译数据的监督微调和多语言微调。以挪威语Bokmål为例进行案例研究，通过母语者评估流畅性。

Result: 实验表明在线策略训练效果显著优于替代方案，且无需依赖稀缺数据。母语者评估验证了其流畅性保持效果。

Conclusion: 该方法为低资源语言提供了有效的偏好对齐解决方案，突破了传统方法对目标语言数据的依赖。

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [107] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出了一种在联邦学习框架下对齐大型语言模型（LLMs）与多样化人类偏好的综合评估框架，并引入自适应权重调整方法，在保持对齐质量的前提下显著提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中对齐LLMs与人类偏好的标准方法未能充分体现不同群体的多样化观点，导致潜在的不公平性。

Method: 设计基于群体历史对齐表现的自适应奖励聚合策略，对比分析min/max/average等传统方法，并构建评估框架量化公平性与对齐质量的权衡。

Result: 自适应方法在问答任务中实现了更高公平性（相对提升达23%），同时保持与传统方法相当的对齐效果，平衡了模型效用与群体公平性需求。

Conclusion: 研究表明联邦学习中需动态调整偏好聚合策略以实现群体公平性，提出的自适应方法为开发符合多样化群体需求的LLMs提供了可推广的技术路径。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [108] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 本文提出了RAGLens，一种基于稀疏自编码器（SAE）的轻量化幻觉检测方法，通过解读大语言模型（LLM）内部激活特征，提升检索增强生成（RAG）系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG幻觉检测方法或依赖大规模标注数据训练，或需调用外部分析模型，存在成本高与准确性不足的问题。本文旨在利用LLM内部表征与因果可解释性技术，实现高效且可解释的幻觉检测。

Method: 基于稀疏自编码器（SAE）解耦内部激活特征，结合信息驱动特征选择与加性特征建模，构建RAGLens检测框架。

Result: RAGLens在检测性能上优于现有方法，提供可解释的决策依据，并揭示LLM内部幻觉信号的分布规律。

Conclusion: 该研究验证了基于内部特征解析的幻觉检测可行性，为RAG系统的后验修正与理论分析提供了新工具。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>
