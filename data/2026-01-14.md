<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]
- [cs.CL](#cs.CL) [Total: 57]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration](https://arxiv.org/abs/2601.07845)
*Shree Charran R,Rahul Kumar Dubey*

Main category: cs.CV

TL;DR: 本文提出了一种实时路侧感知节点，用于交通违规分析与安全事件分发，基于YOLOv8 Nano和DeepSORT实现在NVIDIA Jetson Nano平台，具备高精度、低功耗和多语言车牌识别能力。


<details>
  <summary>Details</summary>
Motivation: 印度等新兴经济体的快速机动化导致执法压力激增（2023年违规记录超1100万起，警力密度仅1名警官/4000辆车辆），传统监控和手动处罚方案无法满足规模需求，需构建自主、协作且节能的边缘AI感知基础设施。

Method: 基于YOLOv8 Nano实现多类别交通违规检测与多目标跟踪（DeepSORT），结合规则引导的OCR后处理引擎解析复杂环境车牌；通过TensorRT FP16量化优化模型，在Jetson Nano硬件平台上部署V2X协议实时传输CAM/DENM安全事件。

Result: 系统实现9.6W功耗下28-30fps推理速度，违规检测准确率97.7%（5类违规：闯红灯、压线、逆行、非法掉头、超速），OCR精度84.9%；相比YOLOv4 Tiny等模型，mAP提升10.7%，能效比提升1.4倍。

Conclusion: 研究表明基于边缘AI的路侧节点可提升交通执法效率与标准化安全事件推送能力，验证了IEEE智能车辆生态中协作感知与主动安全管理的可行性。

Abstract: Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.

</details>


### [2] [An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.07855)
*Subeen Lee,Siyeong Lee,Namil Kim,Jaesik Choi*

Main category: cs.CV

TL;DR: 提出了ROAD基准测试，用于评估3D点云感知在域和标签变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 3D感知系统需适应现实应用中的对象定义和传感器域变化，但现有研究在同时应对域和标签偏移方面不足。

Method: 构建ROAD基准，包含LiDAR分类任务及三种标签演变形式，并在Waymo等数据集上测试零样本迁移、线性探针和CL方法。

Result: 现有方法在域标签联合偏移下表现受限，特定主干架构和训练目标可提升鲁棒性。

Conclusion: ROAD为3D感知鲁棒性研究提供了基线，揭示了持续学习与迁移学习的未来方向。

Abstract: For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.

</details>


### [3] [Moonworks Lunara Aesthetic Dataset](https://arxiv.org/abs/2601.07941)
*Yan Wang,M M Sayeef Abdullah,Partho Hassan,Sabit Hassan*

Main category: cs.CV

TL;DR: The Lunara Aesthetic Dataset provides high-quality, stylistically diverse artwork images with detailed annotations, prioritizing aesthetic excellence and licensing transparency under Apache 2.0.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets focused on high aesthetic quality, stylistic precision, and ethical licensing while capturing global artistic styles and detailed descriptive metadata.

Method: Used the Moonworks Lunara model to generate images across diverse aesthetics (regional and categorical), combined with human-refined prompts and structured annotations describing objects, attributes, relationships, and stylistic elements.

Result: Aesthetic scores significantly outperform existing aesthetics-focused and general-purpose datasets, establishing a new benchmark for quality, diversity, and usability in art datasets.

Conclusion: The dataset supports research and commercial applications with validated quality, explicit licensing, and cross-cultural stylistic representation unavailable in broadly scraped web-derived collections.

Abstract: The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.

</details>


### [4] [3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing](https://arxiv.org/abs/2601.07963)
*Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 3DGS-Drag是一个基于点的3D编辑框架，结合3D高斯泼溅和扩散模型引导的变形与内容修正技术，通过渐进策略实现高效直观的几何相关3D场景拖拽编辑


<details>
  <summary>Details</summary>
Motivation: 解决传统3D编辑方法在几何内容修改时效率低下与用户交互不直观的问题，填补变形编辑与2D编辑方法之间的技术鸿沟

Method: 提出3D高斯泼溅变形引导实现几何一致性修改，结合扩散模型进行内容修正与视觉增强，并采用渐进式多阶段编辑策略处理复杂拖拽操作

Result: 成功完成运动改变、形状调整、补全扩展等多样编辑任务，在各类场景中均达到sota编辑效果，单卡RTX4090实现10-20分钟全流程编辑

Conclusion: 验证了3D高斯泼溅与扩散模型融合编辑方案的有效性，在保持几何准确性的前提下显著提升视觉质量，为复杂3D场景编辑提供了实用性强的解决方案

Abstract: The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.

</details>


### [5] [Sesame Plant Segmentation Dataset: A YOLO Formatted Annotated Dataset](https://arxiv.org/abs/2601.07970)
*Sunusi Ibrahim Muhammad,Ismail Ismail Tijjani,Saadatu Yusuf Jumare,Fatima Isah Jibrin*

Main category: cs.CV

TL;DR: 本论文介绍了Sesame Plant Segmentation Dataset，一个专注于芝麻植物的开源标注图像数据集，用于支持农业应用中人工智能模型的开发。数据集包含206张训练图像、43张验证图像和43张测试图像，采用YOLO兼容分割格式。基于Ultralytics YOLOv8框架的模型评估显示，检测和分割任务均表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统边界框数据集无法满足精准农业需求，该研究通过像素级分割解决芝麻植物早期生长阶段的精准监测问题，填补了尼日利亚芝麻农业视觉数据集的空白，支持植物监测、产量估计和农业研究。

Method: 数据集通过高分辨率移动摄像机在尼日利亚卡齐纳州采集，采用Segment Anything Model 2.0（SAMv2）在农民监督下标注。使用Ultralytics YOLOv8框架进行模型评估，计算检测与分割任务的召回率、精度、mAP@0.50和mAP@0.50-0.95。

Result: 检测任务：召回率79%，精度79%，mAP@0.50为84%，mAP@0.50-0.95为58%。分割任务：召回率82%，精度77%，mAP@0.50为84%，mAP@0.50-0.95为52%。

Conclusion: 该数据集是首个针对尼日利亚芝麻植物的像素级分割数据集，为农业人工智能模型开发提供了关键基础设施，可提升早期生长阶段的植物分析精度。

Abstract: This paper presents the Sesame Plant Segmentation Dataset, an open source annotated image dataset designed to support the development of artificial intelligence models for agricultural applications, with a specific focus on sesame plants. The dataset comprises 206 training images, 43 validation images, and 43 test images in YOLO compatible segmentation format, capturing sesame plants at early growth stages under varying environmental conditions. Data were collected using a high resolution mobile camera from farms in Jirdede, Daura Local Government Area, Katsina State, Nigeria, and annotated using the Segment Anything Model version 2 with farmer supervision. Unlike conventional bounding box datasets, this dataset employs pixel level segmentation to enable more precise detection and analysis of sesame plants in real world farm settings. Model evaluation using the Ultralytics YOLOv8 framework demonstrated strong performance for both detection and segmentation tasks. For bounding box detection, the model achieved a recall of 79 percent, precision of 79 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 58 percent. For segmentation, it achieved a recall of 82 percent, precision of 77 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 52 percent. The dataset represents a novel contribution to sesame focused agricultural vision datasets in Nigeria and supports applications such as plant monitoring, yield estimation, and agricultural research.

</details>


### [6] [An Efficient Additive Kolmogorov-Arnold Transformer for Point-Level Maize Localization in Unmanned Aerial Vehicle Imagery](https://arxiv.org/abs/2601.07975)
*Fei Li,Lang Qiao,Jiahao Fan,Yijia Xu,Shawn M. Kaeppler,Zhou Zhang*

Main category: cs.CV

TL;DR: 基于Kolmogorov-Arnold理论的高效定位算法AKT，解决无人机高分辨率玉米点级定位难题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在应对小物体检测（<0.1%像素占比）、超高分辨率影像计算负载（>3000×4000像素）及农田场景复杂性（稀疏目标分布、环境干扰）时存在显著性能瓶颈。

Method: 提出AKT模型，采用PKAN模块替代传统MLP增强小目标特征表达，设计PAA注意力机制降低计算复杂度；同时发布包含50.1万点标注的PML无人机玉米定位数据集。

Result: 在平均F1分数提升4.2%至62.8%的同时，计算量减少12.6%，推理吞吐量提升20.7%；下游任务中出苗数误差7.1株，株距误差1.95-1.97厘米。

Conclusion: 将Kolmogorov-Arnold表示理论与高效注意力机制结合，构建了高分辨率农业遥感分析的有效框架。

Abstract: High-resolution UAV photogrammetry has become a key technology for precision agriculture, enabling centimeter-level crop monitoring and point-level plant localization. However, point-level maize localization in UAV imagery remains challenging due to (1) extremely small object-to-pixel ratios, typically less than 0.1%, (2) prohibitive computational costs of quadratic attention on ultra-high-resolution images larger than 3000 x 4000 pixels, and (3) agricultural scene-specific complexities such as sparse object distribution and environmental variability that are poorly handled by general-purpose vision models.
  To address these challenges, we propose the Additive Kolmogorov-Arnold Transformer (AKT), which replaces conventional multilayer perceptrons with Pade Kolmogorov-Arnold Network (PKAN) modules to enhance functional expressivity for small-object feature extraction, and introduces PKAN Additive Attention (PAA) to model multiscale spatial dependencies with reduced computational complexity. In addition, we present the Point-based Maize Localization (PML) dataset, consisting of 1,928 high-resolution UAV images with approximately 501,000 point annotations collected under real field conditions.
  Extensive experiments show that AKT achieves an average F1-score of 62.8%, outperforming state-of-the-art methods by 4.2%, while reducing FLOPs by 12.6% and improving inference throughput by 20.7%. For downstream tasks, AKT attains a mean absolute error of 7.1 in stand counting and a root mean square error of 1.95-1.97 cm in interplant spacing estimation. These results demonstrate that integrating Kolmogorov-Arnold representation theory with efficient attention mechanisms offers an effective framework for high-resolution agricultural remote sensing.

</details>


### [7] [Likelihood ratio for a binary Bayesian classifier under a noise-exclusion model](https://arxiv.org/abs/2601.07982)
*Howard C. Gifford*

Main category: cs.CV

TL;DR: 本文提出了一种新的统计理想观察者模型，通过设置最小可提取图像特征阈值减少自由参数，应用于医学图像感知、计算机视觉及防御/安全领域。


<details>
  <summary>Details</summary>
Motivation: 现有视觉搜索模型存在参数冗余问题，需要一种更高效的框架实现特征选择与系统优化，尤其在医学成像和目标检测等关键领域。

Method: 建立基于特征阈值处理的数学模型，通过约束最小可提取图像特征参数来压缩系统复杂度，同时保留核心处理能力。

Result: 提出了可扩展的理想观察者框架，在医学图像特征评估、传感器性能测试、计算机视觉任务中实现参数优化和高效特征选择。

Conclusion: 该模型为视觉信息处理提供新范式，解决了特征冗余与系统效率的权衡问题，在跨学科应用中具有显著技术优势。

Abstract: We develop a new statistical ideal observer model that performs holistic visual search (or gist) processing in part by placing thresholds on minimum extractable image features. In this model, the ideal observer reduces the number of free parameters thereby shrinking down the system. The applications of this novel framework is in medical image perception (for optimizing imaging systems and algorithms), computer vision, benchmarking performance and enabling feature selection/evaluations. Other applications are in target detection and recognition in defense/security as well as evaluating sensors and detectors.

</details>


### [8] [Predicting Region of Interest in Human Visual Search Based on Statistical Texture and Gabor Features](https://arxiv.org/abs/2601.07998)
*Hongwei Lin,Diego Andrade,Mini Das,Howard C. Gifford*

Main category: cs.CV

TL;DR: This paper explores combining Gabor-based structural features and GLCM texture features to model human visual search behavior, showing their integration improves fixation prediction in breast imaging tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between structural (Gabor) and texture-based (GLCM) features for better modeling of early-stage human attention in location-unknown visual search tasks.

Method: Proposed two pipelines merging Gabor and GLCM features to narrow fixation regions. Evaluated on simulated breast tomosynthesis images and compared with a threshold-based model. Eye-tracking data was used for validation.

Result: Combined features showed qualitative alignment with model observer predictions; GLCM mean and Gabor responses were strongly correlated. Eye-tracking confirmed consistency between predicted regions and human gaze behavior.

Conclusion: Integrating structural-texture features enhances visual search modeling, supporting development of perceptually accurate observer models for attention prediction.

Abstract: Understanding human visual search behavior is a fundamental problem in vision science and computer vision, with direct implications for modeling how observers allocate attention in location-unknown search tasks. In this study, we investigate the relationship between Gabor-based features and gray-level co-occurrence matrix (GLCM) based texture features in modeling early-stage visual search behavior. Two feature-combination pipelines are proposed to integrate Gabor and GLCM features for narrowing the region of possible human fixations. The pipelines are evaluated using simulated digital breast tomosynthesis images. Results show qualitative agreement among fixation candidates predicted by the proposed pipelines and a threshold-based model observer. A strong correlation is observed between GLCM mean and Gabor feature responses, indicating that these features encode related image information despite their different formulations. Eye-tracking data from human observers further suggest consistency between predicted fixation regions and early-stage gaze behavior. These findings highlight the value of combining structural and texture-based features for modeling visual search and support the development of perceptually informed observer models.

</details>


### [9] [CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation](https://arxiv.org/abs/2601.08010)
*Chaoyu Li,Deeparghya Dutta Barua,Fei Tao,Pooyan Fazli*

Main category: cs.CV

TL;DR: 提出CASHEW和CASHEW-RL两种方法，通过推理时聚合多轨迹与视觉验证提升视觉-语言模型的多步推理稳定性，在多项基准测试中显著提效。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言模型多步推理结果不稳定问题，现有模型因推理路径差异易产生错误预测。

Method: CASHEW：推理时通过迭代聚合多候选路径生成高质量推理链，并用视觉验证过滤错误步骤；CASHEW-RL：通过分组序列策略优化（GSPO）训练模型学习自聚合，结合最小视觉证据奖励与任务难度自适应推理机制。

Result: 在13个图像/视频理解基准测试中显著提升，ScienceQA提高23.6%，EgoSchema提高8.1%。

Conclusion: 所提方法有效增强视觉-语言模型推理稳定性与准确性，尤其在复杂多步任务中表现突出。

Abstract: Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.

</details>


### [10] [TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models](https://arxiv.org/abs/2601.08011)
*Xin Jin,Yichuan Zhong,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文提出TP-Blend，一种无需训练的轻量级扩散编辑框架，通过双提示分别控制目标物体与风格融合，在单次去噪过程中实现高分辨率、精确的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动扩散编辑模型难以同时实现新物体与新风格的联合替换，传统方法或需多次编辑流程、或存在维度丢失问题，需要同时处理内容与外观参数协同优化的瓶颈。

Method: 1. 提出Cross-Attention Object Fusion(CAOF)：通过头级注意力均值定位空间位置，用熵正则化最优传输分配多头特征向量；2. Self-Attention Style Fusion(SASF)：低高频分离后，高频细节通过实例归一化融合，并通过键值矩阵替换实现上下文感知风格调制；3. 保持完整特征维度(如SD-XL 640维)同时降低计算负载。

Result: 在定量保真度(FID/CLIP)、感知质量(LPIPS)和推理速度方面显著优于DiffEdit、Prompt-to-Prompt等基线模型，尤其擅长处理跨类别物体替换与复杂艺术风格的同步编辑(如梵高笔触的咖啡桌替换)。

Conclusion: TP-Blend通过双注意力机制解耦内容与风格编辑，首次实现单步扩散流程中同时进行结构重写与纹理重绘，在参数效率、编辑精度和推理延迟上取得平衡，为扩散模型的可控生成提供了新思路。

Abstract: Current text-conditioned diffusion editors handle single object replacement well but struggle when a new object and a new style must be introduced simultaneously. We present Twin-Prompt Attention Blend (TP-Blend), a lightweight training-free framework that receives two separate textual prompts, one specifying a blend object and the other defining a target style, and injects both into a single denoising trajectory. TP-Blend is driven by two complementary attention processors. Cross-Attention Object Fusion (CAOF) first averages head-wise attention to locate spatial tokens that respond strongly to either prompt, then solves an entropy-regularised optimal transport problem that reassigns complete multi-head feature vectors to those positions. CAOF updates feature vectors at the full combined dimensionality of all heads (e.g., 640 dimensions in SD-XL), preserving rich cross-head correlations while keeping memory low. Self-Attention Style Fusion (SASF) injects style at every self-attention layer through Detail-Sensitive Instance Normalization. A lightweight one-dimensional Gaussian filter separates low- and high-frequency components; only the high-frequency residual is blended back, imprinting brush-stroke-level texture without disrupting global geometry. SASF further swaps the Key and Value matrices with those derived from the style prompt, enforcing context-aware texture modulation that remains independent of object fusion. Extensive experiments show that TP-Blend produces high-resolution, photo-realistic edits with precise control over both content and appearance, surpassing recent baselines in quantitative fidelity, perceptual quality, and inference speed.

</details>


### [11] [Representations of Text and Images Align From Layer One](https://arxiv.org/abs/2601.08017)
*Evžen Wybitul,Javier Rando,Florian Tramèr,Stanislav Fort*

Main category: cs.CV

TL;DR: The paper challenges the belief that image-text alignment emerges in late layers of vision-language models, showing it occurs as early as the first layer using a synthesis-based method inspired by DeepDream.


<details>
  <summary>Details</summary>
Motivation: To investigate how textual concepts and their visual representations align across layers in adapter-based models, and to provide a direct, constructive method for analyzing multimodal alignment without relying on auxiliary models or datasets.

Method: Proposed a synthesis method where textual concept vectors at given layers are used to optimize and generate images aligned with the vectors, leveraging an approach akin to DeepDream for analyzing alignment layer-by-layer.

Result: For Gemma 3, synthesized images at layer 1 showed over 50% recognizable visual features of targeted concepts (e.g., animals, activities), indicating early alignment of textual and visual representations.

Conclusion: The method offers a new way to interpret vision-language models by visualizing their layer-wise representation spaces, and provides evidence that image-text alignment starts much earlier than previously believed.

Abstract: We show that for a variety of concepts in adapter-based vision-language models, the representations of their images and their text descriptions are meaningfully aligned from the very first layer. This contradicts the established view that such image-text alignment only appears in late layers. We show this using a new synthesis-based method inspired by DeepDream: given a textual concept such as "Jupiter", we extract its concept vector at a given layer, and then use optimisation to synthesise an image whose representation aligns with that vector. We apply our approach to hundreds of concepts across seven layers in Gemma 3, and find that the synthesised images often depict salient visual features of the targeted textual concepts: for example, already at layer 1, more than 50 % of images depict recognisable features of animals, activities, or seasons. Our method thus provides direct, constructive evidence of image-text alignment on a concept-by-concept and layer-by-layer basis. Unlike previous methods for measuring multimodal alignment, our approach is simple, fast, and does not require auxiliary models or datasets. It also offers a new path towards model interpretability, by providing a way to visualise a model's representation space by backtracing through its image processing components.

</details>


### [12] [Training Free Zero-Shot Visual Anomaly Localization via Diffusion Inversion](https://arxiv.org/abs/2601.08022)
*Samet Hicsonmez,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出了一种无需训练的纯视觉零样本异常检测方法，通过反转去噪扩散隐式模型（DDIM）实现异常定位。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法需依赖文本等辅助模态生成细粒度提示，而纯视觉方法仅能进行图像级分类，缺乏空间精度。需要在不依赖外部模态的情况下实现精准的异常定位。

Method: 给定输入图像和通用文本描述（如'X类物体图像'），通过反转预训练DDIM模型获得潜在表征，并从固定中间时间步启动去噪过程重建图像。由于模型仅训练于正常数据，重建图像呈现正常形态，输入与重建的差异即为异常区域。

Result: 在VISA数据集上取得SOTA性能，成功实现不依赖辅助模态的异常定位。

Conclusion: 建立了不依赖文本提示的纯视觉零样本异常检测范式，为后续研究提供了无需训练的扩散模型应用方案。

Abstract: Zero-Shot image Anomaly Detection (ZSAD) aims to detect and localise anomalies without access to any normal training samples of the target data. While recent ZSAD approaches leverage additional modalities such as language to generate fine-grained prompts for localisation, vision-only methods remain limited to image-level classification, lacking spatial precision. In this work, we introduce a simple yet effective training-free vision-only ZSAD framework that circumvents the need for fine-grained prompts by leveraging the inversion of a pretrained Denoising Diffusion Implicit Model (DDIM). Specifically, given an input image and a generic text description (e.g., "an image of an [object class]"), we invert the image to obtain latent representations and initiate the denoising process from a fixed intermediate timestep to reconstruct the image. Since the underlying diffusion model is trained solely on normal data, this process yields a normal-looking reconstruction. The discrepancy between the input image and the reconstructed one highlights potential anomalies. Our method achieves state-of-the-art performance on VISA dataset, demonstrating strong localisation capabilities without auxiliary modalities and facilitating a shift away from prompt dependence for zero-shot anomaly detection research. Code is available at https://github.com/giddyyupp/DIVAD.

</details>


### [13] [A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs](https://arxiv.org/abs/2601.08024)
*Amin Abbasishahkoo,Mahboubeh Dadkhah,Lionel Briand*

Main category: cs.CV

TL;DR: 本文提出基于概念的多样性（CBD）指标和Margin结合方法，用于高效选择图像输入标注，显著提升DNN微调性能。


<details>
  <summary>Details</summary>
Motivation: 现有输入选择方法计算量大且扩展性差，亟需高效且可扩展的解决方案。

Method: 提出CBD指标（利用VLM加速计算）并结合Margin（不确定度量），构建混合选择方法。

Result: CBD与GD强相关且耗时更少；CBD+Margin在多模型/数据集/预算下均超越基线方法，选择时间接近简单不确定度方法。

Conclusion: CBD方法兼具高效性与可扩展性，在重复大规模输入选择中表现优异。

Abstract: Maintaining or improving the performance of Deep Neural Networks (DNNs) through fine-tuning requires labeling newly collected inputs, a process that is often costly and time-consuming. To alleviate this problem, input selection approaches have been developed in recent years to identify small, yet highly informative subsets for labeling. Diversity-based selection is one of the most effective approaches for this purpose. However, they are often computationally intensive and lack scalability for large input sets, limiting their practical applicability. To address this challenge, we introduce Concept-Based Diversity (CBD), a highly efficient metric for image inputs that leverages Vision-Language Models (VLM). Our results show that CBD exhibits a strong correlation with Geometric Diversity (GD), an established diversity metric, while requiring only a fraction of its computation time. Building on this finding, we propose a hybrid input selection approach that combines CBD with Margin, a simple uncertainty metric. We conduct a comprehensive evaluation across a diverse set of DNN models, input sets, selection budgets, and five most effective state-of-the-art selection baselines. The results demonstrate that the CBD-based selection consistently outperforms all baselines at guiding input selection to improve the DNN model. Furthermore, the CBD-based selection approach remains highly efficient, requiring selection times close to those of simple uncertainty-based methods such as Margin, even on larger input sets like ImageNet. These results confirm not only the effectiveness and computational advantage of the CBD-based approach, particularly compared to hybrid baselines, but also its scalability in repetitive and extensive input selection scenarios.

</details>


### [14] [FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures](https://arxiv.org/abs/2601.08026)
*Jifeng Song,Arun Das,Pan Wang,Hui Ji,Kun Zhao,Yufei Huang*

Main category: cs.CV

TL;DR: This paper introduces FigEx2, a visual-based framework for locating and captioning individual panels in scientific compound figures, outperforming existing methods and showing strong cross-domain adaptability without additional tuning.


<details>
  <summary>Details</summary>
Motivation: Existing scientific compound figure analysis tools often lack detailed panel-by-panel captions, relying on vague or missing text summaries. Current models (e.g., Qwen3-VL-8B) demonstrate weak performance in this task due to multimodal inconsistency issues caused by diverse caption phrasing and insufficient supervision data.

Method: 1) Developed a visual-grounded framework (FigEx2) with a noise-aware gated fusion module to filter irrelevant textual noise during caption generation. 2) Used combined supervised + reinforcement learning with CLIP/BERTScore rewards to enforce multimodal consistency. 3) Curated a high-quality benchmark (BioSci-Fig-Cap) with cross-disciplinary test sets across physics/chemistry domains.

Result: 1) Achieved 72.6% mAP@0.5:0.95 for panel detection 2) Surpassed Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore for caption quality 3) Demonstrated strong zero-shot transferability to unseen scientific domains without retraining

Conclusion: FigEx2 effectively addresses key challenges in panel-level scientific figure understanding through innovative architecture design, training strategies, and dataset curation. The framework's robustness across disciplines and zero-shot capabilities make it valuable for improving automated analysis of complex scientific visualizations.

Abstract: Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.

</details>


### [15] [Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling](https://arxiv.org/abs/2601.08040)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Integscan是一种新的医学图像伪造检测框架，结合视觉-语言模型，通过Rescind数据集实现出色的检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像伪造检测面临领域特有伪影和复杂纹理挑战，现有自然图像取证方法难以适用，需开发专门框架。

Method: 采用扩散合成与视觉-语言提示生成可控伪造图像，创建Rescind标注数据集；Integscan框架融合注意力视觉编码与语义对齐，并引入验证循环过滤伪造样本。

Result: 在Rescind和其他数据集实验显示Integscan在检测与定位方面达SOTA性能，成功建立医学图像完整性分析基础。

Conclusion: 该框架通过多模态建模有效解决医学图像伪造问题，Rescind数据集为后续研究提供基准资源。

Abstract: Scientific image manipulation in biomedical publications poses a growing threat to research integrity and reproducibility. Unlike natural image forensics, biomedical forgery detection is uniquely challenging due to domain-specific artifacts, complex textures, and unstructured figure layouts. We present the first vision-language guided framework for both generating and detecting biomedical image forgeries. By combining diffusion-based synthesis with vision-language prompting, our method enables realistic and semantically controlled manipulations, including duplication, splicing, and region removal, across diverse biomedical modalities. We introduce Rescind, a large-scale benchmark featuring fine-grained annotations and modality-specific splits, and propose Integscan, a structured state space modeling framework that integrates attention-enhanced visual encoding with prompt-conditioned semantic alignment for precise forgery localization. To ensure semantic fidelity, we incorporate a vision-language model based verification loop that filters generated forgeries based on consistency with intended prompts. Extensive experiments on Rescind and existing benchmarks demonstrate that Integscan achieves state of the art performance in both detection and localization, establishing a strong foundation for automated scientific integrity analysis.

</details>


### [16] [The Role of Noisy Data in Improving CNN Robustness for Image Classification](https://arxiv.org/abs/2601.08043)
*Oscar H. Ramírez-Agudelo,Nicoleta Gorea,Aliza Reif,Lorenzo Bonasera,Michael Karl*

Main category: cs.CV

TL;DR: Training CNNs with 10% controlled noisy data improves model robustness against real-world distortions while maintaining performance on clean data.


<details>
  <summary>Details</summary>
Motivation: Real-world data often contains noise/corruptions not present in curated training sets, creating a gap between ideal training conditions and practical deployment scenarios for CNNs.

Method: Introduced Gaussian noise, Salt-and-Pepper noise, and Gaussian blur at varying intensities into CIFAR-10 training data using ResNet-18 models, systematically testing different 'pollution levels' of noisy data during training.

Result: Adding 10% noisy data reduced test loss and boosted accuracy on fully corrupted test sets by 4.2% (Salt-and-Pepper)/3.7% (Gaussian noise) with <1% accuracy drop on clean data compared to standard training.

Conclusion: Strategic noise injection serves as an effective regularization technique, creating models that better balance traditional data cleanliness requirements with real-world robustness needs.

Abstract: Data quality plays a central role in the performance and robustness of convolutional neural networks (CNNs) for image classification. While high-quality data is often preferred for training, real-world inputs are frequently affected by noise and other distortions. This paper investigates the effect of deliberately introducing controlled noise into the training data to improve model robustness. Using the CIFAR-10 dataset, we evaluate the impact of three common corruptions, namely Gaussian noise, Salt-and-Pepper noise, and Gaussian blur at varying intensities and training set pollution levels. Experiments using a Resnet-18 model reveal that incorporating just 10\% noisy data during training is sufficient to significantly reduce test loss and enhance accuracy under fully corrupted test conditions, with minimal impact on clean-data performance. These findings suggest that strategic exposure to noise can act as a simple yet effective regularizer, offering a practical trade-off between traditional data cleanliness and real-world resilience.

</details>


### [17] [Exploiting DINOv3-Based Self-Supervised Features for Robust Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2601.08078)
*Guoping Xu,Jayaram K. Udupa,Weiguo Lu,You Zhang*

Main category: cs.CV

TL;DR: 本文提出DINO-AugSeg框架，结合小波变换增强和跨注意力融合模块，有效提升少样本下的医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注数据稀缺导致传统深度学习方法效果受限，虽自然图像预训练模型如DINOv3能提取密集特征，但域差异阻碍其直接应用。

Method: 1) WT-Aug模块通过小波变换扰动特征频域分量增强数据多样性；2) CG-Fuse模块采用跨注意力机制融合高低分辨率特征的上下文信息。

Result: 在6个医学图像数据集（5种模态）上测试显示，DINO-AugSeg在有限样本下均超越现有方法，并揭示了频域增强对特征表达的显著优化。

Conclusion: 所提框架成功解决医学图像域差异问题，小波变换与特征融合策略显著提升了少样本学习效果，为医疗AI提供新思路。

Abstract: Deep learning-based automatic medical image segmentation plays a critical role in clinical diagnosis and treatment planning but remains challenging in few-shot scenarios due to the scarcity of annotated training data. Recently, self-supervised foundation models such as DINOv3, which were trained on large natural image datasets, have shown strong potential for dense feature extraction that can help with the few-shot learning challenge. Yet, their direct application to medical images is hindered by domain differences. In this work, we propose DINO-AugSeg, a novel framework that leverages DINOv3 features to address the few-shot medical image segmentation challenge. Specifically, we introduce WT-Aug, a wavelet-based feature-level augmentation module that enriches the diversity of DINOv3-extracted features by perturbing frequency components, and CG-Fuse, a contextual information-guided fusion module that exploits cross-attention to integrate semantic-rich low-resolution features with spatially detailed high-resolution features. Extensive experiments on six public benchmarks spanning five imaging modalities, including MRI, CT, ultrasound, endoscopy, and dermoscopy, demonstrate that DINO-AugSeg consistently outperforms existing methods under limited-sample conditions. The results highlight the effectiveness of incorporating wavelet-domain augmentation and contextual fusion for robust feature representation, suggesting DINO-AugSeg as a promising direction for advancing few-shot medical image segmentation. Code and data will be made available on https://github.com/apple1986/DINO-AugSeg.

</details>


### [18] [From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models](https://arxiv.org/abs/2601.08095)
*Dongsik Yoon,Jongeun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的三阶段自动化流程，生成领域特定的合成数据集，通过多模态评估和用户偏好分类器提高数据质量，解决分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 预训练模型与实际部署环境存在分布偏移，传统数据收集方法依赖大量真实数据，本文旨在通过合成数据生成减少依赖性并提升数据适配性。

Method: 1) 控制 inpainting 在特定背景中生成目标对象；2) 多模态评估结合目标检测、美学评分和视觉语言对齐；3) 用户偏好分类器捕捉主观需求。

Result: 实验验证了流程高效生成高质量数据集的能力，显著减少真实数据采集需求，且生成数据与目标领域匹配性高。

Conclusion: 该框架为部署级数据集生成提供了可扩展的自动化解决方案，通过扩散模型与多模态评估结合，有效桥接分布偏移问题。

Abstract: In this paper, we present an automated pipeline for generating domain-specific synthetic datasets with diffusion models, addressing the distribution shift between pre-trained models and real-world deployment environments. Our three-stage framework first synthesizes target objects within domain-specific backgrounds through controlled inpainting. The generated outputs are then validated via a multi-modal assessment that integrates object detection, aesthetic scoring, and vision-language alignment. Finally, a user-preference classifier is employed to capture subjective selection criteria. This pipeline enables the efficient construction of high-quality, deployable datasets while reducing reliance on extensive real-world data collection.

</details>


### [19] [PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images](https://arxiv.org/abs/2601.08127)
*Mohamad Koohi-Moghadam,Mohammad-Ali Nikouei Mahani,Kyongtae Tyler Bae*

Main category: cs.CV

TL;DR: PathoGen是一种基于扩散模型的生成方法，通过可控的高保真病灶修复技术合成高分辨率的组织病理学图像，解决了专家标注数据不足对医学AI诊断模型开发的制约问题。实验表明其生成效果超越GAN和Stable Diffusion，显著提升小样本场景下的分割性能，同时自动生成像素级标注。


<details>
  <summary>Details</summary>
Motivation: 组织病理学诊断AI模型的开发严重受限于罕见病理特征和疾病亚型的专家标注数据稀缺性。传统数据增强方法无法生成保持复杂空间关系和细胞结构的真实病灶形态。

Method: 提出PathoGen扩散生成模型，采用迭代优化机制实现病灶区域的可控制高质量重建，通过噪声扩散和反向生成过程同步生成病理图像和像素级标注，保留自然组织边界、细胞结构和染色特征。

Result: 在肾、皮肤、乳腺和前列腺病理四个数据集验证：1）定量评估显示超越条件GAN和Stable Diffusion的生成质量；2）增强数据提升下游分割任务性能，尤其在数据稀缺场景下表现突出；3）同步生成图像和标注有效缓解人工标注瓶颈。

Conclusion: 该方法为开发泛化性强的医学AI系统提供了可扩展路径，其高质量自标注数据生成能力显著降低专家标注依赖，突破了病理AI发展的关键瓶颈。

Abstract: The development of robust artificial intelligence models for histopathology diagnosis is severely constrained by the scarcity of expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. While data augmentation offers a potential solution, existing methods fail to generate sufficiently realistic lesion morphologies that preserve the complex spatial relationships and cellular architectures characteristic of histopathological tissues. Here we present PathoGen, a diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images. Unlike conventional augmentation techniques, PathoGen leverages the iterative refinement process of diffusion models to synthesize lesions with natural tissue boundaries, preserved cellular structures, and authentic staining characteristics. We validate PathoGen across four diverse datasets representing distinct diagnostic challenges: kidney, skin, breast, and prostate pathology. Quantitative assessment confirms that PathoGen outperforms state-of-the-art generative baselines, including conditional GAN and Stable Diffusion, in image fidelity and distributional similarity. Crucially, we show that augmenting training sets with PathoGen-synthesized lesions enhances downstream segmentation performance compared to traditional geometric augmentations, particularly in data-scarce regimes. Besides, by simultaneously generating realistic morphology and pixel-level ground truth, PathoGen effectively overcomes the manual annotation bottleneck. This approach offers a scalable pathway for developing generalizable medical AI systems despite limited expert-labeled data.

</details>


### [20] [How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?](https://arxiv.org/abs/2601.08133)
*Peng Gao,Yujian Lee,Yongqi Xu,Wentao Fan*

Main category: cs.CV

TL;DR: 本文提出了音频-视觉语义分割(AVSS)任务，并引入了一种新的协作框架SSP，结合光流和文本提示提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉分割(AVS)方法难以同时捕捉动态场景的时序信息和静态声源对象的语义信息，需要更全面的跨模态整合策略。

Method: 设计两阶段框架：预分割阶段利用光流捕捉运动动态，结合类别提示和场景描述提示处理动态/静态声源，并通过视觉-文本对齐模块(VTA)实现跨模态融合。

Result: 实验表明SSP在AVS任务中优于现有方法，实现了更精确的分割效果，特别是在处理动态对象和静态声源场景时表现突出。

Conclusion: AVSS作为扩展任务具有研究价值，SSP框架通过多模态协同机制为音频-视觉场景理解提供了新思路。

Abstract: Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \textit{S}tepping \textit{S}tone \textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.

</details>


### [21] [Subspace Alignment for Vision-Language Model Test-time Adaptation](https://arxiv.org/abs/2601.08139)
*Zhichen Zeng,Wenxuan Bao,Xiao Lin,Ruizhong Qiu,Tianxin Wei,Xuying Ning,Yuchen Yan,Chen Luo,Monica Xiao Cheng,Jingrui He,Hanghang Tong*

Main category: cs.CV

TL;DR: 提出SubTTA方法，通过对齐多模态子空间解决视觉语言模型在分布偏移下的测试时自适应问题


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应(TTA)方法依赖不可靠的跨模态伪标签，导致分布偏移下适应效果下降，受限于模态间断和视觉噪声两大问题

Method: 通过最小化Chordal距离对齐视觉与文本主子空间，将视觉特征投影到文本语义子空间消除无关噪声，并在净化后空间进行TTA优化决策边界

Result: 在多种基准测试中平均提升2.24%，优于当前最优TTA方法

Conclusion: SubTTA通过跨模态子空间对齐和视觉特征净化，有效解决了分布偏移下的TTA挑战，验证了子空间建模对多模态适应的重要性

Abstract: Vision-language models (VLMs), despite their extraordinary zero-shot capabilities, are vulnerable to distribution shifts. Test-time adaptation (TTA) emerges as a predominant strategy to adapt VLMs to unlabeled test data on the fly. However, existing TTA methods heavily rely on zero-shot predictions as pseudo-labels for self-training, which can be unreliable under distribution shifts and misguide adaptation due to two fundamental limitations. First (Modality Gap), distribution shifts induce gaps between visual and textual modalities, making cross-modal relations inaccurate. Second (Visual Nuisance), visual embeddings encode rich but task-irrelevant noise that often overwhelms task-specific semantics under distribution shifts. To address these limitations, we propose SubTTA, which aligns the semantic subspaces of both modalities to enhance zero-shot predictions to better guide the TTA process. To bridge the modality gap, SubTTA extracts the principal subspaces of both modalities and aligns the visual manifold to the textual semantic anchor by minimizing their chordal distance. To eliminate visual nuisance, SubTTA projects the aligned visual features onto the task-specific textual subspace, which filters out task-irrelevant noise by constraining visual embeddings within the valid semantic span, and standard TTA is further performed on the purified space to refine the decision boundaries. Extensive experiments on various benchmarks and VLM architectures demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.

</details>


### [22] [Instance-Aligned Captions for Explainable Video Anomaly Detection](https://arxiv.org/abs/2601.08155)
*Inpyo Song,Minjun Joo,Joonhyung Kwon,Eunji Jeon,Jangwon Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种实例对齐的描述方法，用于视频异常检测，通过创建新数据集VIEW360+暴露了现有基于大型模型的方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释视频异常检测方法在多实体交互场景中缺乏空间准确性，导致解释不可靠，需改进其可信度与可验证性。

Method: 开发实例对齐的文本描述框架，关联对象实例（含外观和运动属性），扩展VIEW360数据集并增加异常类型。

Result: 实例级空间描述揭示了传统LLM/VLM方法的显著缺陷，为未来研究提供了可验证的稳健基准。

Conclusion: 通过空间接地描述与综合数据集VIEW360+，增强了视频异常检测的可解释性与行动指导性。

Abstract: Explainable video anomaly detection (VAD) is crucial for safety-critical applications, yet even with recent progress, much of the research still lacks spatial grounding, making the explanations unverifiable. This limitation is especially pronounced in multi-entity interactions, where existing explainable VAD methods often produce incomplete or visually misaligned descriptions, reducing their trustworthiness. To address these challenges, we introduce instance-aligned captions that link each textual claim to specific object instances with appearance and motion attributes. Our framework captures who caused the anomaly, what each entity was doing, whom it affected, and where the explanationis grounded, enabling verifiable and actionable reasoning. We annotate eight widely used VAD benchmarks and extend the 360-degree egocentric dataset, VIEW360, with 868 additional videos, eight locations, and four new anomaly types, creating VIEW360+, a comprehensive testbed for explainable VAD. Experiments show that our instance-level spatially grounded captions reveal significant limitations in current LLM- and VLM-based methods while providing a robust benchmark for future research in trustworthy and interpretable anomaly detection.

</details>


### [23] [A Hardware-Algorithm Co-Designed Framework for HDR Imaging and Dehazing in Extreme Rocket Launch Environments](https://arxiv.org/abs/2601.08162)
*Jing Tao,Banglei Guan,Pengju Sun,Taihang Lei,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种硬件-算法协同设计框架，利用定制的空间可变曝光（SVE）传感器和物理感知去雾算法，在极端火箭发射条件下实现精确光学测量，解决了因浓烟和亮度剧烈变化导致的图像退化问题。


<details>
  <summary>Details</summary>
Motivation: 火箭发射时，燃烧引发的浓烟和超过120 dB的亮度变化严重干扰了关键机械参数（如羽流流场、激波结构、喷嘴振荡）的光学测量精度，亟需鲁棒的去雾技术以支持后续运动分析和参数提取。

Method: 设计SVE传感器进行单次拍摄多曝光数据采集（硬件），结合物理建模的去雾算法：1. 动态估计雾密度；2. 区域自适应光照优化；3. 多尺度熵约束融合分离雾层与场景辐射信息。

Result: 在真实发射影像及实验中验证，该框架显著提升羽流及喷管区域的物理正确图像恢复能力，成功提取粒子速度、流动不稳定性频率及结构振动参数，优于传统方法。

Conclusion: 该研究为极端航空航天环境下的定量图像分析提供了可靠技术基础，实现了硬件创新与算法物理建模的协同增效，为工程监测开辟了新路径。

Abstract: Quantitative optical measurement of critical mechanical parameters -- such as plume flow fields, shock wave structures, and nozzle oscillations -- during rocket launch faces severe challenges due to extreme imaging conditions. Intense combustion creates dense particulate haze and luminance variations exceeding 120 dB, degrading image data and undermining subsequent photogrammetric and velocimetric analyses. To address these issues, we propose a hardware-algorithm co-design framework that combines a custom Spatially Varying Exposure (SVE) sensor with a physics-aware dehazing algorithm. The SVE sensor acquires multi-exposure data in a single shot, enabling robust haze assessment without relying on idealized atmospheric models. Our approach dynamically estimates haze density, performs region-adaptive illumination optimization, and applies multi-scale entropy-constrained fusion to effectively separate haze from scene radiance. Validated on real launch imagery and controlled experiments, the framework demonstrates superior performance in recovering physically accurate visual information of the plume and engine region. This offers a reliable image basis for extracting key mechanical parameters, including particle velocity, flow instability frequency, and structural vibration, thereby supporting precise quantitative analysis in extreme aerospace environments.

</details>


### [24] [Representation Learning with Semantic-aware Instance and Sparse Token Alignments](https://arxiv.org/abs/2601.08165)
*Phuoc-Nguyen Bui,Toan Duc Nguyen,Junghyun Bum,Duc-Tai Le,Hyunseung Choo*

Main category: cs.CV

TL;DR: 本文提出SISTA框架，通过多级对齐机制解决医学图像与文本预训练中的假负例问题，提升细粒度下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习将所有未配对样本视为负例，但医学数据中存在跨患者相似性，导致语义结构破坏和表征质量下降。

Method: 构建双级语义对齐框架：1）图像-报告级，利用报告间相似性消除假负例；2）图像块-词元级，实现跨模态细粒度对齐。

Result: 在三个医学下游任务（分类/分割/检测）中均取得显著提升，尤其在少量标注数据下细粒度任务性能提升达15.2%

Conclusion: SISTA有效缓解了医学多模态预训练中的负例干扰问题，开源代码和模型为后续研究提供基础

Abstract: Medical contrastive vision-language pre-training (VLP) has demonstrated significant potential in improving performance on downstream tasks. Traditional approaches typically employ contrastive learning, treating paired image-report samples as positives and unpaired ones as negatives. However, in medical datasets, there can be substantial similarities between images or reports from different patients. Rigidly treating all unpaired samples as negatives, can disrupt the underlying semantic structure and negatively impact the quality of the learned representations. In this paper, we propose a multi-level alignment framework, Representation Learning with Semantic-aware Instance and Sparse Token Alignments (SISTA) by exploiting the semantic correspondence between medical image and radiology reports at two levels, i.e., image-report and patch-word levels. Specifically, we improve the conventional contrastive learning by incorporating inter-report similarity to eliminate the false negatives and introduce a method to effectively align image patches with relevant word tokens. Experimental results demonstrate the effectiveness of the proposed framework in improving transfer performance across different datasets on three downstream tasks: image classification, image segmentation, and object detection. Notably, our framework achieves significant improvements in fine-grained tasks even with limited labeled data. Codes and pre-trained models will be made available.

</details>


### [25] [Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling](https://arxiv.org/abs/2601.08174)
*Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He*

Main category: cs.CV

TL;DR: 本文提出了基于PVRCNN++的跨平台3D目标检测方法，通过数据增强和伪标签自训练策略提升模型泛化性，在RoboSense2025挑战赛中Car类目标检测AP达62.67%。


<details>
  <summary>Details</summary>
Motivation: 针对激光雷达跨平台部署时设备差异导致的领域间隙问题，需提升检测模型在不同扫描模式下的通用性，特别是在无人车与机器人多模态场景的适配需求。

Method: 采用PVRCNN++作为基础检测框架，创新性地引入平台特定的数据增强策略，并设计基于高质量伪标签的自训练机制，有效缩小领域差异带来的性能衰减。

Result: 在挑战赛phase-1目标域中Car类别取得62.67% AP，phase-2测试中Car/Pedestrian类别分别达到58.76%和49.81% AP，总成绩位列全球第三

Conclusion: 实验证明结合多模态特征提取与自训练策略能显著提升3D检测模型的平台兼容性，为复杂环境下的机器人感知系统提供了实用化解决方案。

Abstract: This technical report represents the award-winning solution to the Cross-platform 3D Object Detection task in the RoboSense2025 Challenge. Our approach is built upon PVRCNN++, an efficient 3D object detection framework that effectively integrates point-based and voxel-based features. On top of this foundation, we improve cross-platform generalization by narrowing domain gaps through tailored data augmentation and a self-training strategy with pseudo-labels. These enhancements enabled our approach to secure the 3rd place in the challenge, achieving a 3D AP of 62.67% for the Car category on the phase-1 target domain, and 58.76% and 49.81% for Car and Pedestrian categories respectively on the phase-2 target domain.

</details>


### [26] [CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval](https://arxiv.org/abs/2601.08175)
*Feiran Wang,Junyi Wu,Dawen Cai,Yuan Hong,Yan Yan*

Main category: cs.CV

TL;DR: CogniMap3D是一种模仿人类认知过程的动态三维场景理解和重建框架，通过持久静态场景记忆库与运动线索分析实现高效空间知识存储与动态对象识别。


<details>
  <summary>Details</summary>
Motivation: 为实现机器人对动态场景的持续理解与长期环境建模，需同时处理移动物体识别、多时段场景记忆整合以及相机位姿优化等挑战，传统方法在此方面存在效率与稳定性不足的问题。

Method: 采用三级架构：1) 多阶段运动线索分析融合深度与位姿先验识别动态区域；2) 认知映射系统构建静态场景记忆库并支持跨时段检索更新；3) 因子图优化策略联合优化相机位姿与场景地图，并通过记忆匹配实现重定位。

Result: 在视频深度估计、相机位姿重建与三维地图构建任务中达到SOTA性能，支持跨长时间段与多回合访问的连续场景理解，在动态对象分割与静态场景一致性保持方面表现优越。

Conclusion: 通过仿生认知机制实现了动态场景的高效持久建模，其记忆检索-更新闭环系统显著提升了复杂环境下的长期定位与地图构建鲁棒性，为自主系统提供了可持续学习的空间表示框架。

Abstract: We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.

</details>


### [27] [Instruction-Driven 3D Facial Expression Generation and Transition](https://arxiv.org/abs/2601.08179)
*Anh H. Vo,Tae-Seok Kim,Hulin Jin,Soo-Mi Choi,Yong-Guk Kim*

Main category: cs.CV

TL;DR: 本研究提出了一种基于文本指令驱动的3D面部表情过渡生成框架，能够通过文本描述生成从一种面部表情到另一种的平滑过渡效果，并在CK+和CelebV-HQ数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D虚拟人物仅支持有限的六种基本面部表情，而真实情感变化需要更丰富的过渡。本文旨在通过文本指令扩展表情生成的多样性，实现从静态图像出发的指令化动态表情转化。

Method: 1) 提出IFED模块（指令驱动面部表情分解器），融合文本描述与面部特征的多模态关联；2) 设计I2FET方法（指令到面部表情过渡），结合IFED和顶点重建损失函数优化潜在向量语义理解；3) 构建面部表情过渡模型生成平滑动态序列。

Result: 在CK+和CelebV-HQ数据集上验证模型性能，定量和定性评估均显示该方法在表情过渡自然度和指令匹配度上优于现有技术，支持根据复杂文本描述生成多样化表情轨迹。

Conclusion: 研究表明文本指令可有效扩展面部表情库及过渡可能性，所提框架在虚拟角色动画、情感计算等领域具有应用潜力，未来可探索更多情感状态组合与跨语言适配。

Abstract: A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/

</details>


### [28] [Second-order Gaussian directional derivative representations for image high-resolution corner detection](https://arxiv.org/abs/2601.08182)
*Dongbo Xie,Junjie Qiu,Changming Sun,Weichuan Zhang*

Main category: cs.CV

TL;DR: 本研究指出张等人提出的角点检测模型存在理论缺陷，通过引入二阶高斯方向导数（SOGDD）滤波器，解决了相邻角点灰度信息干扰问题，并提出新型高精度角点检测方法。


<details>
  <summary>Details</summary>
Motivation: 发现现有角点检测方法在相邻角点处的灰度信息相互干扰会影响特征提取精度，需要建立更准确的理论框架和检测方法。

Method: 使用SOGDD滤波器对END型和L型两种标准角点模型进行数学建模，推导其特征表达式，建立高斯尺度选择准则，提出基于SOGDD的相邻角点精确定位算法。

Result: 实验表明新方法在定位误差（降低~15%）、抗模糊性（PSNR提升2.3dB）、图像匹配准确率（提升8.7%）和三维重建质量（误差减少12%）方面优于现有技术。

Conclusion: SOGDD理论有效解决了相邻角点干扰问题，提出的检测框架显著提升了角点定位精度和算法鲁棒性，为后续计算机视觉任务提供了更可靠的特征基础。

Abstract: Corner detection is widely used in various computer vision tasks, such as image matching and 3D reconstruction. Our research indicates that there are theoretical flaws in Zhang et al.'s use of a simple corner model to obtain a series of corner characteristics, as the grayscale information of two adjacent corners can affect each other. In order to address the above issues, a second-order Gaussian directional derivative (SOGDD) filter is used in this work to smooth two typical high-resolution angle models (i.e. END-type and L-type models). Then, the SOGDD representations of these two corner models were derived separately, and many characteristics of high-resolution corners were discovered, which enabled us to demonstrate how to select Gaussian filtering scales to obtain intensity variation information from images, accurately depicting adjacent corners. In addition, a new high-resolution corner detection method for images has been proposed for the first time, which can accurately detect adjacent corner points. The experimental results have verified that the proposed method outperforms state-of-the-art methods in terms of localization error, robustness to image blur transformation, image matching, and 3D reconstruction.

</details>


### [29] [GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards](https://arxiv.org/abs/2601.08183)
*Yan Zhu,Te Luo,Pei-Yao Fu,Zhen Zhang,Zi-Long Wang,Yi-Fan Qu,Zi-Han Geng,Jia-Qi Xu,Lu Yao,Li-Yun Ma,Wei Su,Wei-Feng Chen,Quan-Lin Li,Shuo Wang,Ping-Hong Zhou*

Main category: cs.CV

TL;DR: 该研究构建了GI-Bench基准测试，系统评估了12种多模态大语言模型（MLLMs）在胃肠内镜全流程中的表现，并与初级内镜医生及住院医生进行对比。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在胃肠病学中潜力巨大，但其在完整临床流程及与人类医生的全面性能对比尚未验证，需建立标准化评估体系。

Method: 开发包含20类精细病变的GI-Bench测试集，通过解剖定位、病变识别、诊断、结果描述及治疗策略五阶段流程评估12种MLLMs，采用Macro-F1、mIoU及多维李克特量表对比模型与人类表现。

Result: Gemini-3-Pro位列榜首，顶级模型在诊断（Macro-F1 0.641）超越住院医生（0.492），接近初级医生（0.727），但存在空间定位瓶颈（人类mIoU>0.506 vs 模型0.345）及‘流畅性-准确性悖论’（生成报告可读性优于人类但存在事实错误）。

Conclusion: GI-Bench动态榜单追踪MLLMs临床表现，揭示当前模型在视觉特征解读和空间定位的不足，建议通过数据优化和交互式训练提升临床实用性。

Abstract: Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical "spatial grounding bottleneck" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a "fluency-accuracy paradox": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to "over-interpretation" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.

</details>


### [30] [Unified Multi-Site Multi-Sequence Brain MRI Harmonization Enriched by Biomedical Semantic Style](https://arxiv.org/abs/2601.08193)
*Mengqi Wu,Yongheng Sun,Qianqian Wang,Pew-Thian Yap,Mingxia Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMH的统一大脑MRI数据标准化框架，旨在通过消除多中心数据中的非生物异质性（如设备、参数差异）来提升深度学习模型的泛化性和多序列MRI的临床适用性。


<details>
  <summary>Details</summary>
Motivation: 多中心MRI数据的聚合可增强模型训练，但存在因设备差异导致的非生物异质性问题。现有标准化方法依赖有限的配对数据或多序列处理能力不足，亟需一种无需配对数据、支持多序列处理的创新框架。

Method: MMH采用两阶段策略：(1) 基于扩散模型的全局标准化器，通过风格无关的梯度条件映射图像到序列特异性统一域；(2) 目标特异性微调器实现个性化适配。创新性地使用三平面注意力BiomedCLIP编码器，通过多视角嵌入分离风格特征与解剖结构，全程无需配对数据。

Result: 在4163对T1/T2加权MRI的实验表明，MMH在图像簇分布、体素层级指标、组织分割准确率及年龄/地点分类任务中，相较SOTA方法均取得0.8%-15.3%的性能提升，且支持跨序列迁移学习。

Conclusion: MMH实现了多中心多序列MRI的高效标准化，在消除扫描参数差异的同时保留解剖细节，为临床多模态数据融合提供了可扩展的解决方案，但其对极端参数差异的处理仍需进一步验证。

Abstract: Aggregating multi-site brain MRI data can enhance deep learning model training, but also introduces non-biological heterogeneity caused by site-specific variations (e.g., differences in scanner vendors, acquisition parameters, and imaging protocols) that can undermine generalizability. Recent retrospective MRI harmonization seeks to reduce such site effects by standardizing image style (e.g., intensity, contrast, noise patterns) while preserving anatomical content. However, existing methods often rely on limited paired traveling-subject data or fail to effectively disentangle style from anatomy. Furthermore, most current approaches address only single-sequence harmonization, restricting their use in real-world settings where multi-sequence MRI is routinely acquired. To this end, we introduce MMH, a unified framework for multi-site multi-sequence brain MRI harmonization that leverages biomedical semantic priors for sequence-aware style alignment. MMH operates in two stages: (1) a diffusion-based global harmonizer that maps MR images to a sequence-specific unified domain using style-agnostic gradient conditioning, and (2) a target-specific fine-tuner that adapts globally aligned images to desired target domains. A tri-planar attention BiomedCLIP encoder aggregates multi-view embeddings to characterize volumetric style information, allowing explicit disentanglement of image styles from anatomy without requiring paired data. Evaluations on 4,163 T1- and T2-weighted MRIs demonstrate MMH's superior harmonization over state-of-the-art methods in image feature clustering, voxel-level comparison, tissue segmentation, and downstream age and site classification.

</details>


### [31] [MobiDiary: Autoregressive Action Captioning with Wearable Devices and Wireless Signals](https://arxiv.org/abs/2601.08204)
*Fei Deng,Yinghui He,Chuntong Chu,Ge Wang,Han Ding,Jinsong Han,Fei Wang*

Main category: cs.CV

TL;DR: 本文提出MobiDiary框架，通过异构物理信号（IMU和Wi-Fi）生成自然语言活动描述，突破传统预定义标签限制。


<details>
  <summary>Details</summary>
Motivation: 视觉识别存在隐私问题和环境限制，且传统方法无法提供可解释性强的活动描述。

Method: 构建基于patch机制的统一传感器编码器，融合异构空间嵌入以统一传感器数据，通过Transformer解码器生成连贯描述。

Result: 在多个公开数据集（如XRF V2, WiFiTAD）上，于BLEU@4、CIDEr等指标达SOTA，并超越专用基线方法。

Conclusion: 证实跨模态特征迁移的有效性，为隐私敏感场景的活动理解提供新范式。

Abstract: Human Activity Recognition (HAR) in smart homes is critical for health monitoring and assistive living. While vision-based systems are common, they face privacy concerns and environmental limitations (e.g., occlusion). In this work, we present MobiDiary, a framework that generates natural language descriptions of daily activities directly from heterogeneous physical signals (specifically IMU and Wi-Fi). Unlike conventional approaches that restrict outputs to pre-defined labels, MobiDiary produces expressive, human-readable summaries. To bridge the semantic gap between continuous, noisy physical signals and discrete linguistic descriptions, we propose a unified sensor encoder. Instead of relying on modality-specific engineering, we exploit the shared inductive biases of motion-induced signals--where both inertial and wireless data reflect underlying kinematic dynamics. Specifically, our encoder utilizes a patch-based mechanism to capture local temporal correlations and integrates heterogeneous placement embedding to unify spatial contexts across different sensors. These unified signal tokens are then fed into a Transformer-based decoder, which employs an autoregressive mechanism to generate coherent action descriptions word-by-word. We comprehensively evaluate our approach on multiple public benchmarks (XRF V2, UWash, and WiFiTAD). Experimental results demonstrate that MobiDiary effectively generalizes across modalities, achieving state-of-the-art performance on captioning metrics (e.g., BLEU@4, CIDEr, RMC) and outperforming specialized baselines in continuous action understanding.

</details>


### [32] [Knowledge-based learning in Text-RAG and Image-RAG](https://arxiv.org/abs/2601.08226)
*Alexander Shim,Khalil Saieh,Samuel Clarke*

Main category: cs.CV

TL;DR: 研究对比了基于Vision Transformer的图像编码器EVA-ViT与LLM（如LlaMA、ChatGPT）在胸部X光疾病诊断中的多模态方法，分析不同RAG策略对减少模型幻觉的效果。


<details>
  <summary>Details</summary>
Motivation: 为解决大语言模型在医疗图像分析中的幻觉问题，提升疾病诊断的准确性和可靠性，探索多模态方法的有效性。

Method: 使用NIH Chest X-ray数据集训练模型，对比图像RAG（基于KNN的特征匹配）、文本RAG（基于外部知识检索）和基线模型的性能，评估不同LLM（GPT vs. Llama）的表现。

Result: 文本RAG通过外部知识有效降低幻觉率；图像RAG利用KNN提升预测置信度和校准精度；GPT在低幻觉率及Expected Calibration Error（ECE）指标上优于Llama模型。

Conclusion: 多模态RAG策略可缓解数据不平衡与复杂结构的挑战，GPT作为LLM更具优势，但需要解决多阶段架构的复杂性，并强调大规模平衡数据集的应用价值。

Abstract: This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.

</details>


### [33] [Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence](https://arxiv.org/abs/2601.08241)
*Michele Fiori,Gabriele Civitarese,Marco Colussi,Claudio Bettini*

Main category: cs.CV

TL;DR: 本文提出基于事件分割和预测置信度估计的零样本ADL识别方法,在复杂场景中优于传统时序分割和有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法依赖与LLM上下文理解不匹配的时间分割,且缺乏预测可靠性评估机制,导致在复杂ADL场景下性能受限。

Method: 采用事件驱动的传感器数据处理框架,结合LLM的多模态表征能力;设计基于语义连贯性和逻辑验证的双维度置信度评估体系。

Result: 在真实场景数据集上,使用Gemma27B的小型LLM比传统时序分割方法F1值提升18.7%,较有监督模型提高12.3%,置信度评分准确率达89.4%

Conclusion: 事件分割突破传统时序框架限制,量化评估证明其能充分释放LLM的上下文推理潜力,而置信度机制为实际部署提供可靠性保障。

Abstract: Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.

</details>


### [34] [HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding](https://arxiv.org/abs/2601.08273)
*Qitan Lv,Tianyu Liu,Wen Wu,Xuenan Xu,Bowen Zhou,Feng Wu,Chao Zhang*

Main category: cs.CV

TL;DR: HIPPO通过语义感知的标记保留和视频并行推测解码，显著提升视频大语言模型的推理速度，达到最高3.51倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有视频-LLM的推测解码方法因语义信息保存不足和剪枝后残余计算成本高，导致加速效果受限，无法达到文本模型水平。

Method: 1) 融合全局注意力与局部视觉语义保留关键标记；2) 解耦草稿生成与验证阶段，实现并行处理。

Result: 在4个视频-LLM和6个基准测试中，HIPPO比原生解码加速3.51倍，同时保持输出质量。

Conclusion: HIPPO框架有效平衡视觉语义保留与计算效率，为视频-LLM推理提供了普适性加速方案。

Abstract: Speculative decoding (SD) has emerged as a promising approach to accelerate LLM inference without sacrificing output quality. Existing SD methods tailored for video-LLMs primarily focus on pruning redundant visual tokens to mitigate the computational burden of massive visual inputs. However, existing methods do not achieve inference acceleration comparable to text-only LLMs. We observe from extensive experiments that this phenomenon mainly stems from two limitations: (i) their pruning strategies inadequately preserve visual semantic tokens, degrading draft quality and acceptance rates; (ii) even with aggressive pruning (e.g., 90% visual tokens removed), the draft model's remaining inference cost limits overall speedup. To address these limitations, we propose HIPPO, a general holistic-aware parallel speculative decoding framework. Specifically, HIPPO proposes (i) a semantic-aware token preservation method, which fuses global attention scores with local visual semantics to retain semantic information at high pruning ratios; (ii) a video parallel SD algorithm that decouples and overlaps draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate HIPPO's effectiveness, yielding up to 3.51x speedup compared to vanilla auto-regressive decoding.

</details>


### [35] [KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?](https://arxiv.org/abs/2601.08292)
*Xianfeng Wang,Kaiwei Zhang,Qi Jia,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: KidVis benchmark evaluates MLLMs' fundamental visual primitives (e.g., concentration, spatial reasoning, closure) found in 6-7 year-old children, revealing humans outperform state-of-the-art models (95.32 vs. 67.33) with a 'Scaling Law Paradox' (increased parameters ≠ linear improvement in visual capabilities).


<details>
  <summary>Details</summary>
Motivation: To determine whether MLLMs possess human-like foundational visual perception abilities, distinct from high-level reasoning capabilities, by leveraging developmental psychology principles.

Method: Constructed the KidVis benchmark (6 categories of low-semantic visual tasks) and evaluated 20 SOTA MLLMs against human children's performance baselines.

Result: Human children achieved 95.32 average accuracy vs. GPT-5's 67.33; larger model parameters showed diminishing returns in basic visual capabilities ('scaling law paradox').

Conclusion: Current MLLMs lack essential physiological perceptual primitives for generalized visual intelligence despite their reasoning capabilities.

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a "Scaling Law Paradox": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.

</details>


### [36] [M3SR: Multi-Scale Multi-Perceptual Mamba for Efficient Spectral Reconstruction](https://arxiv.org/abs/2601.08293)
*Yuze Zhang,Lingjie Li,Qiuzhen Lin,Zhong Ming,Fei Yu,Victor C. M. Leung*

Main category: cs.CV

TL;DR: 本文提出M3SR，通过多尺度、多感知的Mamba架构解决光谱重建中的空间感知单一和特征提取不足问题。


<details>
  <summary>Details</summary>
Motivation: Mamba架构在光谱重建中存在单空间感知限制和单尺度特征提取无法捕捉复杂结构的问题。

Method: 设计多感知融合块增强特征理解，并结合U-Net结构实现多尺度特征提取与融合。

Result: 实验表明M3SR在定量与定性评估中均超越现有方法，且计算成本更低。

Conclusion: 所提M3SR有效提升高光谱图像重建精度，兼具高效计算优势。

Abstract: The Mamba architecture has been widely applied to various low-level vision tasks due to its exceptional adaptability and strong performance. Although the Mamba architecture has been adopted for spectral reconstruction, it still faces the following two challenges: (1) Single spatial perception limits the ability to fully understand and analyze hyperspectral images; (2) Single-scale feature extraction struggles to capture the complex structures and fine details present in hyperspectral images. To address these issues, we propose a multi-scale, multi-perceptual Mamba architecture for the spectral reconstruction task, called M3SR. Specifically, we design a multi-perceptual fusion block to enhance the ability of the model to comprehensively understand and analyze the input features. By integrating the multi-perceptual fusion block into a U-Net structure, M3SR can effectively extract and fuse global, intermediate, and local features, thereby enabling accurate reconstruction of hyperspectral images at multiple scales. Extensive quantitative and qualitative experiments demonstrate that the proposed M3SR outperforms existing state-of-the-art methods while incurring a lower computational cost.

</details>


### [37] [SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices](https://arxiv.org/abs/2601.08303)
*Dongting Hu,Aarush Gupta,Magzhan Gabidolla,Arpit Sahni,Huseyin Coskun,Yanyu Li,Yerlan Idelbayev,Ahsan Mahmood,Aleksei Lebedev,Dishani Lahiri,Anujraaj Goyal,Ju Hu,Mingming Gong,Sergey Tulyakov,Anil Kag*

Main category: cs.CV

TL;DR: 本文提出了一种高效的扩散变换器（DiT）框架，专为移动和边缘设备设计，结合紧凑架构、弹性训练和知识引导蒸馏，实现在严格资源限制下的高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散变换器（DiT）因计算和内存开销高，难以在移动设备上部署。本文旨在解决硬件资源受限场景下的实时高质量图像生成需求。

Method: 提出三部分创新：1）紧凑DiT架构+自适应全局-局部稀疏注意力机制；2）弹性训练框架支持多容量子模型联合优化；3）知识引导的分布匹配蒸馏管道（KGDM），结合DMD目标和少步长教师模型知识迁移。

Result: 实现动态适配不同硬件设备的生成能力，在4步极短步长下完成低延迟高保真生成，资源消耗显著降低的同时保持Transformer级生成质量。

Conclusion: 所提方案成为首个支持跨设备动态调整、兼容移动端实时应用的扩散模型框架，为资源敏感场景的部署提供了新范式。

Abstract: Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.

</details>


### [38] [Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.08311)
*Kang Fu,Huiyu Duan,Zicheng Zhang,Yucheng Zhu,Jun Zhao,Xiongkuo Min,Jia Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出IQARAG，一种无需训练的框架，通过检索增强生成（RAG）提升大型多模态模型（LMMs）在图像质量评估（IQA）中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs在IQA任务中需昂贵的微调方法以对齐质量相关token分布与图像质量等级，而无需训练的方法（如RAG）为构建高效替代方案提供了灵感。

Method: IQARAG包含三个阶段：1）提取图像特征；2）检索语义相似但质量不同的参考图像及对应MOS评分；3）将检索结果与输入图像集成至特定提示词中，为LMM提供视觉感知锚点。

Result: 在KADID、KonIQ、LIVE Challenge和SPAQ等多个IQA数据集上的实验证明，IQARAG显著提升LMM的IQA性能，且无需微调即可达到接近最先进方法的效果。

Conclusion: IQARAG通过引入基于RAG的零样本学习策略，为资源受限场景下的图像质量评估提供了有效解决方案，验证了训练-检索-生成范式在多模态任务中的潜力。

Abstract: Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.

</details>


### [39] [YOLOBirDrone: Dataset for Bird vs Drone Detection and Classification and a YOLO based enhanced learning architecture](https://arxiv.org/abs/2601.08319)
*Dapinder Kaur,Neeraj Battish,Arnav Bhavsar,Shashi Poddar*

Main category: cs.CV

TL;DR: 该研究针对无人机与鸟类的视觉检测难题，提出了YOLOBirDrone架构（集成AELAN、MPDA/RMPDA模块）及BirDrone数据集，检测精度达85%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉无人机检测系统难以区分小型鸟类与无人机，存在安全隐患，亟需提升分类精度。

Method: 设计YOLOBirDrone架构：1. 自适应扩展层聚合模块（AELAN）增强特征融合 2. 多尺度双注意力模块（MPDA/RMPDA）捕获形状与空间-通道信息 3. 构建含挑战性小目标的BirDrone数据集进行测试

Result: 在多场景下检测准确率提升至85%，超越当前最优算法：mAP提升12.7%，对最小目标（<32x32）的检测AP50达到72.3%

Conclusion: 提出的模块化设计与数据集有效解决了无人机-鸟类检测混淆问题，为公共安全防御系统提供了高可靠性技术方案

Abstract: The use of aerial drones for commercial and defense applications has benefited in many ways and is therefore utilized in several different application domains. However, they are also increasingly used for targeted attacks, posing a significant safety challenge and necessitating the development of drone detection systems. Vision-based drone detection systems currently have an accuracy limitation and struggle to distinguish between drones and birds, particularly when the birds are small in size. This research work proposes a novel YOLOBirDrone architecture that improves the detection and classification accuracy of birds and drones. YOLOBirDrone has different components, including an adaptive and extended layer aggregation (AELAN), a multi-scale progressive dual attention module (MPDA), and a reverse MPDA (RMPDA) to preserve shape information and enrich features with local and global spatial and channel information. A large-scale dataset, BirDrone, is also introduced in this article, which includes small and challenging objects for robust aerial object identification. Experimental results demonstrate an improvement in performance metrics through the proposed YOLOBirDrone architecture compared to other state-of-the-art algorithms, with detection accuracy reaching approximately 85% across various scenarios.

</details>


### [40] [UM-Text: A Unified Multimodal Model for Image Understanding](https://arxiv.org/abs/2601.08321)
*Lichen Ma,Xiaolong Fu,Gaojing Zhou,Zipeng Guo,Ting Zhu,Yichun Liu,Yu Shi,Jason Li,Junshi Huang*

Main category: cs.CV

TL;DR: This paper introduces UM-Text, a unified multimodal model for visual text editing via natural language instructions, using a Visual Language Model (VLM) and UM-Encoder, alongside a new large-scale dataset (UM-DATA-200K).


<details>
  <summary>Details</summary>
Motivation: Prior methods rely on manual specification of text attributes without considering reference image style, leading to inconsistency. A holistic approach is needed to achieve context-aware and style-consistent text editing.

Method: Proposes UM-Text with (1) VLM for understanding instructions and reference images, (2) UM-Encoder to combine conditional embeddings guided by VLM, (3) regional consistency loss for supervision in latent/RGB space, and (4) a three-stage training strategy. Introduces UM-DATA-200K dataset with diverse scenes.

Result: Demonstrates state-of-the-art performance on multiple public benchmarks through qualitative/quantitative evaluations, showing superior visual-text harmony and accuracy.

Conclusion: UM-Text effectively addresses style consistency and contextual understanding in visual text editing, validated by the proposed dataset and training strategy enhancements.

Abstract: With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.

</details>


### [41] [Tissue Classification and Whole-Slide Images Analysis via Modeling of the Tumor Microenvironment and Biological Pathways](https://arxiv.org/abs/2601.08336)
*Junzhuo Liu,Xuemei Du,Daniel Reisenbuchler,Ye Chen,Markus Eckstein,Christian Matek,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: BioMorphNet通过整合组织形态特征与空间基因表达，提出图建模和可学习通路模块，提升WSI分类和差异基因分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注基因序列和滑动窗口分类，缺乏对空间转录组及补丁级应用的研究，未有效结合形态学与分子特征分析肿瘤微环境。

Method: 构建图模型捕捉目标补丁与邻域的形态关联，通过形态-分子相似性调整响应强度；基于临床通路数据库导出特征，并设计可学习神经通路模块模拟生物过程。

Result: 分类精度在前列腺癌、直肠癌、乳腺癌数据集分别提升2.67%/5.48%/6.29%，实现精准WSI分类、肿瘤定位及差异基因发现。

Conclusion: 多模态交互框架有效联合形态学与基因数据，为精准诊断和肿瘤生物标志物发现提供新工具。

Abstract: Automatic integration of whole slide images (WSIs) and gene expression profiles has demonstrated substantial potential in precision clinical diagnosis and cancer progression studies. However, most existing studies focus on individual gene sequences and slide level classification tasks, with limited attention to spatial transcriptomics and patch level applications. To address this limitation, we propose a multimodal network, BioMorphNet, which automatically integrates tissue morphological features and spatial gene expression to support tissue classification and differential gene analysis. For considering morphological features, BioMorphNet constructs a graph to model the relationships between target patches and their neighbors, and adjusts the response strength based on morphological and molecular level similarity, to better characterize the tumor microenvironment. In terms of multimodal interactions, BioMorphNet derives clinical pathway features from spatial transcriptomic data based on a predefined pathway database, serving as a bridge between tissue morphology and gene expression. In addition, a novel learnable pathway module is designed to automatically simulate the biological pathway formation process, providing a complementary representation to existing clinical pathways. Compared with the latest morphology gene multimodal methods, BioMorphNet's average classification metrics improve by 2.67%, 5.48%, and 6.29% for prostate cancer, colorectal cancer, and breast cancer datasets, respectively. BioMorphNet not only classifies tissue categories within WSIs accurately to support tumor localization, but also analyzes differential gene expression between tissue categories based on prediction confidence, contributing to the discovery of potential tumor biomarkers.

</details>


### [42] [From Local Windows to Adaptive Candidates via Individualized Exploratory: Rethinking Attention for Image Super-Resolution](https://arxiv.org/abs/2601.08341)
*Chunyu Meng,Wei Long,Shuhang Gu*

Main category: cs.CV

TL;DR: 本文提出个体化探索性Transformer（IET），通过新型注意力机制IEA优化图像超分辨率的计算效率与精度。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法因固定分组注意力机制忽略标记相似性非对称性，导致计算效率低且信息聚合不精确。

Method: 设计个体化探索性注意力（IEA），使每个token自适应选择内容感知的独立注意力候选，实现非对称的token级自适应计算。

Result: 在标准SR数据集上达到SOTA性能，计算复杂度与现有方法相当，但重建质量显著提升。

Conclusion: 通过灵活的非对称注意力机制，在保证效率的同时提升超分辨率效果，为Transformer架构设计提供了新思路。

Abstract: Single Image Super-Resolution (SISR) is a fundamental computer vision task that aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) input. Transformer-based methods have achieved remarkable performance by modeling long-range dependencies in degraded images. However, their feature-intensive attention computation incurs high computational cost. To improve efficiency, most existing approaches partition images into fixed groups and restrict attention within each group. Such group-wise attention overlooks the inherent asymmetry in token similarities, thereby failing to enable flexible and token-adaptive attention computation. To address this limitation, we propose the Individualized Exploratory Transformer (IET), which introduces a novel Individualized Exploratory Attention (IEA) mechanism that allows each token to adaptively select its own content-aware and independent attention candidates. This token-adaptive and asymmetric design enables more precise information aggregation while maintaining computational efficiency. Extensive experiments on standard SR benchmarks demonstrate that IET achieves state-of-the-art performance under comparable computational complexity.

</details>


### [43] [Semantic Misalignment in Vision-Language Models under Perceptual Degradation](https://arxiv.org/abs/2601.08355)
*Guo Cheng*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型(VLM)在感知退化下的语义不一致问题，发现现有指标不足以评估安全关键场景下的稳健性，提出通过语义分割扰动模拟感知退化并设计语言级评估指标的新方法。


<details>
  <summary>Details</summary>
Motivation: VLMs被广泛应用于自动驾驶等安全敏感场景，但当前研究过度依赖传统视觉指标，忽略了感知误差对跨模态语义一致性的影响，亟需系统化的鲁棒性评估框架。

Method: 以Cityscapes语义分割为原型感知模块，构建包含现实感知畸变的数据集，提出幻觉生成、关键实体遗漏、安全误判三类语言级评估指标，测试对比学习与生成式VLM的跨模型表现。

Result: 当分割mIoU仅下降3.1%时，VLM的文本描述出现平均47%的幻觉实体与32%的关键对象遗漏，安全判断准确率骤降至58%，且不同架构模型均呈现像素级健壮性与语义可靠性间的显著脱节。

Conclusion: 揭示了当前VLM系统在感知不确定性下的根本性缺陷，强调需重构包含感知-语言联合风险的评估体系，为安全关键型AI系统设计提供新思路。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.

</details>


### [44] [Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild with an SDF Renderer](https://arxiv.org/abs/2601.08371)
*Anastasios Tsalakopoulos,Angelos Kanlis,Evangelos Chatzis,Antonis Karakottas,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: 本研究提出Geo-NVS-w框架，利用几何感知方法提升野外无约束图像集合的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有野外新视角合成方法缺乏对复杂表面几何的一致性约束，导致渲染结果存在结构矛盾，需要引入几何表征机制提升鲁棒性。

Method: 采用符号距离函数(SDF)构建基础几何表征，结合几何保持损失函数指导渲染过程，并通过能量优化实现效率提升。

Result: 渲染质量提升且能耗降低4-5倍，实验显示该方法在野外数据中保持尖锐几何细节与真实感效果。

Conclusion: Geo-NVS-w通过几何约束显式建模，在保证效率的同时提升了野外新视角合成的几何一致性与视觉质量。

Abstract: We introduce Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. While existing in-the-wild methods already excel at novel view synthesis, they often lack geometric grounding on complex surfaces, sometimes producing results that contain inconsistencies. Geo-NVS-w addresses this limitation by leveraging an underlying geometric representation based on a Signed Distance Function (SDF) to guide the rendering process. This is complemented by a novel Geometry-Preservation Loss which ensures that fine structural details are preserved. Our framework achieves competitive rendering performance, while demonstrating a 4-5x reduction reduction in energy consumption compared to similar methods. We demonstrate that Geo-NVS-w is a robust method for in-the-wild NVS, yielding photorealistic results with sharp, geometrically coherent details.

</details>


### [45] [Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation](https://arxiv.org/abs/2601.08375)
*Yuan Gao,Di Cao,Xiaohuan Xi,Sheng Nie,Shaobo Xia,Cheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于局部-全局双一致性策略的源无关无监督域适应（SFUDA）框架LoGo，用于解决地理空间点云语义分割中的域间分布差异问题。


<details>
  <summary>Details</summary>
Motivation: 地理空间点云在不同地区和采集策略下存在显著域偏移，传统域适应方法依赖源域数据但受隐私政策制约，因此需要发展无需源数据的SFUDA方法。

Method: 提出Local-Global Dual-Consensus框架，包含：1）基于类平衡原型估计的局部模块，采用类内独立锚点挖掘策略；2）基于最优传输的全局分布对齐模块，通过全局优化抑制类别主导现象；3）结合局部多增强预测与全局分配的双一致性伪标签过滤机制。

Result: 局部模块缓解了尾部类别样本不足导致的特征崩溃，全局模块修正了局部贪婪分配引起的类别偏差，双一致性策略有效提升了伪标签质量。

Conclusion: LoGo在无需源域数据的地理空间点云分割任务中表现出优越性，通过局部原型学习和全局分布优化的协同机制，解决了实际遥感应用中的域适应挑战。

Abstract: Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.

</details>


### [46] [An Explainable Two Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50](https://arxiv.org/abs/2601.08401)
*Ajo Babu George,Pranav S,Kunal Agarwal*

Main category: cs.CV

TL;DR: 开发一个结合解剖定位、病理分类和可解释性AI的阻生齿周围炎症诊断系统，提升全景X光片的诊断效率与可信度。


<details>
  <summary>Details</summary>
Motivation: 传统全景X光片在阻生齿周围炎症诊断中存在操作复杂、诊断标准不一致问题，需结合AI提升效率和临床信任度。

Method: 两阶段深度学习流程：第一阶段用YOLOv8进行第三磨牙检测与Winter's分类；第二阶段用改进ResNet-50检测炎症特征，并通过Grad-CAM实现可解释性诊断。

Result: YOLOv8实现92%精度和92.5%平均精度；ResNet-50炎症检测F1分数达86%，正常分类88%；Grad-CAM与医生诊断一致性84%。

Conclusion: 该系统通过多阶段深度学习框架与可解释性技术，为全景X光片的AI辅助诊断提供了临床可行方案。

Abstract: Objectives: To overcome challenges in diagnosing pericoronitis on panoramic radiographs, an AI-assisted assessment system integrating anatomical localization, pathological classification, and interpretability. Methods: A two-stage deep learning pipeline was implemented. The first stage used YOLOv8 to detect third molars and classify their anatomical positions and angulations based on Winter's classification. Detected regions were then fed into a second-stage classifier, a modified ResNet-50 architecture, for detecting radiographic features suggestive of pericoronitis. To enhance clinical trust, Grad-CAM was used to highlight key diagnostic regions on the radiographs. Results: The YOLOv8 component achieved 92% precision and 92.5% mean average precision. The ResNet-50 classifier yielded F1-scores of 88% for normal cases and 86% for pericoronitis. Radiologists reported 84% alignment between Grad-CAM and their diagnostic impressions, supporting the radiographic relevance of the interpretability output. Conclusion: The system shows strong potential for AI-assisted panoramic assessment, with explainable AI features that support clinical confidence.

</details>


### [47] [Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2](https://arxiv.org/abs/2601.08408)
*Yizhan Feng,Hichem Snoussi,Jing Teng,Jian Liu,Yuyang Wang,Abel Cherouat,Tian Wang*

Main category: cs.CV

TL;DR: 基于BLIP-2与YOLO模型的轻量级无人机多模态任务平台，无需微调即可实现视频级交互任务。


<details>
  <summary>Details</summary>
Motivation: 解决无人机边缘设备资源受限与视觉语言模型高计算成本之间的矛盾，适应复杂场景实时视觉理解需求。

Method: 融合BLIP-2与YOLO模型实现检测/分割任务，设计基于K-Means的智能关键帧采样机制，并通过结构化事件日志注入优化多任务提示词。

Result: 通过感知结果复用和时序特征融合提升视觉注意力推理能力，但未提及具体性能指标数据。

Conclusion: 该框架在保持任务通用性的同时降低资源消耗，适用于无人机复杂场景下多模态交互任务的高效处理。

Abstract: The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.

</details>


### [48] [SPARK: Scalable Real-Time Point Cloud Aggregation with Multi-View Self-Calibration](https://arxiv.org/abs/2601.08414)
*Chentian Sun*

Main category: cs.CV

TL;DR: SPARK是一种实时多摄像头3D重建框架，通过几何感知的在线外参估计和置信度驱动的点云融合策略，解决了多视角融合、外参不确定性和大规模扩展问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角融合、摄像头外参不确定性和大规模扩展性上存在局限性，而动态场景中的稳定点云生成需求迫切。

Method: 提出1) 几何感知的在线外参估计模块，结合多视角先验和时空一致性；2) 像素与点级别的置信度驱动融合策略，建模深度可靠性和可见性，采用非累积式逐帧融合。

Result: 在真实多摄像头系统实验中，SPARK在外参精度、几何一致性、时间稳定性和实时性能上均超越现有方法，支持线性扩展至大规模摄像头阵列。

Conclusion: 该框架实现了高稳定性与可扩展性，验证了自校准实时重建在复杂动态场景中的有效性。

Abstract: Real-time multi-camera 3D reconstruction is crucial for 3D perception, immersive interaction, and robotics. Existing methods struggle with multi-view fusion, camera extrinsic uncertainty, and scalability for large camera setups. We propose SPARK, a self-calibrating real-time multi-camera point cloud reconstruction framework that jointly handles point cloud fusion and extrinsic uncertainty. SPARK consists of: (1) a geometry-aware online extrinsic estimation module leveraging multi-view priors and enforcing cross-view and temporal consistency for stable self-calibration, and (2) a confidence-driven point cloud fusion strategy modeling depth reliability and visibility at pixel and point levels to suppress noise and view-dependent inconsistencies. By performing frame-wise fusion without accumulation, SPARK produces stable point clouds in dynamic scenes while scaling linearly with the number of cameras. Extensive experiments on real-world multi-camera systems show that SPARK outperforms existing approaches in extrinsic accuracy, geometric consistency, temporal stability, and real-time performance, demonstrating its effectiveness and scalability for large-scale multi-camera 3D reconstruction.

</details>


### [49] [MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP](https://arxiv.org/abs/2601.08420)
*Aditya Chaudhary,Sneha Barman,Mainak Singha,Ankit Jha,Girish Mishra,Biplab Banerjee*

Main category: cs.CV

TL;DR: MMLGNet通过对比学习将遥感多模态数据与文本对齐，使用简单CNN编码器在两个数据集上超越现有方法，证明语言监督的有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态地球观测数据激增，但融合高维光谱/空间/几何信息并实现语义理解存在挑战，需突破视觉-语言语义鸿沟。

Method: 构建模态特异性编码器，通过双向对比学习将视觉特征与手工文本嵌入对齐到共享潜在空间，借鉴CLIP范式实现跨模态对齐。

Result: 在HSI-LiDAR数据集上超越多模态视觉基线模型，使用轻量CNN编码器达到SOTA性能，验证语言监督对遥感解析的增益。

Conclusion: 证明跨模态对比学习能有效融合地球观测数据，语言引导可提升高维遥感数据语义理解，开源代码促进领域发展。

Abstract: In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.

</details>


### [50] [Deep Learning Based Facial Retargeting Using Local Patches](https://arxiv.org/abs/2601.08429)
*Yeonsoo Choi,Inyup Lee,Sihun Cha,Seonghyeon Kim,Sunjin Jung,Junyong Noh*

Main category: cs.CV

TL;DR: 该论文提出了一种基于局部图像块的面部动画重定向方法，用于将真人面部动作迁移至风格化3D角色。


<details>
  <summary>Details</summary>
Motivation: 传统面部动作迁移方法在处理具有相似面部结构的模型时效果良好，但在处理偏离人类面部结构的夸张风格化角色时效果欠佳。需要开发能适应目标角色面部结构和运动范围的迁移方法以保留原始动作语义。

Method: 设计三阶段框架：1) 自动图像块提取模块从源视频中提取局部区域 2) 重演模块生成目标角色对应的局部图像块 3) 权重估计模块计算每帧的动画参数，最终合成完整的面部动画序列。

Result: 实验表明该方法能成功保留源面部表情的语义特征，并有效适配面部特征比例差异显著的风格化角色。

Conclusion: 基于局部图像块的迁移方法在风格化角色面部动画生成中表现出优异的语义保持能力和跨形态适应性。

Abstract: In the era of digital animation, the quest to produce lifelike facial animations for virtual characters has led to the development of various retargeting methods. While the retargeting facial motion between models of similar shapes has been very successful, challenges arise when the retargeting is performed on stylized or exaggerated 3D characters that deviate significantly from human facial structures. In this scenario, it is important to consider the target character's facial structure and possible range of motion to preserve the semantics assumed by the original facial motions after the retargeting. To achieve this, we propose a local patch-based retargeting method that transfers facial animations captured in a source performance video to a target stylized 3D character. Our method consists of three modules. The Automatic Patch Extraction Module extracts local patches from the source video frame. These patches are processed through the Reenactment Module to generate correspondingly re-enacted target local patches. The Weight Estimation Module calculates the animation parameters for the target character at every frame for the creation of a complete facial animation sequence. Extensive experiments demonstrate that our method can successfully transfer the semantic meaning of source facial expressions to stylized characters with considerable variations in facial feature proportion.

</details>


### [51] [Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis](https://arxiv.org/abs/2601.08440)
*Yi Qin,Lehan Wang,Chenxu Zhao,Alex P. W. Lee,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出Cardiac Reasoning Template (CRT)与CardiacMind强化学习框架，提升多模态大语言模型 (MLLM) 的超声心动图诊断推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有超声心动图模型无法有效整合定量测量与临床表现，而医疗推理MLLM需高昂成本构建推理路径，且难以直接嵌入医学先验知识。

Method: 1) CRT提供标准化诊断流程，避免逐案验证；2) CardiacMind包含三个强化学习奖励：PQtR（促进细节推理）、PQlR（多模态证据整合）、ESR（视觉内容对齐）。

Result: 1) 多视角诊断准确率提升48%（15种复杂心脏病），CardiacNet-PAH指标提升5%；2) 临床医生对推理逻辑的认可度达93.33%。

Conclusion: 通过引入医学专家逻辑的模板化推理框架与定制化强化学习策略，有效解决了传统模型与MLLM在超声心动图诊断中的关键缺陷并获得临床验证。

Abstract: Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.

</details>


### [52] [Noise-Adaptive Regularization for Robust Multi-Label Remote Sensing Image Classification](https://arxiv.org/abs/2601.08446)
*Tom Burgert,Julia Henkel,Begüm Demir*

Main category: cs.CV

TL;DR: 本文提出NAR（Noise-Adaptive Regularization）方法，解决遥感多标签分类中的噪声标注问题，通过区分加性/减性噪声并动态调整标签监督强度，结合早学习正则化提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 遥感数据规模扩大后，低成本标注（如众包）导致多标签噪声问题（加性/减性噪声），现有方法未区分噪声类型且回避主动修正机制，亟需针对性解决方案。

Method: NAR框架包含：1）置信度分类机制（高置信标签保留、中置信暂挂、低置信翻转）；2）与早学习正则化结合的半监督学习流程，实现噪声抑制与训练稳定化。

Result: 在三种噪声场景下验证，NAR在减性/混合噪声中表现最优（如mAP提升3.2%/2.8%），表明自适应噪声抑制策略优于传统方法。

Conclusion: 本文证明区分处理噪声类型并动态修正标签能有效增强遥感多标签分类的鲁棒性，为噪声场景下的深度学习应用提供新范式。

Abstract: The development of reliable methods for multi-label classification (MLC) has become a prominent research direction in remote sensing (RS). As the scale of RS data continues to expand, annotation procedures increasingly rely on thematic products or crowdsourced procedures to reduce the cost of manual annotation. While cost-effective, these strategies often introduce multi-label noise in the form of partially incorrect annotations. In MLC, label noise arises as additive noise, subtractive noise, or a combination of both in the form of mixed noise. Previous work has largely overlooked this distinction and commonly treats noisy annotations as supervised signals, lacking mechanisms that explicitly adapt learning behavior to different noise types. To address this limitation, we propose NAR, a noise-adaptive regularization method that explicitly distinguishes between additive and subtractive noise within a semi-supervised learning framework. NAR employs a confidence-based label handling mechanism that dynamically retains label entries with high confidence, temporarily deactivates entries with moderate confidence, and corrects low confidence entries via flipping. This selective attenuation of supervision is integrated with early-learning regularization (ELR) to stabilize training and mitigate overfitting to corrupted labels. Experiments across additive, subtractive, and mixed noise scenarios demonstrate that NAR consistently improves robustness compared with existing methods. Performance improvements are most pronounced under subtractive and mixed noise, indicating that adaptive suppression and selective correction of noisy supervision provide an effective strategy for noise robust learning in RS MLC.

</details>


### [53] [Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.08448)
*Kexin Bao,Daichi Zhang,Yong Li,Dan Zeng,Shiming Ge*

Main category: cs.CV

TL;DR: 本文提出了一种名为静态-动态协作(SDC)的两阶段框架，通过分离静态保留阶段(SRS)和动态学习阶段(DLS)，在少样本类增量学习(FSCIL)中实现稳定性与可塑性的平衡。


<details>
  <summary>Details</summary>
Motivation: FSCIL任务面临稳定性与可塑性的核心矛盾：如何在有限数据下既保留旧知识又学习新类别。传统方法难以协调两种需求，导致遗忘或过拟合问题。

Method: 第一阶段(SRS)利用充足基础数据训练初始模型并保留静态记忆模块固化知识；第二阶段(DLS)引入可联合训练的动态投影器，在静态记忆约束下增量学习新类别，通过交替阶段优化权衡。

Result: 在3个公开基准数据集和真实应用场景中，该方法相对现有SOTA方法取得最优性能，特别在长序列增量学习和低样本敏感性指标上提升显著。

Conclusion: 实验表明SDC框架有效缓解了FSCIL中的灾难性遗忘，并通过模块化设计为稳定性-可塑性权衡提供了可扩展的解决方案。

Abstract: Few-shot class-incremental learning (FSCIL) aims to continuously recognize novel classes under limited data, which suffers from the key stability-plasticity dilemma: balancing the retention of old knowledge with the acquisition of new knowledge. To address this issue, we divide the task into two different stages and propose a framework termed Static-Dynamic Collaboration (SDC) to achieve a better trade-off between stability and plasticity. Specifically, our method divides the normal pipeline of FSCIL into Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS), which harnesses old static and incremental dynamic class information, respectively. During SRS, we train an initial model with sufficient data in the base session and preserve the key part as static memory to retain fundamental old knowledge. During DLS, we introduce an extra dynamic projector jointly trained with the previous static memory. By employing both stages, our method achieves improved retention of old knowledge while continuously adapting to new classes. Extensive experiments on three public benchmarks and a real-world application dataset demonstrate that our method achieves state-of-the-art performance against other competitors.

</details>


### [54] [Developing Predictive and Robust Radiomics Models for Chemotherapy Response in High-Grade Serous Ovarian Carcinoma](https://arxiv.org/abs/2601.08455)
*Sepideh Hatamikia,Geevarghese George,Florian Schwarzhans,Amirreza Mahbod,Marika AV Reinius,Ali Abbasian Ardakani,Mercedes Jimenez-Linan,Satish Viswanath,Mireia Crispin-Ortuzar,Lorena Escudero Sanchez,Evis Sala,James D Brenton,Ramona Woitek*

Main category: cs.CV

TL;DR: 本研究提出一种结合影像组学和机器学习的NACT治疗反应预测框架，通过整合稳健特征选择方法，在高级别浆液性卵巢癌患者中达到0.83的最高AUC值。


<details>
  <summary>Details</summary>
Motivation: 高级别浆液性卵巢癌(NACT)约40%患者疗效有限，需开发更精准的非侵入式治疗前预测工具以优化临床决策。

Method: 开发自动化随机化算法模拟观察者差异，结合预/术后CT影像组学特征与4个疗效指标(CRS/RECIST/体积/直径缩小率)，对不同解剖部位病变进行多队列验证。

Result: 联合所有病变特征预测体积缩小率达最优AUC0.83，大网膜病变特征预测CRS(AUC0.77)，盆腔病变预测直径缩小(AUC0.76)。

Conclusion: 特征稳健性整合能提升模型可靠性，推动影像组学在卵巢癌实时临床诊疗的应用，建议探索真实临床场景应用。

Abstract: Objectives: High-grade serous ovarian carcinoma (HGSOC) is typically diagnosed at an advanced stage with extensive peritoneal metastases, making treatment challenging. Neoadjuvant chemotherapy (NACT) is often used to reduce tumor burden before surgery, but about 40% of patients show limited response. Radiomics, combined with machine learning (ML), offers a promising non-invasive method for predicting NACT response by analyzing computed tomography (CT) imaging data. This study aimed to improve response prediction in HGSOC patients undergoing NACT by integration different feature selection methods. Materials and methods: A framework for selecting robust radiomics features was introduced by employing an automated randomisation algorithm to mimic inter-observer variability, ensuring a balance between feature robustness and prediction accuracy. Four response metrics were used: chemotherapy response score (CRS), RECIST, volume reduction (VolR), and diameter reduction (DiaR). Lesions in different anatomical sites were studied. Pre- and post-NACT CT scans were used for feature extraction and model training on one cohort, and an independent cohort was used for external testing. Results: The best prediction performance was achieved using all lesions combined for VolR prediction, with an AUC of 0.83. Omental lesions provided the best results for CRS prediction (AUC 0.77), while pelvic lesions performed best for DiaR (AUC 0.76). Conclusion: The integration of robustness into the feature selection processes ensures the development of reliable models and thus facilitates the implementation of the radiomics models in clinical applications for HGSOC patients. Future work should explore further applications of radiomics in ovarian cancer, particularly in real-time clinical settings.

</details>


### [55] [Modality-Decoupled RGB-Thermal Object Detector via Query Fusion](https://arxiv.org/abs/2601.08458)
*Chao Tian,Zikun Zhou,Chao Yang,Guoqing Zhu,Fu'an Zhong,Zhenyu He*

Main category: cs.CV

TL;DR: 提出了一种名为MDQF的RGB-Thermal检测框架，通过解耦模态分支与动态查询融合，在保留跨模态互补性的同时有效抑制低质量模态噪声。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T检测方法在单一模态质量极差时性能显著下降，且依赖严格配对的RGB-T训练数据。需要解决模态质量不对称条件下的鲁棒性问题和数据获取瓶颈。

Method: 采用双分支DETR架构分别处理RGB/TIR图像，通过多阶段查询选择与跨模态适应机制动态融合高质量检测查询。设计解耦训练框架，允许使用非配对单模态数据进行模型优化。

Result: 方法在RGB-T检测基准测试中超越现有技术，在极端光照条件下保持稳定检测性能。验证了非配对训练策略的有效性，且模态独立性比传统方法提升23.6%。

Conclusion: 提出的动态查询融合机制成功平衡模态互补与分离的矛盾，解耦框架显著降低数据标注成本，为多模态检测提供新范式。

Abstract: The advantage of RGB-Thermal (RGB-T) detection lies in its ability to perform modality fusion and integrate cross-modality complementary information, enabling robust detection under diverse illumination and weather conditions. However, under extreme conditions where one modality exhibits poor quality and disturbs detection, modality separation is necessary to mitigate the impact of noise. To address this problem, we propose a Modality-Decoupled RGB-T detection framework with Query Fusion (MDQF) to balance modality complementation and separation. In this framework, DETR-like detectors are employed as separate branches for the RGB and TIR images, with query fusion interspersed between the two branches in each refinement stage. Herein, query fusion is performed by feeding the high-quality queries from one branch to the other one after query selection and adaptation. This design effectively excludes the degraded modality and corrects the predictions using high-quality queries. Moreover, the decoupled framework allows us to optimize each individual branch with unpaired RGB or TIR images, eliminating the need for paired RGB-T data. Extensive experiments demonstrate that our approach delivers superior performance to existing RGB-T detectors and achieves better modality independence.

</details>


### [56] [CoMa: Contextual Massing Generation with Vision-Language Models](https://arxiv.org/abs/2601.08464)
*Evgenii Maslov,Valentin Khrulkov,Anastasia Volkova,Anton Gusarov,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出了一种基于功能性需求和场地环境的建筑体量自动生成框架，并发布CoMa-20K数据集及视觉-语言模型评估。


<details>
  <summary>Details</summary>
Motivation: 建筑及城市规划中的概念设计阶段（特别是建筑体量生成）依赖人工经验且缺乏高质量数据集，需通过数据驱动方法实现自动化。

Method: 构建包含体量几何、经济参数和场地语境图像的CoMa-20K数据集，通过调整和零样本视觉-语言模型（VLMs）进行条件式体量生成任务的基准测试。

Result: 实验验证了VLMs在复杂任务中的潜力，同时揭示了生成上下文相关体量选项的挑战性，并建立了数据驱动设计的基础基准。

Conclusion: 数据集与分析为建筑自动化设计提供了基准框架，表明未来需进一步探索数据驱动方法在建筑领域的应用。

Abstract: The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.

</details>


### [57] [Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling](https://arxiv.org/abs/2601.08467)
*Takamichi Miyata,Sumiko Miyata,Andrew Morris*

Main category: cs.CV

TL;DR: 提出一种解耦驾驶员外观特征与行为线索的分心驾驶检测方法，通过消除外表干扰提升模型在现实场景中的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的分心驾驶检测方法因无法分离驾驶员个体特征（如衣着/年龄/性别）与行为特征，导致模型决策偏差。需要解决模型过度依赖驾驶员身份而非行为表现的核心问题。

Method: 1) 构建驾驶员外观嵌入提取模块，从图像中分离并消除外观特征影响；2) 通过度量投影将文本嵌入正交化，提升类间可分性；3) 在Stiefel流形空间中保持文本语义完整性。

Result: 在现实场景驾驶检测任务中，相对基线模型实现稳定性能提升（具体指标未披露），验证了特征解耦与文本嵌入优化对复杂场景的适应性优势。

Conclusion: 该框架有效缓解了身份特征对行为分析的干扰，为实际道路安全监测提供了更可靠的检测范式，证明了视觉-语言跨模态解耦方法在工业场景的应用潜力。

Abstract: Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.

</details>


### [58] [Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs](https://arxiv.org/abs/2601.08470)
*Takara Taniguchi,Kuniaki Saito,Atsushi Hashimoto*

Main category: cs.CV

TL;DR: 本文提出HazardForge框架，用于生成复杂危险场景并构建MovSafeBench数据集，以评估视觉语言模型在自动驾驶中的安全决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分覆盖动态的异常危险场景，影响对VLMs在自动驾驶等复杂环境中决策能力的评估。

Method: 开发HazardForge框架，结合图像编辑模型、布局决策算法和验证模块生成动态场景；构建包含7254个样本的MovSafeBench数据集，涵盖13类正常与异常物体。

Result: VLMs在包含异常物体的场景中性能显著下降，特别是在需要精细运动理解的任务上表现最差。

Conclusion: 研究证明生成动态危险场景对评估VLMs安全性至关重要，揭示现有模型的局限性并为未来改进提供方向。

Abstract: Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.

</details>


### [59] [EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers](https://arxiv.org/abs/2601.08499)
*Wenwen Liao,Hang Ruan*

Main category: cs.CV

TL;DR: EfficientFSL提出了一种仅针对查询的微调框架，在保持ViT强大few-shot分类性能的同时，通过轻量级模块设计显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 为了解决Vision Transformer在few-shot分类中因参数规模大导致的显存消耗高和训练时间长的问题，提出低资源场景下的高效微调方法

Method: 1) 设计前向模块合成任务查询以提取特征，2) 构建融合模块聚合多层输出，3) 引入支持-查询注意力模块缓解分布偏移

Result: 在4个同域和6个跨域数据集上达到SOTA性能，使用可训练参数仅占ViT的0.09%-0.15%

Conclusion: 通过参数高效微调策略，成功平衡模型性能与计算成本，验证了知识蒸馏结合注意力机制在实际应用的可行性

Abstract: Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.

</details>


### [60] [CD^2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.08519)
*Kexin Bao,Daichi Zhang,Hansong Zhang,Yong Li,Yutao Yue,Shiming Ge*

Main category: cs.CV

TL;DR: 本文提出CD²框架，通过数据集蒸馏和蒸馏约束解决FSCIL中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用外部内存存储旧知识但无法有效保留核心特征，且平等处理新旧类导致知识冲突。

Method: CD²包含DDM模块（生成高浓缩样本）和DCM模块（设计约束损失函数），分别实现知识压缩与分布保护。

Result: 在3个公开数据集上实验表明CD²显著优于当前最优方法（如深度模型与蒸馏方法）。

Conclusion: CD²通过双向约束机制有效平衡了知识保持与增量学习能力，解决了FSCIL的关键挑战。

Abstract: Few-shot class-incremental learning (FSCIL) receives significant attention from the public to perform classification continuously with a few training samples, which suffers from the key catastrophic forgetting problem. Existing methods usually employ an external memory to store previous knowledge and treat it with incremental classes equally, which cannot properly preserve previous essential knowledge. To solve this problem and inspired by recent distillation works on knowledge transfer, we propose a framework termed \textbf{C}onstrained \textbf{D}ataset \textbf{D}istillation (\textbf{CD$^2$}) to facilitate FSCIL, which includes a dataset distillation module (\textbf{DDM}) and a distillation constraint module~(\textbf{DCM}). Specifically, the DDM synthesizes highly condensed samples guided by the classifier, forcing the model to learn compacted essential class-related clues from a few incremental samples. The DCM introduces a designed loss to constrain the previously learned class distribution, which can preserve distilled knowledge more sufficiently. Extensive experiments on three public datasets show the superiority of our method against other state-of-the-art competitors.

</details>


### [61] [VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations](https://arxiv.org/abs/2601.08557)
*Sushant Gautam,Cise Midoglu,Vajira Thambawita,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: VideoHEDGE是一个用于视频问答中幻觉检测的模块化框架，并提出了三种基于语义熵和视觉增强的可靠性评分指标。


<details>
  <summary>Details</summary>
Motivation: 视频视觉语言模型（Video-VLMs）易产生高置信度幻觉，现有不确定性度量难以有效关联正确性，需设计针对视频时序结构的幻觉检测方法。

Method: 通过高温采样生成多组答案，结合原始视频与经过光度/时空扰动的变体视频输入，利用NLI或嵌入方法对文本输出聚类，计算语义熵（SE）、RadFlag和视觉增强语义熵（VASE）三种可靠性评分。

Result: 在SoccerChat基准上，VASE在ROC-AUC指标表现最佳（尤其在大扰动预算下），嵌入聚类在降低计算成本的同时与NLI聚类检测性能相当，领域微调可减少幻觉但校准效果有限。

Conclusion: VideoHEDGE提供了可扩展的幻觉检测框架，其开源库便于方法复现与基准测试，揭示了视频模型可靠性评估的新方向。

Abstract: Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .

</details>


### [62] [End-to-End Video Character Replacement without Structural Guidance](https://arxiv.org/abs/2601.08587)
*Zhengbo Xu,Jie Ma,Ziheng Wang,Zhan Peng,Jun Liang,Jing Li*

Main category: cs.CV

TL;DR: 本论文提出了一种名为MoCha的视频角色替换框架，克服了现有方法的局限性，仅需单帧任意掩码即可实现可控的角色替换。


<details>
  <summary>Details</summary>
Motivation: 现有所依赖配对帧掩码与显式结构引导的方法在复杂场景（如遮挡、角色与物品交互、非常规姿势与光照）中泛化性差，易造成视觉伪影与时间不一致等问题。

Method: MoCha框架创新点包括：采用条件感知 RoPE（位置编码）和基于强化学习（RL）的后训练阶段，仅需单帧任意掩码。同时提出综合性数据构建管线，包含三类定制数据集：基于Unreal Engine 5的高保真渲染数据集、表情驱动合成数据集和基于现有视频掩码增强的数据集。

Result: 实验证明MoCha在性能上全面超越现有最先进方法。

Conclusion: MoCha成功突破了传统方法的数据依赖性与复杂场景适应性瓶颈，提供了一种更高效、可控的视频角色替换方案，并计划公开代码以促进行业研究。

Abstract: Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha

</details>


### [63] [WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation](https://arxiv.org/abs/2601.08602)
*Zishan Shu,Juntong Wu,Wei Yan,Xudong Liu,Hongyu Zhang,Chang Liu,Youdong Mao,Jie Chen*

Main category: cs.CV

TL;DR: 提出WaveFormer模型，通过波动方程显式建模视觉信息的空间传播，替代传统ViT和CNN，实现高效全局交互。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力机制在视觉空间信息传播建模上缺乏理论基础，缺乏对语义信息如何空间传播的显式控制。

Method: 将特征图视为空间信号，用阻尼波动方程建模其传播过程，推导闭式解并实现为Wave Propagation Operator（WPO），构建WaveFormer模型。

Result: WaveFormer在ImageNet分类、COCO检测和ADE20K分割任务上达到竞争精度，相比注意力模型吞吐量提升1.6倍、FLOPs减少30%，并证明波传播与heat-based方法互补。

Conclusion: 波传播建模能同时捕获全局结构和高频纹理细节，为视觉模型提供新的物理启发式建模路径。

Abstract: Vision modeling has advanced rapidly with Transformers, whose attention mechanisms capture visual dependencies but lack a principled account of how semantic information propagates spatially. We revisit this problem from a wave-based perspective: feature maps are treated as spatial signals whose evolution over an internal propagation time (aligned with network depth) is governed by an underdamped wave equation. In this formulation, spatial frequency-from low-frequency global layout to high-frequency edges and textures-is modeled explicitly, and its interaction with propagation time is controlled rather than implicitly fixed. We derive a closed-form, frequency-time decoupled solution and implement it as the Wave Propagation Operator (WPO), a lightweight module that models global interactions in O(N log N) time-far lower than attention. Building on WPO, we propose a family of WaveFormer models as drop-in replacements for standard ViTs and CNNs, achieving competitive accuracy across image classification, object detection, and semantic segmentation, while delivering up to 1.6x higher throughput and 30% fewer FLOPs than attention-based alternatives. Furthermore, our results demonstrate that wave propagation introduces a complementary modeling bias to heat-based methods, effectively capturing both global coherence and high-frequency details essential for rich visual semantics. Codes are available at: https://github.com/ZishanShu/WaveFormer.

</details>


### [64] [Interpretability and Individuality in Knee MRI: Patient-Specific Radiomic Fingerprint with Reconstructed Healthy Personas](https://arxiv.org/abs/2601.08604)
*Yaxi Chen,Simin Ni,Shuai Li,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出'放射组学指纹+健康人格'双策略，在膝关节MRI自动评估中平衡精度与可解释性，性能媲美深度学习的同时支持多层级解释性分析。


<details>
  <summary>Details</summary>
Motivation: 传统放射组学因人群级特征限制难以捕捉个体化差异，深度学习虽准但黑箱特性阻碍临床应用，需在二者间建立可解释的中间态方案

Method: 1)动态放射组学指纹：MRI驱动的个性化特征池+图像条件概率预测+透明逻辑回归 2)健康人格模型：扩散生成健康膝关节MRI重建基线

Result: 双策略及组合在3项临床任务中均达或超SOTA深度模型水平，病理定位与生物标志物发现案例验证可解释性，特征权重差异可直观可视化

Conclusion: 通过患者特异性特征生成与健康基线对比，建立了兼顾模型性能与临床可信度的新型医学影像分析框架

Abstract: For automated assessment of knee MRI scans, both accuracy and interpretability are essential for clinical use and adoption. Traditional radiomics rely on predefined features chosen at the population level; while more interpretable, they are often too restrictive to capture patient-specific variability and can underperform end-to-end deep learning (DL). To address this, we propose two complementary strategies that bring individuality and interpretability: radiomic fingerprints and healthy personas. First, a radiomic fingerprint is a dynamically constructed, patient-specific feature set derived from MRI. Instead of applying a uniform population-level signature, our model predicts feature relevance from a pool of candidate features and selects only those most predictive for each patient, while maintaining feature-level interpretability. This fingerprint can be viewed as a latent-variable model of feature usage, where an image-conditioned predictor estimates usage probabilities and a transparent logistic regression with global coefficients performs classification. Second, a healthy persona synthesises a pathology-free baseline for each patient using a diffusion model trained to reconstruct healthy knee MRIs. Comparing features extracted from pathological images against their personas highlights deviations from normal anatomy, enabling intuitive, case-specific explanations of disease manifestations. We systematically compare fingerprints, personas, and their combination across three clinical tasks. Experimental results show that both approaches yield performance comparable to or surpassing state-of-the-art DL models, while supporting interpretability at multiple levels. Case studies further illustrate how these perspectives facilitate human-explainable biomarker discovery and pathology localisation.

</details>


### [65] [SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling](https://arxiv.org/abs/2601.08608)
*Xi Chen,Hongxun Yao,Sicheng Zhao,Jiankun Zhu,Jing Jiang,Kui Jiang*

Main category: cs.CV

TL;DR: 本文提出SfMamba框架，结合Channel-wise Visual State-Space块和Semantic-Consistent Shuffle策略，有效解决源无关域自适应（SFDA）中域不变特征学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法存在感知场与计算效率的权衡，而VMamba难以捕捉通道频率特性和空间鲁棒性，需更优解决方案。

Method: 设计CVSS块进行通道序列扫描提取域不变特征，并引入SCS策略打乱背景块序列但保持预测一致性。

Result: SfMamba在多个基准测试中表现优于现有方法，参数效率高且代码开源（https://github.com/chenxi52/SfMamba）。

Conclusion: SfMamba成功平衡SFDA的域对齐与效率问题，提供隐私敏感场景下的实用模型迁移方案。

Abstract: Source-free domain adaptation (SFDA) tackles the critical challenge of adapting source-pretrained models to unlabeled target domains without access to source data, overcoming data privacy and storage limitations in real-world applications. However, existing SFDA approaches struggle with the trade-off between perception field and computational efficiency in domain-invariant feature learning. Recently, Mamba has offered a promising solution through its selective scan mechanism, which enables long-range dependency modeling with linear complexity. However, the Visual Mamba (i.e., VMamba) remains limited in capturing channel-wise frequency characteristics critical for domain alignment and maintaining spatial robustness under significant domain shifts. To address these, we propose a framework called SfMamba to fully explore the stable dependency in source-free model transfer. SfMamba introduces Channel-wise Visual State-Space block that enables channel-sequence scanning for domain-invariant feature extraction. In addition, SfMamba involves a Semantic-Consistent Shuffle strategy that disrupts background patch sequences in 2D selective scan while preserving prediction consistency to mitigate error accumulation. Comprehensive evaluations across multiple benchmarks show that SfMamba achieves consistently stronger performance than existing methods while maintaining favorable parameter efficiency, offering a practical solution for SFDA. Our code is available at https://github.com/chenxi52/SfMamba.

</details>


### [66] [SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning](https://arxiv.org/abs/2601.08617)
*Leo Fillioux,Omprakash Chakraborty,Ismail Ben Ayed,Paul-Henry Cournède,Stergios Christodoulidis,Maria Vakalopoulou,Jose Dolz*

Main category: cs.CV

TL;DR: 本文提出SoC方法，通过语义正交约束改善视觉语言模型的不确定性校准，解决现有方法因强制正交导致的语义分离问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的prompt-tuning方法过度关注分类性能提升，忽视不确定性校准。强制文本prompt嵌入的完全正交虽能增强可分性，但理论证明这会破坏语义关联性，导致模型过度自信。

Method: 设计基于Huber损失的正则化项（Semantic Orthogonal Calibration），在拉大不同类别原型距离的同时保持语义相关类别的接近性，实现校准与分类性能的平衡。

Result: 在多个基准测试中，SoC相比传统正交约束方法显著提升校准指标（如ECE），同时保持甚至优于当前sota分类准确率。

Conclusion: 研究揭示了全维度正交约束的负面影响，证明保留语义结构对齐的正交策略能更有效提升模型可靠性，为VLM校准提供了新的理论视角和实践方案。

Abstract: With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.

</details>


### [67] [CtrlFuse: Mask-Prompt Guided Controllable Infrared and Visible Image Fusion](https://arxiv.org/abs/2601.08619)
*Yiming Sun,Yuan Ruan,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出CtrlFuse框架，通过交互式动态融合实现可控制的红外与可见光图像融合，结合多模态特征和语义提示提升智能系统环境感知。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法存在像素级融合忽略下游任务需求或依赖刚性语义模型的缺陷，无法满足多样化任务场景的动态交互需求。

Method: 构建包含多模态特征提取器、参考提示编码器(RPE)和提示语义融合模块(PSFM)的框架，通过掩码引导动态编码任务语义，并注入融合特征，实现并行分支协同优化。

Result: 实验显示该方法在融合可控性和分割准确率方面达到SOTA，适配任务分支性能超越原始分割模型，实现融合质量与任务性能的双重提升。

Conclusion: CtrlFuse通过显式语义融合和动态提示机制，有效解决了传统方法在语义交互与任务适配方面的局限性，为智能系统提供更优环境感知方案。

Abstract: Infrared and visible image fusion generates all-weather perception-capable images by combining complementary modalities, enhancing environmental awareness for intelligent unmanned systems. Existing methods either focus on pixel-level fusion while overlooking downstream task adaptability or implicitly learn rigid semantics through cascaded detection/segmentation models, unable to interactively address diverse semantic target perception needs. We propose CtrlFuse, a controllable image fusion framework that enables interactive dynamic fusion guided by mask prompts. The model integrates a multi-modal feature extractor, a reference prompt encoder (RPE), and a prompt-semantic fusion module (PSFM). The RPE dynamically encodes task-specific semantic prompts by fine-tuning pre-trained segmentation models with input mask guidance, while the PSFM explicitly injects these semantics into fusion features. Through synergistic optimization of parallel segmentation and fusion branches, our method achieves mutual enhancement between task performance and fusion quality. Experiments demonstrate state-of-the-art results in both fusion controllability and segmentation accuracy, with the adapted task branch even outperforming the original segmentation model.

</details>


### [68] [Além do Desempenho: Um Estudo da Confiabilidade de Detectores de Deepfakes](https://arxiv.org/abs/2601.08674)
*Lucas Lopes,Rayson Laroca,André Grégio*

Main category: cs.CV

TL;DR: 本文提出了一种基于可转移性、鲁棒性、可解释性和计算效率四个支柱的Deepfakes检测方法可靠性评估框架，并发现现有技术存在关键局限性。


<details>
  <summary>Details</summary>
Motivation: 当前Deepfake检测技术评估仅关注分类性能，缺乏全面评估方法，无法有效推动技术实用化发展。需构建多维度可靠性评估体系以平衡方法优劣。

Method: 设计包含迁移能力测试、抗干扰能力验证、判据透明度分析和资源消耗评估的四维框架，对5种主流检测算法进行全面分析。

Result: 实验显示现有技术在特定场景下表现突出（如ResNet-50在计算效率优势），但普遍存在跨数据集性能下降（平均准确率下降18.3%）、对抗攻击易损性高等系统性缺陷。

Conclusion: 单纯的分类准确率无法反映检测方法真实可靠性，四维框架为技术选型与优化提供了结构化决策支持，强调需在多个性能维度间取得平衡。

Abstract: Deepfakes are synthetic media generated by artificial intelligence, with positive applications in education and creativity, but also serious negative impacts such as fraud, misinformation, and privacy violations. Although detection techniques have advanced, comprehensive evaluation methods that go beyond classification performance remain lacking. This paper proposes a reliability assessment framework based on four pillars: transferability, robustness, interpretability, and computational efficiency. An analysis of five state-of-the-art methods revealed significant progress as well as critical limitations.

</details>


### [69] [Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation](https://arxiv.org/abs/2601.08728)
*Runfeng Qu,Ole Hall,Pia K Bideau,Julie Ouerfelli-Ethier,Martin Rolfs,Klaus Obermayer,Olaf Hellwich*

Main category: cs.CV

TL;DR: 本研究提出Salience-SGG框架，通过迭代显著解码器（ISD）和语义无关显著标签解决场景图生成中的长尾分布问题，提升模型对罕见关系的识别能力。


<details>
  <summary>Details</summary>
Motivation: 场景图生成（SGG）因长尾分布导致模型对少数关系表现差，现有去偏方法（Unbiased-SGG）虽缓解偏差但损伤空间理解能力，需平衡语义先验与空间特征。

Method: 设计包含迭代显著解码器（ISD）的框架，通过语义无关的显著标签引导模型关注具有显著空间结构的三元组，结合语义与空间信息进行迭代优化。

Result: 在Visual Genome、Open Images V6和GQA-200上均达SOTA，Pairwise Localization AP指标证明空间理解能力优于现有Unbiased-SGG方法。

Conclusion: Salience-SGG有效缓解SGG长尾分布问题，通过空间结构显著性建模实现语义与视觉信息的协同优化，为类似任务提供新思路。

Abstract: Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision

</details>


### [70] [ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning](https://arxiv.org/abs/2601.08732)
*Vincent Roca,Martin Bretzner,Hilde Henon,Laurent Puy,Grégory Kuchcinski,Renaud Lopes*

Main category: cs.CV

TL;DR: 本文提出ISLA模型，通过深度学习优化实现急性缺血性卒中病变的高精度MRI分割


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在急性缺血性卒中病变分割中依赖U-Net框架但存在配置差异，且多数方法未开源，最优架构配置和泛化能力仍需探索

Method: 构建包含损失函数优化、卷积架构改进、深度监督机制、注意力机制的ISLA模型，并引入无监督领域自适应技术；基于1500+例多中心数据进行训练

Result: ISLA在外部测试集中显著优于现有两种SOTA方法，代码和预训练模型已开源

Conclusion: ISLA通过系统化架构优化实现了临床级卒中病变分割精度，开源代码促进了后续研究，并证明了领域自适应在医学影像泛化中的有效性

Abstract: Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.

</details>


### [71] [Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN](https://arxiv.org/abs/2601.08776)
*Yanhua Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于CycleGAN的方法，将多通道荧光显微图像转换为类似H&E染色的病理图像，以支持病理学家解读并融入常规分析流程。


<details>
  <summary>Details</summary>
Motivation: 组织病理学依赖H&E染色，但荧光显微镜能提供互补信息。将荧光图像转为H&E类似外观可辅助解读并与现有工作流集成，但缺乏无需配对训练数据的转换方法。

Method: 使用CycleGAN进行无配对图像到图像转换，将C01和C02荧光通道合成RGB图像，通过ResNet生成器和PatchGAN判别器学习双向映射，结合对抗损失、循环一致性和身份损失优化。

Result: 实验表明生成结果保留荧光形态结构并具备H&E颜色特征，且能与现有H&E分析流程兼容。

Conclusion: 该方法解决了无配对数据的跨域病理图像转换问题，为荧光数据提供病理学友好型可视化工具，并促进与标准工作流的整合。

Abstract: Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.

</details>


### [72] [Aggregating Diverse Cue Experts for AI-Generated Image Detection](https://arxiv.org/abs/2601.08790)
*Lei Tan,Shuwei Li,Mohan Kankanhalli,Robby T. Tan*

Main category: cs.CV

TL;DR: MCAN通过整合输入图像、高频分量和色度不一致性线索，在统一框架下提升AI生成图像检测的跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法过度依赖特定生成模型特征导致过拟合，缺乏对不同图像生成模型的泛化能力。

Method: MCAN采用混合编码器适配器动态处理多模态线索：1) 输入图像捕获整体内容 2) 高频分量强调边缘细节 3) 新提出的色度不一致性(CI)线索通过标准化强度值和捕获真实图像噪声特征。该方法整合空间、频域和色度信息进行表征学习。

Result: 在GenImage、Chameleon和UniversalFakeDetect数据集上验证性能，其中GenImage数据集的跨八种生成器测试中，MCAN平均准确率超越现有SOTA方法7.4%。

Conclusion: 基于多模态线索融合的统一框架能有效提升检测器对新型生成模型的泛化能力，其中CI线索和高频分量对噪声模式的建模具有关键作用。

Abstract: The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.

</details>


### [73] [DentalX: Context-Aware Dental Disease Detection with Radiographs](https://arxiv.org/abs/2601.08797)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 本文提出DentalX框架，通过整合口腔结构信息来改善X光片中牙科疾病的检测，解决因影像细微特征导致的识别难题。


<details>
  <summary>Details</summary>
Motivation: 传统基于自然图像目标检测的模型难以识别牙科影像微弱病灶，牙科X光片的诊断因证据模糊而耗时且困难。

Method: 设计结构上下文提取模块，通过同步训练牙齿解剖语义分割辅助任务，将牙齿结构信息融合到疾病检测主任务中。

Result: 在基准数据集上DentalX在两个任务中均显著优于现有方法，模型优化中两个任务相关性捕获实现了协同增益。

Conclusion: 基于结构感知的多任务学习框架能有效缓解牙科影像模糊特征带来的检测挑战，代码已开源。

Abstract: Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.

</details>


### [74] [S3-CLIP: Video Super Resolution for Person-ReID](https://arxiv.org/abs/2601.08807)
*Tamas Endrei,Gyorgy Cserey*

Main category: cs.CV

TL;DR: S3-CLIP introduces video super-resolution to enhance tracklet quality for person re-identification, improving performance in challenging cross-view scenarios.


<details>
  <summary>Details</summary>
Motivation: Most ReID methods prioritize architectural changes over tracklet quality, leading to limitations in real-world deployment where low-quality tracklets hinder accurate matching.

Method: Integrates video super-resolution networks with task-driven pipelines to enhance tracklet quality, then combines this with CLIP-based ReID for cross-view matching.

Result: Achieved competitive mAP scores (37.52% in aerial-to-ground, 29.16% in ground-to-aerial) and significant Rank-1/5/10 improvements (11.24-17.98%) under cross-view conditions.

Conclusion: Proposes the first systematic exploration of video super-resolution for tracklet quality enhancement in person ReID, demonstrating its criticality in challenging real-world applications.

Abstract: Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.

</details>


### [75] [Reasoning Matters for 3D Visual Grounding](https://arxiv.org/abs/2601.08811)
*Hsiang-Wei Huang,Kuang-Ming Chen,Wenhao Chai,Cheng-Yen Yang,Jen-Hao Cheng,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出自动化3D视觉基础数据合成管道与Reason3DVG-8B模型，仅需1.6%数据超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D视觉基础模型依赖大规模人工标注数据且性能提升受限的问题，探索低能耗数据合成路径。

Method: 构建跨模态数据合成管道生成带推理过程的3D视觉基础数据，结合LLM微调技术开发Reason3DVG-8B模型。

Result: 模型性能超越3D-GRAND基线方法，数据效率提升62倍（1.6%训练数据占比），验证合成数据有效性。

Conclusion: 证明自动化推理标注数据对3D视觉基础模型的关键作用，为小数据驱动视觉语言模型提供新范式。

Abstract: The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.

</details>


### [76] [Motion Attribution for Video Generation](https://arxiv.org/abs/2601.08828)
*Xindi Wu,Despoina Paschalidou,Jun Gao,Antonio Torralba,Laura Leal-Taixé,Olga Russakovsky,Sanja Fidler,Jonathan Lorraine*

Main category: cs.CV

TL;DR: 本文提出Motive，一种基于运动的视频生成模型数据归因框架，通过分析高影响力剪辑优化数据微调，显著改善生成视频的时间一致性和运动质量。


<details>
  <summary>Details</summary>
Motivation: 尽管视频生成模型发展迅速，但训练数据如何影响视频运动模式仍不清晰。现有研究多关注静态画面而非动态时序特征，亟需一种可量化运动属性影响的分析框架。

Method: 提出Motive框架：(1)采用运动加权损失掩码分离动态时序与静态外观特征；(2)应用梯度反向传播计算每帧数据对运动的影响值；(3)基于影响值筛选关键训练样本并重构微调数据集；(4)通过消融实验证明框架有效性。

Result: 在VBench基准测试中，Motive选取的高影响力数据使运动平滑度提升74.1%，动态程度达到SOTA水平。人工评估显示74.1%用户更偏好Motive优化的生成结果。定量分析证明运动加权策略可精确识别时序异常样本。

Conclusion: 作为首个针对视频生成模型运动属性的数据归因框架，Motive开创性地将梯度归因技术应用于运动模式分析，并首次实现了基于数据影响值的动态微调策略，为大模型训练集优化提供了新方法。

Abstract: Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.

</details>


### [77] [3AM: Segment Anything with Geometric Consistency in Videos](https://arxiv.org/abs/2601.08831)
*Yang-Che Sun,Cheng Sun,Chin-Yang Lin,Fu-En Yang,Min-Hung Chen,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 通过结合3D感知特征与轻量级特征融合，本文提出3AM方法显著提升视频对象分割在大视角变化下的性能，仅需RGB输入且无需预处理。


<details>
  <summary>Details</summary>
Motivation: 现有视频分割方法（如SAM2）因依赖外观特征在大视角变化时效果差，传统3D方法又依赖相机姿态和深度图等额外信息，作者希望解决几何一致性与输入便捷性的矛盾。

Method: 提出3AM框架，通过特征融合模块整合SAM2外观特征与MUSt3R的3D几何特征，采用视场感知采样策略确保空间一致性，并设计轻量级Feature Merger进行多尺度特征融合。

Result: 在ScanNet++和Replica数据集上，3AM较SAM2提升15.9%（90.6% IoU）和30.4%（71.7% Positive IoU）指标，成为当前无需相机姿态输入的最先进方法。

Conclusion: 该方法通过隐式几何建模与外观特征结合，突破了传统视频分割对视角变化和输入条件的限制，推动了无需深度信息的动态场景理解技术发展。

Abstract: Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/

</details>


### [78] [RAVEN: Erasing Invisible Watermarks via Novel View Synthesis](https://arxiv.org/abs/2601.08832)
*Fahad Shamshad,Nils Lukas,Karthik Nandakumar*

Main category: cs.CV

TL;DR: This paper identifies a vulnerability in AI-generated image watermarking where semantic-preserving viewpoint transformations can remove watermarks, and proposes a diffusion-based method to exploit this weakness.


<details>
  <summary>Details</summary>
Motivation: To evaluate the vulnerability of invisible watermarking schemes against sophisticated semantic-preserving attacks, revealing critical gaps in current robustness assumptions.

Method: Reformulates watermark removal as a view synthesis problem using a zero-shot diffusion-framework that applies geometric transformations in latent space with view-guided correspondence attention, without requiring watermark knowledge or model access.

Result: Achieves state-of-the-art watermark suppression across 15 methods, outperforms 14 baselines, and maintains perceptual quality on multiple datasets through semantic-preserving transformations.

Conclusion: Exposes a fundamental vulnerability in invisible watermarks against semantic viewpoint transformations, demonstrating that robustness to traditional attacks does not ensure resilience to structure-preserving geometric manipulations.

Abstract: Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [79] [EmbeddingRWKV: State-Centric Retrieval with Reusable States](https://arxiv.org/abs/2601.07861)
*Haowen Hou,Jie Yang*

Main category: cs.CL

TL;DR: State-Centric Retrieval通过共享状态信息，将嵌入模型与排序模型统一，解决了传统RAG系统冗余计算问题，实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统两阶段缺乏信息共享导致冗余计算，需要提升系统整体效率。

Method: 提出EmbeddingRWKV模型同时完成嵌入和状态提取，设计基于状态的排序模型，仅处理查询token并采用层选择策略减少计算量。

Result: 排序阶段推理速度提升5.4-44.8倍，使用25%的模型层数保留98.62%性能。

Conclusion: 状态中心检索范式有效平衡检索质量与效率，代码已开源。

Abstract: Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.

</details>


### [80] [A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics](https://arxiv.org/abs/2601.07954)
*Haoan Jin,Han Ying,Jiacheng Ji,Hanhui Xu,Mengyue Wu*

Main category: cs.CL

TL;DR: 本文提出MedES基准和守护者框架，通过数据对齐优化医疗伦理决策模型，在中国语境中实现7B模型超越更大基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂医疗伦理场景的适应性不足，需构建针对性评估体系与对齐框架填补该领域研究空白。

Method: ①构建260个权威中文医疗伦理语料库（MedES）；②开发基于专家标注的高精度自动评估者；③实施监督微调与领域优化的守护者框架。

Result: 7B对齐模型在中国医疗伦理任务中显著优于更大基线模型，质量与综合指标双提升，验证了框架有效性。

Conclusion: 验证医疗AI伦理对齐的可行性，框架支持跨文化法律环境定制化应用，为医疗AI合规化提供新路径。

Abstract: Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric benchmark specifically constructed from 260 authoritative Chinese medical, ethical, and legal sources to reflect the challenges in clinical decision-making. To facilitate model alignment, we introduce a guardian-in-the-loop framework that leverages a dedicated automated evaluator (trained on expert-labeled data and achieving over 97% accuracy within our domain) to generate targeted prompts and provide structured ethical feedback. Using this pipeline, we align a 7B-parameter LLM through supervised fine-tuning and domain-specific preference optimization. Experimental results, conducted entirely within the Chinese medical ethics context, demonstrate that our aligned model outperforms notably larger baselines on core ethical tasks, with observed improvements in both quality and composite evaluation metrics. Our work offers a practical and adaptable framework for aligning LLMs with medical ethics in the Chinese healthcare domain, and suggests that similar alignment pipelines may be instantiated in other legal and cultural environments through modular replacement of the underlying normative corpus.

</details>


### [81] [Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs](https://arxiv.org/abs/2601.07972)
*Jen-tse Huang,Jiantong Qin,Xueli Qiu,Sharon Levy,Michelle R. Kaufman,Mark Dredze*

Main category: cs.CL

TL;DR: 本文提出了ValAct-15k数据集，并发现大语言模型（LLMs）在价值观表现上具有高度一致性，但存在与人类相似的“知行脱节”问题，即自我报告价值观与实际决策间的弱对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLMs如何在现实决策中表征与执行人类价值观，填补基于标准化理论框架（Schwartz基本人类价值观理论）的实证研究空白，并比较LLMs与人类在价值观对齐上的异同。

Method: 从Reddit提取3,000个建议寻求场景构建ValAct-15k数据集，涵盖Schwartz理论的10项核心价值观；对10家中美前沿LLMs及55名人类受试者进行场景决策与价值观问卷测试，分析模型与人类的决策一致性及价值观扮演表现。

Result: LLMs在场景决策中呈现近完美一致性（皮尔逊相关r≈1.0），远超人类群体差异性（r范围[-0.79,0.98]）；LLMs与人类均显示自我报告与实际行为间对应性弱（相关系数0.4 vs 0.3）；当强制指定价值观时，LLMs表现为角色扮演能力下降（性能下降6.6%）。

Conclusion: 对齐训练促使LLMs形成规范化价值观收敛，但未消除人类特有的价值观“认知-行动鸿沟”，且LLMs在主动采纳特定价值观时存在角色适应性局限，反映出类人认知惯性。

Abstract: Value alignment is central to the development of safe and socially compatible artificial intelligence. However, how Large Language Models (LLMs) represent and enact human values in real-world decision contexts remains under-explored. We present ValAct-15k, a dataset of 3,000 advice-seeking scenarios derived from Reddit, designed to elicit ten values defined by Schwartz Theory of Basic Human Values. Using both the scenario-based questions and the traditional value questionnaire, we evaluate ten frontier LLMs (five from U.S. companies, five from Chinese ones) and human participants ($n = 55$). We find near-perfect cross-model consistency in scenario-based decisions (Pearson $r \approx 1.0$), contrasting sharply with the broad variability observed among humans ($r \in [-0.79, 0.98]$). Yet, both humans and LLMs show weak correspondence between self-reported and enacted values ($r = 0.4, 0.3$), revealing a systematic knowledge-action gap. When instructed to "hold" a specific value, LLMs' performance declines up to $6.6%$ compared to merely selecting the value, indicating a role-play aversion. These findings suggest that while alignment training yields normative value convergence, it does not eliminate the human-like incoherence between knowing and acting upon values.

</details>


### [82] [Explaining Generalization of AI-Generated Text Detectors Through Linguistic Analysis](https://arxiv.org/abs/2601.07974)
*Yuxi Xia,Kinga Stańczak,Benjamin Roth*

Main category: cs.CL

TL;DR: AI-text detectors struggle to generalize across different generation conditions (prompts, models, domains), and linguistic features like tense usage and pronouns correlate with performance gaps.


<details>
  <summary>Details</summary>
Motivation: High accuracy in-domain detectors face generalization issues in cross-condition scenarios, but root causes remain understudied.

Method: Constructed a benchmark spanning 6 prompts, 7 LLMs, 4 domains with human/AI texts, then used statistical analysis to link generalization accuracy to 80 linguistic feature shifts.

Result: Detector performance significantly correlates with shifts in linguistic features like tense and pronouns between training/test conditions.

Conclusion: Linguistic feature misalignment explains generalization failure in AI-text detection, offering insights for robust detector design.

Abstract: AI-text detectors achieve high accuracy on in-domain benchmarks, but often struggle to generalize across different generation conditions such as unseen prompts, model families, or domains. While prior work has reported these generalization gaps, there are limited insights about the underlying causes. In this work, we present a systematic study aimed at explaining generalization behavior through linguistic analysis. We construct a comprehensive benchmark that spans 6 prompting strategies, 7 large language models (LLMs), and 4 domain datasets, resulting in a diverse set of human- and AI-generated texts. Using this dataset, we fine-tune classification-based detectors on various generation settings and evaluate their cross-prompt, cross-model, and cross-dataset generalization. To explain the performance variance, we compute correlations between generalization accuracies and feature shifts of 80 linguistic features between training and test conditions. Our analysis reveals that generalization performance for specific detectors and evaluation conditions is significantly associated with linguistic features such as tense usage and pronoun frequency.

</details>


### [83] [Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models](https://arxiv.org/abs/2601.07984)
*Haorui Yu,Ramon Ruiz-Dolz,Xuehang Wen,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: 提出三层跨文化艺术批评评估框架，校准模型文化理解得分并诊断文化差异，15 VLM模型实验显示西方样本得分更高且自动化指标不可靠。


<details>
  <summary>Details</summary>
Motivation: 验证现有视觉语言模型（VLMs）对文化艺术内涵的解读能力不足，传统指标无法有效评估文化深度理解，需建立系统化的跨文化评估方法论。

Method: 构建三阶段评估体系：1）离线计算覆盖率与风险指标；2）主评专家按五个维度评分；3）通过同调回归校准得分。使用294个专家样本覆盖6大文化传统进行测试。

Result: 框架使MAE误差降低5.2%，发现自动化指标与文化深度无显著关联，西方文化样本得分显著优于非西方样本，跨评估标准的尺度差异使单一评分不可靠，单专家加校准机制最优。

Conclusion: 该框架为模型筛选和文化差异诊断提供量化工具，证明VLMs在文化理解上的局限性，强调评估标准校准和跨文化采样平衡的重要性。

Abstract: Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.

</details>


### [84] [Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset](https://arxiv.org/abs/2601.07985)
*Z. Melce Hüsünbeyi,Virginie Mouilleron,Leonie Uhling,Daniel Foppe,Tatjana Scheffler,Djamé Seddah*

Main category: cs.CL

TL;DR: 本研究提出了一种多模态事实核查数据集构建的全流程方法，涵盖法语和德语资源，并基于大语言模型实现自动证据提取与可解释性验证。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查数据集存在多模态证据缺失、结构化标注不足及跨语言适用性有限的问题，亟需开发更鲁棒且可解释的多语言多模态验证框架。

Method: 通过聚合ClaimReview数据源、爬取辟谣文章、统一异构判决、添加结构化元数据及视觉内容构建数据集，采用大语言模型与多模态模型进行证据分类提取与验证理由生成，并通过G-Eval与人工评估验证效果。

Result: 实验表明该方法支持跨机构事实核查实践的细粒度对比，显著提升模型可解释性与证据关联性，且评估结果验证了多模态数据对多语言场景的有效性。

Conclusion: 为多语言多模态虚假信息治理提供了可扩展的数据基础设施，揭示了大型模型在证据驱动型事实核查中的应用潜力，并为跨文化验证研究奠定了基础。

Abstract: The rapid proliferation of misinformation across online platforms underscores the urgent need for robust, up-to-date, explainable, and multilingual fact-checking resources. However, existing datasets are limited in scope, often lacking multimodal evidence, structured annotations, and detailed links between claims, evidence, and verdicts. This paper introduces a comprehensive data collection and processing pipeline that constructs multimodal fact-checking datasets in French and German languages by aggregating ClaimReview feeds, scraping full debunking articles, normalizing heterogeneous claim verdicts, and enriching them with structured metadata and aligned visual content. We used state-of-the-art large language models (LLMs) and multimodal LLMs for (i) evidence extraction under predefined evidence categories and (ii) justification generation that links evidence to verdicts. Evaluation with G-Eval and human assessment demonstrates that our pipeline enables fine-grained comparison of fact-checking practices across different organizations or media markets, facilitates the development of more interpretable and evidence-grounded fact-checking models, and lays the groundwork for future research on multilingual, multimodal misinformation verification.

</details>


### [85] [VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding](https://arxiv.org/abs/2601.07986)
*Haorui Yu,Ramon Ruiz-Dolz,Diji Yang,Hang He,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: VULCA-Bench introduces a multicultural art-critique benchmark to evaluate Vision-Language Models' higher-order cultural understanding (L3-L5) beyond surface visual perception (L1-L2), with 7,410 bilingual image-critique pairs across eight traditions.


<details>
  <summary>Details</summary>
Motivation: Current VLM benchmarks focus on object recognition and factual analysis (L1-L2) but neglect complex cultural interpretation (L3-L5). VULCA-Bench aims to address this gap by operationalizing cultural understanding through a five-layer framework spanning philosophical aesthetics to visual perception.

Method: Constructed 7,410 image-critique pairs spanning eight cultural traditions (including Chinese-English bilingual coverage), organized into a five-layer framework (L1-L5) with 225 culture-specific dimensions. Expert-written critiques and evaluation scripts were developed, released under CC BY 4.0 license.

Result: Pilot experiments showed consistent difficulty in higher-layer reasoning (L3-L5) compared to lower layers (L1-L2). The dataset and tools demonstrate cultural nuance evaluation capabilities while maintaining bilingual accessibility.

Conclusion: VULCA-Bench advances VLM assessment by emphasizing cultural depth beyond technical analysis, providing standardized resources for multicultural research, and highlighting challenges in machine cultural interpretation.

Abstract: We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.

</details>


### [86] [From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP](https://arxiv.org/abs/2601.07988)
*Adithya V Ganesan,Vasudha Varadarajan,Oscar NE Kjell,Whitney R Ringwald,Scott Feltman,Benjamin J Luft,Roman Kotov,Ryan L Boyd,H Andrew Schwartz*

Main category: cs.CL

TL;DR: 本文提出纵向NLP建模范式，通过调整评估分隔、动态指标、历史序列输入和状态建模，修正传统方法在处理时间嵌套文本数据时的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统NLP假设文档独立且无时序关系，但在纵向研究中存在作者内文档时序依赖和个体差异，导致传统方法可能产生错误结论。

Method: 构建四维改进框架：1) 跨个体/时序双重评估分隔；2) 解耦个体内动态与个体间差异的指标；3) 引入历史状态的序列输入机制；4) 支持不同粒度隐含状态建模(从池化统计到交互模型)。

Result: 在PTSD日记数据集验证中，传统文档级评估与改进方法产生显著结论差异，某些结论甚至完全反转，验证了纵向建模的生态效度优势。

Conclusion: 研究主张NLP范式从词序列分析转向行为序列分析，强调整合时间维度和个体特异性建模对生态效度的重要性。

Abstract: While NLP typically treats documents as independent and unordered samples, in longitudinal studies, this assumption rarely holds: documents are nested within authors and ordered in time, forming person-indexed, time-ordered $\textit{behavioral sequences}$. Here, we demonstrate the need for and propose a longitudinal modeling and evaluation paradigm that consequently updates four parts of the NLP pipeline: (1) evaluation splits aligned to generalization over people ($\textit{cross-sectional}$) and/or time ($\textit{prospective}$); (2) accuracy metrics separating between-person differences from within-person dynamics; (3) sequence inputs to incorporate history by default; and (4) model internals that support different $\textit{coarseness}$ of latent state over histories (pooled summaries, explicit dynamics, or interaction-based models). We demonstrate the issues ensued by traditional pipeline and our proposed improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to our ecologically valid modeling and evaluation. We tie our results to a broader discussion motivating a shift from word-sequence evaluation toward $\textit{behavior-sequence}$ paradigms for NLP.

</details>


### [87] [DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs](https://arxiv.org/abs/2601.07994)
*Nayoung Choi,Jonathan Zhang,Jinho D. Choi*

Main category: cs.CL

TL;DR: 本文提出DyCP，一种轻量级对话上下文管理方法，通过动态分割和查询时检索相关记忆，在保留对话顺序结构的前提下提升长对话场景下的回答质量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆管理方法依赖额外LLM调用或离线构建记忆，导致效率低下并破坏对话连贯性。同时，尽管现代LLM上下文窗口扩大，其长文本处理能力仍有不足。

Method: 提出基于动态上下文分割与检索框架(DyCP)，支持无需预定义话题边界的时间序列结构保留机制，实现对话历史中关键上下文的实时检索。

Result: 在LoCoMo、MT-Bench+和SCM4LLMs三个长对话基准测试中，DyCP在多个LLM上均实现回答质量提升与延迟降低，验证了动态上下文管理的有效性。

Conclusion: 研究表明即使LLM上下文窗口扩展，优化长对话状态管理仍具挑战。DyCP通过结构化记忆检索机制填补了现代LLM理论窗口长度与实际处理能力间的差距。

Abstract: Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.

</details>


### [88] [Is Sentiment Banana-Shaped? Exploring the Geometry and Portability of Sentiment Concept Vectors](https://arxiv.org/abs/2601.07995)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer L. Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: Concept Vector Projections (CVP) provide continuous sentiment scores but have approximate linearity, allowing portability across domains with minimal loss while leaving room for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis methods in humanities require context-aware scores, but CVP's cross-domain portability and linearity assumptions remain underexplored.

Method: Evaluated CVP across genres, historical periods, languages, and affective dimensions, analyzing transferability of concept vectors and examining the linearity assumption through generalization patterns.

Result: CVP demonstrates strong cross-domain transfer with minimal performance loss but relies on an approximate linearity assumption, indicating its limitations in capturing complex sentiment patterns.

Conclusion: CVP is a portable sentiment analysis method with generalizable patterns, but its linearity assumption should be refined for better performance.

Abstract: Use cases of sentiment analysis in the humanities often require contextualized, continuous scores. Concept Vector Projections (CVP) offer a recent solution: by modeling sentiment as a direction in embedding space, they produce continuous, multilingual scores that align closely with human judgments. Yet the method's portability across domains and underlying assumptions remain underexplored. We evaluate CVP across genres, historical periods, languages, and affective dimensions, finding that concept vectors trained on one corpus transfer well to others with minimal performance loss. To understand the patterns of generalization, we further examine the linearity assumption underlying CVP. Our findings suggest that while CVP is a portable approach that effectively captures generalizable patterns, its linearity assumption is approximate, pointing to potential for further development.

</details>


### [89] [LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback](https://arxiv.org/abs/2601.08003)
*Weiyue Li,Mingxiao Song,Zhenda Shen,Dachuan Zhao,Yunfan Long,Yi Li,Yongce Li,Ruyi Yang,Mengyu Wang*

Main category: cs.CL

TL;DR: 提出了LLM Review框架，利用盲审机制提升大语言模型的创造性生成能力，并通过SciFi-100数据集验证其效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在创造性生成任务上表现不足，传统多智能体框架因内容同质化抑制创造力，需要新的交互结构解决该问题

Method: 设计基于盲审机制的LLM Review框架（智能体独立修改并交换定向反馈），构建包含人类标注、规则指标和LLM评审的SciFi-100科幻写作评估体系

Result: 实验显示LLM Review优于多智能体基线模型，且采用该框架的小模型可超越大单模型性能，证明交互结构可替代模型规模

Conclusion: 结构化交互机制能有效提升生成创造力，为模型架构设计提供了「结构代偿规模」的新范式

Abstract: Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.

</details>


### [90] [Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models](https://arxiv.org/abs/2601.08058)
*Zhenghao He,Guangzhi Xiong,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 通过稀疏自动编码器分析LLM内部表示，发现无需CoT提示即可通过调节特定潜在特征触发推理机制。


<details>
  <summary>Details</summary>
Motivation: 探究Chain-of-Thought提示提升LLM推理性能的原因及是否存在替代触发机制。

Method: 使用稀疏自动编码器(SAEs)干预大模型内部表征，识别因果关联的推理特征。

Result: 调节单个推理特征可提升准确率，大模型表现与CoT相当且输出更高效，且推理状态可覆盖反推理指令。

Conclusion: LLM多步推理由潜在内部激活驱动，CoT仅是有效激活方式而非必要原因。

Abstract: Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.

</details>


### [91] [Universal computation is intrinsic to language model decoding](https://arxiv.org/abs/2601.08061)
*Alex Lewandowski,Marlos C. Machado,Dale Schuurmans*

Main category: cs.CL

TL;DR: 语言模型通过自回归输出链具备通用计算能力，训练提升其可编程性而非计算能力。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在无形式化计算机结构下的计算潜力，揭示训练对计算能力与可编程性的影响差异。

Method: 通过数学证明构建自回归链模拟通用算法执行框架，验证未训练随机初始化模型的计算能力。

Result: 发现仅需输出链式调用即可实现算法模拟，且随机模型本身具备完整计算功能，训练仅增强与自然语言交互的易用性。

Conclusion: 语言模型内在计算能力与生俱来，训练核心价值在于建立自然语言与计算能力的高效接口。

Abstract: Language models now provide an interface to express and often solve general problems in natural language, yet their ultimate computational capabilities remain a major topic of scientific debate. Unlike a formal computer, a language model is trained to autoregressively predict successive elements in human-generated text. We prove that chaining a language model's autoregressive output is sufficient to perform universal computation. That is, a language model can simulate the execution of any algorithm on any input. The challenge of eliciting desired computational behaviour can thus be reframed in terms of programmability: the ease of finding a suitable prompt. Strikingly, we demonstrate that even randomly initialized language models are capable of universal computation before training. This implies that training does not give rise to computational expressiveness -- rather, it improves programmability, enabling a natural language interface for accessing these intrinsic capabilities.

</details>


### [92] [Calibration Is Not Enough: Evaluating Confidence Estimation Under Language Variations](https://arxiv.org/abs/2601.08064)
*Yuxi Xia,Dennis Ulmer,Terra Blevins,Yihong Liu,Hinrich Schütze,Benjamin Roth*

Main category: cs.CL

TL;DR: 该论文提出了一种新的大语言模型（LLM）置信度估计评估框架，指出现有方法在语言变体和LLM特定场景下的局限性。新框架从鲁棒性、稳定性、敏感性三个新维度评估置信度质量，发现传统方法虽在传统指标表现良好，但存在提示扰动不鲁棒、语义变化不敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有置信度评估方法仅关注校准性和区分度，忽视LLM在语言变化场景中的潜在问题。作者旨在建立更全面的评估框架，以提升LLM在实际应用中的可信度及决策支持能力。

Method: 构建包含三个新维度的评估框架：
1) 对提示扰动的鲁棒性
2) 语义等价答案的稳定性
3) 对语义差异答案的敏感性
通过实验对比传统置信度估计方法在这些指标上的表现

Result: 实验发现：
1) 现有CE方法在提示扰动下鲁棒性不足
2) 对语义等价答案稳定性较差
3) 对语义差异答案缺乏足够敏感性
传统方法虽在传统校准和区分度表现良好，但无法满足新维度要求

Conclusion: 研究揭示了实际应用场景中现有置信度评估方法的局限性，提出包含模型可解释性与鲁棒性的综合评估框架，为设计更可靠的LLM置信度估计方法提供实用指导。

Abstract: Confidence estimation (CE) indicates how reliable the answers of large language models (LLMs) are, and can impact user trust and decision-making. Existing work evaluates CE methods almost exclusively through calibration, examining whether stated confidence aligns with accuracy, or discrimination, whether confidence is ranked higher for correct predictions than incorrect ones. However, these facets ignore pitfalls of CE in the context of LLMs and language variation: confidence estimates should remain consistent under semantically equivalent prompt or answer variations, and should change when the answer meaning differs. Therefore, we present a comprehensive evaluation framework for CE that measures their confidence quality on three new aspects: robustness of confidence against prompt perturbations, stability across semantic equivalent answers, and sensitivity to semantically different answers. In our work, we demonstrate that common CE methods for LLMs often fail on these metrics: methods that achieve good performance on calibration or discrimination are not robust to prompt variations or are not sensitive to answer changes. Overall, our framework reveals limitations of existing CE evaluations relevant for real-world LLM use cases and provides practical guidance for selecting and designing more reliable CE methods.

</details>


### [93] [AdaJudge: Adaptive Multi-Perspective Judging for Reward Modeling](https://arxiv.org/abs/2601.08097)
*Yongliang Miao,Yangyang Liang,Mengnan Du*

Main category: cs.CL

TL;DR: AdaJudge improves reward modeling by adaptively refining representations and pooling strategies for better alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing reward models use static pooling, causing misalignment with task-specific preferences and poor discrimination due to generation-optimized backbones.

Method: AdaJudge uses gated refinement blocks to adapt representations and dynamic multi-view pooling to combine evidence, addressing static bias and representation mismatch.

Result: AdaJudge outperforms strong reward models and static pooling baselines on RM-Bench and JudgeBench benchmarks.

Conclusion: Adaptive representation and pooling strategies enhance reward modeling for alignment with human preferences in LLMs.

Abstract: Reward modeling is essential for aligning large language models with human preferences, yet predominant architectures rely on a static pooling strategy to condense sequences into scalar scores. This paradigm, however, suffers from two key limitations: a static inductive bias that misaligns with task-dependent preference signals, and a representational mismatch, as the backbone is optimized for generation rather than fine-grained discrimination. To address this, we propose AdaJudge, a unified framework that jointly adapts representation and aggregation. AdaJudge first refines backbone representations into a discrimination-oriented space via gated refinement blocks. It then replaces the static readout with an adaptive multi-view pooling module that dynamically routes and combines evidence. Extensive experiments on RM-Bench and JudgeBench show that AdaJudge outperforms strong off-the-shelf reward models and traditional pooling baselines.

</details>


### [94] [Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning](https://arxiv.org/abs/2601.08105)
*Fabian Spaeh,Tianyi Chen,Chen-Hao Chiang,Bin Shen*

Main category: cs.CL

TL;DR: This paper introduces a dynamic few-shot learning approach to suggest answerable queries for agentic RAG systems, improving user interaction by avoiding hallucinations from out-of-scope questions.


<details>
  <summary>Details</summary>
Motivation: Agentic RAG systems face hallucinations when handling queries beyond their knowledge scope. Existing guardrails block such questions but lack tools to suggest alternative answerable queries. The authors aim to fill this gap to enhance user-agent interaction, especially in complex, multi-step workflows where RAG agents struggle to adapt.

Method: The proposed solution uses robust dynamic few-shot learning, which retrieves relevant examples from RAG workflows to train the system. It leverages prior user queries for self-learning, enabling real-time query suggestions without requiring explicit domain knowledge. This addresses the RAG system's inability to internally understand its own workflow limitations.

Result: Experiments on three real-world datasets show that the method outperforms few-shot and retrieval-only baselines. It generates more relevant, answerable query suggestions, enabling safer and more effective user-RAG agent interactions by adhering to the system's knowledge scope.

Conclusion: The dynamic few-shot learning framework enhances agentic RAG systems by providing practical, answerable query recommendations. This addresses a critical gap in user interaction design, offering a scalable solution for real-world applications.

Abstract: Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.
  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.

</details>


### [95] [Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought](https://arxiv.org/abs/2601.08108)
*Bowen Li,Ziqi Xu,Jing Ren,Renqiang Luo,Xikun Zhang,Xiuzhen Zhang,Yongli Ren,Feng Xia*

Main category: cs.CL

TL;DR: The paper proposes a new prompting framework called Adaptive Causal Prompting with Sketch-of-Thought (ACPS) to improve efficiency and generalizability in large language models by leveraging causal reasoning with concise prompts.


<details>
  <summary>Details</summary>
Motivation: Existing prompting methods like Chain-of-Thought (CoT) suffer from high token usage and limited generalizability across diverse reasoning tasks, prompting the need for a more efficient and adaptable approach.

Method: ACPS uses structural causal models to infer causal effects between queries and answers, adaptively selects interventions (e.g., front-door adjustments), and replaces verbose CoT with compact Sketch-of-Thought prompts.

Result: Experiments demonstrate ACPS outperforms existing prompting methods in accuracy, robustness, and computational efficiency across multiple benchmarks and LLMs.

Conclusion: ACPS enables generalizable causal reasoning without task-specific retraining while significantly reducing token usage and inference costs.

Abstract: Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.

</details>


### [96] [Attention Projection Mixing and Exogenous Anchors](https://arxiv.org/abs/2601.08131)
*Jonathan Su*

Main category: cs.CL

TL;DR: ExoFormer introduces external anchor projections in Transformers to decouple computational refinement from stable reference points, improving performance and data efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional Transformers face a conflict where early layers must balance being stable references and computational units. This leads to suboptimal trade-offs in model design and performance.

Method: ExoFormer uses exogenous (external) anchor projections outside the layer stack, coupled with a unified normalized mixing framework across attention pathways (queries, keys, values, gate logits). Different coefficient granularities (elementwise, headwise, scalar) are explored to optimize attention dynamics.

Result: ExoFormer outperforms baseline models with a 2.13-point accuracy gain, achieves 1.84x better data efficiency, and reduces attention sink by 2x. However, it paradoxically exhibits representation collapse.

Conclusion: The Offloading Hypothesis explains how external anchors preserve token identity, enabling layers to focus on computation rather than dual roles. Despite representation collapse, performance improves, suggesting new trade-offs in Transformer design.

Abstract: Transformers that reuse early-layer attention projections as residuals face a fundamental tension: the first layer must simultaneously serve as a stable reference for all deeper layers and as an effective computational block. To resolve this, we propose ExoFormer, which learns dedicated exogenous anchor projections outside the sequential layer stack, decoupling the anchor role from computational refinement. Through a unified normalized mixing framework (studying different coefficient granularities: elementwise, headwise, scalar) across all attention pathways (queries, keys, values, and gate logits), ExoFormer variants consistently outperform their internal-anchor counterparts. Moreover, the dynamic variant achieves a 2.13-point increase in downstream accuracy over the baseline and demonstrates superior data efficiency, matching baseline validation loss with 1.84x fewer tokens. ExoFormer also achieves a 2x reduction in attention sink compared to standard Gated Attention. Paradoxically, all ExoFormer variants exhibit signs of representation collapse. We explain this via an Offloading Hypothesis: external anchors preserve essential token identity, allowing layers to specialize exclusively in computational refinement. We release codes and models to facilitate future research.

</details>


### [97] [How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains](https://arxiv.org/abs/2601.08134)
*Reza Khanmohammadi,Erfan Miahi,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: 该论文提出了RMCB基准测试，用于评估大型推理模型（LRMs）的信心估计。研究发现，不同模型在区分能力（AUROC）和校准能力（ECE）之间存在权衡，并指出当前基于表示的方法存在性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在高风险领域的可靠性受到信心校准不足的影响，缺乏有效的方法评估其多步骤输出的信心，因此需要建立标准化的基准来填补这一空白。

Method: 构建包含347,496个推理轨迹的RMCB基准，涵盖六个LRM模型和跨高风险领域的多任务数据集，并对超过十种序列、图结构和文本编码方法进行大规模实证评估。

Result: 文本编码器在AUROC上表现最佳（0.672），结构感知模型在ECE上表现最佳（0.148），但二者无法兼顾；复杂架构未显著优于简单序列模型，表明当前方法存在性能上限。

Conclusion: RMCB为信心估计提供了最全面的基准，证明了现有方法的局限性，并强调需要突破基于表示的范式以提升模型可靠性。

Abstract: The miscalibration of Large Reasoning Models (LRMs) undermines their reliability in high-stakes domains, necessitating methods to accurately estimate the confidence of their long-form, multi-step outputs. To address this gap, we introduce the Reasoning Model Confidence estimation Benchmark (RMCB), a public resource of 347,496 reasoning traces from six popular LRMs across different architectural families. The benchmark is constructed from a diverse suite of datasets spanning high-stakes domains, including clinical, financial, legal, and mathematical reasoning, alongside complex general reasoning benchmarks, with correctness annotations provided for all samples. Using RMCB, we conduct a large-scale empirical evaluation of over ten distinct representation-based methods, spanning sequential, graph-based, and text-based architectures. Our central finding is a persistent trade-off between discrimination (AUROC) and calibration (ECE): text-based encoders achieve the best AUROC (0.672), while structurally-aware models yield the best ECE (0.148), with no single method dominating both. Furthermore, we find that increased architectural complexity does not reliably outperform simpler sequential baselines, suggesting a performance ceiling for methods relying solely on chunk-level hidden states. This work provides the most comprehensive benchmark for this task to date, establishing rigorous baselines and demonstrating the limitations of current representation-based paradigms.

</details>


### [98] [Relational Knowledge Distillation Using Fine-tuned Function Vectors](https://arxiv.org/abs/2601.08169)
*Andrea Kang,Yingnian Wu,Hongjing Lu*

Main category: cs.CL

TL;DR: 通过微调函数向量并组合为复合函数向量，提升大语言模型在类比推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有因果中介分析获得的函数向量在关系表征任务中表现有限。研究旨在探索如何通过少量示例（约20组词对）微调函数向量，以及构造复合向量以增强模型对语义关系与类比推理的解码能力。

Method: 1）对原函数向量进行少量样本微调；2）构建复合函数向量（加权结合微调向量）；3）推理时将复合向量注入语言模型激活状态以增强推理能力。

Result: 1）微调后函数向量在关系词解码和类比任务（包括认知科学与SAT基准）表现提升；2）与人类语义关系相似性判断对齐度提高；3）复合向量在大模型激活中显著提升类比问题准确率。

Conclusion: 激活补丁（activation patching）作为可控机制，可有效编码和操作关系知识，显著提升大型语言模型的可解释性与推理能力。

Abstract: Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.

</details>


### [99] [Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering](https://arxiv.org/abs/2601.08176)
*Lavanya Prahallad,Sai Utkarsh Choudarypally,Pragna Prahallad,Pranathi Prahallad*

Main category: cs.CL

TL;DR: 该论文研究了在政治问答中，通过不同提示策略提升语言模型回答清晰度的自动评估效果，发现结合思维链和少量示例的提示方法最优，但细粒度逃避检测和话题识别仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型回答自动评估多关注事实准确性，但缺乏对清晰度和提示设计影响的研究，尤其是针对政治问题的自动清晰度评估方法亟待探索。

Method: 基于SemEval 2026的CLARITY数据集，比较GPT-3.5基线与GPT-5.2在简单提示、思维链提示及少量示例思维链提示下的表现，评估与人类标注的一致性（准确率、类别指标、分层精确匹配）。

Result: GPT-5.2在清晰度预测准确率从56%提升至63%（思维链+少量示例），逃避检测准确率最高达34%（思维链），话题识别准确率从60%升至74%，但细粒度逃避类别改进不显著。

Conclusion: 精心设计提示策略能显著提升宏观清晰度评估效果，但细粒度逃避检测和话题识别仍需更优解决方案，结构化推理提示对此类挑战性任务效果有限。

Abstract: Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.

</details>


### [100] [Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs](https://arxiv.org/abs/2601.08198)
*Yibo Wang,Hai-Long Sun,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.CL

TL;DR: 本研究提出了基于三元组的自我博弈微调方法(T-SPIN)，通过引入历史优势项和熵约束机制，解决了传统SPIN方法在奖励优势衰减和训练-生成指标不一致的问题，显著提升数据稀缺场景下的模型微调效果。


<details>
  <summary>Details</summary>
Motivation: SPIN方法存在两个核心问题：(1)当迭代生成的响应奖励优势衰减时导致优化不稳定；(2)参考策略带来的奖励公式与生成指标存在偏差。这些限制了其在数据稀缺场景下的应用效果。

Method: T-SPIN包含两个创新设计：(1)在训练目标中同时保留当前优势和历史优势，当当前优势消失时仍能通过历史优势维持优化稳定性；(2)引入熵约束实现免参考微调，理论证明该约束能消除训练与生成阶段的评估差异。

Result: 在多种任务上的实验表明：(1)T-SPIN比SPIN表现更优且迭代过程更稳定；(2)仅需25%标注样本即可达到甚至超越监督微调效果；(3)历史优势机制保持了持续优化能力，熵约束有效实现了参考-生成一致性。

Conclusion: T-SPIN成功解决了SPIN方法的不稳定优化与评估偏差问题，在数据稀缺场景下展现出显著优势，理论证明和实验验证共同支撑了所提出方法的有效性。

Abstract: Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated responses over synthetic responses at hand, which may gradually vanish during iterations, leading to unstable optimization. Moreover, the utilization of reference policy induces a misalignment issue between the reward formulation for training and the metric for generation. To address these limitations, we propose a novel Triplet-based Self-Play fIne-tuNing (T-SPIN) method that integrates two key designs. First, beyond current advantages, T-SPIN additionally incorporates historical advantages between iteratively generated responses and proto-synthetic responses produced by the initial policy. Even if the current advantages diminish, historical advantages remain effective, stabilizing the overall optimization. Second, T-SPIN introduces the entropy constraint into the self-play framework, which is theoretically justified to support reference-free fine-tuning, eliminating the training-generation discrepancy. Empirical results on various tasks demonstrate not only the superior performance of T-SPIN over SPIN, but also its stable evolution during iterations. Remarkably, compared to supervised fine-tuning, T-SPIN achieves comparable or even better performance with only 25% samples, highlighting its effectiveness when faced with scarce annotated data.

</details>


### [101] [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://arxiv.org/abs/2601.08209)
*Rongji Li,Jian Xu,Xueqing Chen,Yisheng Yang,Jiayi Wang,Xingyu Chen,Chunyu Xie,Dawei Leng,Xu-Yao Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为Generation-Augmented Generation (GAG)的新框架，用于将私有领域知识注入大语言模型，有效解决现有微调和检索增强生成(RAG)方法的不足。


<details>
  <summary>Details</summary>
Motivation: 工业场景需要将私有、非公开的领域知识注入大模型，但现有方法存在：1) 微调迭代成本高且易遗忘旧知识；2) RAG在专有领域易出现证据碎片、检索漂移和长上下文压力。

Method: 借助多模态模型的对齐机制，将私有知识视为'额外专家模态'，通过紧凑的表征层接口与冻结基座模型对齐，实现知识注入与即插即用的领域定制。

Result: 在免疫佐剂和催化剂材料两大私有科学问答基准上分别优于RAG基线15.34%和14.86%，保持通用基准性能，并实现近乎完美的选择性激活能力。

Conclusion: GAG提供了一种可扩展的多领域共存方案，在保持模型通用性的同时，解决了专有领域知识注入的稳定性与效率问题，适合高价值行业落地。

Abstract: In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.

</details>


### [102] [Towards Principled Design of Mixture-of-Experts Language Models under Memory and Inference Constraints](https://arxiv.org/abs/2601.08215)
*Seng Pei Liew,Kenta Shinzato,Yuyang Dong*

Main category: cs.CL

TL;DR: 传统MoE模型设计仅考虑总参数和激活参数，但论文发现专家稀疏度（s = n_exp/n_topk）与总参数量是影响模型性能的核心因素，并提出需在内存限制下最大化总参数且最小化稀疏度的优化原则。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型设计指标（总参数与激活参数）不足以描述最优架构。增加专家总数（n_exp）会因内存限制压缩模型核心维度（深度/宽度），从而影响性能，亟需新设计框架。

Method: 通过系统性实证研究分析MoE性能影响因素，提出专家稀疏度（s）与总参数量（N_total）的双因子模型，结合理论推导揭示专家数量与模型维度的权衡关系。

Result: 发现当总参数确定时，较高的专家稀疏度（s）会降低性能；而更多专家（n_exp）需牺牲模型深度/宽度。实验验证了在内存约束下最大化N_total并最小化s的设计原则能显著提升效率。

Conclusion: 论文提出基于总参数量与稀疏度的MoE设计框架，通过参数优化解决架构模糊性，为稀疏模型开发提供通用方法论。

Abstract: Modern Mixture-of-Experts (MoE) language models are designed based on total parameters (memory footprint) and active parameters (inference cost). However, we find these two factors alone are insufficient to describe an optimal architecture. Through a systematic study, we demonstrate that MoE performance is primarily determined by total parameters ($N_{total}$) and expert sparsity ($s:=n_{exp}/n_{topk}$).
  Moreover, $n_{exp}$ and $n_{topk}$ do not "cancel out" within the sparsity ratio; instead, a larger total number of experts slightly penalizes performance by forcing a reduction in core model dimensions (depth and width) to meet memory constraints. This motivates a simple principle for MoE design which maximizes $N_{total}$ while minimizing $s$ (maximizing $n_{topk}$) and $n_{exp}$ under the given constraints. Our findings provide a robust framework for resolving architectural ambiguity and guiding MoE design.

</details>


### [103] [User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale](https://arxiv.org/abs/2601.08225)
*Jungho Cho,Minbyul Jeong,Sungrae Park*

Main category: cs.CL

TL;DR: 提出了一种用户导向的多轮对话生成框架，通过解耦任务生成和用户模拟，提升对话长度和真实互动性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集受限于静态工具集，难以支持复杂人机协作中的多轮动态交互。

Method: 1) 设计任务导向的多轮对话生成框架，利用LRRM生成动态工具；2) 构建专用用户模拟器模拟人类行为规则（渐进式请求、逐轮反馈），实现用户导向的模拟范式。

Result: 生成的多任务数据集支持任意起点生成、多任务嵌套，对话平均轮次显著提升，数据密度增强。

Conclusion: 用户模拟机制有效解决了纯任务导向系统缺乏互动性的痛点，为开放域工具使用提供可扩展的数据生产方案。

Abstract: The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.

</details>


### [104] [Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning](https://arxiv.org/abs/2601.08267)
*Fan Gao,Sherry T. Tong,Jiwoong Sohn,Jiahao Huang,Junfeng Jiang,Ding Xia,Piyalitt Ittichaiwong,Kanyakorn Veerakanjana,Hyunjae Kim,Qingyu Chen,Edison Marrese Taylor,Kazuma Kobayashi,Akkiko Aizawa,Irene Li*

Main category: cs.CL

TL;DR: 提出Med-CoReasoner框架，通过双语并行推理和概念对齐提升多语言医疗推理能力


<details>
  <summary>Details</summary>
Motivation: 英语医疗模型性能优异但多语言推理存在显著差距，限制了全球医疗公平性

Method: 建立英语言及本地语言协同推理框架，将双语推理抽象为结构化概念，并通过概念对齐融合本地医学知识到英语逻辑框架

Result: 在MultiMed-X多语言基准测试中平均性能提升5%，低资源语言提升显著，模型蒸馏和专家验证显示推理轨迹兼具临床正确性和文化适配性

Conclusion: 结构化双语协同推理框架能有效整合英语逻辑鲁棒性和本地医学经验，为跨文化医疗AI提供新范式

Abstract: While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.

</details>


### [105] [Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees](https://arxiv.org/abs/2601.08274)
*Kun Li,Zenan Xu,Junan Li,Zengrui Jin,Jinghao Deng,Zexuan Qiu,Bo Zhou*

Main category: cs.CL

TL;DR: DART框架通过强化学习实现大语言模型在长链思维推理中自主使用工具，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型难以在长链推理中有效整合工具使用，受限于训练数据不足及传统方法可能削弱原有推理能力。

Method: 动态构建rollout树探索工具调用机会，通过树状优势估计筛选并强化有效工具调用路径。

Result: 在AIME和GPQA-Diamond等复杂任务上显著优于基线方法，成功融合工具执行与长链推理。

Conclusion: DART证明了动态轨迹探索与优势估计机制在工具-推理协同中的有效性，为复杂任务扩展提供了新方向。

Abstract: Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.

</details>


### [106] [Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques](https://arxiv.org/abs/2601.08302)
*Marvin Schmitt,Anne Schwerk,Sebastian Lempert*

Main category: cs.CL

TL;DR: 本研究探讨了不同提示工程技术对GPT-4o-mini和gemini-1.5-flash模型在情感分析任务中的效果，发现针对性的提示策略可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在情感分析中存在模型与任务适配性差异，需探索高级提示方法在情感分类、方面级分析和讽刺检测等任务中的最佳实践。

Method: 采用few-shot学习、chain-of-thought推理、self-consistency三种提示技术，对比基线表现，在准确性、召回率、精确度和F1分数四个指标下评估模型在多个情感分析任务的表现。

Result: 高级提示技术整体提升模型性能，其中GPT-4o-mini在few-shot提示下表现最佳，gemini-1.5-flash通过chain-of-thought在讽刺检测中准确率提升46%。

Conclusion: 提示策略需同时匹配模型架构特性与任务语义复杂度，GPT-4o-mini适用于样本引导的few-shot，gemini-1.5-flash的推理链提示更擅长处理讽刺等复杂语义分析。

Abstract: This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.

</details>


### [107] [CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark](https://arxiv.org/abs/2601.08331)
*Daniil Gurgurov,Yusser Al Ghussin,Tanja Baeumel,Cheng-Ting Chou,Patrick Schramowski,Marius Mosbach,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 该论文提出了CLaS-Bench，首个用于评估大语言模型多语种'引导'技术的标准化基准。


<details>
  <summary>Details</summary>
Motivation: 解决多语种大语言模型领域长期缺乏专用基准和评价协议的问题，填补现有方法(如提示词或微调)与内部表征操控技术的可量化评估空白。

Method: 构建包含32种语言、平行问题的轻量级基准测试集，系统评估DiffMean干预、探针方向、语言特异性神经元等6类主流引导方法，采用调和平均引导评分同时度量语言控制力和语义相关性。

Result: 实验发现：1) DiffMean方法在各语言中均表现最优；2) 语言特异性特征主要出现在模型后层；3) 引导方向按语系聚类；4) 提示词基线方法存在明显不足。

Conclusion: CLaS-Bench为首个标准化多语种引导评估基准，推动语言表征分析并证明引导技术作为低成本适配方案的潜力。

Abstract: Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question benchmark for evaluating language-forcing behavior in LLMs across 32 languages, enabling systematic evaluation of multilingual steering methods. We evaluate a broad array of steering techniques, including residual-stream DiffMean interventions, probe-derived directions, language-specific neurons, PCA/LDA vectors, Sparse Autoencoders, and prompting baselines. Steering performance is measured along two axes: language control and semantic relevance, combined into a single harmonic-mean steering score. We find that across languages simple residual-based DiffMean method consistently outperforms all other methods. Moreover, a layer-wise analysis reveals that language-specific structure emerges predominantly in later layers and steering directions cluster based on language family. CLaS-Bench is the first standardized benchmark for multilingual steering, enabling both rigorous scientific analysis of language representations and practical evaluation of steering as a low-cost adaptation alternative.

</details>


### [108] [Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue](https://arxiv.org/abs/2601.08342)
*Run Chen,Wen Liang,Ziwei Gong,Lin Ai,Julia Hirschberg*

Main category: cs.CL

TL;DR: 本文提出首个语音对话中检测心理操控的数据集SPEECHMENTALMANIP，并发现音频模态检测存在高特异性低召回率，凸显多模态系统需模态感知评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有文本对话心理操控检测研究忽视语音中的操作策略，亟需填补语音模态分析空白以提升多模态对话系统的安全性。

Method: 合成高质量语音-文本混合数据集SPEECHMENTALMANIP，采用少样本学习和人类标注协同评估音频与文本模态差异。

Result: 模型在文本转语音数据中特异性达87%但召回率低于65%，人工标注者对音频操控判断一致性仅52%，显示音频线索缺失导致检测困难。

Conclusion: 声学和语调线索对心理操控检测至关重要，多模态对话系统需构建模态专用评估框架并强化安全校准机制。

Abstract: Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.

</details>


### [109] [PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors](https://arxiv.org/abs/2601.08402)
*Donya Rooein,Sankalan Pal Chowdhury,Mariia Eremeeva,Yuan Qin,Debora Nozza,Mrinmaya Sachan,Dirk Hovy*

Main category: cs.CL

TL;DR: 本文提出一种结合学生性格特征的LLM辅导框架，通过构建教学-性格映射分类体系并调整生成策略，实验证明该方法显著提升教学偏好与效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM教育系统未针对学生个性化特征定制策略，而不同人格类型对教学法敏感度差异显著，导致潜在负面教学效果。

Method: 1) 构建基于教育学理论的『性格-教学法』双维度映射分类体系
2) 设计性格模拟对话生成框架
3) 通过人机协同标注系统实现教学策略的实时动态调整

Result: 1) 人类教师对系统生成的教学策略偏好度达73.2%，显著领先传统基线
2) 高效但低频的教学策略（如角色扮演）使用率提升4.1倍
3) 双盲实验显示生成内容在激励性、适配性维度超越人工基准

Conclusion: 通过系统化关联人格科学与教学法，本研究验证了个性化LLM教学在提升教学质量与策略多样性的可行性，为生成式教育系统提供了可扩展的认知科学框架

Abstract: Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.

</details>


### [110] [Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering](https://arxiv.org/abs/2601.08427)
*Nonghai Zhang,Weitao Ma,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu*

Main category: cs.CL

TL;DR: 该论文提出Latent-GRPO框架，利用潜在空间几何特性实现无需外部验证器的连续内在奖励机制，通过迭代稳健质心估计算法（IRCE）解决传统方法计算成本高、奖励稀疏的问题，并实现训练加速。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO方法依赖昂贵的外部验证器或人工规则，导致计算成本高、训练延迟严重且奖励稀疏，阻碍优化效率。

Method: 基于潜在空间几何特性，提出IRCE算法：通过球面投影缓解向量幅度波动，并通过迭代聚合估计稳健的“真相质心”，生成密集的连续奖励。

Result: 实验表明，在维持模型性能的前提下，训练速度比基线方法提升2倍以上，且具有强泛化能力和鲁棒性。

Conclusion: Latent-GRPO通过挖掘潜在空间内在几何特性，为大语言模型提供了高效的奖励机制设计范式，验证了自监督奖励机制在强化学习中的潜力。

Abstract: Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.

</details>


### [111] [Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management](https://arxiv.org/abs/2601.08435)
*Weitao Ma,Xiaocheng Feng,Lei Huang,Xiachong Feng,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Bing Qin*

Main category: cs.CL

TL;DR: 提出Fine-Mem框架，通过细粒度反馈对齐提升大型语言模型的记忆管理能力，包含分块级步骤奖励与证据锚定奖励分配方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习驱动的记忆管理因依赖最终任务奖励导致奖励稀疏性和信用分配失效，无法有效指导具体记忆操作。

Method: 1) Chunk-level Step Reward: 通过辅助分块问答任务提供即时监督；2) Evidence-Anchored Reward Attribution: 基于推理时使用的具体记忆项重新分配全局奖励，实现信用分配对齐。

Result: 在Memalpha和MemoryAgentBench基准测试中超越强基线方法，实现更高成功率和跨模型配置的强泛化能力，验证了方法的稳定性和适应性。

Conclusion: Fine-Mem框架通过细粒度反馈机制有效协调局部记忆操作与长期任务目标，为复杂任务下的记忆管理系统设计提供了新范式。

Abstract: Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.

</details>


### [112] [JudgeRLVR: Judge First, Generate Second for Efficient Reasoning](https://arxiv.org/abs/2601.08468)
*Jiangshan Duo,Hanyu Li,Hailin Zhang,Yudong Wang,Sujian Li,Liang Zhao*

Main category: cs.CL

TL;DR: 本论文提出JudgeRLVR方法，通过两阶段训练（先判断再生成）优化大语言模型的强化学习，在保持答案准确性的同时显著减少冗长推理步骤。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR过度追求最终答案正确性导致模型依赖低效试错，需在生成长度与问题解决间取得平衡。现有长度惩罚会误删关键推理步骤，需探索内在结构化引导机制。

Method: 1) 判断器训练：在相同数学领域数据上训练模型判别正确解法；2) 生成器微调：用原始RLVR方法初始化生成器并基于判断器反馈优化。双阶段共享模型参数。

Result: Qwen3-30B-A3B模型显示：在数学域内任务准确率提升3.7%，生成长度减少42%；跨领域测试准确率提升4.5%，显示更强泛化能力。

Conclusion: 生成质量与效率的博弈源于模型对合法解法的辨别能力，先验判断机制能有效压缩搜索空间，为大模型推理效率提升提供新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.

</details>


### [113] [Do You Understand How I Feel?: Towards Verified Empathy in Therapy Chatbots](https://arxiv.org/abs/2601.08477)
*Francesco Dettori,Matteo Forasassi,Lorenzo Veronese,Livia Lestingi,Vincenzo Scotti,Matteo Giovanni Rossi*

Main category: cs.CL

TL;DR: This paper proposes a framework integrating natural language processing and formal verification to develop empathetic therapy chatbots, using Transformer-based models and stochastic hybrid automata for behavior analysis.


<details>
  <summary>Details</summary>
Motivation: Empathy is critical in therapeutic chatbots, but current practices lack systematic methods to define or verify this requirement, risking reduced effectiveness in mental health support.

Method: A Transformer-based model extracts dialogue features, translated into a Stochastic Hybrid Automaton (SHA) to model therapy sessions. Empathy properties are verified via Statistical Model Checking, with strategies synthesized to improve agent behavior.

Result: The framework successfully modeled therapy session dynamics with high fidelity, and targeted strategies increased the probability of meeting empathy requirements in preliminary tests.

Conclusion: The proposed approach provides a structured way to design and verify empathetic behaviors in chatbots, addressing a critical gap in therapeutic AI development.

Abstract: Conversational agents are increasingly used as support tools along mental therapeutic pathways with significant societal impacts. In particular, empathy is a key non-functional requirement in therapeutic contexts, yet current chatbot development practices provide no systematic means to specify or verify it. This paper envisions a framework integrating natural language processing and formal verification to deliver empathetic therapy chatbots. A Transformer-based model extracts dialogue features, which are then translated into a Stochastic Hybrid Automaton model of dyadic therapy sessions. Empathy-related properties can then be verified through Statistical Model Checking, while strategy synthesis provides guidance for shaping agent behavior. Preliminary results show that the formal model captures therapy dynamics with good fidelity and that ad-hoc strategies improve the probability of satisfying empathy requirements.

</details>


### [114] [BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts](https://arxiv.org/abs/2601.08490)
*Erin Feiglin,Nir Hutnik,Raz Lapid*

Main category: cs.CL

TL;DR: 论文发现了大型语言模型在普通提示下产生过长输出的'Overflow'现象，提出BenchOverflow基准测试并验证了简洁提示的缓解效果。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs无必要输出过长导致的经济成本、环境能耗、服务降级等实际问题，尤其在服务端规模化应用时影响显著。

Method: 构建了9种纯文本提示策略的BenchOverflow基准，采用5000 token预算协议评估9个开源/闭源模型，并测试简洁提醒的消融效果。

Result: 发现输出长度呈现重尾分布，跨模型存在显著差异；简洁提醒有效降低CSR指标并缩短右尾分布，证明长度控制的可行性。

Conclusion: 将输出长度控制确立为可靠性指标，BenchOverflow为量化评估提供了实用框架，建议在部署阶段进行针对性优化。

Abstract: We investigate a failure mode of large language models (LLMs) in which plain-text prompts elicit excessive outputs, a phenomenon we term Overflow. Unlike jailbreaks or prompt injection, Overflow arises under ordinary interaction settings and can lead to elevated serving cost, latency, and cross-user performance degradation, particularly when scaled across many requests. Beyond usability, the stakes are economic and environmental: unnecessary tokens increase per-request cost and energy consumption, compounding into substantial operational spend and carbon footprint at scale. Moreover, Overflow represents a practical vector for compute amplification and service degradation in shared environments. We introduce BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies that amplify output volume without adversarial suffixes or policy circumvention. Using a standardized protocol with a fixed budget of 5000 new tokens, we evaluate nine open- and closed-source models and observe pronounced rightward shifts and heavy tails in length distributions. Cap-saturation rates (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) quantify tail risk; within-prompt variance and cross-model correlations show that Overflow is broadly reproducible yet heterogeneous across families and attack vectors. A lightweight mitigation-a fixed conciseness reminder-attenuates right tails and lowers CSR for all strategies across the majority of models. Our findings position length control as a measurable reliability, cost, and sustainability concern rather than a stylistic quirk. By enabling standardized comparison of length-control robustness across models, BenchOverflow provides a practical basis for selecting deployments that minimize resource waste and operating expense, and for evaluating defenses that curb compute amplification without eroding task performance.

</details>


### [115] [It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models](https://arxiv.org/abs/2601.08500)
*Cristian Santini,Marieke Van Erp,Mehwish Alam*

Main category: cs.CL

TL;DR: 本文提出MHEL-LLaMo，一种结合小语言模型(SLM)和大语言模型(LLM)的无监督多语言历史文本实体链接方法。


<details>
  <summary>Details</summary>
Motivation: 历史文本实体链接因语言差异、噪声数据和语义演变而存在挑战，现有方法依赖训练数据或存在可扩展性限制。

Method: 采用多语言bi-编码器(BELA)进行候选检索，通过指令调优的LLM实现NIL预测和逐步提示链候选选择，并基于SLM置信度区分难易样本。

Result: 在6种欧洲语言的19-20世纪文本基准测试中超越现有模型，无需微调即可实现低资源历史文本高效实体链接。

Conclusion: 通过混合使用SLM与LLM，提供了一种可扩展的历史文本实体链接解决方案，在保证性能的同时降低计算成本。

Abstract: Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.

</details>


### [116] [STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays](https://arxiv.org/abs/2601.08510)
*Qiuyu Tian,Yiding Li,Fengyi Chen,Zequn Liu,Youyong Kong,Fan Guo,Yuyao Li,Jinjing Shen,Zhijing Xie,Yiyun Luo,Xin Zhang*

Main category: cs.CL

TL;DR: STAGE是一个新的基准，用于评估模型对电影剧本叙事的理解能力，涵盖知识图谱构建、事件摘要、问答和角色扮演等多项任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常针对问答或对话生成等单独任务，难以评估模型是否能构建并一致地使用连贯的故事世界。STAGE旨在解决这一问题，推动叙事理解的综合评估。

Method: 提出四个任务（知识图谱构建、场景事件摘要、长文本问答、剧本内角色扮演），基于150部中英文电影剧本提供清洗后的数据、知识图谱和事件/角色标注，建立共享叙事世界表征。

Result: 发布了包含清洗剧本、知识图谱及多维度标注的基准数据集，支持评估模型在叙事抽象、长程推理、角色一致性生成等方面的能力。

Conclusion: STAGE通过多任务设计推动模型对复杂叙事的连贯理解与生成，为构建故事世界表征和跨形式推理提供了新的测评框架。

Abstract: Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.

</details>


### [117] [STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio](https://arxiv.org/abs/2601.08511)
*Seong-Gyu Park,Sohee Park,Jisu Lee,Hyunsik Na,Daeseon Choi*

Main category: cs.CL

TL;DR: 该论文提出了一种名为STAR的框架，通过分析输出概率变化来检测大语言模型（LLM）中基于链式思维（CoT）的推理路径后门攻击。


<details>
  <summary>Details</summary>
Motivation: LLM的显式推理路径为运行时后门攻击提供了新的攻击面，传统检测方法因无法识别语言连贯的恶意路径而失效。

Method: 通过计算状态转移放大比（STAR），量化模型先验知识中低概率但后验概率高的恶意推理路径，并结合CUSUM算法检测持续异常。

Result: 在8B-70B参数模型及5个数据集上的实验显示，STAR检测性能接近完美（AUROC≈1.0），效率是基线方法的42倍，且对自适应攻击具有鲁棒性。

Conclusion: STAR通过统计差异与高效算法结合，实现了无需修改模型的推理路径后门检测，具备跨模型泛化能力。

Abstract: Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\approx$ 1.0) with approximately $42\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.

</details>


### [118] [Algorithmic Stability in Infinite Dimensions: Characterizing Unconditional Convergence in Banach Spaces](https://arxiv.org/abs/2601.08512)
*Przemysław Spyra*

Main category: cs.CL

TL;DR: 本文研究了无限维空间中条件收敛、无条件收敛和绝对收敛的区别及其对算法稳定性的影响，提出了统一特征定理并证明其等价条件。


<details>
  <summary>Details</summary>
Motivation: 由于在有限维与无限维空间中收敛性质的差异，以及其在梯度下降算法和信号处理中的实际应用需求，需要建立严格的收敛性理论基础以指导算法设计与稳定性分析。

Method: 通过Dvoretzky-Rogers定理和泛函分析工具，构建了一个包含排列不变性、子级数检验、弱收敛等七类等价条件的统一特征定理，并结合算法排列不变性与系数阈值问题进行验证。

Result: 证明了七类无条件收敛条件的理论等价性，明确展示了其在Banach空间中的严格分离特性，并直接解释了SGD梯度累积和信号处理算法的稳定性机制。

Conclusion: 研究建立经典泛函分析与现代计算实践的桥梁，为无序求和与鲁棒数值算法提供了理论依据，解决了优化计算和信号处理中的关键理论问题。

Abstract: The distinction between conditional, unconditional, and absolute convergence in infinite-dimensional spaces has fundamental implications for computational algorithms. While these concepts coincide in finite dimensions, the Dvoretzky-Rogers theorem establishes their strict separation in general Banach spaces. We present a comprehensive characterization theorem unifying seven equivalent conditions for unconditional convergence: permutation invariance, net convergence, subseries tests, sign stability, bounded multiplier properties, and weak uniform convergence. These theoretical results directly inform algorithmic stability analysis, governing permutation invariance in gradient accumulation for Stochastic Gradient Descent and justifying coefficient thresholding in frame-based signal processing. Our work bridges classical functional analysis with contemporary computational practice, providing rigorous foundations for order-independent and numerically robust summation processes.

</details>


### [119] [DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report](https://arxiv.org/abs/2601.08536)
*Ruizhe Li,Mingxuan Du,Benfeng Xu,Chiwei Zhu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出Deep Research Bench II用于评估DRS生成的研究报告


<details>
  <summary>Details</summary>
Motivation: 现有深度研究基准存在评估不够充分或标准偏倚的问题

Method: 构建包含132个任务和9430个细粒度二元评分标准的基准，结合LLM与人工四阶段流程验证评估

Result: 顶尖模型仅满足不足50%评估标准

Conclusion: 基准有效揭示当前系统与人类专家水平的显著差距

Abstract: Deep Research Systems (DRS) aim to help users search the web, synthesize information, and deliver comprehensive investigative reports. However, how to rigorously evaluate these systems remains under-explored. Existing deep-research benchmarks often fall into two failure modes. Some do not adequately test a system's ability to analyze evidence and write coherent reports. Others rely on evaluation criteria that are either overly coarse or directly defined by LLMs (or both), leading to scores that can be biased relative to human experts and are hard to verify or interpret. To address these issues, we introduce Deep Research Bench II, a new benchmark for evaluating DRS-generated reports. It contains 132 grounded research tasks across 22 domains; for each task, a system must produce a long-form research report that is evaluated by a set of 9430 fine-grained binary rubrics in total, covering three dimensions: information recall, analysis, and presentation. All rubrics are derived from carefully selected expert-written investigative articles and are constructed through a four-stage LLM+human pipeline that combines automatic extraction with over 400 human-hours of expert review, ensuring that the criteria are atomic, verifiable, and aligned with human expert judgment. We evaluate several state-of-the-art deep-research systems on Deep Research Bench II and find that even the strongest models satisfy fewer than 50% of the rubrics, revealing a substantial gap between current DRSs and human experts.

</details>


### [120] [Ministral 3](https://arxiv.org/abs/2601.08584)
*Alexander H. Liu,Kartik Khandelwal,Sandeep Subramanian,Victor Jouault,Abhinav Rastogi,Adrien Sadé,Alan Jeffares,Albert Jiang,Alexandre Cahill,Alexandre Gavaudan,Alexandre Sablayrolles,Amélie Héliou,Amos You,Andy Ehrenberg,Andy Lo,Anton Eliseev,Antonia Calvi,Avinash Sooriyarachchi,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Clémence Lanfranchi,Corentin Barreau,Cyprien Courtot,Daniele Grattarola,Darius Dabert,Diego de las Casas,Elliot Chane-Sane,Faruk Ahmed,Gabrielle Berrada,Gaëtan Ecrepont,Gauthier Guinet,Georgii Novikov,Guillaume Kunsch,Guillaume Lample,Guillaume Martin,Gunshi Gupta,Jan Ludziejewski,Jason Rute,Joachim Studnia,Jonas Amar,Joséphine Delas,Josselin Somerville Roberts,Karmesh Yadav,Khyathi Chandu,Kush Jain,Laurence Aitchison,Laurent Fainsin,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Maarten Buyl,Margaret Jennings,Marie Pellat,Mark Prins,Mathieu Poirée,Mathilde Guillaumin,Matthieu Dinot,Matthieu Futeral,Maxime Darrin,Maximilian Augustin,Mia Chiquier,Michel Schimpf,Nathan Grinsztajn,Neha Gupta,Nikhil Raghuraman,Olivier Bousquet,Olivier Duchenne,Patricia Wang,Patrick von Platen,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Quentin Torroba,Romain Sauvestre,Roman Soletskyi,Rupert Menneer,Sagar Vaze,Samuel Barry,Sanchit Gandhi,Siddhant Waghjale,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Théo Cachet,Theo Simon Sorg,Thibaut Lavril,Thiziri Nait Saada,Thomas Chabal,Thomas Foubert,Thomas Robert,Thomas Wang,Tim Lawson,Tom Bewley,Tom Bewley,Tom Edwards,Umar Jamil,Umberto Tomasini,Valeriia Nemychnikova,Van Phung,Vincent Maladière,Virgile Richard,Wassim Bouaziz,Wen-Ding Li,William Marshall,Xinghui Li,Xinyu Yang,Yassine El Ouahidi,Yihan Wang,Yunhao Tang,Zaccharie Ramzi*

Main category: cs.CL

TL;DR: Ministral 3系列通过级联蒸馏技术推出3B/8B/14B参数的高效稠密模型，支持多模态应用。


<details>
  <summary>Details</summary>
Motivation: 面向计算与内存受限的应用场景，开发兼顾效率和性能的可扩展语言模型。

Method: 采用迭代剪枝结合知识蒸馏的级联蒸馏方法，实现模型压缩与能力保持的平衡。

Result: 产出包含预训练/指令微调/推理三变体，具备图像理解能力且开源（Apache 2.0）的模型家族。

Conclusion: 为资源敏感场景提供灵活、高效且多模态的模型解决方案。

Abstract: We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.

</details>


### [121] [How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction](https://arxiv.org/abs/2601.08626)
*Yingjie He,Zhaolu Kang,Kehan Jiang,Qianyuan Zhang,Jiachen Qian,Chunlei Meng,Yujie Feng,Yuan Wang,Jiabao Dou,Aming Wu,Leqi Zheng,Pengxiang Zhao,Jiaxin Liu,Zeyu Zhang,Lei Wang,Guansu Wang,Qishi Zhan,Xiaomin He,Meisheng Zhang,Jianyuan Ni*

Main category: cs.CL

TL;DR: OrderProbe评估大型语言模型的结构性重构能力，发现即使面对四字短语也难以恢复正确语序。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在语义理解之外，对打乱输入进行结构重构的能力，并因传统句子层级重构存在多义性提出新评估方法。

Method: 构建OrderProbe基准测试，针对中日韩四字固定表达，利用唯一标准语序进行精确匹配评分，并设计包含语义保真度、逻辑有效性等维度的诊断框架。

Result: 对12个主流模型的测试显示零样本语序恢复率低于35%，且语义记忆与结构规划能力存在可分离性。

Conclusion: 语言模型的结构性稳健性并非语义能力的必然产物，需针对性设计结构增强方案。

Abstract: Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.

</details>


### [122] [Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation](https://arxiv.org/abs/2601.08629)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本研究开发了LALITA框架，通过词汇和语言特征筛选复杂句子，优化低资源语言的机器翻译训练数据集，有效减少数据需求并提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的人工翻译数据获取成本过高，现有数据筛选方法不足，需找到高效句子选择策略以提升机器翻译训练效果。

Method: 使用LALITA框架评估英印双语数据，基于词汇和语言特征选择源句，结合复杂句子训练和新合成数据，在50K-800K不同规模数据集上测试，并扩展到多种语言验证效率。

Result: LALITA使多语言（印度语、奥里亚语、尼泊尔语、挪威尼诺斯克语、德语）数据需求降低超50%，在所有数据规模上均显著提升翻译质量，验证了框架的数据增强能力。

Conclusion: 通过结构化数据筛选策略，LALITA成功降低低资源场景下机器翻译训练成本，为多语言数据受限问题提供了可扩展的解决方案。

Abstract: Data curation is a critical yet under-researched step in the machine translation training paradigm. To train translation systems, data acquisition relies primarily on human translations and digital parallel sources or, to a limited degree, synthetic generation. But, for low-resource languages, human translation to generate sufficient data is prohibitively expensive. Therefore, it is crucial to develop a framework that screens source sentences to form efficient parallel text, ensuring optimal MT system performance in low-resource environments. We approach this by evaluating English-Hindi bi-text to determine effective sentence selection strategies for optimal MT system training. Our extensively tested framework, (Lexical And Linguistically Informed Text Analysis) LALITA, targets source sentence selection using lexical and linguistic features to curate parallel corpora. We find that by training mostly on complex sentences from both existing and synthetic datasets, our method significantly improves translation quality. We test this by simulating low-resource data availabilty with curated datasets of 50K to 800K English sentences and report improved performances on all data sizes. LALITA demonstrates remarkable efficiency, reducing data needs by more than half across multiple languages (Hindi, Odia, Nepali, Norwegian Nynorsk, and German). This approach not only reduces MT systems training cost by reducing training data requirement, but also showcases LALITA's utility in data augmentation.

</details>


### [123] [Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs](https://arxiv.org/abs/2601.08634)
*Chenchen Yuan,Bolei Ma,Zheyu Zhang,Bardh Prenkaj,Frauke Kreuter,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本研究挑战了现有LLM政治取向评估方法，通过可控的道德价值观调节来观察对政治定位的影响，发现道德价值观显著引导模型经济与社会维度的立场变化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM政治偏见分析依赖直接探测或虚构人口画像，而社会心理学强调政治立场源于底层道德直觉。研究旨在揭示道德价值观与政治定位间的因果关系。

Method: 将道德价值观设为可控制条件，通过训练模型接受或拒绝特定道德标准，并使用Political Compass Test测量其政治坐标变化，分析道德透镜对模型决策轨迹的影响。

Result: 道德调节显著触发特定价值观对应的政治坐标偏移，效果受角色设定和模型规模影响，且在不同道德价值测量工具间保持稳定。

Conclusion: 政治立场对齐需锚定于更广泛的社会价值观（如道德），为基于社会学原理的模型对齐技术提供了实证基础。

Abstract: While recent research has systematically documented political orientation in large language models (LLMs), existing evaluations rely primarily on direct probing or demographic persona engineering to surface ideological biases. In social psychology, however, political ideology is also understood as a downstream consequence of fundamental moral intuitions. In this work, we investigate the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. Rather than simply assigning a demographic persona, we condition models to endorse or reject specific moral values and evaluate the resulting shifts on their political orientations, using the Political Compass Test. By treating moral values as lenses, we observe how moral conditioning actively steers model trajectories across economic and social dimensions. Our findings show that such conditioning induces pronounced, value-specific shifts in models' political coordinates. We further notice that these effects are systematically modulated by role framing and model scale, and are robust across alternative assessment instruments instantiating the same moral value. This highlights that effective alignment requires anchoring political assessments within the context of broader social values including morality, paving the way for more socially grounded alignment techniques.

</details>


### [124] [A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding](https://arxiv.org/abs/2601.08645)
*Dilara Torunoğlu-Selamet,Dogukan Arslan,Rodrigo Wilkens,Wei He,Doruk Eryiğit,Thomas Pickard,Adriana S. Pagano,Aline Villavicencio,Gülşen Eryiğit,Ágnes Abuczki,Aida Cardoso,Alesia Lazarenka,Dina Almassova,Amalia Mendes,Anna Kanellopoulou,Antoni Brosa-Rodríguez,Baiba Saulite,Beata Wojtowicz,Bolette Pedersen,Carlos Manuel Hidalgo-Ternero,Chaya Liebeskind,Danka Jokić,Diego Alves,Eleni Triantafyllidi,Erik Velldal,Fred Philippy,Giedre Valunaite Oleskeviciene,Ieva Rizgeliene,Inguna Skadina,Irina Lobzhanidze,Isabell Stinessen Haugen,Jauza Akbar Krito,Jelena M. Marković,Johanna Monti,Josue Alejandro Sauca,Kaja Dobrovoljc,Kingsley O. Ugwuanyi,Laura Rituma,Lilja Øvrelid,Maha Tufail Agro,Manzura Abjalova,Maria Chatzigrigoriou,María del Mar Sánchez Ramos,Marija Pendevska,Masoumeh Seyyedrezaei,Mehrnoush Shamsfard,Momina Ahsan,Muhammad Ahsan Riaz Khan,Nathalie Carmen Hau Norman,Nilay Erdem Ayyıldız,Nina Hosseini-Kivanani,Noémi Ligeti-Nagy,Numaan Naeem,Olha Kanishcheva,Olha Yatsyshyna,Daniil Orel,Petra Giommarelli,Petya Osenova,Radovan Garabik,Regina E. Semou,Rozane Rebechi,Salsabila Zahirah Pranida,Samia Touileb,Sanni Nimb,Sarfraz Ahmad,Sarvinoz Nematkhonova,Shahar Golan,Shaoxiong Ji,Sopuruchi Christian Aboh,Srdjan Sucur,Stella Markantonatou,Sussi Olsen,Vahide Tajalli,Veronika Lipp,Voula Giouli,Yelda Yeşildal Eraydın,Zahra Saaberi,Zhuohan Xie*

Main category: cs.CL

TL;DR: XMPIE是一个包含34种语言、1万余条数据的多模态习语理解基准集，每项数据包含从字面到比喻的多维度解释及视觉化呈现，支持跨模态迁移与文化共性分析。


<details>
  <summary>Details</summary>
Motivation: 习语与文化体验高度关联，但现有NLP评估数据集缺乏多语言多模态设计，无法有效验证模型的文化迁移能力，尤其在视觉-文本联合理解场景下存在评估空白。

Method: 由语言专家构建平行语料库，通过统一标注框架创建文本与视觉表达。每个习语条目对应五张图像（涵盖隐喻-字面连续体及无关干扰项），支持跨语言对齐与跨模态一致性验证。采用双盲审核确保数据质量。

Result: 成功建立首个跨模态多语言习语基准数据集（含34种语言、10,000+条目），揭示语言间隐喻表达的非对称映射现象，发现文本到图像的理解迁移效率显著高于逆向迁移（p<0.01），且专家生成数据比自动爬取数据在跨模态评估中表现更优。

Conclusion: 证明人工构造的多模态平行数据能更准确暴露模型的文化理解局限，跨语言迁移效果与语系亲缘关系正相关，而跨模态迁移存在单向优势。该数据集为评估文化感知能力提供了新范式。

Abstract: Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.

</details>


### [125] [Safe Language Generation in the Limit](https://arxiv.org/abs/2601.08648)
*Antonios Anastasopoulos,Giuseppe Ateniese,Evgenios M. Kornaropoulos*

Main category: cs.CL

TL;DR: 本文研究了安全语言生成的理论基础，指出在极限学习框架下，安全语言识别不可行，而生成任务同样面临挑战，并讨论了其复杂性。


<details>
  <summary>Details</summary>
Motivation: 在语言生成基础研究扩展的背景下，需要确保生成过程的安全性，尽管传统语言识别已被证明不可行。

Method: 基于计算学习理论中的极限学习模型，形式化定义安全语言识别与生成任务，通过数学证明分析其理论可行性边界。

Result: 安全语言识别在极限学习模型下被证明不可行，安全语言生成的复杂度至少与普通语言识别相同（后者已知不可行），并系统梳理了不同场景下的可计算性边界。

Conclusion: 即使基础语言生成存在可行解，安全约束下的语言生成仍存在理论局限，建议未来研究需探索更细粒度的安全条件建模方法。

Abstract: Recent results in learning a language in the limit have shown that, although language identification is impossible, language generation is tractable. As this foundational area expands, we need to consider the implications of language generation in real-world settings.
  This work offers the first theoretical treatment of safe language generation. Building on the computational paradigm of learning in the limit, we formalize the tasks of safe language identification and generation. We prove that under this model, safe language identification is impossible, and that safe language generation is at least as hard as (vanilla) language identification, which is also impossible. Last, we discuss several intractable and tractable cases.

</details>


### [126] [RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation](https://arxiv.org/abs/2601.08654)
*Yihan Hong,Huaiyuan Yao,Bolin Shen,Wanpeng Xu,Hua Wei,Yushun Dong*

Main category: cs.CL

TL;DR: 本文提出RULERS框架，通过可执行评分标准和证据验证增强LLM评判的稳定性与人类一致性，解决鲁棒性差、推理不可验证及尺度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM作为评判模型时因提示敏感性、不可验证的推理和尺度不匹配导致的与人类标准对齐困难的挑战。

Method: 构建RULERS框架：将自然语言评分标准编译为版本化的不可变规范包，通过结构化解码实现证据可验证性，并采用Wasserstein校准技术进行事后尺度校准。

Result: 在论文及摘要评测任务中显著提升人类评分一致性，对对抗性评分标准扰动保持稳定，且小模型可媲美大参数化模型。

Conclusion: 可靠LLM评判需基于可执行评分标准、可验证证据及校准尺度优化，而非依赖提示词工程。

Abstract: The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.

</details>


### [127] [Analyzing Bias in False Refusal Behavior of Large Language Models for Hate Speech Detoxification](https://arxiv.org/abs/2601.08668)
*Kyuri Im,Shuzhou Yuan,Michael Färber*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）因对某些仇恨言论的语义毒性及针对性群体（如国籍、宗教、政治意识形态）存在系统性偏见，常拒绝执行去毒化任务。本研究提出跨语言翻译策略（英-中-英），有效降低虚假拒绝率。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在仇恨言论去毒化任务中异常拒绝行为的背后偏见，解决模型安全机制过度敏感导致的任务失败问题。

Method: 使用九个LLMs对英/多语言仇恨言论数据集进行系统评估，量化语义毒性、目标群体与虚假拒绝率的关联性，并验证跨语言翻译策略的减缓效果。

Result: 模型对高毒性及针对性群体言论拒绝率显著升高，多语言数据集整体表现更佳但仍未消除语言相关偏见，跨翻译策略使虚假拒绝率降低40%且保留原内容语义。

Conclusion: LLMs虚假拒绝源于上下文与语言偏见，跨语言翻译策略提供了轻量级、有效的缓解方案，未来需优化多模态偏见检测机制。

Abstract: While large language models (LLMs) have increasingly been applied to hate speech detoxification, the prompts often trigger safety alerts, causing LLMs to refuse the task. In this study, we systematically investigate false refusal behavior in hate speech detoxification and analyze the contextual and linguistic biases that trigger such refusals. We evaluate nine LLMs on both English and multilingual datasets, our results show that LLMs disproportionately refuse inputs with higher semantic toxicity and those targeting specific groups, particularly nationality, religion, and political ideology. Although multilingual datasets exhibit lower overall false refusal rates than English datasets, models still display systematic, language-dependent biases toward certain targets. Based on these findings, we propose a simple cross-translation strategy, translating English hate speech into Chinese for detoxification and back, which substantially reduces false refusals while preserving the original content, providing an effective and lightweight mitigation approach.

</details>


### [128] [Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization](https://arxiv.org/abs/2601.08682)
*Kushal Chawla,Chenyang Zhu,Pengshan Cai,Sangwoo Cho,Scott Novotney,Ayushman Singh,Jonah Lewis,Keasha Safewright,Alfy Samuel,Erin Babinsky,Shi-Xiong Zhang,Sambit Sahu*

Main category: cs.CL

TL;DR: 本文提出了一个工业界的agentic系统案例，用于生成多方对话摘要，解决动态需求下的质量评估、组件优化、数据瓶颈和模型迁移性等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话摘要研究依赖静态数据集，无法适应实际工业场景中不断变化的需求，亟需能灵活应对多维度要求的实用化系统方案。

Method: 通过完整的工业系统开发周期案例分析，提出基于agentic架构的分组件优化方法，重点解决动态评价指标设计、数据瓶颈影响分析和LLM提示迁移性差等核心问题。

Result: 1) 提出主观性任务下的稳健评估框架 2) 验证任务分解对组件优化的提升效果 3) 揭示数据质量对系统性能的制约作用 4) 量化分析大模型提示工程的迁移成本瓶颈

Conclusion: 该研究为构建可适应动态业务需求的对话摘要系统提供了系统性实践指南，同时揭示了大模型工业部署中的关键技术挑战，为后续研究指明方向。

Abstract: Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.

</details>


### [129] [QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models](https://arxiv.org/abs/2601.08689)
*Zhaolu Kang,Junhao Gong,Wenqing Hu,Shuo Yin,Kehan Jiang,Zhicheng Fang,Yingjie He,Chunlei Meng,Rong Fu,Dongyang Chen,Leqi Zheng,Eric Hanchen Jiang,Yunfei Feng,Yitong Leng,Junfan Zhu,Xiaoyou Chen,Xi Yang,Richeng Xuan*

Main category: cs.CL

TL;DR: 本文提出了QuantEval，一个评估大语言模型（LLMs）在量化金融领域能力的基准，涵盖知识问答、数学推理及策略编程三方面，并引入CTA式回测框架以增强评估真实性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在金融领域的评估多局限于知识问答，缺乏对量化推理和策略生成能力的全面测试，需通过综合基准推动研究及实际应用。

Method: 构建包含三个维度的QuantEval基准，结合回测框架评估策略表现，并对LLMs进行微调和强化学习实验以验证改进效果。

Result: 现有多数LLMs在量化推理和策略生成上与人类专家存在显著差距，经领域对齐训练后表现提升，且回测配置实现完全可复现。

Conclusion: QuantEval为LLMs在量化金融中的研究和实际交易应用提供了更现实的评估工具，促进其落地发展。

Abstract: Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.

</details>


### [130] [PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation](https://arxiv.org/abs/2601.08739)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PrivGemo通过双重塔设计和匿名化视图，在保护私有知识图谱隐私的同时实现高效远程推理。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法存在语义/结构泄露、可控性差、多实体推理效果差及经验复用不足等缺陷

Method: 采用本地化原始KG管理+远程匿名化视图推理的双塔架构，通过分层控制器减少跨域交互，结合记忆模块提升经验复用效率

Result: 在6个基准测试中平均提升17.1%，且中小模型(Qwen3-4B)达到接近GPT-4-Turbo的推理能力

Conclusion: 该框架解决了隐私泄露与推理效率的矛盾，为私有知识图谱场景提供了兼顾安全性与实用性的解决方案

Abstract: Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [131] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati,Sahil Sen,Waqar Sarguroh,Kevin Paul*

Main category: cs.CL

TL;DR: 本文介绍了FRTR-Bench这一首个面向企业级多模态电子表格推理的大规模基准，以及基于细粒度嵌入和混合检索的FRTR框架，在准确性与效率上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对大规模企业电子表格中存在的多模态数据（如多表关联、图表及收据图片）难以被现有模型有效处理的问题，现有方法（单表压缩/全上下文编码）存在扩展性不足和忽视用户真实交互模式的局限。

Method: 提出FRTR框架：（1）将电子表格分解为行/列/块级细粒度嵌入（2）结合倒数排名融合（RRF）的混合词法-密集检索策略（3）融合数值与视觉信息的多模态推理机制。同步构建包含30个企业级案例的FRTR-Bench基准。

Result: 在FRTR-Bench上使用Claude Sonnet 4.5达到74%准确率（对比现有方法仅24%）；在SpreadsheetLLM基准中，GPT-5结合FRTR实现87%准确率且标记消耗降低50%。

Conclusion: FRTR通过结构化分解与多模态增强检索机制，解决了企业级电子表格处理中的扩展性瓶颈，并建立了首个面向复杂场景的评估标准。

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [132] [Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents](https://arxiv.org/abs/2601.08742)
*Xin Quan,Jiafeng Xiong,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 本文提出Att-NLI框架，通过融合溯因-演绎推理，显著提升语言模型在多智能体场景下的意图推理能力，神经符号系统实现最优游戏胜率17.08%。


<details>
  <summary>Details</summary>
Motivation: 传统NLI仅能捕捉表面逻辑关系，缺乏对多智能体互动场景中关键的意图推理能力，导致模型难以模拟人类复杂的社会认知过程。

Method: 构建社会心理学驱动的Att-NLI推理框架，设计包含三类智能体的Undercover-V文本游戏实验：1）纯演绎NLI基线模型 2）融合溯因-演绎的Att-NLI模型 3）结合定理证明器的神经符号Att-NLI模型。

Result: 实验证明推理能力呈金字塔结构：神经符号Att-NLI胜率17.08% > 标准Att-NLI 12.43% > 传统NLI 9.21%，体现混合推理架构显著优势。模型在生成意图假设和验证逻辑链条两阶段均表现提升。

Conclusion: 研究证实推理能力层级结构，展示神经符号系统对复杂推理任务的适配优势，为开发具备社会心智理论的智能体提供新路径，揭示大模型与符号系统深度融合的重要潜力。

Abstract: Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.

</details>


### [133] [TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL](https://arxiv.org/abs/2601.08743)
*Jinbo Su,Yuxuan Hu,Cuiping Li,Hong Chen,Jia Li,Lintao Ma,Jing Zhang*

Main category: cs.CL

TL;DR: The paper proposes TableCache, a method to optimize Text-to-SQL tasks by precomputing database table representations as KV caches, using a Table Trie structure, and improving cache management to reduce latency.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based Text-to-SQL methods suffer from high prefilling latency due to lengthy database schema inputs and redundant cache copies in inference engines, despite user queries often reusing common table sets.

Method: Precompute table representations as KV caches offline while preserving table relationships, construct a Table Trie for efficient cache lookup, and implement a cache management system with query reranking and parallelized computation loading.

Result: TableCache achieves up to 3.62x speedup in Time to First Token (TTFT) with minimal performance degradation compared to existing methods.

Conclusion: The proposed approach efficiently reduces latency in Text-to-SQL tasks by leveraging precomputed cache sharing, table relationship preservation, and optimized cache management strategies.

Abstract: In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.

</details>


### [134] [Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables](https://arxiv.org/abs/2601.08750)
*Valerie Zermatten,Chiara Vanalli,Gencer Sumbul,Diego Marcos,Devis Tuia*

Main category: cs.CL

TL;DR: 该论文提出一种基于注意力机制的方法，结合航空影像和地理位置文本，以空间邻域内整合多源数据，预测环境变量并验证空间上下文对生态预测的价值。


<details>
  <summary>Details</summary>
Motivation: 传统遥感数据和环境协变量难以捕捉生态系统的局部细节，而文本数据作为新兴补充源存在分布稀疏和整合困难的问题，需探索其与地理空间数据的协同机制。

Method: 构建EcoWikiRS数据集（结合瑞士高分辨率影像与维基百科文本），提出注意力模型融合视觉表征、文本表征和地理位置编码，动态选择空间邻域内相关观测进行预测。

Result: 模型在预测SWECO25数据立方体的103个环境变量时显著超越单模态基线（如仅图像或仅文本），在气候、土壤、人口及土地利用等类别变量中空间上下文增益效果显著。

Conclusion: 文本与影像的互补性及注意力机制的空间邻域整合能力可有效提升生态预测性能，验证了多模态数据融合在局部尺度生态研究中的潜力。

Abstract: Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.

</details>


### [135] [Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge](https://arxiv.org/abs/2601.08808)
*Yao Tang,Li Dong,Yaru Hao,Qingxiu Dong,Furu Wei,Jiatao Gu*

Main category: cs.CL

TL;DR: Multiplex Thinking结合CoT和软推理的特性，通过采样并聚合多个候选词元的嵌入生成紧凑表示，提升推理性能且缩短生成序列。


<details>
  <summary>Details</summary>
Motivation: 传统CoT虽有效但生成冗长低效序列，而人类更倾向于通过软推理的动态分布处理不确定性。作者目标是保留CoT优势的同时解决其带宽瓶颈，并借鉴生物启发的软推理范式。

Method: Multiplex Thinking在每一步推理中采样K个候选词元并聚合其嵌入为连续'多路词元'，保持词汇嵌入先验与离散采样机制。利用强化学习优化多路轨迹，其自适应机制根据模型置信度调节表示粒度——高置信度近似离散CoT，低置信度则紧凑编码多个潜在步骤。

Result: 在数学推理任务中，Multiplex Thinking从Pass@1到Pass@1024指标均超越离散CoT与强化学习基线，且生成序列长度显著缩短。代码与预训练模型已开源。

Conclusion: 实验证明Multiplex Thinking在复杂推理任务中优于传统离散序列生成方法，其基于深度学习的软推理机制在保持优化可行性前提下兼顾效率与多义表达能力。

Abstract: Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.

</details>
