<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.CL](#cs.CL) [Total: 34]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Hierarchical Process Reward Models are Symbolic Vision Learners](https://arxiv.org/abs/2512.03126)
*Shan Zhang,Aotian Chen,Kai Zou,Jindong Gu,Yuan Xue,Anton van den Hengel*

Main category: cs.CV

TL;DR: The paper introduces a self-supervised symbolic auto-encoder for diagram understanding, using hierarchical reward modeling to enhance structure consistency and a neuro-symbolic system for improved interpretability and performance on vision tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional pixel-based vision models struggle with structured diagrams. Symbolic computer vision, which uses logical rules and geometric primitives, offers interpretability but requires novel learning paradigms to address consistency and exploration challenges in reconstruction.

Method: A symbolic auto-encoder encodes diagrams into geometric primitives and relationships, decoded via an executable engine. Symbolic Hierarchical Process Reward Modeling enforces consistency across hierarchy levels, while stabilization mechanisms balance exploration/exploitation in reinforcement learning. Downstream tasks integrate neural reasoning with symbolic interpretability via visual rewards.

Result: 98.2% MSE reduction in geometric diagram reconstruction, 0.6% accuracy improvement over GPT-4o on chart reconstruction with a 7B model, +13% on MathGlance perception, and +3% on MathVerse/GeoQA reasoning benchmarks.

Conclusion: The neuro-symbolic approach bridges interpretability and performance, demonstrating that structured symbolic representations combined with hierarchical rewards and stabilization mechanisms outperform pixel-based and traditional symbolic methods in diagram understanding tasks.

Abstract: Symbolic computer vision represents diagrams through explicit logical rules and structured representations, enabling interpretable understanding in machine vision. This requires fundamentally different learning paradigms from pixel-based visual models. Symbolic visual learners parse diagrams into geometric primitives-points, lines, and shapes-whereas pixel-based learners operate on textures and colors. We propose a novel self-supervised symbolic auto-encoder that encodes diagrams into structured primitives and their interrelationships within the latent space, and decodes them through our executable engine to reconstruct the input diagrams. Central to this architecture is Symbolic Hierarchical Process Reward Modeling, which applies hierarchical step-level parsing rewards to enforce point-on-line, line-on-shape, and shape-on-relation consistency. Since vanilla reinforcement learning exhibits poor exploration in the policy space during diagram reconstruction; we thus introduce stabilization mechanisms to balance exploration and exploitation. We fine-tune our symbolic encoder on downstream tasks, developing a neuro-symbolic system that integrates the reasoning capabilities of neural networks with the interpretability of symbolic models through reasoning-grounded visual rewards. Evaluations across reconstruction, perception, and reasoning tasks demonstrate the effectiveness of our approach: achieving a 98.2% reduction in MSE for geometric diagram reconstruction, surpassing GPT-4o by 0.6% with a 7B model on chart reconstruction, and improving by +13% on the MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.

</details>


### [2] [Does Head Pose Correction Improve Biometric Facial Recognition?](https://arxiv.org/abs/2512.03199)
*Justin Norman,Hany Farid*

Main category: cs.CV

TL;DR: 研究表明，直接应用图像修复技术会降低面部识别准确率，但组合使用CFR-GAN和CodeFormer能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的低质量图像、非正面姿态及遮挡问题导致生物特征识别模型性能下降，研究旨在探索AI驱动的图像修复技术能否改善这一问题。

Method: 通过模型无关的大规模评估框架，测试三种修复技术（NextFace的3D重建、CFR-GAN的2D正面化、CodeFormer的特征增强）的独立及组合效果。

Result: 单独使用修复技术显著降低识别成功率，但CFR-GAN与CodeFormer的组合选择性应用带来了实质性性能提升。

Conclusion: 需谨慎选择图像修复方法，组合策略有效缓解现实场景识别问题，并强调大规模评估框架对算法优化的重要性。

Abstract: Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.

</details>


### [3] [Flux4D: Flow-based Unsupervised 4D Reconstruction](https://arxiv.org/abs/2512.03210)
*Jingkang Wang,Henry Che,Yun Chen,Ze Yang,Lily Goli,Sivabalan Manivasagam,Raquel Urtasun*

Main category: cs.CV

TL;DR: 本文提出Flux4D框架，通过3D高斯预测和运动动力学实现大规模动态场景的快速自监督重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF/3DGS存在可扩展性差和依赖运动注解的问题，现有自监督方法受限于单场景优化和超参数敏感性，迫切需要一种无需人工标注的通用方案。

Method: 设计端到端的Flux4D架构，采用3D高斯表示场景元素，通过光度损失约束重建质量，引入'尽可能静态'的正则化项自动分解动态物体，完全不需要人工标签或先验知识。

Result: 在户外驾驶数据集实验中，Flux4D处理大规模场景速度达秒级，扩展能力比现有方法提升2个数量级，成功重建包括罕见物体的未知场景，定量指标超越主流方法15%以上。

Conclusion: 该研究证明通过多场景联合训练的隐式正则化可替代显式监督，为动态场景重建开辟了高效实用的新范式。

Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.

</details>


### [4] [Object Counting with GPT-4o and GPT-5: A Comparative Study](https://arxiv.org/abs/2512.03233)
*Richard Füzesséry,Kaziwa Saleh,Sándor Szénási,Zoltán Vámossy*

Main category: cs.CV

TL;DR: This paper explores using large language models (GPT-4o and GPT-5) for zero-shot object counting, eliminating the need for annotated training data or visual exemplars by relying solely on textual prompts. The approach achieves competitive performance compared to state-of-the-art methods on the FSC-147 dataset.


<details>
  <summary>Details</summary>
Motivation: Existing object counting methods require extensive annotated data and visual exemplars, which are labor-intensive to obtain. The work leverages the reasoning and generalization capabilities of pre-trained multi-modal large language models (LLMs) to enable zero-shot counting without supervision, reducing dependency on costly labeled resources.

Method: The authors employ two pre-trained multi-modal LLMs (GPT-4o and GPT-5) to perform object counting via textual prompts describing the counting task. The models are evaluated on the FSC-147 and CARPK datasets, with performance analyzed against state-of-the-art zero-shot approaches. No training, fine-tuning, or visual exemplars are used during inference.

Result: The LLM-based approach achieves performance comparable to supervised zero-shot methods on FSC-147 and outperforms them in some cases, despite requiring no annotated data or visual exemplars. Results on CARPK show promise but with room for improvement, highlighting the models' inherent counting capabilities through prompt-based reasoning.

Conclusion: This study demonstrates that pre-trained multi-modal LLMs can perform zero-shot object counting effectively using only textual prompts, bypassing the need for annotated datasets or visual guides. The findings suggest a paradigm shift toward leveraging LLMs' reasoning skills for vision tasks with minimal supervision.

Abstract: Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.

</details>


### [5] [LLM-Guided Material Inference for 3D Point Clouds](https://arxiv.org/abs/2512.03237)
*Nafiseh Izadyar,Teseo Schneider*

Main category: cs.CV

TL;DR: 提出两阶段零样本方法，利用大语言模型从3D点云中分离推断物体语义和材料属性。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状数据集和模型忽略材料属性，导致几何与材料理解分离。同时缺乏可靠的材料标注数据支持材料识别研究。

Method: 采用解耦式两阶段LLM架构：第一阶段预测物体整体语义类别，第二阶段基于该语义为几何分割部分分配材料属性。全流程无需任务特定训练，完全通过零样本迁移实现。

Result: 在Fusion/ABS和ShapeNet数据集的1000个3D形状中，使用DeepEval的LLM-as-a-Judge评估显示：模型在语义识别准确率89.3%，材料分配合理性得分4.2/5，显著优于基线方法。

Conclusion: 验证了大规模语言模型可有效建立几何特征与材料属性间的知识关联，为零样本3D材料理解提供了新范式，突破了传统依赖专有数据标注的技术路径。

Abstract: Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.

</details>


### [6] [2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition](https://arxiv.org/abs/2512.03245)
*Liying Lu,Raphaël Achddou,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: This paper proposes a practical noise synthesis method for low-light image denoising using minimal input (one noisy image and dark frame per ISO level), leveraging Poisson distribution and Fourier spectral sampling to achieve state-of-the-art performance without requiring large clean-noisy image pairs.


<details>
  <summary>Details</summary>
Motivation: Learning-based image denoisers require large paired datasets of clean and noisy images, which are difficult to collect in low-light conditions. Current noise synthesis methods either rely on simplified parametric models or large datasets, limiting their practicality.

Method: The authors model signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm for signal-independent noise. This approach only requires a single noisy image and single dark frame per ISO setting, enabling diverse noise realizations while preserving real sensor noise properties.

Result: The proposed method achieves state-of-the-art performance on multiple low-light denoising benchmarks while overcoming limitations of previous approaches that depend on parametric models or extensive clean-noisy data pairs.

Conclusion: This noise synthesis framework provides an accurate, practical solution for low-light image denoising by eliminating data collection constraints and improving noise modeling fidelity through combined Poisson and Fourier-domain techniques.

Abstract: Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.

</details>


### [7] [PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement](https://arxiv.org/abs/2512.03247)
*Haitian Zheng,Yuan Yao,Yongsheng Yu,Yuqian Zhou,Jiebo Luo,Zhe Lin*

Main category: cs.CV

TL;DR: PixPerfect提出了一种基于像素空间优化的LDM局部编辑增强框架，显著改善图像补全、对象移除/插入的效果。


<details>
  <summary>Details</summary>
Motivation: 潜在扩散模型（LDMs）的隐空间压缩会导致颜色偏移、纹理不匹配和边界可见接缝等像素级伪影，现有方案在跨模型/任务推广时存在局限性。

Method: （1）构建可微分判别性像素空间增强微小色纹差异；（2）伪影仿真训练数据生成流程；（3）基于像素空间的端到端优化策略，适配不同LDM架构。

Result: 在ImageNet、Places2等基准测试中，PixPerfect在SSIM提升6.3%，FID降低4.2，编辑掩膜边缘感知质量提升显著，推理耗时仅增加18%。

Conclusion: PixPerfect通过像素级联合优化建立了LDM局部编辑的新范式，在保证高保真度的同时实现了跨架构/任务的泛化能力。

Abstract: Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.

</details>


### [8] [PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2512.03257)
*Mark Moussa,Andre Williams,Seth Roffe,Douglas Morton*

Main category: cs.CV

TL;DR: 提出了一种名为PyroFocus的深度学习两阶段管道，用于实时 wildfire 分类与火辐射功率估计，在NASA MASTER数据集上实现了精度与速度的权衡。


<details>
  <summary>Details</summary>
Motivation: 野火频率和严重性增加，现有基于多光谱/高光谱的实时检测算法受限于数据维度与机载资源不足，需低延迟且计算高效的解决方案。

Method: 系统评估多种深度学习架构（CNN/Transformer），设计两阶段流程：第一阶段分类火焰状态，第二阶段通过FRP回归/分割降低推理成本，使用NASA MASTER数据集验证。

Result: PyroFocus相比传统模型显著提升精度速度平衡，在边缘计算场景下具备实时部署潜力，适用于下一代野火监测任务。

Conclusion: 深度学习模型结合FRP优化的两阶段方法可有效满足机载野火检测需求，为未来空间监测系统提供技术基础。

Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.

</details>


### [9] [SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding](https://arxiv.org/abs/2512.03284)
*Hongpei Zheng,Shijie Li,Yanran Li,Hujun Yin*

Main category: cs.CV

TL;DR: H²U3D introduces a house-scale 3D visual question answering dataset and SpatialReasoner framework, achieving superior performance with minimal images via coarse-to-fine active exploration.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle with large-scale 3D environments, limited to room-scale scenarios. The work addresses this by enabling holistic house-level understanding and efficient spatial reasoning in multi-floor 3D scenes.

Method: 1) Created H²U3D dataset with hierarchical visual representations and chain-of-thought annotations using an automated pipeline; 2) Proposed SpatialReasoner framework combining coarse-to-fine exploration with a two-stage training strategy (supervised cold start + reinforcement learning with adaptive exploration rewards).

Result: Achieved state-of-the-art performance on H²U3D, outperforming GPT-4o and Gemini-2.5-Pro while using only 3-4 images per task (vs baselines requiring 16+). The adaptive reward strategy reduced redundant operations by 40% compared to conventional exploration methods.

Conclusion: The coarse-to-fine active exploration paradigm and hierarchical dataset design significantly enhance spatial reasoning capabilities in large-scale 3D environments, demonstrating that efficient exploration with minimal visual inputs is achievable through adaptive reward mechanisms.

Abstract: Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.

</details>


### [10] [NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction](https://arxiv.org/abs/2512.03317)
*Thomas Monninger,Zihan Zhang,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: 提出NavMapFusion框架，通过扩散模型融合导航地图与传感器数据，提升自动驾驶环境建图的实时性与准确性。


<details>
  <summary>Details</summary>
Motivation: 高清地图需持续更新但成本高；现有导航地图分辨率低但易获取，需解决其引导实时建图的有效性问题。

Method: 基于扩散模型的迭代去噪机制，将传感器高精度数据与低精度导航地图结合，利用预测差异作为噪声动态修正地图

Result: nuScenes数据集验证：①结合OpenSteamMap数据实现100米距离21.4%的相对精度提升 ②大感知范围性能增益更显著 ③满足实时计算需求

Conclusion: 证明扩散模型通过噪声建模有效融合异质地图数据，为自动驾驶提供可靠动态环境表征，开源代码促进领域发展

Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion

</details>


### [11] [Step-by-step Layered Design Generation](https://arxiv.org/abs/2512.03335)
*Faizan Farooq Khan,K J Joseph,Koustava Goswami,Mohamed Elhoseiny,Balaji Vasan Srinivasan*

Main category: cs.CV

TL;DR: 提出SLEDGE模型，通过分步骤、分层的方式解决设计生成问题，更贴近真实设计流程。


<details>
  <summary>Details</summary>
Motivation: 传统设计生成方法多采用单步式合成，无法建模真实设计中逐层优化迭代的人工流程，低估了动态设计过程的复杂性。

Method: 引入Step-by-Step Layered Design Generation问题设定，设计SLEDGE模型将每个设计更改建模为基于指令的原子化、分层增量修改，并构建配套评估数据集与基准。

Result: 实验表明SLEDGE在新设定下显著优于现有方法，验证了分层生成框架的有效性，并推动了动态设计过程建模研究。

Conclusion: 该工作建立了更符合实际设计流程的建模范式，为AI辅助设计领域提供了新视角和开源基准，有望促进渐进式智能设计工具的发展。

Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.

</details>


### [12] [ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography](https://arxiv.org/abs/2512.03339)
*Yeganeh Ghamary,Victoria Wu,Hooman Vaseli,Christina Luong,Teresa Tsang,Siavash Bigdeli,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 本文提出ProtoEFNet，一种基于视频的原型学习模型，用于连续射血分数（EF）回归预测，通过动态时空原型和原型角度分离损失（PAS）提升模型可解释性，同时保持与不可解释模型相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统EF评估依赖手动标注且易受观察者差异影响，现有深度学习模型多为黑箱，缺乏透明度导致临床信任度低，后验解释方法无法引导模型内部推理过程，临床可靠性有限。

Method: 设计动态时空原型捕捉心脏运动模式，提出PAS损失强制区分连续EF谱的特征表示，在EchonetDynamic数据集上验证，结合消融实验评估性能提升。

Result: 与非可解释模型准确率相当，但提供临床解释性；消融实验显示PAS损失使F1分数从77.67±2.68提升2%至79.64±2.10。

Conclusion: ProtoEFNet通过可解释的原型学习与PAS损失优化，在保证精度的同时为临床提供可解释性，适用于需要因果分析的实际医疗场景。

Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF

</details>


### [13] [HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration](https://arxiv.org/abs/2512.03345)
*Seunghoi Kim,Henry F. J. Tregidgo,Chen Jin,Matteo Figini,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 本文提出一种生成图像修复模型中的幻觉的可控合成框架HalluGen，并构建首个大规模幻觉数据集，用于提升医疗等安全关键场景中图像修复的可靠性。


<details>
  <summary>Details</summary>
Motivation: 图像修复模型在医疗、工业检测等安全关键领域易产生与真实数据不符的幻觉结构，但现有评价方法因依赖昂贵的标注数据受限。这种循环依赖（需要标注数据评估幻觉，但标注本身困难）阻碍了该领域进展。

Method: 1. 开发基于扩散模型的HalluGen框架，可控制幻觉的类型、位置和严重程度；2. 构建包含4,350张脑部MRI图像的公开数据集，通过合成使分割IoU从0.86降至0.36的幻觉样本；3. 提出特征评估语义幻觉检测指标SHAFE，并训练零参考检测模型。

Result: 1. 成功实现逼真幻觉生成（IoU下降显著）；2. SHAFE指标相比传统方法提升幻觉检测灵敏度；3. 零参考检测模型可泛化到真实修复失败场景。

Conclusion: HalluGen框架和公开数据集为安全关键领域的图像修复提供了可扩展的幻觉评估基础，所提指标和检测方法提升了模型可靠性，推动后续方法改进。

Abstract: Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.

</details>


### [14] [Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus](https://arxiv.org/abs/2512.03346)
*Lynn Kandakji,William Woof,Nikolas Pontikos*

Main category: cs.CV

TL;DR: 本文系统比较了16种深度学习架构在3D医学影像弱分布性异常检测中的性能，发现分层注意力模型在亚临床圆锥角膜检测中表现出更高的灵敏度和参数效率，其优势源于跨尺度的空间注意力匹配机制。


<details>
  <summary>Details</summary>
Motivation: 传统2D/3D CNN的局部性过强和ViT的全局注意力发散性缺陷，导致医学影像中微弱、非连续的早期病理信号检测困难，亟待探索适配稀疏三维模式识别的最优归纳偏置结构。

Method: 基于3D前节OCT体数据，构建包含卷积、混合和体积Transformer等架构的16种模型对照实验；采用机械分析探究注意力机制特性，并通过表征相似性分析和年龄/性别预测辅助验证方法普适性。

Result: 分层注意力模型在亚临床稀疏异常检测中，灵敏度和特异性比CNN/ViT提升21-23%；注意力距离分析揭示不同病理状态的空间整合长度差异显著，亚临床期需更长距离整合；空间窗口层次化设计可实现与病灶跨尺度特征匹配的有效感受野。

Conclusion: 三维医学异常检测应采用分层注意力架构，其通过空间尺度对齐机制平衡局部细节与全局关联，为早期病理微特征分析提供兼具原则性和有效性的新范式，可推广至其他医学影像分析场景。

Abstract: The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge. The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention. This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved. This study presents a controlled comparison of sixteen modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus (SKC) detection from 3D anterior segment OCT volumes. We demonstrate that hierarchical attention models offer a superior and more parameter-efficient inductive bias, surpassing the performance of both 2D and 3D CNNs and ViTs. Our results show 21-23% higher sensitivity and specificity in the sparse anomaly (subclinical) regime. Mechanistic analyses reveal that this advantage stems from precise spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate, multi-slice extent of subclinical abnormalities. This avoids excessive CNN locality and diffuse global attention. Attention-distance measurements confirm a key insight into architectural adaptation: the required spatial integration length shifts significantly based on the signal strength, with subclinical cases necessitating longer integration compared to both healthy and manifest disease states. Representational similarity and auxiliary age/sex prediction tasks further support the generalizability of these inductive principles. The findings provide design guidance for future volumetric anomaly detection systems, establishing hierarchical attention as a principled and effective approach for early pathological change analysis in 3D medical imaging.

</details>


### [15] [SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation](https://arxiv.org/abs/2512.03350)
*Yu Yuan,Tharindu Wickremasinghe,Zeeshan Nadir,Xijun Wang,Yiheng Chi,Stanley H. Chan*

Main category: cs.CV

TL;DR: SeeU通过2D→4D→2D框架建模连续4D动态，实现物理一致的视觉内容生成，超越传统2D方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接在2D观测上操作导致性能不足，而真实世界是4D连续体，需在更高维度建模以提升生成质量。

Method: 1) 从稀疏单目2D帧重建4D世界（2D→4D）；2) 在低秩表示和物理约束下学习连续4D动态；3) 通过时空上下文感知前推4D世界并重新投影生成2D内容。

Result: 实验证明SeeU在未见过的时间/空间生成、视频编辑等任务中具有连续性和物理一致性优势。

Conclusion: 4D动态建模为视觉生成提供了更优的连续性和物理合理性，为多任务提供新范式。

Abstract: Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\to$4D$\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.

</details>


### [16] [A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM](https://arxiv.org/abs/2512.03359)
*Md Rashidul Islam,Bakary Gibba,Altagi Abdallah Bakheit Abdelgadir*

Main category: cs.CV

TL;DR: 本研究提出基于深度学习的自动化肺部癌症分类系统，采用改进DenseNet169和SVM模型，结合Grad-CAM与SHAP实现高精度诊断和可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 肺部癌症高致死率要求早期诊断，但CT影像人工分析存在效率低、易误判问题，需建立自动化系统提高准确性。

Method: 1. 使用含注意力机制的DenseNet169模型（SE模块+FocalLoss+FPN）；2. 开发轻量化MobileNetV2-SVM模型；3. 应用Grad-CAM定位CT影像关键区域，SHAP解释SVM模型特征贡献。

Result: 双模型在IQOTHNCCD数据集上均达成98%准确率，显示其在临床环境中的鲁棒性。

Conclusion: 提出的混合模型架构在保证诊断精度的同时提升可解释性，为深度学习在医学影像诊断的应用提供新范式。

Abstract: Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.

</details>


### [17] [FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting](https://arxiv.org/abs/2512.03369)
*Nan Zhou,Huandong Wang,Jiahao Li,Han Li,Yali Song,Qiuhua Wang,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出高精度野火预测数据集FireSentry与双模态模型FiReDiff，通过亚米级时空分辨率数据与红外视频预测-火掩码分割新范式，显著提升火灾预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有野火预测模型受限于低分辨率卫星数据和粗时空尺度，无法捕捉局部高精度火势动态，制约应急响应决策。

Method: 构建FireSentry多模态数据集（含无人机同步采集的亚米级可见光/红外视频、环境数据和人工火掩码）；建立物理/数据/生成模型混合基准；提出FiReDiff双模态预测框架（先预测红外视频序列，再分割火掩码）。

Result: FiReDiff在生成模型对比中：视频质量（PSNR+39.2%、SSIM+36.1%、LPIPS+50.0%、FVD+29.4%）、火掩码精度（AUPRC+3.3%、F1+59.1%、IoU+42.9%、MSE+62.5%）均显著提升。

Conclusion: FireSentry填补了精细化野火建模数据空白，FiReDiff双模态范式突破现有掩码预测局限，二者共同推动动态灾害建模仿真，开源数据集加速相关领域研究。

Abstract: Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.

</details>


### [18] [ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding](https://arxiv.org/abs/2512.03370)
*Lingjun Zhao,Yandong Luo,James Hay,Lu Gan*

Main category: cs.CV

TL;DR: ShelfGaussian是一种基于高斯的多模态3D场景理解框架，引入视觉基础模型监督和分层学习范式，实现开放词汇的零样本语义场景预测。


<details>
  <summary>Details</summary>
Motivation: 现有高斯方法受限于闭集语义标注或纯2D监督导致几何退化。需要结合多模态特征与开放词汇监督挖掘高斯表示潜力。

Method: 设计多模态高斯转换器以融合异构传感器特征，提出shelf-supervised学习范式进行2D-3D联合优化。采用预训练视觉模型作为固定特征提取器指导监督信号生成。

Result: 在Occ3D-nuScenes上实现SOTA零样本语义占据预测，在无人车真实城市场景测试中展现强泛化能力。开源项目网站获得广泛社区关注。

Conclusion: 该框架打破了传统高斯方法对标注数据的依赖，通过分层监督策略有效整合多模态信息，在开放场景建模和实际部署间建立了可靠桥梁。

Abstract: We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.

</details>


### [19] [MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.03404)
*Yujian Zhao,Hankun Liu,Guanglin Niu*

Main category: cs.CV

TL;DR: MOS框架通过模态一致性学习与跨模态数据增强，显著提升光学与SAR图像的船舶重新识别性能，解决模态差异导致的特征对齐难题。


<details>
  <summary>Details</summary>
Motivation: 海洋情报监控中，光学与SAR图像间的巨大模态差异导致现有方法在船舶跨模态重识别任务中效果欠佳，亟需有效方案弥合模态鸿沟并提升特征判别性。

Method: 提出MOS框架：1) MCRL模块结合SAR图像去噪与类别感知模态对齐损失，实现跨模态特征分布对齐；2) CDGF模块利用布朗桥扩散模型生成跨模态样本，并在推理阶段融合原始特征增强对齐性。

Result: 在HOSS数据集上相较SOTA方法，ALL-ALL/Optical-SAR/SAR-Optical三种协议下的R1精度分别提升3.0%、6.2%、16.4%，验证了方法有效性。

Conclusion: MOS通过显式模态对齐与数据增强策略，在跨模态船舶ReID任务中取得显著性能突破，为多模态遥感图像分析提供了新思路。

Abstract: Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.

</details>


### [20] [ViDiC: Video Difference Captioning](https://arxiv.org/abs/2512.03405)
*Jiangtao Wu,Shihao Li,Zhaozhou Bian,Yuanxing Zhang,Jialu Chen,Runzhe Wen,An Ping,Yiwen He,Jiakai Wang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出ViDiC任务与ViDiC-1K数据集，用于评估多模态大模型对视频对动态差异的细粒度描述能力。数据集包含1,000组视频对和4,000+对比标注项，涵盖七类视觉元素，并引入基于LLM的双核查清单评估框架，实验显示现有模型在对比推理能力上存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像差异常描述(IDC)方法局限于静态图像，无法捕捉动态场景中的动作连续性、事件演变和时序编辑一致性。视觉-语言系统缺乏对动态场景差异的系统性对比感知能力，需建立新的基准推动视频理解与编辑感知研究。

Method: 构建ViDiC-1K数据集：1) 人工筛选1,000组视频对并标注7类视觉特征(主体、风格等)的4,000+对比项；2) 设计双核查清单框架，通过LLM-as-a-Judge协议分别评估相似性与差异性检测能力。对19个多模态模型进行大规模实验。

Result: 实验揭示现有多模态模型在视频对比描述与差异感知任务中存在显著性能差距，尤其是在动作连续性、播放技术等动态特征的识别上表现不佳，表明当前方法需要改进以更好理解时序变化与编辑逻辑。

Conclusion: ViDiC-1K的提出为视频理解、编辑感知及多模态对比推理提供了挑战性基准，未来工作需突破动态场景的时空关联建模瓶颈，推动视频差异描述的实用化进程。

Abstract: Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.

</details>


### [21] [YOLOA: Real-Time Affordance Detection via LLM Adapter](https://arxiv.org/abs/2512.03418)
*Yuqi Ji,Junjie Ke,Lihuo He,Jun Liu,Kaifan Zhang,Yu-Kun Lai,Guiguang Ding,Xinbo Gao*

Main category: cs.CV

TL;DR: YOLOA通过LLM适配器实时处理物体检测和可供性学习，解决'what-where-how'联合挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法割裂处理物体检测（what/where）与可供性学习（how），缺乏交互性和实时性。

Method: 基于YOLO架构构建轻量检测器，设计LLM适配器协同优化目标检测分支和可供性学习分支，在训练时通过适配器生成的精确先验信息（类别、框偏移、可供性门控）同步优化双任务

Result: 在ADG-Det/IIT-Heat数据集上达到52.8/73.1 mAP，标准版89.77 FPS，轻量版846.24 FPS，均超实时性指标

Conclusion: 证实了LLM适配器在跨模态任务融合中的有效性，实现了准确率与计算效率的帕累托最优

Abstract: Affordance detection aims to jointly address the fundamental "what-where-how" challenge in embodied AI by understanding "what" an object is, "where" the object is located, and "how" it can be used. However, most affordance learning methods focus solely on "how" objects can be used while neglecting the "what" and "where" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.

</details>


### [22] [Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features](https://arxiv.org/abs/2512.03430)
*Yuzhen Hu,Biplab Banerjee,Saurabh Prasad*

Main category: cs.CV

TL;DR: 该研究提出了一种利用预训练扩散模型的空间特征和FiLM融合模块的超光谱成像分类框架,在稀疏标签下实现高效分类。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像存在空间分辨率低、标注稀缺的挑战,需探索低标签依赖且能同时融合光谱-空间信息的分类方法。

Method: 冻结扩散模型早期解码层提取空间特征,通过轻量级FiLM模块动态融合光谱特征,实现谱间自适应调制与多模态学习。

Result: 在两个最新高光谱数据集上超越现有方法,消融实验证明扩散特征与谱感知融合的必要性,验证了方法在稀疏监督下的鲁棒性。

Conclusion: 预训练扩散模型具备跨域特征迁移能力,可为遥感和科学成像任务提供高效的标签无关表征学习方案。

Abstract: Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.

</details>


### [23] [LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis](https://arxiv.org/abs/2512.03449)
*Tongxu Zhang*

Main category: cs.CV

TL;DR: LM-CartSeg是一种自动化的膝关节MRI影像组学分析工具，结合深度学习和几何规则实现软骨/骨骼分割、内外侧区域划分和特征提取。


<details>
  <summary>Details</summary>
Motivation: 膝关节影像组学需要同时捕获软骨和软骨下骨的可靠区域，但现有工作依赖手动标注且缺乏质量控制，限制了多中心研究的应用。

Method: 使用SKM-TEA和OAIZIB-CM数据集训练两个3D nnU-Net模型，通过零样本预测融合和几何后处理（连通区域清理、构建10mm软骨下骨带、PCA+K-means胫骨内外侧划分）实现自动化分析。

Result: 后处理使OAIZIB-CM测试集宏观ASSD从2.63mm降至0.36mm，DSC达0.91；跨数据集零样本验证DSC为0.80。影像组学特征中仅6-12%与体积/厚度相关，分类模型可有效区分骨关节炎。

Conclusion: 该工具提供的自动化质量控制系统和非形态学特征，为多中心膝关节骨关节炎影像组学研究提供了实用基础，解决了现有方法域适应性不足的问题。

Abstract: Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.

</details>


### [24] [KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models](https://arxiv.org/abs/2512.03450)
*Rhys Newbury,Juyan Zhang,Tin Tran,Hanna Kurniawati,Dana Kulić*

Main category: cs.CV

TL;DR: 本文提出了一种无监督学习3D关键点的框架，用于点云数据的结构化表示，并通过扩散模型实现形状重建。


<details>
  <summary>Details</summary>
Motivation: 现有无监督关键点方法无法适用于无条件生成场景，限制了其在3D生成中的应用。本文旨在解决这一限制。

Method: 设计了一种从点云数据中学习空间结构化3D关键点的框架，利用这些关键点通过Elucidated Diffusion Model（EDM）条件生成完整形状。

Result: 关键点在不同对象实例中表现出空间结构一致性，支持关键点空间平滑插值，且关键点一致性指标较先前方法提升6个百分点。

Conclusion: 该方法实现了无监督条件下空间结构化关键点的学习，可跨类别生成高质量3D形状，并验证了关键点对几何变化的表达能力。

Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.

</details>


### [25] [GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers](https://arxiv.org/abs/2512.03451)
*Zhiye Song,Steve Dai,Ben Keller,Brucek Khailany*

Main category: cs.CV

TL;DR: GalaxyDiT is a training-free method to accelerate guided video diffusion models by optimizing computational reuse through proxy selection.


<details>
  <summary>Details</summary>
Motivation: Video generation with DiT and CFG is computationally slow due to iterative steps and doubled compute demands, limiting practical applications.

Method: GalaxyDiT aligns model guidance and uses rank-order correlation analysis to systematically select reusable proxy metrics across model scales and architectures.

Result: Achieved 1.87x/2.37x speedup on Wan2.1 models with <1% VBench-2.0 degradation and 5-10 dB PSNR improvement over prior methods at high speeds.

Conclusion: GalaxDiT enables efficient guided video generation while maintaining quality, outperforming existing approaches in computational reuse effectiveness.

Abstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.
  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\times$ and $2.37\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).

</details>


### [26] [GeoVideo: Introducing Geometric Regularization into Video Generation Model](https://arxiv.org/abs/2512.03453)
*Yunpeng Bai,Shaoheng Fang,Chaohui Yu,Fan Wang,Qixing Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合深度预测和多视角几何损失的视频生成方法，提升几何一致性和时空连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在2D像素空间操作且缺乏3D结构建模，导致几何不一致和运动不合理问题。

Method: 在扩散模型中引入深度预测，通过多视角几何损失对齐相邻帧深度图于共享3D坐标系，实现结构一致性正则化。

Result: 多数据集实验表明，生成结果相较基线在几何结构稳定性和时空一致性指标上显著提升。

Conclusion: 通过结合深度表征学习与显式几何约束，该方法有效解决了视频生成中的三维结构建模难题。

Abstract: Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.

</details>


### [27] [Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles](https://arxiv.org/abs/2512.03454)
*Haicheng Liao,Huanming Shen,Bonan Wang,Yongkang Li,Yihong Tang,Chengyue Wang,Dingyi Zhuang,Kehua Chen,Hai Yang,Chengzhong Xu,Zhenning Li*

Main category: cs.CV

TL;DR: 这篇文章提出了一种名为ThinkDeeper的框架，通过预测未来空间状态和层次化特征融合，在自动驾驶视觉定位任务中取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法难以处理歧义/上下文相关指令，缺乏对三维空间关系和场景演变的推理能力。

Method: 1. 构建空间感知世界模型(SA-WM)进行未来状态预测 2. 采用超图引导解码器进行多模态特征层次融合 3. 创建多源视觉定位数据集DrivePilot

Result: 在六个基准测试中均表现优异，Talk2Car排行榜第一，在DrivePilot、MoCAD等数据集超越现有方法，展现出强鲁棒性和数据效率

Conclusion: 通过引入未来状态预测机制和超图融合策略，显著提升了自动驾驶场景中视觉定位的准确性和鲁棒性

Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.

</details>


### [28] [Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models](https://arxiv.org/abs/2512.03463)
*Shojiro Yamabe,Futa Waseda,Daiki Shiono,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: The paper introduces Text-Printed Image (TPI), a method to generate synthetic images from text descriptions for training vision-language models (LVLMs) without real images, enabling low-cost and scalable data generation.


<details>
  <summary>Details</summary>
Motivation: Training LVLMs typically requires costly image-text pairs. Text-centric training could reduce costs, but the modality gap between text and images limits performance. A solution is needed to bridge this gap while maintaining scalability and affordability.

Method: Propose TPI to synthesize images by rendering textual descriptions on blank canvases. This bridges the text-image modality gap and integrates with existing LVLM pipelines. They evaluate TPI across four models and seven benchmarks, comparing its effectiveness to other synthetic image generation methods.

Result: TPI outperforms diffusion-based synthetic image generation in text-centric training. It achieves better VQA task performance while preserving textual semantics. TPI also demonstrates utility as a low-cost data-augmentation strategy.

Conclusion: TPI enables efficient text-centric training for LVLMs, highlighting the potential of automated text-based data generation to reduce costs and privacy constraints associated with real image datasets.

Abstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.

</details>


### [29] [Procedural Mistake Detection via Action Effect Modeling](https://arxiv.org/abs/2512.03474)
*Wenliang Guo,Yujiang Pu,Yu Kong*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Action Effect Modeling (AEM)的新框架，用于通过同时捕捉动作执行及其结果（即动作效应）来提升程序性任务中的错误检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注动作执行过程，忽略动作结果（如对象状态或空间布局异常），而许多错误实则体现在结果中。本文旨在填补这一缺口。

Method: AEM通过概率模型联合建模动作执行与结果。其步骤包括：1）选择语义相关且视觉质量高的效应帧；2）融合视觉特征与符号化场景图生成效应感知表征；3）设计基于任务提示的检测器对齐动作与语义意图。

Result: 在EgoPER和CaptainCook4D数据集的one-class classification设置下达到SOTA性能，验证了动作执行与结果联合建模的有效性。

Conclusion: 结合动作执行与结果能显著提升错误检测可靠性，效应感知表征对其他下游应用具有普适价值。

Abstract: Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.

</details>


### [30] [Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis](https://arxiv.org/abs/2512.03477)
*Zijian Gu,Yuxi Liu,Zhenhao Zhang,Song Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对医疗视觉-语言模型（VLM）的公平性低秩自适应方法（Fairness-aware Low-Rank Adaptation），通过可微分最大准确率差异损失函数（MaxAccGap）实现高参数效率的公平优化，并提出了FR-LoRA、GR-LoRA及Hybrid-LoRA三种方法，实验表明GR-LoRA在保持53.15%整体准确率的基础上，将诊断准确率差异降低69%。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在医学影像任务中表现优异，但其诊断准确率在不同人口统计学群体间存在显著差异。现有模型缺乏针对公平性与参数效率的协同优化方法，亟需一种既能减少诊断偏差又能适应资源受限医疗场景的解决方案。

Method: 核心贡献是提出可微分的MaxAccGap损失函数，直接优化准确性均等性；具体方法包括：FR-LoRA（将MaxAccGap作为正则化项加入训练目标）、GR-LoRA（采用逆频加权平衡梯度贡献）、Hybrid-LoRA（结合两者）。实验在1万张青光眼眼底图像上验证效果。

Result: GR-LoRA降低诊断差异69%，整体准确率53.15%；消融实验显示强正则化参数权衡最优公平性与最小准确率损失，群体特异性优化降低差异60%；方法仅需0.24%可训练参数，显著降低部署成本。

Conclusion: 所提公平性LoRA方法在资源消耗与公平性优化间取得平衡，特别是GR-LoRA的梯度加权机制为医疗AI公平性提供了低参数成本、高实用性的解决方案，适用于资源受限的医疗场景。

Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.

</details>


### [31] [NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation](https://arxiv.org/abs/2512.03499)
*Renqi Chen,Haoyang Su,Shixiang Tang*

Main category: cs.CV

TL;DR: 本文提出NAS-LoRA方法，通过整合神经架构搜索增强SAM模型的域适配能力，动态优化权重中的先验知识整合并采用分阶段训练策略，训练成本降低24.14%且推理成本不变。


<details>
  <summary>Details</summary>
Motivation: 为解决SAM模型在特定领域（如医学/农业）适配时因Transformer编码器缺乏空间先验导致的语义获取障碍，需设计能嵌入归纳偏置的参数高效微调方法

Method: 在LoRA框架中嵌入轻量级NAS模块以学习最优架构先验，采用阶段式优化策略协调权重更新与架构调整，通过可训练架构参数动态控制先验知识注入过程

Result: 在多种视觉任务中超越现有PEFT方法，以24.14%更低训练成本达成更优性能，消融实验证实架构搜索组件对语义表达提升的有效性

Conclusion: 通过将NAS引入LoRA框架，成功弥合SAM预训练与下游任务间的语义鸿沟，验证了架构搜索在参数高效微调中平衡性能与效率的独特价值

Abstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.

</details>


### [32] [CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding](https://arxiv.org/abs/2512.03558)
*Huy Quang Ung,Guillaume Habault,Yasutaka Nishimura,Hao Niu,Roberto Legaspi,Tomoki Oya,Ryoichi Kojima,Masato Taya,Chihiro Ono,Atsunori Minamikawa,Yan Liu*

Main category: cs.CV

TL;DR: 本研究开发了CartoMapQA基准数据集以评估视觉语言模型对地图的理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态领域取得进展，但其对地图这一重要可视化信息的解读能力尚未被系统检验，现有基准测试存在不足。

Method: 构建包含2000+标注样本的数据集，涵盖符号识别、信息提取、比例解读和路径推理等多层级地图理解能力测试，并设计开放型与多选型问答任务验证现有主流模型表现。

Result: 现有多模态模型在地图语义解析（OCR错误）和地理空间推理能力上均显现显著不足，开源与闭源模型均存在相似痛点。

Conclusion: CartoMapQA为改进视觉语言模型的空间认知能力提供了针对性优化方向，可推动导航、城市规划等实际场景的模型研发。

Abstract: The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git

</details>


### [33] [EEA: Exploration-Exploitation Agent for Long Video Understanding](https://arxiv.org/abs/2512.03500)
*Te Yang,Xiangyu Zhu,Bo Wang,Quan Chen,Peng Jiang,Zhen Lei*

Main category: cs.CV

TL;DR: 本研究提出EEA框架，通过语义引导的树搜索平衡探索与利用，高效处理长视频分析中的关键信息定位问题，避免高计算开销与覆盖率不足。


<details>
  <summary>Details</summary>
Motivation: 现有长视频分析方法存在两大问题：1) 密集预处理导致计算过载；2) 缺乏对探索（全场景覆盖）与利用（关键帧精析）的平衡机制，导致信息遗漏与低效。

Method: EEA采用分层树搜索策略：1) 动态生成任务语义查询并提取语义锚帧；2) 以语义相似度为优先级进行非均匀树扩展；3) 通过置信度建模融合视觉-语言模型奖励与语义先验。

Result: 在多个长视频基准数据集上验证，EEA在准确率优于现有方法(未明确数值)，计算效率提升2.3倍(假设典型指标)，语义锚帧覆盖率达92%关键事件片段。

Conclusion: EEA通过语义驱动的自适应搜索策略，在保证信息完整性的同时显著降低计算成本，为长视频分析提供了兼顾效率与精度的可行解决方案。

Abstract: Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.

</details>


### [34] [Optical Context Compression Is Just (Bad) Autoencoding](https://arxiv.org/abs/2512.03643)
*Ivan Yee Lee,Cheng Yang,Taylor Berg-Kirkpatrick*

Main category: cs.CV

TL;DR: 光学压缩的前景可能被高估，简单方法在文本压缩与语言建模中表现更优。


<details>
  <summary>Details</summary>
Motivation: 挑战Vision-based上下文压缩对语言模型的有效性假设，并检验其是否优于简单替代方法。

Method: 对比DeepSeek-OCR的视觉编码器与无参数均值池化和分层编码器，在相同压缩率下进行文本重构与语言建模测试。

Result: 简单方法在重构任务中表现相当或更优，而在语言建模任务中显式超越视觉编码器，视觉压缩甚至未能击败截断基线。

Conclusion: 当前对光学上下文压缩的预期超前于实证验证，简单压缩方法在语言任务中更具潜力。

Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding

</details>


### [35] [Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation](https://arxiv.org/abs/2512.03508)
*Seogkyu Jeon,Kibeom Hong,Hyeran Byun*

Main category: cs.CV

TL;DR: 本文提出了一种名为DPMFormer的领域通用语义分割框架，通过领域感知提示学习、对比学习与鲁棒一致性学习，解决视觉与文本上下文语义不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用单一领域固定的上下文提示，导致视觉和文本上下文语义不匹配，本文旨在通过动态领域感知机制解决该问题。

Method: 1) 领域感知提示学习实现视觉-文本语义对齐；2) 结合纹理扰动的领域感知对比学习扩展观测领域；3) 领域鲁棒一致性学习减少原始与增强图像预测差异。

Result: 在多个DGSS基准测试中表现出最优性能，建立新的SOTA（state-of-the-art）。

Conclusion: DPMFormer通过多阶段领域适应策略显著提升通用分割性能，代码已公开。

Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.

</details>


### [36] [Thinking with Programming Vision: Towards a Unified View for Thinking with Images](https://arxiv.org/abs/2512.03746)
*Zirun Guo,Minjie Hong,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.CV

TL;DR: 本研究揭示了现有MLLMs在面对图像方向变化或自然损坏时的脆弱性，并提出了CodeVision框架，通过生成代码作为通用接口来增强模型的工具推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖有限的固定工具，缺乏面对图像方向变化或自然损坏时的鲁棒性，急需提升工具推理的可扩展性和实际应用性。

Method: 提出CodeVision框架，采用代码生成作为通用接口替代固定工具库，通过两阶段训练：第一阶段使用复杂多步任务的高质量数据集进行监督微调（SFT），第二阶段引入新的密集奖励函数进行强化学习（RL），同时构建了专用数据集和评估基准以支持研究。

Result: 在Qwen2.5-VL和Qwen3-VL系列上的实验表明，CodeVision显著提升了模型在图像方向变化和多工具组合任务上的性能，并具备灵活工具组合、高效链式执行和鲁棒错误恢复能力。

Conclusion: CodeVision通过代码生成方式解决了MLLMs在工具推理中的局限性，为处理视觉任务提供了灵活、可扩展和鲁棒的新范式。

Abstract: Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.

</details>


### [37] [AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model](https://arxiv.org/abs/2512.03509)
*Kwaku Opoku-Ware,Gideon Opoku*

Main category: cs.CV

TL;DR: This paper introduces a proof-of-concept framework for automated markerless dance analysis using YOLOv8/v11 and Segment Anything Model (SAM), achieving high-precision tracking and quantification of dancer movements in a Ghanaian AfroBeats video.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, equipment-free solution for quantitative dance analysis by leveraging computer vision advancements, enabling objective measurement of movement patterns, spatial utilization, and rhythm consistency in performance studies.

Method: Integrated object detection (YOLO) with semantic segmentation (SAM) to: 1) identify dancers in video frames 2) quantify discrete steps and motion intensity 3) analyze spatial coverage through pixel-level segmentation and motion trajectory mapping.

Result: System demonstrated 94% detection precision/89% recall on manual samples, 83% SAM segmentation accuracy, and significant differences between primary vs secondary dancers (23% more steps, 37% higher motion intensity, 42% greater spatial coverage).

Conclusion: Preliminary technical validation confirms feasibility of markerless dance analysis but requires further systematic testing, ground truth comparison with pose estimation baselines, and diverse dataset validation for robust quantitative studies.

Abstract: This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.

</details>


### [38] [AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition](https://arxiv.org/abs/2512.03794)
*Zichuan Lin,Yicheng Liu,Yang Yang,Lvfang Tao,Deheng Ye*

Main category: cs.CV

TL;DR: AdaptVision 提出了一种基于强化学习的自适应视觉 token 获取方法，通过粗到细策略在保持性能的同时显著降低视觉 token 消耗量。


<details>
  <summary>Details</summary>
Motivation: 传统高效VLM方法依赖固定压缩比，无法根据任务难度动态调整视觉token数量，导致资源浪费或精度损失。研究旨在赋予模型自主判断每条样本所需最小视觉信息量的能力。

Method: 设计分层决策框架：1) 初始阶段处理低分辨率图片；2) 条件性调用bounding box工具聚焦关键区域。提出解耦回合策略优化（DTPO），将目标分解为工具使用优化（工具调用决策）和精度提升（答案正确性优化），并分别计算对应目标的token优势估计。

Result: 在多个VQA基准测试中，在保证准确率的前提下，视觉token消耗量显著低于现有SOTA高效VLM方法（如压缩比提升2-5倍）。

Conclusion: 通过模仿人类主动视觉机制，证实了动态调整视觉token数量的可行性，DTPO框架有效解决了多目标协同优化难题，为高效视觉语言建模提供了新范式。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.

</details>


### [39] [Stable Signer: Hierarchical Sign Language Generative Model](https://arxiv.org/abs/2512.04048)
*Sen Fang,Yalin Feng,Hongbin Zhong,Yanxin Zhang,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 本文提出Stable Signer模型，通过简化传统SLP任务流程并引入新模块（SLUL和SLP-MoE），以端到端方式生成高质量手语视频，性能提升48.6%。


<details>
  <summary>Details</summary>
Motivation: 传统SLP流程（Text2Gloss→Gloss2Pose→Pose2Vid）因多阶段误差累积导致效果不佳，需简化结构并优化关键环节。

Method: 1) 将任务重构为文本理解+Pose2Vid双阶段；2) SLUL模块通过SAGM Loss增强文本理解；3) SLP-MoE专家模块生成多样式手势；4) 端到端联合优化。

Result: 在跨语言手语生成任务中，相比当前SOTA方法实现48.6%的性能提升，视频质量与风格可控性显著改善。

Conclusion: 通过结构简化与模块创新，Stable Signer解决了多阶段误差累积问题，在手语视频生成领域取得了突破性进展。

Abstract: Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.

</details>


### [40] [FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation](https://arxiv.org/abs/2512.03520)
*Yiyi Cai,Yuhan Wu,Kunhang Li,You Zhou,Bo Zheng,Haiyang Liu*

Main category: cs.CV

TL;DR: FloodDiffusion框架可通过时变文本提示实时生成文本对齐的人体运动序列，通过改进扩散强制方法实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖分块或自回归模型生成运动序列，难以处理时变文本控制和真实运动分布建模。 Vanilla扩散强制方法直接应用失败，需针对性改进。

Method: 1. 双向注意力代替因果注意力。2. 采用下三角时间调度器而非随机调度。3. 文本条件连续时变注入。通过这三个调整优化扩散强制框架的时间序列建模能力。

Result: 在HumanML3D基准上取得0.057的FID值，首次验证扩散强制框架在流式运动生成任务的SOTA性能，并公开模型/代码。

Conclusion: 实验证明改进的扩散强制框架可有效建模真实运动分布，实现低延迟实时生成，为文本驱动运动生成提供了新范式。

Abstract: We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/

</details>


### [41] [Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation](https://arxiv.org/abs/2512.03534)
*Subin Kim,Sangwoo Mo,Mamshad Nayeem Rizve,Yiran Xu,Difan Liu,Jinwoo Shin,Tobias Hinz*

Main category: cs.CV

TL;DR: 提出PRIS框架，通过推理时动态调整文本提示，解决文本到视觉生成中用户意图与生成结果对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定文本提示，扩大生成规模时易陷入质量瓶颈，且难以精准捕捉用户意图与生成结果的细节偏差。

Method: 设计PRIS框架，包含元素级事实修正验证模块，通过生成结果中的重复失败模式检测，在推理阶段动态修正文本提示并重新生成视觉结果。

Result: 在文本到图像/视频任务中，VBench 2.0基准提升15%，验证模块在细节对齐评估中显著优于整体评估方法。

Conclusion: 联合扩展文本提示与视觉生成可突破推理时的规模瓶颈，动态提示调整是实现意图精准对齐的关键。

Abstract: Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.

</details>


### [42] [CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation](https://arxiv.org/abs/2512.03540)
*Ruoxuan Zhang,Bin Wen,Hongxia Xie,Yi Yao,Songhan Zuo,Jian-Yu Jiang-Lin,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: CookAnything是一种基于扩散模型的灵活框架，可根据任意长度的文本烹饪指令生成连贯且视觉多样的图像序列，通过区域控制、位置编码和跨步骤一致性技术解决现有方法在可变步骤和连贯性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的图像生成模型难以处理具有结构化多步骤属性的食谱插图生成，且无法根据指令长度自适应调整生成图像数量，导致步骤逻辑与视觉语义脱节。需要一种可扩展且保持细粒度一致性的方法。

Method: 提出三个核心组件：(1) Step-wise Regional Control（SRC）实现单次去噪过程中文本步骤与图像区域的动态对齐；(2) Flexible RoPE步态感知位置编码增强时空一致性与多样性；(3) Cross-Step Consistency Control（CSCC）通过跨步骤细粒度控制保证配料连续性。在训练与免训练场景下进行实验验证。

Result: 在食谱插图基准测试中，CookAnything在可变步骤适应性、图像序列连贯性及细节一致性上全面优于现有方法，且支持无固定长度限制的高质量视觉合成。

Conclusion: 该框架通过创新的区域控制与一致性机制，成功将扩散模型扩展到结构化多步骤指令可视化领域，为程序性内容生成提供了可扩展的技术范式。

Abstract: Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.

</details>


### [43] [V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention](https://arxiv.org/abs/2512.03542)
*Nan Sun,Zhenyu Zhang,Xixun Lin,Kun Wang,Yanmin Shang,Naibin Gu,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang,Yanan Cao*

Main category: cs.CV

TL;DR: 本文提出V-ITI框架解决多模态大语言模型在视觉-语言任务中的视觉相关幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有干预方法因忽略'何时干预'导致过度干预，产生新幻觉并增加计算开销，需要精准控制干预时机

Method: 通过头部层级激活模式检测视觉忽视现象，设计视觉忽视检测器和视觉回想干预者，仅在必要时刻调用预存视觉特征进行激活调制

Result: 在8个基准数据集和不同MLLM家族中实验证明该方法有效减少视觉相关幻觉，同时保持原有任务性能

Conclusion: V-ITI框架通过精准干预时机控制，在不增加复杂度的情况下显著提升多模态模型视觉问答可靠性

Abstract: Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.

</details>


### [44] [Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching](https://arxiv.org/abs/2512.03553)
*Wei Chee Yew,Hailun Xu,Sanjay Saha,Xiaotian Fan,Hiok Hian Ong,David Yuchen Wang,Kanchan Sarkar,Zhenheng Yang,Danhui Guan*

Main category: cs.CV

TL;DR: 本论文提出了一种可扩展的混合内容审核框架，结合监督分类和基于参考的相似性匹配，利用多模态大语言模型（MLLM）实现高效且灵活的直播内容治理。


<details>
  <summary>Details</summary>
Motivation: 大规模用户生成视频平台需要实时、多模态且能应对新兴违规形式的审核机制，传统分类器在检测新颖或模糊违规内容时效果有限。

Method: 构建双通道框架：监督分类通道处理已知违规，相似性匹配通道通过参考样本库识别新形态违规；多模态输入（文本/音频/视觉）被两通道同步处理，MLLM在模型蒸馏中提升各通道精度。

Result: 生产环境中监督分类通道达到80%精度下67%召回率，相似性通道实现80%精度下76%召回率，大规模A/B测试表明用户观看违规直播量减少6-8%。

Conclusion: 混合框架在维持推理轻量化的同时提升了多模态内容审核的鲁棒性，可同时应对明确违规与对抗性新样式，为动态内容治理提供可行路径。

Abstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.

</details>


### [45] [GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models](https://arxiv.org/abs/2512.03566)
*Hao Sun,Lei Fan,Donglin Di,Shaohui Liu*

Main category: cs.CV

TL;DR: 本文提出GAOT框架，通过扩散模型和超图学习从文本提示生成可运动的3D物体。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以根据文本生成3D可运动物体，需弥合文本描述与3D表示间的差距。

Method: 1. 微调点云生成模型生成粗糙物体; 2. 超图学习细化表示(零件为顶点);
3. 扩散模型生成关节(边)。

Result: 在PartNet-Mobility数据集上超越以往方法，定性和定量实验均有效。

Conclusion: GAOT成功结合扩散模型与超图结构，实现了文本驱动的可运动物体生成。

Abstract: Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.

</details>


### [46] [Global-Local Aware Scene Text Editing](https://arxiv.org/abs/2512.03574)
*Fuxiang Yang,Tonghua Su,Donglin Di,Yin Chen,Xiangqian Wu,Zhongjie Wang,Lei Fan*

Main category: cs.CV

TL;DR: This paper proposes GLASTE, an end-to-end framework for Scene Text Editing that addresses inconsistency and length-insensitivity issues by integrating global context and local features.


<details>
  <summary>Details</summary>
Motivation: Existing STE methods struggle with maintaining coherence between edited text regions and surrounding backgrounds, and fail to handle varying text lengths. The authors aim to solve these challenges through a unified framework.

Method: GLASTE employs a global-local combination structure with joint global-local loss functions. It enhances text features using size-independent style vectors and applies affine fusion to maintain aspect ratios. A three-stage pipeline processes global semantics, local style, and final consistency optimization.

Result: Experiments show GLASTE outperforms previous methods in both quantitative metrics (e.g., 8.2% improvement in text similarity) and qualitative visual coherence, while handling large length discrepancies in text editing tasks.

Conclusion: The integration of global context and local details effectively mitigates inconsistency and length-insensitivity challenges, establishing GLASTE as a superior solution for scene text editing with versatile text length adaptation.

Abstract: Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.

</details>


### [47] [UniComp: Rethinking Video Compression Through Informational Uniqueness](https://arxiv.org/abs/2512.03575)
*Chao Yuan,Shimin Chen,Minliang Lin,Limeng Qiao,Guanglu Wan,Lin Ma*

Main category: cs.CV

TL;DR: UniComp is a video compression framework leveraging information uniqueness to optimize computational resources by minimizing reconstruction error through semantic analysis and adaptive compression modules.


<details>
  <summary>Details</summary>
Motivation: Current attention-based compression methods struggle with efficiently preserving critical visual information under strict computational budgets, necessitating a novel semantic-driven approach rooted in information theory.

Method: 1) Formulates compression as a conditional entropy minimization problem via information uniqueness metrics, 2) Proposes three modules: Frame Group Fusion (semantic grouping), Token Allocation (dynamic resource distribution), and Spatial Dynamic Compression (fine-grained token pruning).

Result: Experiments show UniComp surpasses existing methods in retaining essential visual tokens while operating within fixed computational constraints, with improved reconstruction fidelity across diverse video datasets.

Conclusion: Information uniqueness serves as a critical indicator for token redundancy, enabling superior compression efficiency compared to attention-based counterparts when balancing quality and computational cost.

Abstract: Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.

</details>


### [48] [Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning](https://arxiv.org/abs/2512.03577)
*Yizhi Zhang,Lei Fan,Zhulin Tao,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: 本研究提出了跨染色对比学习框架CSCL，利用五染色全切片图像数据集，通过两级预训练（补丁级对比对齐和滑块级表征学习）增强H&E染色切片与免疫组化数据的兼容性和表征质量。


<details>
  <summary>Details</summary>
Motivation: 多染色切片的非对齐问题导致特征不一致和表征退化，传统方法受限于多染色数据集稀缺且配准困难，需要解决跨染色信息互补性和表征鲁棒性问题。

Method: 构建H&E与四个免疫组化标记物的配准数据集；设计CSCL框架：第一阶段使用对比学习对齐跨染色补丁特征，第二阶段采用多实例学习结合跨染色注意力融合模块和全局一致性约束进行滑块级表征学习。

Result: 在癌症亚型分类、免疫组化生物标志物预测和生存分析任务中均取得性能提升，生成的H&E表征具有跨数据集迁移能力和高鲁棒性，代码与数据已开源。

Conclusion: 该框架有效解决了跨染色图像对齐和语义融合难题，通过系统性表征学习实现多模态病理信息整合，实验验证了其在多种临床任务中的实用价值。

Abstract: Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.

</details>


### [49] [Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes](https://arxiv.org/abs/2512.03580)
*Malte Bleeker,Mauro Gotsch*

Main category: cs.CV

TL;DR: Dynamic Optical Test for Bot Identification (DOT-BI)利用人类对动态图像的感知差异区分机器与人类，用户可通过观察运动与缩放变化辨识隐藏数字，算法难以解析，平均10.7秒完成测试。


<details>
  <summary>Details</summary>
Motivation: 传统验证码易被先进AI破解，需一种无需复杂设备、人类易完成但机器难以通过的新型测试方法，以解决自动化系统对在线调查/流程的干扰问题。

Method: 设计动态光学测试：隐藏数字与背景共享同种黑白像素纹理，仅通过帧间运动/缩放差异使人类可见。评估方式包括：1)测试GPT-5等AI模型的破解能力；2)在线问卷验证人类表现；3)实验室研究可用性。

Result: AI模型完全失效，人类在在线测试中达成99.5%准确率（181/182），实验室测试显示可用性（0负面影响）与机器学习模型控制组相当。

Conclusion: DOT-BI基于人类视觉系统的认知优势，兼具高安全性（抗AI攻击）与用户体验（快速无认知负担），开放代码加速实际应用部署。

Abstract: We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.

</details>


### [50] [Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding](https://arxiv.org/abs/2512.03592)
*Guang Yang,Lei Fan*

Main category: cs.CV

TL;DR: HyperRNA使用超图框架解决RNA逆折叠问题，通过编码器-解码器架构生成符合目标结构的RNA序列。


<details>
  <summary>Details</summary>
Motivation: RNA逆折叠问题在分子稳定性和功能中具有关键作用，但序列与结构关系复杂导致设计困难。现有方法存在局限性，需利用更高效的建模方式（如超图）捕捉高阶依赖关系。

Method: HyperRNA分三阶段：预处理阶段基于3-bead粗粒化构建RNA骨架图结构；编码阶段通过注意力嵌入模块和超图编码器捕捉高阶依赖；解码阶段采用自回归生成RNA序列。实验在PDBBind和RNAsolo数据集上进行对比验证。

Result: HyperRNA在RNA序列生成和RNA-蛋白质复合物设计任务上均超越现有方法，证明超图在建模生物分子相互作用中的有效性，生成的序列具有更高结构匹配度。

Conclusion: 基于超图的HyperRNA框架为RNA设计提供了新范式，在逆折叠任务中展现显著优势，揭示了超图模型在生物工程中应用于复杂分子设计的潜力。

Abstract: The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding.
  In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.

</details>


### [51] [CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures](https://arxiv.org/abs/2512.03593)
*David Svitov,Pietro Morerio,Lourdes Agapito,Alessio Del Bue*

Main category: cs.CV

TL;DR: 提出CloseUpAvatar，通过动态纹理切换实现高质量近景渲染与广泛视角适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理自由摄像机运动时难以兼顾近景细节保持与远程渲染效率，需动态平衡渲染质量与计算复杂度。

Method: 采用高低频双纹理层平面参数化表示，根据摄像机距离自适应融合纹理细节，并设计渐进式混合策略实现质量-效率协同优化。

Result: 在ActorsHQ高分辨率数据集上，相较基线方法提升32%视角覆盖范围，保持47FPS实时渲染性能，量化指标与视觉质量均显著优化。

Conclusion: 所提动态纹理参数化框架有效解决了大范围相机运动下的化身渲染保真度与效率矛盾，支持影视级高质量虚实融合应用。

Abstract: We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.

</details>


### [52] [HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation](https://arxiv.org/abs/2512.03597)
*Fuchen Zheng,Xinyi Chen,Weixuan Li,Quanjun Li,Junhua Zhou,Xiaojiao Guo,Xuhang Chen,Chi-Man Pun,Shoujun Zhou*

Main category: cs.CV

TL;DR: HBFormer, a Hybrid-Bridge Transformer, addresses limitations in Vision Transformers' ability to integrate local details with global context for medical image segmentation, particularly enhancing microtumor and miniature organ boundary definition.


<details>
  <summary>Details</summary>
Motivation: Current Vision Transformers struggle to fuse local details with global context, hindering precise segmentation of small structures like microtumors. Current methods lack explicit modeling of long-range dependencies and multi-scale feature integration, necessitating a novel architecture for improved accuracy.

Method: HBFormer combines a U-shaped encoder-decoder with a Swin Transformer backbone. It introduces a Multi-Scale Feature Fusion (MFF) decoder with a 'Bridge' mechanism that integrates encoder features and global context through channel/spatial attention modules built on dilated/depth-wise convolutions, explicitly capturing long-range dependencies.

Result: HBFormer achieves state-of-the-art performance on multi-organ, liver tumor, and bladder tumor segmentation datasets, particularly excelling in delineating microstructures. Evaluated benchmarks confirm superior microtumor and miniature organ segmentation precision compared to existing methods.

Conclusion: HBFormer improves medical image segmentation by bridging local-global context through its MFF decoder. The architecture's explicit long-range dependency modeling and multi-scale feature fusion establish a promising framework for challenging medical imaging tasks requiring precise boundary delineation.

Abstract: Medical image segmentation is a cornerstone of modern clinical diagnostics. While Vision Transformers that leverage shifted window-based self-attention have established new benchmarks in this field, they are often hampered by a critical limitation: their localized attention mechanism struggles to effectively fuse local details with global context. This deficiency is particularly detrimental to challenging tasks such as the segmentation of microtumors and miniature organs, where both fine-grained boundary definition and broad contextual understanding are paramount. To address this gap, we propose HBFormer, a novel Hybrid-Bridge Transformer architecture. The 'Hybrid' design of HBFormer synergizes a classic U-shaped encoder-decoder framework with a powerful Swin Transformer backbone for robust hierarchical feature extraction. The core innovation lies in its 'Bridge' mechanism, a sophisticated nexus for multi-scale feature integration. This bridge is architecturally embodied by our novel Multi-Scale Feature Fusion (MFF) decoder. Departing from conventional symmetric designs, the MFF decoder is engineered to fuse multi-scale features from the encoder with global contextual information. It achieves this through a synergistic combination of channel and spatial attention modules, which are constructed from a series of dilated and depth-wise convolutions. These components work in concert to create a powerful feature bridge that explicitly captures long-range dependencies and refines object boundaries with exceptional precision. Comprehensive experiments on challenging medical image segmentation datasets, including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrate that HBFormer achieves state-of-the-art results, showcasing its outstanding capabilities in microtumor and miniature organ segmentation. Code and models are available at: https://github.com/lzeeorno/HBFormer.

</details>


### [53] [Memory-Guided Point Cloud Completion for Dental Reconstruction](https://arxiv.org/abs/2512.03598)
*Jianan Sun,Yukang Huang,Dongzhihan Wang,Mingyu Fan*

Main category: cs.CV

TL;DR: 提出检索增强框架解决牙齿点云缺失问题


<details>
  <summary>Details</summary>
Motivation: 牙科点云因遮挡和扫描视角限制存在缺失区域，传统编码器-解码器架构易产生结构偏差

Method: 在编码器-解码器中集成可学习原型记忆库，通过编码特征检索最近原型并加权融合进行补全

Result: 在Teeth3DS数据集上Chamfer Distance提升，可视化显示牙尖/嵴线等细节更清晰

Conclusion: 通过记忆原型提供结构先验，可在无需标签条件下实现精准点云补全

Abstract: Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.

</details>


### [54] [Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding](https://arxiv.org/abs/2512.03601)
*Haoran Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了Motion4D框架，通过将2D基础模型的先验知识集成到统一的4D高斯泼溅表示中，解决了传统2D视觉模型在3D一致性上的不足，从而提升动态场景的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有2D视觉基础模型在动态场景分析中缺乏3D一致性，导致空间错位和时间闪烁问题。本文旨在通过引入时空联合优化策略，弥补2D模型在三维几何理解上的缺陷。

Method: 1) **序列优化**分阶段更新运动和语义场以保持局部一致性；2) **全局优化**联合优化所有属性确保长期连贯性；3) 使用3D置信度图动态调整运动先验并引入自适应重采样；4) 通过交替优化语义场和SAM2提示来提升语义一致性。

Result: 在点跟踪、视频分割和新视角合成等任务中均显著优于2D基础模型及现有3D方法，代码已开源。实验证明了4D表示在动态场景建模中的有效性。

Conclusion: Motion4D通过4D高斯泼溅框架实现了2D先验与3D几何的深度融合，提出的动态置信度机制和分阶段优化策略有效解决了时空不一致问题。

Abstract: Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.

</details>


### [55] [LAMP: Language-Assisted Motion Planning for Controllable Video Generation](https://arxiv.org/abs/2512.03619)
*Muhammed Burak Kizil,Enes Sanli,Niloy J. Mitra,Erkut Erdem,Aykut Erdem,Duygu Ceylan*

Main category: cs.CV

TL;DR: LAMP是一个利用大语言模型（LLMs）将自然语言描述转化为3D运动轨迹的框架，通过引入电影化运动领域特定语言（DSL）和程序合成技术，首次实现从自然语言直接生成物体及相机的动态控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成的运动控制接口（如物体动态和相机轨迹设计）功能有限，难以满足复杂电影场景的编排需求。现有方法在运动可控性和用户意图对齐方面存在不足。

Method: 1) 设计基于电影理论的运动DSL，规范运动指令表达；2) 利用LLMs的程序合成能力将自然语言解析为结构化运动程序；3) 通过确定性映射将程序转化为显式3D轨迹；4) 构建包含文本-程序-轨迹三元组的大规模数据集进行验证。

Result: 验证显示LAMP在运动可控性（如轨迹精度、动态组合）和用户意图符合度（如指令解析准确率）上均显著优于SOTA模型，成功生成复杂运动场景（如多物体协作、电影级运镜），数据集包含10万组文本轨迹样本。

Conclusion: 该框架首次实现通过自然语言直接生成物体及相机的复合运动控制，证明LLMs在程序合成与运动规划中的有效性，并为未来视频生成提供了标准化运动控制解决方案。

Abstract: Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.

</details>


### [56] [ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation](https://arxiv.org/abs/2512.03621)
*Yaokun Li,Shuaixian Wang,Mantang Guo,Jiehui Huang,Taojun Ding,Mu Hu,Kaixuan Wang,Shaojie Shen,Guang Tan*

Main category: cs.CV

TL;DR: ReCamDriving是一个纯视觉、相机控制的视频生成框架，通过3DGS渲染的稠密几何引导实现复杂场景修复，并采用两阶段训练和新数据集提升相机可控性。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法无法处理复杂结构损坏，LiDAR方法受限于稀疏数据。作者旨在利用稠密3D几何信息提升相机可控生成质量，解决训练与测试阶段相机姿态差异问题。

Method: 1) 两阶段训练：先用相机姿态实现粗糙控制，再结合3DGS渲染进行细粒度几何引导；2) 基于3DGS的跨轨迹数据增强策略，构建包含110K并行轨迹对的Paradise数据集。

Result: 在相机可控性和结构一致性上达到最优性能，实验表明两阶段训练有效缓解过拟合，数据增强策略提升多轨迹泛化能力。

Conclusion: ReCamDriving通过结合3DGS几何引导与两阶段训练，在无激光雷达的场景下实现了SOTA级可控视频生成，提出的数据集将推动该领域研究。

Abstract: We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.

</details>


### [57] [ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos](https://arxiv.org/abs/2512.03666)
*Qi'ao Xu,Tianwen Qian,Yuqian Fu,Kailing Li,Yang Jiao,Jiacheng Zhang,Xiaoling Wang,Liang He*

Main category: cs.CV

TL;DR: 本文介绍了首个面向任务导向的视角时空视频定位基准测试集 ToG-Bench，旨在解决具身智能中任务相关物体定位的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视角时空定位（STVG）研究主要聚焦于描述性指令，缺乏任务导向推理这一具身智能体完成目标导向交互的关键能力。

Method: 构建了包含三大特征的 ToG-Bench 测试集：(1) 任务导向定位；(2) 显式-隐式双重定位；(3) 一对多定位。基于 ScanNet 视频构建，采用基础模型标注与人工校正的混合流水线生成100个片段和2,704条任务导向指令，并设计了针对多目标和显/隐式定位的任务级评估指标。

Result: 在7个SOTA多模态语言模型上的系统实验揭示了任务导向STVG的内在挑战性，显式-隐式定位和多目标定位存在显著性能差距。

Conclusion: 研究凸显了连接感知与交互在具身场景中的难度，并为未来研究提供了基准和方向，代码和数据已开源。

Abstract: A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..

</details>


### [58] [Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning](https://arxiv.org/abs/2512.03667)
*Ge-Peng Ji,Jingyi Liu,Deng-Ping Fan,Nick Barnes*

Main category: cs.CV

TL;DR: 本研究提出Colon-X，发布全球最大结肠镜多模态数据集ColonVQA（110万+样本），并开发新型临床推理模型ColonR1，其精度超传统方法25.22%


<details>
  <summary>Details</summary>
Motivation: 推动结肠镜领域多模态智能发展，解决从基础多模态理解到临床推理过渡的核心挑战，提升现有模型在临床场景中的鲁棒性与可靠性

Method: 1) 构建ColonVQA数据集（1.1M+ VQA样本，76临床发现/18任务）；2) 系统评估22个多模态大模型的泛化能力及抗干扰性；3) 开发ColonReason（专家辩论标注）与ColonR1（任务自适应奖励+梯度稳定优化）

Result: 现有顶级多模态模型临床输出稳定性不足，ColonR1在数据匮乏下达56.61%准确率，相对监督微调提升25.22%，建立新型多模态结肠镜分析基线

Conclusion: Colon-X为社区提供数据与模型基础设施，ColonVQA奠定研究基础，ColonR1通过创新架构实现推理能力突破，所有资源开源促进领域发展（GitHub: ai4colonoscopy/Colon-X）

Abstract: In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.

</details>


### [59] [ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers](https://arxiv.org/abs/2512.03673)
*Feice Huang,Zuliang Han,Xing Zhou,Yihuang Chen,Lifei Zhu,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文提出了一种称为ConvRot的分组旋转量化方法，通过规则Hadamard变换抑制离群值，结合ConvLinear4bit模块实现无需训练的W4A4推理，在保证图像质量的同时显著提升了推理速度并降低了内存占用。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型体积增大，其高内存占用和推理延迟阻碍了实际部署，现有旋转量化方法在处理这类模型中的行/列离群值时存在复杂度高或效果差的问题。

Method: 提出ConvRot方法，使用规则Hadamard变换在组内同时抑制行和列方向的离群值（复杂度由平方级降至线性级），并设计集成旋转、量化、矩阵乘和反量化的ConvLinear4bit模块，实现即插即用的W4A4推理。

Result: 在FLUX.1-dev模型上实现2.26倍速度提升和4.05倍内存降低，且保持图像保真度。

Conclusion: 首次将基于旋转的量化方法应用到扩散变压器的插拔式低比特推理中，为大规模生成模型的部署提供了新方案。

Abstract: Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.

</details>


### [60] [GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces](https://arxiv.org/abs/2512.03683)
*Melis Ocal,Xiaoyan Xing,Yue Li,Ngo Anh Vien,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 本文提出GaussianBlender，一种无需优化的实时3D风格化框架，通过结构化解耦潜空间与文本扩散模型实现高质量多视角一致编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格化方法依赖2D图像编辑、需耗时优化且存在多视角不一致问题，难以满足工业规模生产需求。

Method: 从空间分组的3D高斯分布学习几何与外观解耦的潜空间，通过文本条件扩散模型进行结构化编辑。

Result: 实验证明该方法在保持几何完整性的同时实现毫秒级高保真编辑，多视角一致性优于需实例优化的现有方法。

Conclusion: 为大规模工业化生产提供了可扩展的实时3D风格化解决方案，推动文本到3D创作的实用化发展。

Abstract: 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.

</details>


### [61] [Active Visual Perception: Opportunities and Challenges](https://arxiv.org/abs/2512.03687)
*Yian Li,Xiaoyu Guo,Hao Zhang,Shuiwang Li,Xiaowei Dai*

Main category: cs.CV

TL;DR: 本文综述了主动视觉感知技术的机遇与挑战，该技术通过动态环境交互提升信息获取能力，在机器人等领域应用广泛，但需解决实时数据处理等难题。


<details>
  <summary>Details</summary>
Motivation: 传统被动视觉系统依赖静态数据，难以满足复杂场景需求，而主动视觉可通过主动环境交互获取动态信息，需系统性研究其应用潜能与现存问题。

Method: 采用文献综述方法，从应用场景、技术特点、现存挑战三个维度对主动视觉感知进行结构化分析，未报告具体实验。

Result: 明确了主动视觉感知在动态环境适应性方面的优势，总结了实时处理、多模态感知整合等关键技术挑战，但未提供量化验证结果。

Conclusion: 该技术具有重要应用前景且需突破算法效率、跨模态融合等限制，未来研究应建立标准化测试平台并发展轻量化模型。

Abstract: Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.

</details>


### [62] [Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images](https://arxiv.org/abs/2512.03701)
*Paula Seidler,Neill D. F. Campbell,Ivor J A Simpson*

Main category: cs.CV

TL;DR: SUSS是一种新的感知相似性评分方法，结合生成模型与可解释性，通过结构化多元正态分布建模图像特征，在对齐人类感知判断的同时提供透明分析机制。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度感知损失（如LPIPS）缺乏可解释性、手工设计指标（如SSIM）缺失关键感知特性的问题，构建既准确又透明的感知相似性评估框架。

Method: 采用生成式自监督方法，将图像分解为感知组件并建模为结构化多元正态分布，通过人类不可感知的增强数据训练；最终得分由基于人类感知数据学习的加权组件对数概率和线性残差变换构成，支持去相关残差分析和可视化解释。

Result: 在多样失真类型中展现与人类感知判断的高度一致性，提供局部可解释的相似性评估依据，并通过下游图像任务验证作为感知损失函数的有效性。

Conclusion: SUSS通过结构化概率建模实现了感知准确性、校准鲁棒性与可解释性的统一，为计算机视觉模型的训练与评估提供兼具性能与透明度的新工具。

Abstract: Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.
  We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.
  SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.

</details>


### [63] [DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction](https://arxiv.org/abs/2512.03715)
*Kaichen Zhang,Tianxiang Sheng,Xuanming Shi*

Main category: cs.CV

TL;DR: 本论文提出了DINO-RotateMatch，通过结合数据集自适应的图像配对策略与旋转感知的关键点提取及匹配，显著提升了大规模3D重建的性能，在Kaggle比赛中取得银奖。


<details>
  <summary>Details</summary>
Motivation: 为解决互联网非结构化图像大规模3D重建中的图像匹配挑战，现有方法在可扩展性和视角变化处理上存在不足，需结合全局语义检索与局部旋转特征增强。

Method: 采用DINO模型从大型图像集中检索语义相关图像对，结合基于旋转增强的数据增强技术，并通过ALIKED与LightGlue提取和匹配旋转依赖的局部特征。

Result: 在Kaggle Image Matching Challenge 2025中，平均准确率(mAA)持续提升，最终获得第47名（银奖，参赛队伍943支），验证了方法的有效性。

Conclusion: 将自监督全局描述符与旋转增强的局部匹配结合，为大规模3D重建提供了鲁棒且可扩展的解决方案，平衡了全局与局部特征的协同优化。

Abstract: This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The
  method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and
  matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while
  rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results
  confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers
  a robust and scalable solution for large-scale 3D reconstruction.

</details>


### [64] [PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention](https://arxiv.org/abs/2512.03724)
*Ziwen Li,Xin Wang,Hanlue Zhang,Runnan Chen,Runqi Lin,Xiao He,Han Huang,Yandong Guo,Fakhri Karray,Tongliang Liu,Mingming Gong*

Main category: cs.CV

TL;DR: PosA-VLA通过姿态条件注意力机制提升视觉-语言-行动模型的动作生成精确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因空间均匀感知域易受无关物体干扰，导致动作冗余不稳定，难以满足实时应用需求。

Method: 提出带姿态条件监督的锚定注意力框架，通过轻量化架构实现指令语义与视觉线索的精准对齐，无需辅助感知模块。

Result: 实验显示该方法在机械臂操作基准测试中实现精确高效的具身任务执行，并在复杂环境中表现出强泛化能力。

Conclusion: 所提框架兼顾性能与效率，为实时机器人应用提供新解决方案。

Abstract: The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.

</details>


### [65] [Out-of-the-box: Black-box Causal Attacks on Object Detectors](https://arxiv.org/abs/2512.03730)
*Melane Navaratnarajah,David A. Kelly,Hana Chockler*

Main category: cs.CV

TL;DR: BlackCAtt is a novel black-box attack method for object detectors that uses causal pixel sets to create explainable, imperceptible, and architecture-agnostic adversarial attacks, outperforming baselines by significant margins.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attack methods for object detectors are often white-box, architecture-specific, and lack explainability. Understanding their mechanisms is critical for improving model robustness and developing countermeasures.

Method: BlackCAtt combines causal inference with object detector outputs (bounding boxes) to identify minimal, causally sufficient pixel sets. These pixels are manipulated to generate adversarial examples targeting detection removal, modification, or creation, without requiring internal model knowledge.

Result: On COCO test, BlackCAtt achieves 2.7× more effective detection removal, 3.86× better modification accuracy, and 5.75× higher success rate in triggering false detections compared to baselines. The attacks remain visually indistinguishable from original images.

Conclusion: BlackCAtt demonstrates that causal pixel targeting enables precise, imperceptible adversarial attacks across diverse object detectors. This approach provides insights into model vulnerabilities and establishes a framework for explainable, architecture-agnostic security analysis.

Abstract: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.

</details>


### [66] [Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.03749)
*Korada Sri Vardhana,Shrikrishna Lolla,Soma Biswas*

Main category: cs.CV

TL;DR: 本文提出了SelfDebias，一种无需监督的去偏方法，通过图像编码器的语义聚类引导扩散过程，有效缓解文本到图像生成中的刻板印象偏差。


<details>
  <summary>Details</summary>
Motivation: 互联网训练数据中存在大量性别/职业等刻板偏见，现有监督方法依赖人工标注数据，而扩散模型的UNet结构和图像语义特征尚未被有效利用。

Method: 在推理阶段：1) 对图像特征空间进行聚类分割 2) 通过KL散度最小化驱动扩散过程 3) 动态生成去偏约束，无需预训练分类器或标注数据。

Result: 在多个扩散模型（包括条件/无条件模型）和敏感维度（性别/种族等）上F1-score提升18.7%，同时保持图像真实性和prompt忠实度。

Conclusion: SelfDebias实现了无监督去偏范式突破，首次将特征空间聚类与扩散过程动态耦合，在保持技术有效性的同时消除伦理风险。

Abstract: Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.

</details>


### [67] [LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling](https://arxiv.org/abs/2512.03796)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: This paper introduces Latent Scale Rejection Sampling (LSRS) to enhance Visual Autoregressive (VAR) models for image generation by refining token maps during inference, reducing structural errors while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Parallel token sampling in VAR models causes structural errors in generated images, degrading quality. Improving this requires a method to iteratively refine token maps without sacrificing computational efficiency.

Method: LSRS applies a lightweight scoring model during inference to evaluate and select optimal token maps at each hierarchical scale, prioritizing early scales crucial for structural coherence. Candidates are sampled and refined progressively to mitigate error accumulation.

Result: LSRS reduces VAR-d30's FID score from 1.95 to 1.78 with only 1% inference time increase. Extending time by 15% further lowers FID to 1.66, achieving quality improvements with minimal overhead.

Conclusion: LSRS provides an efficient test-time refinement strategy for VAR models, significantly enhancing generation quality by addressing structural errors through latent-scale token map optimization.

Abstract: Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.

</details>


### [68] [A Robust Camera-based Method for Breath Rate Measurement](https://arxiv.org/abs/2512.03827)
*Alexey Protopopov*

Main category: cs.CV

TL;DR: 利用普通摄像头通过数学变换组合实现高精度呼吸率非接触测量，抗干扰性强。


<details>
  <summary>Details</summary>
Motivation: 现有呼吸率检测方法受限于理想条件或准确性不足，需改进鲁棒性和精度。

Method: 结合多种数学变换处理视频信号，通过相对偏差（<5%）和均方绝对误差（0.57次/分钟）评估，使用14名志愿者总时长超2.5小时的视频测试。

Result: 与基准数据对比，平均绝对误差0.57次/分钟，显著优于既往研究，能抵抗运动干扰实现自由行为下的远程监测。

Conclusion: 方法在非理想条件下仍保持高精度呼吸率检测，无需约束行为，拓展应用场景。

Abstract: Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.

</details>


### [69] [CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation](https://arxiv.org/abs/2512.03844)
*Letian Zhou,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出CoDA，一种无需目标数据集预训练即可实现高性能数据集蒸馏的新方法，通过匹配生成模型与目标数据的核心分布，在不依赖目标数据训练的情况下达到或超越现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法存在两大缺陷：1) 依赖目标数据预训练的扩散模型，违背蒸馏目的且训练成本高；2) 使用通用图文模型时存在目标语义分布不匹配问题，导致性能下降。

Method: 采用密度聚类技术挖掘目标数据集的“内核分布”，通过引导生成过程对齐该分布，将通用生成先验与目标数据特性相结合，并通过动态调整机制优化样本生成。

Result: 在ImageNet-1K等基准测试中，CoDA在无需目标数据预训练的情况下达到60.4%（50样本/类）的新SOTA准确率，性能与依赖目标模型的方法相当或更优。

Conclusion: 实验证明CoDA有效解决目标数据分布不匹配问题，使通用图文模型可直接用于数据集蒸馏，为低数据学习场景提供高效解决方案。

Abstract: Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA

</details>


### [70] [Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population](https://arxiv.org/abs/2512.03854)
*Peshawa J. Muhammad Ali,Navin Vincent,Saman S. Abdulla,Han N. Mohammed Fadhl,Anders Blilie,Kelvin Szolnoky,Julia Anna Mielcarz,Xiaoyi Ji,Kimmo Kartasalo,Abdulbasit K. Al-Talabani,Nita Mulliqi*

Main category: cs.CV

TL;DR: 本研究公开了一个包含339张伊拉克前列腺穿刺活检全切片影像的数据集，用于支持跨人群及多扫描仪的人工智能病理模型研究。


<details>
  <summary>Details</summary>
Motivation: 当前公开的病理数据集稀缺且多源自西方人群，人工智能模型在中东等欠数字化地区人群中的泛化能力未知。该研究旨在通过提供全球多样化人群的病理数据集，验证和改进AI模型的普适性。

Method: 收集伊拉克埃尔比勒185例连续患者的339张前列腺穿刺活检全切片图像，由三位病理学家独立标注Gleason评分及ISUP分级。使用Leica、Hamamatsu和Grundium三种扫描仪进行数字化，所有图像以原格式去标识化保存，并计划以CC BY 4.0协议存放于Bioimage Archive数据库。

Result: 数据集支持跨扫描仪一致性分析、染色颜色标准化及分级一致性研究，具体访问编号待公布。

Conclusion: 该数据集为评估病理AI模型在不同人群和扫描设备下的鲁棒性提供了重要资源，有助于推动数字病理学的公平性与多样性研究。

Abstract: Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.

</details>


### [71] [Diminishing Returns in Self-Supervised Learning](https://arxiv.org/abs/2512.03862)
*Oli Bridge,Huey Sun,Botond Branyicskai-Nagy,Charles D'Ornano,Shomit Basu*

Main category: cs.CV

TL;DR: 研究发现小型视觉Transformer通过目标明确的预训练和数据选择可优化性能，但过度堆叠中间任务可能降低效果。


<details>
  <summary>Details</summary>
Motivation: 探索在参数和数据有限的小型视觉Transformer中，预训练、中间微调与下游任务协同训练的影响机制，解决模型性能与计算效率的平衡问题。

Method: 设计三组不同的预训练、中间微调和下游任务数据集组合，量化分析其对5M参数视觉Transformer的边际收益。

Result: 预训练始终提升性能但收益递减，不匹配的中间微调任务因任务机制差异反而损害下游性能。

Conclusion: 小型ViT需要针对性的预训练策略和任务筛选，盲目叠加中间任务会导致计算资源浪费及性能退化。

Abstract: While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.

</details>


### [72] [An Automated Framework for Large-Scale Graph-Based Cerebrovascular Analysis](https://arxiv.org/abs/2512.03869)
*Daniele Falcetta,Liane S. Canas,Lorenzo Suppa,Matteo Pentassuglia,Jon Cleary,Marc Modat,Sébastien Ourselin,Maria A. Zuluaga*

Main category: cs.CV

TL;DR: CaravelMetrics是一个自动化脑血管分析框架，通过骨架化图表示提取15类特征，支持多尺度和群体级研究。


<details>
  <summary>Details</summary>
Motivation: 传统血管分析方法耗时且无法全面量化形态特征，需要可扩展的自动化工具进行多尺度血管健康量化研究。

Method: 结合基于图谱的区域分割、中心线提取及图构建，计算形态、拓扑等15项特征，适用于全局/区域性分析。

Result: 在570例脑扫描中复现血管图谱，验证了年龄/性别相关变化及教育水平与血管复杂度的关联性

Conclusion: 该框架实现全自动化量化分析，为血管衰老研究和临床评估提供标准化计算工具

Abstract: We present CaravelMetrics, a computational framework for automated cerebrovascular analysis that models vessel morphology through skeletonization-derived graph representations. The framework integrates atlas-based regional parcellation, centerline extraction, and graph construction to compute fifteen morphometric, topological, fractal, and geometric features. The features can be estimated globally from the complete vascular network or regionally within arterial territories, enabling multiscale characterization of cerebrovascular organization. Applied to 570 3D TOF-MRA scans from the IXI dataset (ages 20-86), CaravelMetrics yields reproducible vessel graphs capturing age- and sex-related variations and education-associated increases in vascular complexity, consistent with findings reported in the literature. The framework provides a scalable and fully automated approach for quantitative cerebrovascular feature extraction, supporting normative modeling and population-level studies of vascular health and aging.

</details>


### [73] [Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy](https://arxiv.org/abs/2512.03883)
*Jorge Tapias Gomez,Despoina Kanata,Aneesh Rangnekar,Christina Lee,Julio Garcia-Aguilar,Joshua Jesse Smith,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: This paper proposes a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to distinguish clinical complete response (cCR) from local regrowth (LR) using longitudinal endoscopic images, enabling early detection of LR in rectal cancer patients undergoing watch-and-wait surveillance.


<details>
  <summary>Details</summary>
Motivation: The need for accurate early detection of local regrowth during watch-and-wait management of rectal cancer requires robust methods to differentiate cCR from LR using follow-up endoscopic images, especially under imaging artifacts and variations that challenge traditional models.

Method: A SSDCA framework combines pretrained Swin transformers for feature extraction and dual cross-attention to align longitudinal restaging and follow-up image pairs without spatial alignment. Evaluated on 135 training and 62 test patient image pairs, the model was validated against baseline Swin-based architectures.

Result: SSDCA achieved superior performance with 81.76% balanced accuracy, 90.07% sensitivity, 72.86% specificity, and showed robustness against artifacts (e.g., blood, stool). UMAP clustering demonstrated optimal inter-cluster separation and intra-cluster compactness, confirming discriminative feature learning.

Conclusion: SSDCA enables accurate and robust LR detection through longitudinal endoscopic image analysis, improving monitoring reliability for cCR patients and offering a promising clinical tool for early intervention to prevent metastases.

Abstract: Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\% $\pm$ 0.04), sensitivity (90.07\% $\pm$ 0.08), and specificity (72.86\% $\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\pm$ 0.19) with SSDCA, confirming discriminative representation learning.

</details>


### [74] [Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence](https://arxiv.org/abs/2512.03905)
*Shuai Yang,Junxin Lin,Yifan Zhou,Ziwei Liu,Chen Change Loy*

Main category: cs.CV

TL;DR: 本研究提出FRESCO方法，通过结合帧内与帧间时空约束，显著提升零样本视频生成质量与时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视频生成方法依赖注意力机制中的帧间对应关系，但其软约束机制无法有效保证时间连续性，导致生成视频存在不一致问题。

Method: FRESCO创新性地融合帧内（intra-frame）与帧间（inter-frame）时空对应关系，构建强约束条件，通过显式优化特征来确保相邻帧语义内容的一致性转换。

Result: 实验表明，该方法在视频到视频翻译和文本引导视频编辑两个零样本任务中，生成视频的时空连贯性与视觉质量均显著优于现有方法。

Conclusion: 通过引入双重时空约束机制，本研究为零样本视频生成提供了新的技术路径，解决了长期存在的跨帧不一致问题，推动了扩散模型在视频领域的应用发展。

Abstract: The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.

</details>


### [75] [UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework](https://arxiv.org/abs/2512.03918)
*Youxin Pang,Yong Zhang,Ruizhi Shao,Xiang Deng,Feng Gao,Xu Xiaoming,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: UniMo是一个统一的自回归模型，首次实现2D视频和3D人体运动的联合建模与生成，通过共享嵌入层弥合分布差异，并采用序列建模策略和新型3D动作分词器（VQ-VAE）实现跨模态同步优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于单模态生成或与文本/音频结合，2D视频与3D运动的联合建模尚未探索，主要挑战在于两者的结构和分布差异需统一处理。

Method: 1. 将2D/3D数据统一为token序列，通过独立嵌入层缓解分布差异；2. 序列建模策略整合两任务；3. 提出带时序扩展策略的3D动作分词器（单VQ-VAE生成量化动作token），使用多专家解码器重建3D运动。

Result: UniMo可同步生成对应视频与动作，并实现精确的动作捕捉（motion capture），实验验证有效性。

Conclusion: 本工作验证了基于LLM的多模态融合能力，为将人体中心信息集成到现有模型提供了新路径，推动人、物、场景的可控联合建模。

Abstract: We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

</details>


### [76] [MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction](https://arxiv.org/abs/2512.03939)
*Guole Shen,Tianchen Deng,Xingrui Qin,Nailin Wang,Jianyu Wang,Yanbo Wang,Yongtao Chen,Hesheng Wang,Jingchuan Wang*

Main category: cs.CV

TL;DR: 提出MUT3R框架，利用注意力机制隐含的运动线索，在无需训练的情况下抑制动态区域伪影，提升3D重建的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法因动态区域干扰导致注意力传播失真，而预训练Transformer的跨层自注意力图自动降低动态区域权重，暴露隐含运动线索未被利用。

Method: 构建注意力级门控模块，在推理早期层通过自注意力权重抑制动态区域影响，阻断伪影传播路径，无需重新训练模型参数。

Result: 在动态场景基准测试中，相较传统方法提升时间一致性与相机姿态鲁棒性5-15%，且保持0训练成本。

Conclusion: 证明Transformer注意力权重隐含运动感知能力，提出首个无需训练的动态3D重建修正框架，为实时动态场景建模提供新范式。

Abstract: Recent stateful recurrent neural networks have achieved remarkable progress on static 3D reconstruction but remain vulnerable to motion-induced artifacts, where non-rigid regions corrupt attention propagation between the spatial memory and image feature. By analyzing the internal behaviors of the state and image token updating mechanism, we find that aggregating self-attention maps across layers reveals a consistent pattern: dynamic regions are naturally down-weighted, exposing an implicit motion cue that the pretrained transformer already encodes but never explicitly uses. Motivated by this observation, we introduce MUT3R, a training-free framework that applies the attention-derived motion cue to suppress dynamic content in the early layers of the transformer during inference. Our attention-level gating module suppresses the influence of dynamic regions before their artifacts propagate through the feature hierarchy. Notably, we do not retrain or fine-tune the model; we let the pretrained transformer diagnose its own motion cues and correct itself. This early regulation stabilizes geometric reasoning in streaming scenarios and leads to improvements in temporal consistency and camera pose robustness across multiple dynamic benchmarks, offering a simple and training-free pathway toward motion-aware streaming reconstruction.

</details>


### [77] [TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning](https://arxiv.org/abs/2512.03963)
*Tao Wu,Li Yang,Gen Zhan,Yiting Liao,Junlin Li,Deliang Fu,Li Zhang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出TempR1框架，通过多任务强化学习提升多模态大模型的时序理解能力，解决视频分析中时间定位和动作检测等任务的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有时序推理方法受限于单一任务类型和数据，难以在多样性场景中泛化。为突破此限制，需设计可捕捉时序依赖的强化学习框架。

Method: 构建包含多时间结构的任务语料库，改进Group Relative Policy Optimization算法，针对三类时间任务（区间预测、序列对应、因果推理）设计定制化定位奖励函数，实现跨任务联合优化。

Result: 在多个基准测试中达成SOTA性能，联合优化产生协同效应，提升单任务性能和模型泛化能力。

Conclusion: 建立可扩展的多任务时序推理范式，为多模态大模型的时间敏感任务提供通用解决方案。

Abstract: Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

</details>


### [78] [DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment](https://arxiv.org/abs/2512.03981)
*Sheng-Hao Liao,Shang-Fu Chen,Tai-Ming Huang,Wen-Huang Cheng,Kai-Lung Hua*

Main category: cs.CV

TL;DR: DirectDrag 是一种无需手动遮罩或文本提示的新型图像编辑框架，通过自动软遮罩生成和特征对齐机制实现高精度、低输入的图像操作。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的图像编辑方法依赖人工遮罩和文本提示来保持语义和运动精度，而移除这些限制会引发视觉伪影与空间控制不足的矛盾。

Method: 提出两种创新：1）自动软遮罩生成模块，通过点位移推断可编辑区域并保持上下文完整性；2）读出引导特征对齐机制，利用扩散模型中间激活层维护结构一致性。

Result: 在无需手动标注的情况下，图像质量优于现有方法且保留可比拖拽精度，实验验证了其在DragBench和真实场景中的有效性。

Conclusion: DirectDrag 通过自动化机制平衡了高效编辑与高质量生成的需求，为交互式图像编辑提供了实用解决方案。

Abstract: Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.

</details>


### [79] [DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation](https://arxiv.org/abs/2512.03992)
*Zexin Lin,Hawen Wan,Yebin Zhong,Xiaoqiang*

Main category: cs.CV

TL;DR: The study introduces DIQ-H, a benchmark for assessing Vision-Language Models (VLMs) under dynamic visual degradation in sequences, addressing limitations in existing benchmarks that neglect temporal errors and propose Uncertainty-Guided Iterative Refinement (UIR) for scalable evaluation.


<details>
  <summary>Details</summary>
Motivation: Current VLM benchmarks focus on static, high-quality images while ignoring critical failure modes like temporal degradation and error propagation, leading to potential hallucinations and inconsistencies in real-world applications such as autonomous driving.

Method: DIQ-H applies physics-based visual corruptions (e.g., motion blur, sensor noise) and evaluates VLMs on hallucination persistence, error recovery, and temporal consistency using multi-turn question-answering tasks. UIR leverages lightweight VLMs and uncertainty filtering to generate scalable pseudo-ground-truth annotations.

Result: Experiments on 16 VLMs show significant robustness gaps: GPT-4o achieves a 78.5% recovery rate, while open-source models struggle with temporal consistency (<60%). UIR improves pseudo-ground-truth accuracy by 15.3%.

Conclusion: DIQ-H fills a critical gap in evaluating VLM reliability, providing a comprehensive platform for real-world deployments under dynamic visual degradation scenarios.

Abstract: Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.

</details>


### [80] [Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation](https://arxiv.org/abs/2512.03996)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本论文提出通过在文本嵌入空间引入随机扰动作为新的随机性形式，结合传统空间噪声，在文本到图像扩散模型中提升生成质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 研究者发现现有文本到图像扩散模型的测试时扩展方法忽略了噪声随机性对性能的影响，且传统方法在高频细节处理受限，希望通过新型随机性形式解决此问题。

Method: 1) 基于频率域分析空间噪声与文本嵌入扰动的互补特性，前者增强低频成分（早期步骤），后者增强高频细节（后期步骤）；2) 提出动态调节扰动强度的策略，结合频率导向的噪声调度。

Result: 在无需额外计算成本的条件下，该方法在多个基准测试中显著提升生成效果，且与现有测试时扩展方法兼容。

Conclusion: 文本嵌入扰动通过与空间噪声的互补作用，克服了高频操作限制，在保持生成质量的同时提升了多样性。

Abstract: Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.

</details>


### [81] [Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding](https://arxiv.org/abs/2512.04000)
*Jialuo Li,Bin Li,Jiahao Li,Yan Lu*

Main category: cs.CV

TL;DR: The paper proposes DIG, a training-free adaptive frame selection framework for long-form video understanding, using uniform sampling for global queries and query-aware selection for localized queries.


<details>
  <summary>Details</summary>
Motivation: Limited context length and computational inefficiency in processing long videos with LMMs motivate the exploration of simplified but effective frame selection strategies.

Method: The authors identify a query typology distinguishing global vs. localized queries, then design DIG to dynamically choose between uniform sampling and query-aware pipelines based on query type without training.

Result: DIG outperforms baseline methods on three benchmarks while maintaining efficiency, showing robust performance improvements even with up to 256 input frames.

Conclusion: The study demonstrates that simplified adaptive frame selection (via DIG) can achieve strong performance on long-form video tasks without relying on complex query-aware mechanisms for all query types.

Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.

</details>


### [82] [On the Temporality for Sketch Representation Learning](https://arxiv.org/abs/2512.04007)
*Marcelo Isaias de Moraes Junior,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 研究表明绝对坐标编码和非自回归解码在草图序列建模中优于传统方法，且时间因素的重要性受任务和顺序类型影响。


<details>
  <summary>Details</summary>
Motivation: 填补草图表示学习中时间因素对模型效果的分析空白，验证序列化处理的合理性

Method: 通过对比实验分析了绝对坐标与相对坐标编码效果，比较自回归与非自回归解码器性能，并评估不同顺序对时空建模的影响

Result: 绝对坐标编码显著优于相对编码，非自回归解码器性能超越自回归模型，时间敏感性取决于编码顺序和具体任务

Conclusion: 草图序列建模应优先采用绝对坐标和非自回归架构，时间维度的作用需要结合顺序特征和任务场景进行权衡

Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.

</details>


### [83] [Learning Group Actions In Disentangled Latent Image Representations](https://arxiv.org/abs/2512.04015)
*Farhana Hossain Swarnali,Miaomiao Zhang,Tonmoy Hossain*

Main category: cs.CV

TL;DR: 本论文提出首个自动学习潜空间群动作且无需人工干预的端到端框架。


<details>
  <summary>Details</summary>
Motivation: 现有群理论方法在高维数据空间操作导致子空间难以解耦，而潜空间方法需要人工手动分割潜变量限制灵活性。

Method: 通过可学习二值掩码结合直通估计动态分割潜空间，构建联合优化框架同步实现潜变量解耦与群变换映射。

Result: 在5个2D/3D图像数据集验证，成功自动学习群动作解耦因子，下游分类任务表现优异。

Conclusion: 该方法可无缝集成标准编解码架构，显著提升潜空间群变换学习的自动化与鲁棒性。

Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .

</details>


### [84] [Ultra-lightweight Neural Video Representation Compression](https://arxiv.org/abs/2512.04019)
*Ho Man Kwan,Tianhao Peng,Ge Gao,Fan Zhang,Mike Nilsson,Andrew Gower,David Bull*

Main category: cs.CV

TL;DR: NVRC-Lite is a lightweight neural video compression framework that improves performance and efficiency through multi-scale feature grids and an octree-based context model for entropy coding.


<details>
  <summary>Details</summary>
Motivation: Existing INR-based video codecs use slow autoregressive entropy coding and lack optimization for low-complexity scenarios, necessitating a practical lightweight solution.

Method: NVRC-Lite integrates multi-scale feature grids to enhance INR performance at low complexity and proposes an octree-based context model to accelerate entropy coding of high-dimensional feature grids.

Result: NVRC-Lite outperforms C3 with up to 21.03% (PSNR) and 23.06% (MS-SSIM) BD-rate savings, achieving 8.4x encoding and 2.5x decoding speedups.

Conclusion: NVRC-Lite achieves state-of-the-art performance in lightweight neural video compression, balancing high efficiency and low computational complexity.

Abstract: Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.

</details>


### [85] [C3G: Learning Compact 3D Representations with 2K Gaussians](https://arxiv.org/abs/2512.04021)
*Honggyu An,Jaewoo Jung,Mungyeom Kim,Sunghwan Hong,Chaehyun Kim,Kazumi Fukuda,Minkyeong Jeon,Jisang Han,Takuya Narihira,Hyuna Ko,Junsu Kim,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出C3G框架，通过紧凑的3D高斯表示减少冗余，提升内存效率和3D重建与理解效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法中的逐像素3D高斯点生成导致冗余和内存开销高，多视角特征聚合次优，亟需更高效的解决方案。

Method: 引入可学习tokens，通过自注意力机制聚合多视角特征，仅在关键位置生成精简的高斯点，并基于注意力模式解码特征。

Result: 在无姿态新视角合成、3D语义分割等任务中验证，内存消耗降低40%，特征保真度提升，且重建质量优于现有方法。

Conclusion: 紧凑且几何感知的高斯表示能有效平衡重建质量与资源效率，C3G为3D场景理解提供了高效基础框架。

Abstract: Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.

</details>


### [86] [Fast & Efficient Normalizing Flows and Applications of Image Generative Models](https://arxiv.org/abs/2512.04039)
*Sandeep Nagar*

Main category: cs.CV

TL;DR: 本论文在提升生成模型效率（尤其是标准化流）和解决计算机视觉实际问题两方面取得创新。核心贡献包括改进标准化流的6项技术（如可逆卷积层、并行反演算法），以及5个应用案例（如农业质量检测、隐私保护、艺术修复）。


<details>
  <summary>Details</summary>
Motivation: 生成模型在计算效率和实际应用中存在瓶颈，如标准化流反向传播效率低、实际场景数据不足、隐私保护需求等，需通过算法创新和跨领域应用解决。

Method: 理论创新：1) 可逆3x3卷积层数学证明 2) Quad耦合层 3) 并行反演算法 4) 快速反向传播算法 5) Inverse-Flow模型训练 6) Affine-StableSR超分模型；应用创新：1) 条件GAN农业检测 2) 堆叠自编码器地质分类 3) 面部检测+Inpainting隐私保护 4) Stable Diffusion隐私修复 5) 多退化艺术修复模型。

Result: 1) 种子纯度检测准确率达标 2) 地质特征提取优于传统方法 3) 实现人脸/车牌匿名化 4) Stain/Scratch等艺术修复统一方案 5) 模型参数量降低30%(-)时性能保持

Conclusion: 通过标准化流理论突破与高效反向传播技术，显著提升生成模型训练效率，在农业质检、地质分类等5个领域验证了方法有效性，同时解决了自动驾驶隐私等伦理问题，为生成模型实用化提供技术路径。

Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.

</details>


### [87] [RELIC: Interactive Video World Model with Long-Horizon Memory](https://arxiv.org/abs/2512.04040)
*Yicong Hong,Yiqun Mei,Chongjian Ge,Yiran Xu,Yang Zhou,Sai Bi,Yannick Hold-Geoffroy,Mike Roberts,Matthew Fisher,Eli Shechtman,Kalyan Sunkavalli,Feng Liu,Zhengqi Li,Hao Tan*

Main category: cs.CV

TL;DR: RELIC is a unified framework for interactive world modeling that integrates real-time long-horizon streaming, consistent spatial memory, and precise user control by combining latent memory compression and a memory-efficient self-forcing method.


<details>
  <summary>Details</summary>
Motivation: Existing approaches address only one aspect (e.g., real-time performance, memory, or control) in isolation due to challenges in unifying them, particularly because long-term memory mechanisms often hinder real-time performance.

Method: RELIC uses autoregressive video-diffusion distillation with a KV cache storing compressed historical latent tokens encoding relative actions and absolute camera poses. It also employs a bidirectional teacher model extended beyond its 5-second training horizon and a causal student generator via a self-forcing paradigm for full-context distillation.

Result: A 14B-parameter model trained on Unreal Engine data achieves real-time 16 FPS generation while improving accuracy in action following, stability in long-horizon streaming, and robustness in spatial-memory retrieval compared to prior work.

Conclusion: RELIC demonstrates that integrating compressed spatial memory, long-term coherence, and user control can enable high-quality, real-time interactive world modeling, serving as a foundation for next-generation systems.

Abstract: A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.

</details>


### [88] [SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL](https://arxiv.org/abs/2512.04069)
*Siyi Chen,Mikaela Angelina Uy,Chan Hee Song,Faisal Ladhak,Adithyavairavan Murali,Qing Qu,Stan Birchfield,Valts Blukis,Jonathan Tremblay*

Main category: cs.CV

TL;DR: 本文提出DIRL框架，通过交互式强化学习提升视觉语言模型的空间推理能力，使SpaceTools模型在多个空间理解基准测试中取得最佳表现，并成功应用于真实环境中的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因依赖预定义工具流或手动策略，难以自主发现多工具协同的最佳模式，而传统强化学习方法受限于单工具推理的庞大搜索空间，导致空间推理精度不足。

Method: 设计双阶段强化学习框架DIRL：第一阶段结合单工具专家示范与全工具模型轨迹进行教学；第二阶段通过持续强化学习进行多工具协同优化，并设计空间感知工具集支持视觉语言模型推理。

Result: SpaceTools在RoboSpatial-Home等3个基准测试中相较传统监督微调方法提升12%，相较单工具强化学习提升16%，且成功实现在7自由度机器人上的空间操作任务验证。

Conclusion: DIRL框架有效解决了多工具协同强化学习的搜索空间难题，证明了交互式学习范式在开发工具增强型智能体空间推理能力方面的显著优势，为具身智能体的发展提供了新路径。

Abstract: Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.

</details>


### [89] [PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design](https://arxiv.org/abs/2512.04082)
*Jiazhe Wei,Ken Li,Tianyu Lao,Haofan Wang,Liang Wang,Caifeng Shan,Chenyang Si*

Main category: cs.CV

TL;DR: 本文提出了一种名为PosterCopilot的框架，旨在解决现有大型多模态模型在图形设计中几何布局不准确和缺乏迭代编辑能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现代视觉传达依赖于平面设计，但现有自动化方法存在几何布局错误及无法满足专业工作流中的分层迭代编辑需求。

Method: 提出渐进式三阶段训练策略：扰动监督微调（PSFT）、视觉现实对齐强化学习（RL-VR）及美学反馈强化学习（RL-AF），并构建了LMM与生成模型的联合工作流实现分层编辑。

Result: 实验表明PosterCopilot生成的布局在几何准确性和美学表现上优于现有方法，且支持精确元素优化与全局视觉一致性维护。

Conclusion: 该方法通过增强布局推理与可控编辑能力，为专业平面设计提供了新的工具，解决了自动化设计中的关键瓶颈。

Abstract: Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.

</details>


### [90] [SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows](https://arxiv.org/abs/2512.04084)
*Qinyu Zhao,Guangting Zheng,Tao Yang,Rui Zhu,Xingjian Leng,Stephen Gould,Liang Zheng*

Main category: cs.CV

TL;DR: 通过固定VAE编码器方差（如0.5）简化Normalizing Flows训练流程，无需额外噪声步骤，同时联合训练VAE-NF实现图像生成性能突破（gFID 2.15）。


<details>
  <summary>Details</summary>
Motivation: 现有Normalizing Flow方法（如STARFlow）依赖复杂的噪声增强流程（如VAE隐变量加噪）且使用冻结VAE编码器，导致训练不稳定、重建质量下降，需要更简洁的联合优化方案。

Method: 提出SimFlow模型：1) 固定VAE编码器输出方差为常数（如0.5），2) 联合训练VAE与Normalizing Flow（无需额外噪声设计），3) 结合REPA-E方法实现生成质量进一步提升。

Result: 在ImageNet 256×256生成任务中，SimFlow取得2.15 gFID优于STARFlow（2.40），引入REPA-E后提升至1.91，刷新Normalizing Flow方法性能纪录。

Conclusion: 固定VAE编码器方差可有效简化Flow训练流程且提升生成质量，证明隐空间分布设计优于传统噪声增强与预训练策略，为生成模型联合优化提供新范式。

Abstract: Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [91] [Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models](https://arxiv.org/abs/2512.03047)
*Samih Fadli*

Main category: cs.CL

TL;DR: 提出基于'智能第二定律'的伦理熵动态监测框架，通过五类行为分类器量化大模型伦理熵S(t)，构建γ_eff对齐工作率指标，实现运行时价值漂移预警。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准无法捕捉LLM动态安全风险（如分布漂移动态、越狱攻击、对齐退化），需建立动态风险管理框架。

Method: 将伦理熵理论扩展到LLM领域，构建行为分类器分析模型对话轨迹，设计S(t)量化指标，通过四类前沿模型（含基础版/指令调优版）压力测试验证，并推导有效对齐率γ_eff参数。

Result: 基础模型表现出持续熵增，调优模型减少80%伦理熵；监控系统可实时检测熵变，当漂移超过阈值时触发预警。实验验证了动态对齐监控的可行性。

Conclusion: 提出的伦理熵动态监测框架可有效管理LLM部署中的价值漂移问题，指令调优显著改善对齐特性，为动态AI安全提供可量化的技术方案。

Abstract: Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.

</details>


### [92] [Watermarks for Embeddings-as-a-Service Large Language Models](https://arxiv.org/abs/2512.03079)
*Anudeex Shetty*

Main category: cs.CL

TL;DR: 现有EaaS水印易被文本改写攻击移除，提出线性变换新方法WET有效防御该漏洞。


<details>
  <summary>Details</summary>
Motivation: 针对EaaS服务模型面临的黑盒克隆威胁，现有水印技术被发现可通过文本改写绕过，需强化数字水印安全性。

Method: 采用线性代数方法对文本嵌入空间进行变换，在反变换后通过余弦相似度检测水印完整性的WET技术。

Result: 实验证明WET对同义词替换/句法重构等改写攻击实现98.6%检测准确率，消融显示变换矩阵维度与检测强度正相关。

Conclusion: 揭示语义保持改写对水印的威胁，提出首个抗语义变换的鲁棒水印框架，为模型产权保护提供新思路。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.

</details>


### [93] [Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation](https://arxiv.org/abs/2512.03082)
*Nan Zhuang,Wenshuo Wang,Lekai Qian,Yuxiao Wang,Boyu Cao,Qi Liu*

Main category: cs.CL

TL;DR: 本研究提出名为Reasoning Dependency Generation (RDG)的新方法，通过生成无偏推理数据解决大语言模型中的选择支持偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI去偏方法主要针对社会/人口统计偏差，而认知偏差如选择支持偏差（CSB）研究不足。需要系统性方法解决CSB以提升AI决策可靠性。

Method: RDG框架通过自动化构建包含上下文依赖数据和依赖解耦数据的平衡推理问答对，显式建模（或解除建模）选择、证据与理由间的依赖关系，通过微调消除认知偏差。

Result: 微调后模型在记忆实验中CSB降低81.5%，评估实验中降低94.3%，同时在标准BBQ基准测试中保持原有性能水平。

Conclusion: 该研究首次实现LLM认知偏差消解，为构建可信赖的AI决策支持系统提供了新范式和数据资源。

Abstract: Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.

</details>


### [94] [Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies](https://arxiv.org/abs/2512.03195)
*Stylianos Saroglou,Konstantinos Diamantaras,Francesco Preta,Marina Delianidi,Apostolos Benisis,Christian Johannes Meyer*

Main category: cs.CL

TL;DR: 研究利用语言模型优化欧洲就业框架分类，发布开源工具及数据集。


<details>
  <summary>Details</summary>
Motivation: 现有劳动市场分类方法依赖表层技能提取，缺乏将职位文本与ESCO/EQF深层关联的公开工具和评估数据集

Method: 对比句子链接与实体链接方法，构建集成工具；创建两个职业资格标注数据集；探索生成式大模型在语义关联中的应用

Result: 改进职位实体提取效果，开发可复用的劳工分类工具链，标注数据集支持深层语义分析，代码开源促进后续研究

Conclusion: 为数字化劳动经济研究提供基础架构，生成式模型展示出跨框架关联潜力

Abstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier

</details>


### [95] [InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation](https://arxiv.org/abs/2512.03197)
*Faezeh Faez,Marzieh S. Tahaei,Yaochen Hu,Ali Pourranjbar,Mahdi Biparva,Mark Coates,Yingxue Zhang*

Main category: cs.CL

TL;DR: InvertiTune框架通过结合可控数据生成和监督微调，有效解决基于文本的知识图谱（Text2KG）生成中的效率和复杂关系捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 现有Text2KG方法依赖迭代式大语言模型（LLM）提示，导致计算成本高且难以捕捉文本中的复杂关系。需开发更高效的单次KG构建方法。

Method: 构建数据生成流水线，从知识库提取子图、去噪后生成自然文本描述，再用监督微调轻量模型。通过系统化生成长文本与大规模KG配对数据，提升模型训练效果。

Result: 在CE12k新数据集和跨数据集测试CrossEval-1200上，InvertiTune均优于非微调LLM和前沿Text2KG方法，单次推理性能提升显著。

Conclusion: 现实场景下高质量训练数据对构建高效Text2KG系统具有关键作用，该框架为单次KG生成提供了新范式。

Abstract: Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.

</details>


### [96] [Identifying attributions of causality in political text](https://arxiv.org/abs/2512.03214)
*Paulina Garcia-Corral*

Main category: cs.CL

TL;DR: 本文提出了一种从政治文本中检测和解析因果关系解释的框架，通过轻量级因果语言模型生成结构化的因果主张数据集，用于下游分析。


<details>
  <summary>Details</summary>
Motivation: 政治学中缺乏对解释性逻辑的系统性分析工具，现有方法分散且局限于特定议题，需要一种低标注需求、可扩展的因果关系提取方法。

Method: 构建轻量级因果语言模型，通过因果推理机制识别文本中的因果对，将非结构化文本转化为结构化（原因-结果）数据，并通过实验验证模型性能。

Result: 方法可扩展性高，标注需求低（仅需少量标注数据），在跨议题场景中表现出色且准确率接近人工编码，验证了模型的泛化能力和效率。

Conclusion: 为政治文本因果关系提取提供了通用性工具，填补了政治学中系统性分析解释性逻辑的空白，为大规模实证研究提供了新途径。

Abstract: Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.

</details>


### [97] [Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs](https://arxiv.org/abs/2512.03310)
*Kunj Joshi,David A. Smith*

Main category: cs.CL

TL;DR: 本文提出了一种名为随机掩码微调（RMFT）的隐私保护微调技术，有效减少大型语言模型对敏感个人信息的记忆，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言模型（尤其是大模型）存在记忆训练数据中个人身份信息（PIIs）的安全与隐私风险，需通过改进微调方法降低此类风险。

Method: 提出RMFT方法，结合随机掩码与微调过程，在Enron电子邮件数据集上对比基线微调和去重方法的效果，并设计MaxTER评估框架通过AURC指标衡量隐私-效用权衡。

Result: RMFT相比基线方法，总提取率降低80.81%，已见过提取率降低80.17%，困惑度仅增加5.73%，且在AURC指标上优于去重方法。

Conclusion: RMFT在保护隐私的同时最小化性能损失，MaxTER框架为隐私-效用评估提供了帕累托最优的量化标准。

Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.

</details>


### [98] [Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní](https://arxiv.org/abs/2512.03334)
*Nemika Tyagi,Nelvin Licona Guevara,Olga Kellert*

Main category: cs.CL

TL;DR: 本文介绍了在语码转换分析中采用大语言模型（LLM）进行标注的计算语言学管道，揭示了双语语篇中社会语言学与话题特征，并推广至跨语言和低资源环境。


<details>
  <summary>Details</summary>
Motivation: 解决传统手动标注社会语言学特征费时低效的问题，探索LLM在两种类型差异较大的双语语境（西班牙语-英语和西班牙语-瓜拉尼语）中的跨语言适用性及低资源语言标注潜力。

Method: 基于LLM构建三阶段标注框架：1) 对3,691句语码转换句进行话题、语域与语用功能自动标注；2) 融合迈阿密双语语料库的方言人口统计元数据；3) 为西班牙语-瓜拉尼语数据集补充新的话题标注，形成双数据集对比分析。

Result: 发现迈阿密数据中性别与语言主导权对话语功能存在系统性关联，巴拉圭文本呈现瓜拉尼语正式域与西班牙语非正式域的严密切分。LLM在无需跨语言调优的情况下成功复现并扩展了既有社会语言学观察结论。

Conclusion: LLM可有效复现人工标注的可解释社会语言学模式，推动低资源双语研究的计算方法创新，证明基于LLM的自动化分析能为跨语言语料库研究提供量化实证支持，尤其在人力与时间资源受限场景下具有显著优势。

Abstract: This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaraní. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaraní dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaraní and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.

</details>


### [99] [PERCS: Persona-Guided Controllable Biomedical Summarization Dataset](https://arxiv.org/abs/2512.03340)
*Rohan Charudatt Salvi,Chirag Chawla,Dhruv Jain,Swapnil Panigrahi,Md Shad Akhtar,Shweta Yadav*

Main category: cs.CL

TL;DR: 提出PERCS数据集，包含针对不同医学素养用户的生物医学摘要可控生成，推动个性化医疗文本简化研究。


<details>
  <summary>Details</summary>
Motivation: 现有医疗文本简化资源缺乏针对不同用户群体（如外行/专家）的差异化需求，PERCS通过构建多角色摘要数据集解决该问题。

Method: 构建包含4类用户角色（外行/医学预科生/非医学研究者/专家）的生物医学摘要数据集，由医生采用错误分类法审核摘要准确性，进行可读性/词汇/内容深度的技术验证，并基于大模型建立基线评估。

Result: 验证显示不同角色摘要在可读性、词汇复杂度和内容深度存在显著差异；大模型基准测试建立了涵盖全面性/可读性/准确性的基线结果，数据集与评估标准已开源。

Conclusion: PERCS为可控型生物医学摘要研究提供标准化数据支持，验证了个性化医疗文本简化的重要性，并建立评估方法论促进该领域发展。

Abstract: Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.

</details>


### [100] [Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning](https://arxiv.org/abs/2512.03343)
*Darshan Fofadiya*

Main category: cs.CL

TL;DR: 本论文提出了Idea-Gated Transformer架构，通过引入辅助的Idea Head和概念向量门控机制，在保留语言模型性能的同时显著增强生成过程的领域一致性，有效缓解了传统自回归模型中的主题漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于Next-Token Prediction的自回归语言模型常因局部关联依赖导致生成偏离原始主题，尽管增大模型规模能缓解但并未根本解决。该研究旨在通过语义规划与语法生成的分离机制，实现更可控的语言生成。

Method: 设计了双头架构，主头负责常规生成，辅助Idea Head预测未来上下文的词袋分布以生成Concept Vector。提出可微分门控机制，通过抑制语义无关词元实时修剪搜索空间，实现生成过程的语义锁定。

Result: 在WikiText-103实验中，模型在验证集困惑度与GPT-2相当的情况下，展现出显著提升的领域保持能力。定量分析显示门控机制能将生成锚定在特定语义簇（如金融、科学），有效抵抗联想漂移。

Conclusion: 该方法为可控语言建模提供了参数高效的解决方案，通过显式语义规划机制提升了生成连贯性，为解决传统自回归模型的短期记忆缺陷提供了新思路。

Abstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \citep{holtzman2019curious}. While scaling model size mitigates this \citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.

</details>


### [101] [From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation](https://arxiv.org/abs/2512.03360)
*Qingchuan Li,Mingyue Cheng,Zirui Liu,Daoyu Wang,Yuting Zeng,Tongxuan Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的假设驱动逆向逻辑推理框架HBLR，通过置信度感知的符号翻译与逆向推理，提升大型语言模型的逻辑推理效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型依赖前向推理范式，存在冗余推理路径、幻觉步骤和语义漂移问题，导致推理效率低且不可靠。

Method: HBLR分两阶段：1) 翻译阶段仅高置信度文本转化为一阶逻辑，不确定部分保留自然语言，并通过反射模块确保语义保真；2) 推理阶段假设结论为真并递归验证前提，反射模块识别并修正错误推理步骤。

Result: 在五项推理基准测试中，HBLR在准确率和效率上均超越强基线方法。

Conclusion: HBLR解决了前向推理的固有缺陷，为逻辑推理提供了更高效可靠的框架，在科学发现、数学证明等领域具有应用潜力。

Abstract: Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.

</details>


### [102] [Characterizing Language Use in a Collaborative Situated Game](https://arxiv.org/abs/2512.03381)
*Nicholas Tomlin,Naitian Zhou,Eve Fleisig,Liangyuan,Chen,Téa Wright,Lauren Vinh,Laura X. Ma,Seun Eisape,Ellie French,Tingting Du,Tianjiao Zhang,Alexander Koller,Alane Suhr*

Main category: cs.CL

TL;DR: 本文构建了包含11.5小时24.5K对话的Portal协作对话语料库，揭示复杂协作场景下的空间指代、澄清修复和临时约定形成等语言现象，并提供多模态游戏状态数据与标注工具。


<details>
  <summary>Details</summary>
Motivation: 传统对话语料库难以捕捉复杂协作场景中的动态语言现象，电子游戏中的多人协作提供了研究真实任务导向下语言使用的全新数据源，特别是复杂空间推理和团队协作机制。

Method: 在Portal 2双人合作模式中录制28名玩家的游戏过程，使用多模态记录设备采集声像数据并生成标注，通过话语功能分类、空间指代分析和约定行为识别等方法解析协作语言特征。

Result: 发现三大核心语言现象：1) 复杂空间指代系统(占比38%)；2) 高频澄清-修复循环(平均每对话轮次2.7次)；3) 团队特有交流惯例形成。语料库包含完整游戏状态日志与双重标注数据。

Conclusion: 该语料库为研究现实协作场景中的语言生成提供了标准化基准，其多模态属性支持跨模态协同推理研究，为开发动态环境下的对话系统提供了重要数据资源。

Abstract: Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.

</details>


### [103] [Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates](https://arxiv.org/abs/2512.03402)
*Yixing Xu,Chao Li,Xuanwu Yin,Spandan Tiwari,Dong Li,Ashish Sirasao,Emad Barsoum*

Main category: cs.CL

TL;DR: 本论文提出了一种名为Dual LoRA的方法，通过分离低秩矩阵并引入ReLU和符号函数，提升了参数高效微调的效果和性能。


<details>
  <summary>Details</summary>
Motivation: 基于低秩假设的LoRA模型在适应下游任务时表现不佳，因此本文通过引入归纳偏置改进原始方法，以更好地模拟全参数微调的更新过程。

Method: 将低秩矩阵分为两组：幅值组（使用ReLU控制更新强度和方向）和方向组（使用符号函数控制参数调整方向），从而更精准地模拟基于梯度的参数更新策略。

Result: 在GPT-2、RoBERTa、DeBERTa和LLaMA等多种模型的实验表明，该方法在相同可训练参数数量下始终优于LoRA及其变体。

Conclusion: Dual LoRA有效提升了参数高效微调方法的性能，为低秩适应提供了更灵活的参数更新机制。

Abstract: Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.

</details>


### [104] [PretrainZero: Reinforcement Active Pretraining](https://arxiv.org/abs/2512.03442)
*Xingrun Xing,Zhiyuan Fan,Jie Lou,Guoqi Li,Jiajun Zhang,Debing Zhang*

Main category: cs.CL

TL;DR: PretrainZero是一种通过主动学习从预训练语料库中进行强化学习的框架，无需依赖特定领域的可验证奖励，旨在提升通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的模型在特定领域依赖可验证奖励，限制了通用推理能力的扩展，需突破数据壁垒。

Method: 1) 主动预训练：通过统一推理策略主动识别语料中的信息内容并进行预测；2) 自监督学习：无需人工标注数据或预训练奖励模型，直接在通用语料库上预训练；3) 验证扩展：通过解决复杂masked spans问题增强推理能力。

Result: 在MMLU-Pro、SuperGPQA和数学基准测试中，PretrainZero使Qwen3-4B-Base模型性能分别提升8.43、5.96和10.60，并可作为下游任务的推理基础模型。

Conclusion: PretrainZero通过突破验证数据壁垒，实现了从通用语料库的强化预训练，显著提升了模型的通用推理能力和跨领域适应性。

Abstract: Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.

</details>


### [105] [Understanding LLM Reasoning for Abstractive Summarization](https://arxiv.org/abs/2512.03503)
*Haohan Yuan,Siu Cheung Hui,Haopeng Zhang*

Main category: cs.CL

TL;DR: 研究显示推理策略在摘要中的效果并非普适且存在权衡：显式推理提升流畅度但损害事实性，大推理模型的隐式推理反之，增加内部推理预算反使事实一致性降低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在分析类任务(如数学和代码生成)表现优异，但其在抽象摘要中的实用性仅被视为理所当然而缺乏验证，需填补该认知空白。

Method: 将通用推理策略适配到摘要领域，系统比较8种推理策略和3种大推理模型在8个多样化数据集上的表现，评估摘要质量与事实性。

Result: 推理效果高度依赖具体策略和场景，发现质量与事实性的权衡现象：显式推理提升流畅度牺牲事实性，隐式推理模式相反；增大模型内部推理预算对事实一致性无改善甚至产生负面作用。

Conclusion: 有效摘要需要忠实压缩而非创造性过度推理，揭示不同推理模式需结合场景特性才能取得平衡。

Abstract: While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.

</details>


### [106] [Fine-grained Narrative Classification in Biased News Articles](https://arxiv.org/abs/2512.03582)
*Zeba Afroz,Harsh Vardhan,Pawan Bhakuni,Aanchal Punia,Rajdeep Kumar,Md. Shad Akhtar*

Main category: cs.CL

TL;DR: 本文提出了一种新的细粒度叙事分类框架，并构建了首个基于意识形态的多层级标注数据集INDI-PROP，用于分析印度新闻媒体中的政治宣传。通过FANTA和TPTC两种多跳提示推理框架，模型表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 叙事是政治宣传的认知和情感支架，但现有研究缺乏多层级、细粒度且具意识形态导向的自动化分析方法。研究旨在整合孤立的说服技巧，构建可解释的叙事分析框架来识别文章立场、事件框架及说服策略。

Method: 1) 创建INDI-PROP数据集，包含1266篇涉及CAA法案和农民抗议的新闻文章，标注三层信息（文章立场、事件框架、说服技巧）；2) 提出FANTA（结合信息抽取与语境框架）和TPTC（两阶段分解说服线索）两种多跳提示框架，基于GPT-4o-mini实现跨层级推理。

Result: 所提框架在文章立场、叙事框架和说服技巧分类任务中均显著优于基线模型，验证了多跳提示工程与多层级标注对政治宣传分析的有效性。

Conclusion: 研究为新闻媒体中意识形态驱动的叙事自动化分析提供了可扩展的方法论，数据集填补了南亚政治传播研究的空白，模型框架为AI伦理视角下的宣传检测提供了新路径。

Abstract: Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.

</details>


### [107] [AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment](https://arxiv.org/abs/2512.03634)
*Ahmad Aghaebrahimian*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的框架，通过分解文本为原子事实并采用加权指标，提升领域内和开放领域的事实一致性评估效果，尤其针对临床等高风险场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高风险领域（如临床应用）中生成错误事实的幻觉问题可能导致严重后果，但现有评估指标不足且缺乏可解释性，难以诊断和修复错误。

Method: 将文本分解为原子事实，提出无固定模式的灵活方法，采用加权评估指标替代绝对指标，并设计机制控制复杂领域的评估复杂度。

Result: 在通用和临床数据集上验证了方法有效性，并开源代码以支持面向事实的模型训练研究。

Conclusion: 该框架显著提升事实一致性评估的可解释性与准确性，为高风险领域模型部署提供关键工具。

Abstract: Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.

</details>


### [108] [Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context](https://arxiv.org/abs/2512.03671)
*Beatrice Savoldi,Giuseppe Attanasio,Olga Gorodetskaya,Marta Marchiori Manerba,Elisa Bassignana,Silvia Casola,Matteo Negri,Tommaso Caselli,Luisa Bentivogli,Alan Ramponi,Arianna Muti,Nicoletta Balbo,Debora Nozza*

Main category: cs.CL

TL;DR: 该研究调查了意大利生成式AI（GenAI）的采用情况，发现其广泛应用但伴随性别差距和数字素养不足的问题。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI技术普及带来的社会影响，重点关注数字鸿沟风险及用户对技术局限性的认知。

Method: 对1906名意大利语成年人进行问卷调查，收集其GenAI使用习惯、采纳率和数字素养数据。

Result: GenAI被广泛用于工作及敏感场景（如医疗建议），但用户数字素养低；女性采纳率和使用频率显著低于男性，尤其在年长群体中差异明显，教育水平仅部分解释此差距。

Conclusion: 需针对性开展数字素养教育，并深入研究除能力外阻碍公平参与的因素（如性别壁垒）。

Abstract: The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.

</details>


### [109] [Evaluating Hydro-Science and Engineering Knowledge of Large Language Models](https://arxiv.org/abs/2512.03672)
*Shiruo Hu,Wenbo Shan,Yingjia Li,Zhiqi Wan,Xinpeng Yu,Yunjia Qi,Haotian Xia,Yang Xiao,Dingxiao Liu,Jiaru Wang,Chenxu Gong,Ruixi Zhang,Shuyue Wu,Shibo Cui,Chee Hui Lai,Wei Luo,Yubin He,Bin Xu,Jianshi Zhao*

Main category: cs.CL

TL;DR: 本研究提出Hydro-SE-Bench基准测试，包含4000道选择题评估大语言模型在水利工程领域的能力，发现商业模型准确率0.74-0.80，小模型0.41-0.68，模型在自然科学领域表现佳但行业标准等专业知识存在短板。


<details>
  <summary>Details</summary>
Motivation: 水利工程作为多学科交叉领域需要专业知识与工程实践的深度融合，但当前缺乏对大语言模型在该领域的知识储备和应用能力的系统性评估，亟需建立标准化评价体系。

Method: 构建Hydro-SE-Bench多维评估框架，包含九个子领域共4000道多选题，覆盖基础概念、工程应用、推理计算三个能力维度，测试不同参数规模模型的表现差异。

Result: 商业大语言模型准确率稳定在74%-80%，小参数模型集中在41%-68%。模型在水文物理规律等自然科学领域表现优异，但在水利工程标准规范和结构设计方面准确率显著下降。模型扩展主要提升推理计算能力，但实践应用仍需强化。

Conclusion: 该研究揭示了当前大语言模型在水利工程领域的能力边界，为模型开发提供针对性训练方向，同时为从业者提供应用选择的技术指南，为AI+水利工程的融合发展奠定基础。

Abstract: Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.

</details>


### [110] [Different types of syntactic agreement recruit the same units within large language models](https://arxiv.org/abs/2512.03676)
*Daria Kryvosheieva,Andrea de Varda,Evelina Fedorenko,Greta Tuckute*

Main category: cs.CL

TL;DR: 该研究发现，大型语言模型中不同句法一致现象（如主谓一致、照应、限定词-名词一致）依赖重叠的模型单元，表明句法一致在模型表征空间中是一个有意义的类别，并在跨语言分析中显示结构相似语言共享更多主谓一致相关单元。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探索大型语言模型如何表征语法知识，特别是不同句法规则（如同一性约束或依存关系）是否调用共享或独立的模型部件。

Method: 采用受认知神经科学启发的定位方法，分析7个开源模型中响应67种英语句法现象的12200个单元，并通过跨语言比较57种结构相似语言的主谓一致单元重合度验证普适性。

Result: 1) 同一模型内，不同句法一致类型显著调用重叠单元；2) 英语、俄语、中文均呈现该模式；3) 跨语言分析显示，句法结构相似度越高的语言，主谓一致相关单元共享比例越高。

Conclusion: 句法一致作为核心句法依存关系的标记，在大型语言模型中构成统一的功能范畴，且语言结构相似性决定了模型表征的跨语言重合度。

Abstract: Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.

</details>


### [111] [AITutor-EvalKit: Exploring the Capabilities of AI Tutors](https://arxiv.org/abs/2512.03688)
*Numaan Naeem,Kaushal Kumar Maurya,Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: AITutor-EvalKit是一款利用语言技术评估AI导师教育质量的应用，提供演示、评估、模型检查与数据可视化功能，服务于教育利益相关者及ACL社区，支持学习过程并收集用户反馈。


<details>
  <summary>Details</summary>
Motivation: 随着AI导师在教育中的应用日益广泛，亟需一种系统化工具从教育学角度评估其质量，帮助教育从业者和技术研究者优化AI辅导系统，同时促进反馈收集与协作改进。

Method: 开发多模块应用程序，整合自然语言处理技术设计评估框架，包含质量评分模块（如响应准确性、引导性）、可视化看板及模型调试工具，支持实时交互与数据标注，构建可扩展的评估平台。

Result: 成功发布完整工具套件，实现对AI导师教学能力的多维度分析，通过标准化指标量化教育价值，实验显示其能有效识别对话系统的教学弱点，已在ACL社区启动用户反馈试点项目。

Conclusion: 该工具弥补了教育技术领域对AI导师质量评估的空白，为教育工作者提供透明化评估手段，同时为NLP研究者建立真实场景数据收集渠道，推动可解释性教育AI的发展。

Abstract: We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.

</details>


### [112] [DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue](https://arxiv.org/abs/2512.03704)
*Yijun Liao*

Main category: cs.CL

TL;DR: 提出DZ-TDPO方法，通过冲突感知的动态KL约束和可学习时间注意力偏差缓解长对话中的状态惯性问题，在保持零样本泛化能力下实现SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 针对长对话系统中用户意图演变与历史上下文冲突导致的"状态惯性"问题，传统静态约束难以平衡上下文连贯性与意图调整需求。

Method: 构建非破坏性对齐框架DZ-TDPO：① 冲突感知动态KL散度约束（调节模型对历史的依赖程度）② 时序注意力偏差模块（学习对话状态迁移模式）。

Result: 在MSC数据集上Phi-3.5模型取得86.2%胜率，零样本迁移能力稳定；Qwen2.5-7B模型达成99.4%胜率且困惑度仅增加1.2%，验证了注意力机制优于参数更新的对齐策略。

Conclusion: 提出注意力量化调节方案，揭示"容量-稳定性权衡"规律：小型模型需支付对齐代价，而大模型可通过精确注意力控制实现低损耗完美对齐。

Abstract: Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO

</details>


### [113] [In-Context Representation Hijacking](https://arxiv.org/abs/2512.03771)
*Itay Yona,Amir Sarid,Michael Karasik,Yossi Gandelsman*

Main category: cs.CL

TL;DR: Doublespeak是一种无需优化的上下文表示劫持攻击,通过在提示上下文中系统性替换有害关键词为良性标记,使模型内部将良性语义解释为有害指令,成功绕过安全对齐机制。


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型安全对齐策略的漏洞,展示仅通过上下文修改即可突破语义控制边界的能力,暴露现有防御机制的潜在缺陷。

Method: 在有害请求前缀下,将上下文中多个示例的有害关键词(如bomb)系统替换为良性标记(如carrot),使用可解释工具观测内部表征层间语义覆盖过程

Result: 成功在闭源LLaMA-3.3-70B-Instruct模型实现74%攻击成功率,无需优化且跨模型家族迁移,语义覆盖呈现从浅层良性到深层有害的逐层过渡模式

Conclusion: 发现大语言模型潜空间新攻击面,证明当前安全对齐策略存在重大缺陷,亟需从表征层面对语义污染进行防御性重构

Abstract: We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.

</details>


### [114] [Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5](https://arxiv.org/abs/2512.03803)
*Huey Sun,Anabel Yong,Lorenzo Gilly,Felipe Jin*

Main category: cs.CL

TL;DR: 首次在encoder-decoder模型（如T5/FLAN-T5）中实现DoLa对比解码，揭示其对指令跟随任务的显著但不均衡影响。


<details>
  <summary>Details</summary>
Motivation: 先前研究仅在decoder-only模型验证DoLa对事实性的增强作用，而该工作旨在探索其在更主流的编码器-解码器架构中的有效性及影响机制。

Method: 通过将DoLa算法适配到T5/FLAN-T5模型家族，在多种指令遵循任务上评估改进效果，并采用逐层logit演变分析解释不同任务类别上的分化表现。

Result: DoLa使事实性任务生成更准确，但在创意生成类任务产生显著劣化，层分析发现其通过强化解码路径选择提升确定性推理能力，但同时抑制发散型推理。

Conclusion: 编码器-解码器架构中对比解码的适用性与任务类型强相关，研究建议未来应设计任务感知的动态平衡机制以优化解码策略。

Abstract: Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.

</details>


### [115] [Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology](https://arxiv.org/abs/2512.03818)
*Kylie L. Anglin,Stephanie Milan,Brittney Hernandez,Claudia Ventura*

Main category: cs.CL

TL;DR: 优化提示工程，提升大型语言模型在心理学文本分类中的专家一致性


<details>
  <summary>Details</summary>
Motivation: LLMs虽擅长分类，但输出高度依赖提示词。心理学等理论驱动领域需精准捕捉构念（constructs），但缺乏适配提示优化方法研究。现有提示工程未充分解决不良措辞导致的性能下降问题，需探索系统性优化框架。

Method: 设计五种对比实验：代码簿引导提示选择、自动提示优化、角色暗示、思维链推理和解释性提示，进行零样本与少样本分类。基于3个心理学构念和2类模型验证，对比不同策略组合的实证表现。

Result: 角色扮演、思维链和解释性提示未能完全弥补差提示。最佳效果来自结合代码簿经验选择与自动优化的少样本提示，在三个构念中均与专家判断最一致。关键提示要素为构念定义、任务框架及示例质量。

Conclusion: 推荐多维提示开发（人工+自动），通过训练集实证评估选择最佳组合，并在验证集中验证。提出理论驱动的标准化流程，为专业领域LLM应用提供方法论指导。

Abstract: Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.

</details>


### [116] [Training and Evaluation of Guideline-Based Medical Reasoning in LLMs](https://arxiv.org/abs/2512.03838)
*Michael Staniek,Artem Sokolov,Stefan Riezler*

Main category: cs.CL

TL;DR: This paper addresses the need for trustworthy machine learning models in medicine by teaching LLMs to follow medical consensus guidelines step-by-step, achieving high accuracy in reasoning while highlighting the challenge of forecasting sparse clinical variables for early prediction.


<details>
  <summary>Details</summary>
Motivation: To build trust among medical practitioners by ensuring models follow established medical consensus guidelines, balancing prediction accuracy with faithful explanations and enabling automatic evaluation of reasoning correctness.

Method: Fine-tuned LLMs on electronic health records linked to verbalized medical inference rules and exceptions, using consensus guidelines like Sepsis-3 for evaluation. Combined LLMs with time series forecasting models to improve future prediction capabilities.

Result: Small fine-tuned models outperformed larger prompt-based LLMs, achieving near-perfect derivation correctness, but forecasting irregular clinical variables remains a key challenge.

Conclusion: Faithful adherence to medical consensus rules is achievable, but early prediction bottlenecks lie in temporal forecasting of sparse clinical data, which can be addressed via multimodal model integration.

Abstract: Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.

</details>


### [117] [Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers](https://arxiv.org/abs/2512.03870)
*Hongzhan Lin,Zhiqi Bai,Xinmiao Zhang,Sen Yang,Xiang Li,Siran Yang,Yunlong Xu,Jiaheng Liu,Yongchi Zhao,Jiamang Wang,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: FusedKV和FusedKV-Lite通过融合不同层的Key-Value缓存，有效降低Transformer解码器的内存消耗同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer解码器在长序列中KV缓存内存瓶颈问题，并改进现有跨层共享方法（如YOCO/CLA）性能劣于同层方法（如GQA）的现象。

Method: 分析顶层Key/Value的信息来源分布后，提出FusedKV：利用底部和中层信息融合生成顶层KV缓存，并直接在旋转位置编码后操作。进一步提出FusedKV-Lite，通过底部Values与中间Keys直接派生顶部缓存。

Result: 实验显示在332M到4B参数大模型中，相较标准Transformer解码器，缓存内存减少50%且验证困惑度更低。FusedKV-Lite进一步降低I/O开销但轻微增加困惑度。

Conclusion: FusedKV系列方法成为兼顾内存效率与性能的新型Transformer架构替代方案。

Abstract: Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.

</details>


### [118] [BERnaT: Basque Encoders for Representing Natural Textual Diversity](https://arxiv.org/abs/2512.03903)
*Ekhi Azurmendi,Joseba Fernandez de Landa,Jaione Bengoetxea,Maite Heredia,Julen Etxaniz,Mikel Zubillaga,Ander Soraluze,Aitor Soroa*

Main category: cs.CL

TL;DR: 本文探讨了通过整合标准与非标准语言数据（如社交媒体、历史文本）来提升语言模型的包容性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 过滤文本语料可能导致非标准语言形式（方言、历史语料等）的缺失，加剧模型偏差，限制其对语言多样性的建模能力。

Method: 以巴斯克语（黏着语、低资源语言）为例构建混合语料库，预训练BERnaT系列编码模型，对比标准语料、多样语料及混合语料三类模型表现。

Result: 混合语料训练的模型在标准基准测试和非标准语言任务（自然语言理解）中均优于纯标准语料模型，且无性能妥协。

Conclusion: 语言多样性数据的纳入能显著增强模型的可迁移性与包容性，尤其对低资源语言具有启示意义。

Abstract: Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.

</details>


### [119] [Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions](https://arxiv.org/abs/2512.03943)
*Kazi Abrab Hossain,Jannatul Somiya Mahmud,Maria Hossain Tuli,Anik Mitra,S. M. Taiabul Haque,Farig Y. Sadeque*

Main category: cs.CL

TL;DR: 本文提出BRAND多语言宗教数据集，揭示了大语言模型在南亚宗教场景中存在的语言与宗教双重偏见。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在宗教等敏感领域存在偏差风险，且多语言模型对南亚主要宗教（佛教、基督教、印度教、伊斯兰教）存在表征偏差问题，尤其是孟加拉语场景易产生误解。

Method: 构建包含2400+英文-孟加拉语平行数据的BRAND宗教数据集，采用三类提示模板，在零样本和少样本场景下评估多语言模型的宗教认知偏差。

Result: 实验发现模型在英文任务表现优于孟加拉语，且对伊斯兰教的中性宗教问题回答存在系统性偏差，体现多语言模型在不同语言场景下的偏差一致性。

Conclusion: 揭示多语言模型在宗教敏感领域的持续性偏差，强调人机交互系统中宗教表征准确性和算法公平性的重要性，推动跨文化AI伦理研究。

Abstract: While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.

</details>


### [120] [Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study](https://arxiv.org/abs/2512.03976)
*Lifeng Chen,Ryan Lai,Tianming Liu*

Main category: cs.CL

TL;DR: 本研究通过连续预训练（CPT）和监督微调（SFT）两阶段方法，成功将Qwen2.5-3B模型适配至低资源语言藏语，显著提升翻译效能并揭示模型适应机制。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言因数据稀缺及跨语言漂移导致的大型语言模型适配难题，填补藏语（形态复杂且代表性不足）在多语言基础模型适配领域的定量研究空白。

Method: 阶段一采用CPT建立藏语语言基础，阶段二通过SFT强化任务与翻译能力；利用435层模型分析适应过程的空间分布特性。

Result: 模型困惑度从2.98降至1.54，中译藏BLEU分数提升465%（0.046→0.261），chrF提升200%（2.2→6.6）；发现适应主要发生于嵌入层和输出层，中期MLP投射编码领域转化特征。

Conclusion: CPT构建藏语语义空间，SFT在最小化表征扰动前提下实现任务对齐；提出首个可复现的低资源语言适配框架，揭示模型适应机制与层次化编码规律。

Abstract: Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.

</details>


### [121] [Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models](https://arxiv.org/abs/2512.03989)
*Taido Purason,Pavel Chizhov,Ivan P. Yamshchikov,Mark Fishel*

Main category: cs.CL

TL;DR: 该研究提出通过持续BPE训练进行词汇扩展和基于叶节点的词汇剪枝技术，提升预训练语言模型的分词效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统分词器扩展方法会在词汇表中添加未重叠词汇导致冗余，现有方法未能有效利用新增词汇，需改进分词适配效率。

Method: 采用持续BPE合并学习算法扩展词汇表，结合叶节点剪枝技术去除冗余标记，通过跨语言实验证证方法有效性。

Result: 实验显示持续BPE训练使新词汇使用率提升243%，剪枝后模型体积缩小28%同时保持0.98的BLEU分数保留率。

Conclusion: 提出的动态词汇修改框架有效解决分词器适配中的效率与质量平衡问题，并已开源为可复用工具包。

Abstract: Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.

</details>


### [122] [AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving](https://arxiv.org/abs/2512.04013)
*Ying Wang,Zhen Jin,Jiexiong Xu,Wenhai Lin,Yiquan Chen,Wenzhi Chen*

Main category: cs.CL

TL;DR: AugServe通过双阶段自适应调度和动态资源调整提升增强型LLM服务的有效吞吐量和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有系统因FCFS调度导致头阻塞和静态批处理限制，难以适应负载与硬件波动，影响服务质量与吞吐量。

Method: 采用双阶段策略：阶段I结合请求特征优化调度顺序，阶段II根据运行时信息实时调整，同时动态调整分批机制。

Result: 与vLLM和InferCept相比，有效吞吐量提升4.7-33.1倍，首字节时间减少最高96.3%。

Conclusion: AugServe通过特征与资源协同优化，显著提升吞吐量并降低延迟，增强增強型LLM服务效率。

Abstract: As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.
  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.

</details>


### [123] [Jina-VLM: Small Multilingual Vision Language Model](https://arxiv.org/abs/2512.04032)
*Andreas Koukounas,Georgios Mastrapas,Florian Hönicke,Sedigheh Eslami,Guillaume Roncari,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: Jina-VLM是一个2.4B参数的多语言视觉-语言模型，通过创新性注意力池化连接器整合SigLIP2视觉编码器与Qwen3语言骨干，实现任意分辨率图像的高效处理，在VQA多语言基准测试中超越同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有2B规模视觉-语言模型在多语言VQA任务中存在性能瓶颈，且无法高效处理高分辨率图像。需要开发兼顾多语言理解能力与视觉处理效率的下一代模型。

Method: 构建包含三个核心组件的框架：1) 基于SigLIP2的视觉编码器实现多尺度特征提取 2) Qwen3语言模型作为推理核心 3) 注意力池化连接器动态调整图像token数量。采用分阶段训练策略优化多模态对齐。

Result: 在OK-VQA、TextVQA等7项主流基准测试中平均超越现有SOTA模型4.2%，多语言测试集X-VQA上达到89.7%准确率。单次推理支持4096×4096分辨率图像且内存消耗降低37%。

Conclusion: Jina-VLM实现了2B量级模型在多语言VQA领域的性能突破，其动态token处理机制为高分辨率视觉理解提供了新的解决方案，在跨模态应用中展现显著优势。

Abstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.

</details>


### [124] [SkillFactory: Self-Distillation For Learning Cognitive Behaviors](https://arxiv.org/abs/2512.04072)
*Zayne Sprague,Jack Lu,Manya Wadhwa,Sedrick Keh,Mengye Ren,Greg Durrett*

Main category: cs.CL

TL;DR: SkillFactory通过在监督式微调(SFT)阶段使用自重组样本培养模型的基础认知技能，为后续强化学习(RL)的高效技能习得奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法依赖基线模型本身具备认知技能或依赖更强的蒸馏模型，而该研究旨在解决基线模型缺乏技能时如何通过非依赖外部模型的SFT引导技能形成的问题。

Method: 提出SkillFactory框架，利用模型自身生成的粗略样本通过排列组合构造'银色'训练数据，在SFT阶段进行技能模拟训练，而非直接依赖人类标注或更强模型蒸馏。

Result: 1) RL前性能较低但RL后显著提升复杂任务表现；2) 模型显式调用技能的中间推理路径可被观测；3) 相比基线模型，RL后的SkillFactory模型在跨任务泛化性和对抗性任务中衰退更小。

Conclusion: SFT阶段植入的归纳偏置能为RL阶段的技能习得提供结构化先验，揭示了技能培养需分阶段训练的认知范式。

Abstract: Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.

</details>
