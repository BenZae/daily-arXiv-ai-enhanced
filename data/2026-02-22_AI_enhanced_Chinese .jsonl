{"id": "2602.16843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16843", "abs": "https://arxiv.org/abs/2602.16843", "authors": ["Ahmed Rafid", "Rumman Adib", "Fariya Ahmed", "Ajwad Abrar", "Mohammed Saidul Islam"], "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization", "comment": "Accepted in 2nd LoResLM at EACL 2026", "summary": "Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $\u03c1= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u6587\u672c\u7684\u5b5f\u52a0\u62c9\u8bed\u6458\u8981\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u6846\u67b6BanglaSummEval\uff0c\u901a\u8fc7\u95ee\u7b54\u673a\u5236\u7ed3\u5408BERTScore-Recall\u5b9e\u73b0\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u53ef\u9760\u8bc4\u4f30\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5728\u5b5f\u52a0\u62c9\u8bed\uff08\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e2d\u7f3a\u4e4f\u9002\u7528\u6027\u4e14\u4f9d\u8d56\u53c2\u8003\u6458\u8981\u7684\u95ee\u9898\uff0c\u89e3\u51b3\u533b\u7597\u3001\u6559\u80b2\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u6458\u8981\u53ef\u9760\u6027\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u591a\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u5b8c\u6210\u81ea\u52a8\u95ee\u7b54\u751f\u6210\u4e0e\u7b54\u6848\u5bf9\u6bd4\uff0c\u901a\u8fc7\u95ee\u9898\u751f\u6210\u3001\u7b54\u6848\u63d0\u53d6\u548c\u91cd\u8981\u6027\u52a0\u6743\u8fdb\u884c\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u5185\u5bb9\u8986\u76d6\u7387\u8bc4\u4f30\uff0c\u5e76\u5229\u7528BERTScore-Recall\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\u5224\u65ad\u3002", "result": "\u5728300\u4e2a\u4eba\u5de5\u64b0\u5199\u7684\u6559\u80b2/\u533b\u7597\u9886\u57df\u6458\u8981\u4e2d\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4e13\u5bb6\u5224\u65ad\u5448\u73b0\u5f3a\u76f8\u5173\u6027\uff08Pearson r=0.694\uff0cSpearman \u03c1=0.763\uff09\u3002", "conclusion": "BanglaSummEval\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u6b65\u9aa4\u548c\u8de8\u8bed\u8a00\u80fd\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u9ad8\u6548\u900f\u660e\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u65b9\u6848\u3002"}}
