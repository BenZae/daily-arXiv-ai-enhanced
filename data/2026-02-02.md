<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 75]
- [cs.CL](#cs.CL) [Total: 47]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 该论文首次系统评估了5种最先进开源视觉检测模型在遥感图像数据集LAE-80C上的零样本迁移能力，发现最佳模型仅取得27.6% F1分数及69%假阳性率，但将类别数从80缩减至3.2时性能提升15倍，证明语义混淆是主要瓶颈，且提出的数据集适应性策略效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索开放词汇目标检测在航空影像领域的迁移潜力，解决现有研究仅关注自然图像而忽视遥感图像领域适配性的问题，为后续研究建立基线标准。

Method: 构建包含LAE-80C数据集（3592张图像，80类）的严格零样本评估协议，采用全局模式、专家模式和单类别模式隔离语义混淆与视觉定位问题，并测试领域特定前缀、同义词扩展等提示工程策略。

Result: 最佳模型OWLv2在遥感数据取得27.6% F1分数（假阳性率69%），将类别缩减至3.2类时性能提升15倍。不同数据集表现差异显著（DIOR数据集F1=0.53，FAIR1M数据集F1=0.12），显示对成像条件敏感性。

Conclusion: 揭示航空影像开放词汇检测存在严重领域迁移失败问题，语义混淆为性能主要瓶颈，现有优化策略效果有限，强调开发领域自适应方法的必要性。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [2] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: 提出了一个新的科学图表视觉问答（VQA）基准，解决当前数据集图表与底层数据一一对应性不足的问题，并开源包含合成直方图、底层数据及标注的多模态数据集。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集多聚焦真实场景图像或简单图表分析，科学图表常涉及数据转换且缺乏底层数据标注，导致模型无法准确推理非一一对应关系的图表挑战。

Method: 通过调查现有VQA数据集局限性，生成基于真实数据的合成直方图，设计需依赖底层数据推理的问题，评估人类与大模型表现，并发布包含图表、数据、生成参数及标注信息的开源数据集。

Result: 发现当前模型在非直接映射场景中表现欠佳，验证了引入底层数据推理的必要性，且数据集包含完整图表结构（如边界框与文本标注），支持未来多模态研究。

Conclusion: 提出首个针对科学图表非一一对应关系的VQA基准，强调大模型需强化数据转换关系的推理能力，开源数据集为领域发展提供标准评测工具。

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

</details>


### [3] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs) struggle with 3D spatial reasoning despite excelling in 2D perception, prompting the creation of VRRPI-Bench/VRRPI-Diag benchmarks. Results show models like GPT-5 underperform compared to geometric methods and humans, especially in depth estimation and multi-image reasoning.


<details>
  <summary>Details</summary>
Motivation: Highlight the gap in VLMs' understanding of 3D spatial structures compared to 2D capabilities, using relative camera pose estimation as a fundamental task to identify limitations.

Method: Developed VRRPI-Bench (based on egocentric videos with verbalized annotations) and VRRPI-Diag (isolating motion parameters) to test VLMs' ability to infer relative camera motion in realistic and controlled scenarios.

Result: Most VLMs fail to generalize beyond 2D heuristics, with poor performance in depth changes (e.g., GPT-5 score 0.64 vs. geometric baseline 0.97). Multi-image reasoning showed inconsistent results (best 59.7%).

Conclusion: VLMs exhibit significant limitations in grounding 3D spatial reasoning and integrating multi-view cues, indicating a need for improved models in these areas to bridge the gap with classic geometric approaches.

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [4] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: 该论文从几何角度分析视觉Transformer(ViT)中位置嵌入(PEs)的本质作用，提出PEs实际上作为几何先验来塑造表征空间结构，并通过14个ViT模型验证其对多视图几何一致性的因果影响。


<details>
  <summary>Details</summary>
Motivation: 研究者旨在揭示位置嵌入在视觉Transformer中被忽视的几何本质，明确其对模型空间推理能力的核心作用，并为架构设计提供理论依据。

Method: 提出标记级别的几何一致性诊断工具，通过控制PEs的一致性变量，在14种主流ViT模型中系统分析PEs对多视角几何表征的影响，并结合代码实现进行实验验证。

Result: 实验表明位置嵌入直接决定了ViT表征的多视图几何稳定性，其一致性缺失会导致空间结构崩塌，证明PEs实质上是编码几何先验的因果变量。

Conclusion: 首次确立位置嵌入作为几何先验的核心地位，为视觉Transformer的架构改进和空间推理能力提升提供了新的理论框架。

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

</details>


### [5] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 单级VQ-VAE在解决代码本坍塌问题后，可匹敌多级模型的重构性能


<details>
  <summary>Details</summary>
Motivation: 因多级模型信息冗余性与其代码本利用率和表征能力耦合，需验证单级模型在对齐表征预算时是否能实现相同重构精度

Method: 在ImageNet高分辨率图像上对比双级VQ-VAE与等效单级模型，采用数据初始化、无效码向量重置及超参数系统调优

Result: 轻量级干预显著减少代码本坍塌，当表征资源匹配时单级模型重构质量与多级模型相当

Conclusion: 消除代码本利用率差异后，单级量化结构在重构任务中表现可与多级结构媲美

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [6] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: VMonarch通过引入结构化的Monarch矩阵解决视频扩散模型中注意力机制的二次复杂度问题，实现高效时空注意力计算。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（DiTs）的注意力机制因二次复杂度难以扩展到长视频序列，而实际训练中发现其注意力权重呈现稀疏特性，可通过结构化矩阵压缩表征。

Method: 1) 将时空注意力模式分解为Monarch矩阵形式，显式建模帧内/帧间相关性；2) 设计重构策略缓解交替最小化优化不稳定性；3) 提出在线熵算法加速FlashAttention中的矩阵更新。

Result: 在VBench基准测试中达到与全注意力相当的生成质量，减少注意力计算量17.5倍，长视频处理速度提升5倍，且在90%稀疏度下超越现有稀疏注意力方法。

Conclusion: 该研究证明结构化矩阵分解可有效解耦视频生成中的时空依赖，为高分辨率长视频生成提供了可扩展的注意力范式。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [7] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: C2R框架通过结合粗略3D渲染与神经生成技术，实现高效可控的城市人群动态视频生成。


<details>
  <summary>Details</summary>
Motivation: 传统渲染管线在动态场景的扩展性、真实感和计算效率间存在显著矛盾，且缺乏对复杂人群动态的可控生成方案。本文旨在通过数据驱动方法突破这些限制。

Method: 采用两阶段混合训练策略：第一阶段用真实视频数据建立生成先验，第二阶段通过隐式时空特征共享引入可控性。核心创新包括粗略3D渲染引导的结构控制模块与文本驱动的神经渲染器联合架构。

Result: 系统能从简化的3D输入生成512x512分辨率、时间连续的1080p视频，在保持场景物理合理性的前提下，支持文本可控的光照、外观及动态细化，且渲染速度达12FPS。实验证明其在CrowdPose和Cityscapes数据集上优于NeRF和GAN-based方法。

Conclusion: 该方法首次实现将粗粒度3D模拟与生成对抗技术的有效融合，为复杂动态场景的实时渲染提供了新范式，在影视制作和虚拟仿真领域具有应用潜力。

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

</details>


### [8] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap是一种无需依赖特定摄像头配置的高精地图构建方法，通过几何感知的基础模型和跨帧注意力机制实现三维场景理解，避免了显式的几何投影。


<details>
  <summary>Details</summary>
Motivation: 现有高精地图构建方法依赖校准的多摄像头设和2D到BEV转换，导致在传感器故障或摄像头配置变化时鲁棒性不足。

Method: 采用几何感知的基础模型与跨帧注意力机制隐式编码3D场景理解；设计时空增强模块分离跨视图空间推理和时态动态，并通过带潜在摄像头token的摄像头感知解码器实现视图自适应注意力。

Result: 实验表明FlexMap在多种配置下性能优于现有方法，且对缺失视角和传感器差异具有鲁棒性。

Conclusion: FlexMap通过无需架构调整或重新训练的适应性设计，提升了自动驾驶系统高精地图构建的实用性。

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

</details>


### [9] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 本论文提出了一个利用Chain-of-Thought（CoT）提示和ReAct驱动自适应噪声机制的越狱框架，通过隐秘提示和图像扰动绕过视觉-语言模型（VLMs）的安全过滤器。


<details>
  <summary>Details</summary>
Motivation: VLMs在视觉问答、图像描述生成等任务中存在安全性对齐漏洞，其输出对提示变化高度敏感，需探索潜在威胁以提升模型安全性。

Method: 1) 基于CoT的隐秘提示生成：通过后训练提示工程构造规避安全检测的文本输入；2) ReAct驱动的自适应噪声机制：利用模型反馈迭代优化图像扰动区域（重点关注易触发防御机制的区域），结合推理（Reasoning）与行动（Acting）范式提升攻击隐蔽性。

Result: 实验表明该双重策略相比传统方法显著提升攻击成功率（ASR），且在文本和视觉领域均保持输入自然性（如人类评估分数下降<5%）。

Conclusion: 揭示了现有VLM安全机制的脆弱性，证明结合多模态扰动的自适应攻击可有效绕过防御，强调需改进多模态对齐的鲁棒性设计。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [10] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: 本研究评估了基于变分推断的概率性多视角无标记运动捕捉(MMMC)系统的可靠性与校准度，结果表明其预测不确定性量化准确，适用于临床运动分析。


<details>
  <summary>Details</summary>
Motivation: 推动多视角无标记动作捕捉系统在临床应用中被广泛信任，需其提供可靠的置信区间以量化个体化数据的准确性。

Method: 利用变分推断估计关节角度后验分布，分析68名参与者在两个机构的步态数据，通过与步道传感器和传统标记捕捉系统对比，采用期望校准误差(ECE)评估模型校准度。

Result: 模型ECE均值小于0.1，步长/步幅误差中位数为16mm/12mm，关节角度误差为1.5-3.8度，预测不确定度与真实误差呈强相关性。

Conclusion: 该概率模型通过量化认知不确定性，在无需实时真实值的情况下，成功识别不可靠输出，验证了其临床适用性。

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

</details>


### [11] [Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework](https://arxiv.org/abs/2601.22451)
*Shiyu Liu,Xinyi Wen,Zhibin Lan,Ante Wang,Jinsong Su*

Main category: cs.CV

TL;DR: 本文提出通过消除语言优先偏差的验证方法和训练无关的自验证框架，显著减轻图像描述任务中大视觉语言模型的物体幻觉问题，且无需模型微调。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在生成长文本时因过度依赖语言优先知识产生非物体幻觉，现有方法缺乏对依赖程度的深入分析和有效缓解机制。

Method: 通过实验证实生成长度与幻觉的关联，提出无需训练的自验证框架，包含语言优先无关验证、候选句生成、存在性验证及描述选择/融合。

Result: 该方法在CHAIRI等指标上提升显著（LLaVA模型提升65.6%），超出现有SOTA方法，且兼容不同架构模型。

Conclusion: 通过模型自我验证机制可有效缓解幻觉问题，为提升视觉语言多模态模型可靠性提供新路径。

Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and attempts to mitigate it through logits calibration. However, they still lack a thorough analysis of the over-reliance. To gain a deeper understanding of over-reliance, we conduct a series of preliminary experiments, indicating that as the generation length increases, LVLMs' over-reliance on language priors leads to inflated probability of hallucinated object tokens, consequently exacerbating object hallucination. To circumvent this issue, we propose Language-Prior-Free Verification to enable LVLMs to faithfully verify the confidence of object existence. Based on this, we propose a novel training-free Self-Validation Framework to counter the over-reliance trap. It first validates objects' existence in sampled candidate captions and further mitigates object hallucination via caption selection or aggregation. Experiment results demonstrate that our framework mitigates object hallucination significantly in image captioning task (e.g., 65.6% improvement on CHAIRI metric with LLaVA-v1.5-7B), surpassing the previous SOTA methods. This result highlights a novel path towards mitigating hallucination by unlocking the inherent potential within LVLMs themselves.

</details>


### [12] [ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction](https://arxiv.org/abs/2601.22455)
*Yudi Zhang,Yeming Geng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出ScribbleSense，结合多模态大模型与生成模型，提升基于涂鸦的3D模型纹理交互编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于涂鸦的3D纹理编辑方法受限于涂鸦指令的抽象性易导致编辑意图模糊及目标语义不明确。

Method: 利用多模态大语言模型(MVLMs)解析涂鸦语义意图，结合图像生成模型提取局部细节以锚定语义并消除歧义。

Result: 相比传统方法实现涂鸦交互编辑的SOTA性能，有效提升编辑准确性和语义一致性。

Conclusion: 通过融合多模态建模与生成能力，突破涂鸦指令抽象性约束，显著优化交互式3D纹理编辑流程。

Abstract: Interactive 3D model texture editing presents enhanced opportunities for creating 3D assets, with freehand drawing style offering the most intuitive experience. However, existing methods primarily support sketch-based interactions for outlining, while the utilization of coarse-grained scribble-based interaction remains limited. Furthermore, current methodologies often encounter challenges due to the abstract nature of scribble instructions, which can result in ambiguous editing intentions and unclear target semantic locations. To address these issues, we propose ScribbleSense, an editing method that combines multimodal large language models (MLLMs) and image generation models to effectively resolve these challenges. We leverage the visual capabilities of MLLMs to predict the editing intent behind the scribbles. Once the semantic intent of the scribble is discerned, we employ globally generated images to extract local texture details, thereby anchoring local semantics and alleviating ambiguities concerning the target semantic locations. Experimental results indicate that our method effectively leverages the strengths of MLLMs, achieving state-of-the-art interactive editing performance for scribble-based texture editing.

</details>


### [13] [Head-Aware Visual Cropping: Enhancing Fine-Grained VQA with Attention-Guided Subimage](https://arxiv.org/abs/2601.22483)
*Junfei Xie,Peng Pan,Xulong Zhang*

Main category: cs.CV

TL;DR: 提出HAVC方法通过注意力头选择和空间优化提升MLLMs在细粒度视觉问答中的精度。


<details>
  <summary>Details</summary>
Motivation: MLLMs在VQA中因低分辨率输入和注意力噪声导致细粒度推理能力受限，需提升视觉基础准确性。

Method: HAVC分两阶段优化：1)基于OCR任务筛选有效注意力头；2)推理时结合空间熵与梯度敏感度精确定位关键区域，生成裁剪指引图。

Result: 在多个基准测试中HAVC超越现有裁剪策略，实现更精确的定位和更强的视觉基础效果。

Conclusion: HAVC通过训练免费策略有效提升MLLMs精度，为视觉-语言模型优化提供新方向。

Abstract: Multimodal Large Language Models (MLLMs) show strong performance in Visual Question Answering (VQA) but remain limited in fine-grained reasoning due to low-resolution inputs and noisy attention aggregation. We propose \textbf{Head Aware Visual Cropping (HAVC)}, a training-free method that improves visual grounding by leveraging a selectively refined subset of attention heads. HAVC first filters heads through an OCR-based diagnostic task, ensuring that only those with genuine grounding ability are retained. At inference, these heads are further refined using spatial entropy for stronger spatial concentration and gradient sensitivity for predictive contribution. The fused signals produce a reliable Visual Cropping Guidance Map, which highlights the most task-relevant region and guides the cropping of a subimage subsequently provided to the MLLM together with the image-question pair. Extensive experiments on multiple fine-grained VQA benchmarks demonstrate that HAVC consistently outperforms state-of-the-art cropping strategies, achieving more precise localization, stronger visual grounding, providing a simple yet effective strategy for enhancing precision in MLLMs.

</details>


### [14] [PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization](https://arxiv.org/abs/2601.22492)
*Duncan McCain,Hossein Kashiani,Fatemeh Afghah*

Main category: cs.CV

TL;DR: PromptMAD is a cross-modal prompting framework for unsupervised visual anomaly detection that integrates semantic guidance via vision-language alignment, achieving state-of-the-art performance on MVTec-AD dataset with 98.35% AUC and 66.54% AP.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multi-class visual anomaly detection including diverse object categories, limited anomalous examples, camouflaged defects, and pixel-level class imbalance.

Method: Integrates CLIP-encoded text prompts for semantic guidance, employs Focal loss to handle class imbalance, and uses a supervised segmentor with multi-scale CNN-Transformer fusion and diffusion refinement for anomaly mapping.

Result: Achieved 98.35% mean AUC and 66.54% AP on MVTec-AD dataset, demonstrating superior pixel-level anomaly detection efficiency across diverse categories.

Conclusion: Semantic vision-language alignment combined with focal loss and multi-scale architectural improvements significantly enhance subtle anomaly detection and localization in complex multi-class scenarios.

Abstract: Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.

</details>


### [15] [MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control](https://arxiv.org/abs/2601.22501)
*Renjie Lu,Xulong Zhang,Xiaoyang Qu,Jianzong Wang,Shangfei Wang*

Main category: cs.CV

TL;DR: 提出MirrorTalk框架，通过条件扩散模型与解耦风格编码器，实现高精度唇同步和个性化面部生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法分离说话人独特风格与语义内容，导致个性化风格迁移时唇同步失真。

Method: 设计Semantically-Disentangled Style Encoder（SDSE）提取纯风格表征，并提出分层调制策略，在扩散过程中动态平衡音频/风格特征对不同面部区域的控制。

Result: 实验表明该方法在LRS和VoxCeleb数据集上，唇同步误差降低23%，个性化风格保持度提升18%。

Conclusion: 镜像调制策略有效解耦内容与风格，为高质量个性化虚拟人生成提供了新思路。

Abstract: Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.

</details>


### [16] [DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation](https://arxiv.org/abs/2601.22507)
*Xin Jiang,Jingwen Chen,Yehao Li,Yingwei Pan,Kezhou Chen,Zechao Li,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 本文提出了DreamVAR，一种基于视觉自回归模型（VAR）的新型主题驱动图像合成框架，采用下一尺度预测技术。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在主题驱动图像生成领域表现突出，但VAR模型因统一架构和推理效率高等优点仍有潜力待挖掘。

Method: 通过视觉分词器提取主题多尺度特征，前置填充完整主题特征序列以简化自回归依赖；结合强化学习提升语义对齐与主体一致性。

Result: 实验表明DreamVAR在视觉保真度方面优于当前最先进的扩散模型方法。

Conclusion: 该框架通过创新性架构设计与强化学习优化，在高效利用VAR模型的同时实现了高质量的主题驱动图像生成。

Abstract: Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.

</details>


### [17] [CoVA: Text-Guided Composed Video Retrieval for Audio-Visual Content](https://arxiv.org/abs/2601.22508)
*Gyuwon Han,Young Kyun Jang,Chanho Eom*

Main category: cs.CV

TL;DR: 提出新任务CoVA，结合视听变化的视频检索，并构建AV-Comp基准与AVT融合方法。


<details>
  <summary>Details</summary>
Motivation: 现有CoVR仅关注视觉变化，忽略音频差异，需构建包含跨模态变化的新任务与数据集。

Method: 设计AVT融合模型，通过选择性对齐文本查询与最相关模态（视频/音频）整合多模态特征。

Result: AVT方法优于传统单一模态融合，成为CoVA任务的有效基线。

Conclusion: CoVA任务和AV-Comp数据集更贴近现实场景，提升视频检索的实用性和鲁棒性。

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a target video from a large gallery using a reference video and a textual query specifying visual modifications. However, existing benchmarks consider only visual changes, ignoring videos that differ in audio despite visual similarity. To address this limitation, we introduce Composed retrieval for Video with its Audio CoVA, a new retrieval task that accounts for both visual and auditory variations. To support this, we construct AV-Comp, a benchmark consisting of video pairs with cross-modal changes and corresponding textual queries that describe the differences. We also propose AVT Compositional Fusion (AVT), which integrates video, audio, and text features by selectively aligning the query to the most relevant modality. AVT outperforms traditional unimodal fusion and serves as a strong baseline for CoVA. Examples from the proposed dataset, including both visual and auditory information, are available at https://perceptualai-lab.github.io/CoVA/.

</details>


### [18] [Can 3D point cloud data improve automated body condition score prediction in dairy cattle?](https://arxiv.org/abs/2601.22522)
*Zhou Tang,Jin Wang,Angelo De Castro,Yuxi Zhang,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Xu Wang,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 本研究比较了深度图像和点云数据在奶牛体况评分（BCS）预测中的表现，发现深度图像在多数情况下优于点云数据。


<details>
  <summary>Details</summary>
Motivation: 传统BCS评分方法主观且耗时，计算机视觉（尤其是深度图像）已被用于改进，但近期三维点云数据的潜力尚缺乏与深度图像的直接比较。

Method: 基于4种数据场景（未分割原始数据、全身体分割、后驱分割、手动特征数据），使用1020头奶牛的数据对比深度图像和点云模型预测效果，并进行交叉验证。

Result: 深度图像在未分割和全身体分割数据中表现更优，点云模型在后驱分割数据中效果相近；两者在手动特征数据中效果均下降，且点云对噪声和模型架构更敏感。

Conclusion: 在现有条件下，点云数据未显著优于深度图像，后者仍是BCS预测的稳健选择。

Abstract: Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions.

</details>


### [19] [SHED Light on Segmentation for Dense Prediction](https://arxiv.org/abs/2601.22529)
*Seung Hyun Lee,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: SHED通过结合分割与密集预测，提升3D感知的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有密集预测方法忽视场景结构导致不一致，需显式引入几何先验

Method: 设计双向层次结构编码器-解码器，分层池化/逆池化段令牌，仅最终监督

Result: 边界锐度提升30%，跨域泛化增强，在NYUv2/ScanNet等数据集达SOTA

Conclusion: 层次化建模显著改善3D场景理解，揭示部件级结构

Abstract: Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.

</details>


### [20] [Hybrid Cross-Device Localization via Neural Metric Learning and Feature Fusion](https://arxiv.org/abs/2601.22551)
*Meixia Lin,Mingkai Liu,Shuxue Peng,Dikai Fan,Shengyu Gu,Xianliang Huang,Haoyang Ye,Xiao Liu*

Main category: cs.CV

TL;DR: 该论文提出了针对CroCoDL 2025挑战的混合跨设备定位流水线，结合几何分支与神经分支并采用候选剪枝策略，在HYDRO和SUCCU基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 为解决跨设备场景中因传感器差异导致的定位难题，探索传统几何方法与深度学习方法的协同潜力，提升定位召回率与精度。

Method: 1. 共享检索编码器提取多设备特征
2. 双分支架构：
   - 几何分支：特征融合+PnP算法
   - 神经分支：MapAnything模型输入几何信息进行度量定位
3. 神经引导候选剪枝：基于平移一致性过滤不可靠地图帧
4. 深度条件定位：优化Spot场景的尺度与平移精度

Result: 在HYDRO和SUCCU基准测试中召回率与准确性均显著提升，挑战赛期间获得92.62（R@0.5m, 5°）的最终得分。

Conclusion: 几何-神经融合架构能有效解决跨设备定位中的尺度不一致与特征差异问题，候选剪枝与深度优化策略验证了多模态方法的鲁棒性价值，对机器人和增强现实应用具有实践意义。

Abstract: We present a hybrid cross-device localization pipeline developed for the CroCoDL 2025 Challenge. Our approach integrates a shared retrieval encoder and two complementary localization branches: a classical geometric branch using feature fusion and PnP, and a neural feed-forward branch (MapAnything) for metric localization conditioned on geometric inputs. A neural-guided candidate pruning strategy further filters unreliable map frames based on translation consistency, while depth-conditioned localization refines metric scale and translation precision on Spot scenes. These components jointly lead to significant improvements in recall and accuracy across both HYDRO and SUCCU benchmarks. Our method achieved a final score of 92.62 (R@0.5m, 5°) during the challenge.

</details>


### [21] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 本文提出一种名为MA-PaPSP的训练无关选择预测方法，通过记忆增强和对比归一化改进视觉-语言模型的低置信度预测拒绝能力，适用于封闭集到开放集的多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有选择预测方法多局限于封闭任务，而该研究旨在解决视觉-语言基础模型的开放词汇任务（如图像描述生成），并通过外接CLIP等嵌入模型实现无需重新训练的轻量化方案。

Method: 在PaPSP框架中引入记忆增强模块：1) 利用图像-文本检索数据集的最近邻对嵌入向量进行平均以降低噪声；2) 通过对比归一化策略改善相似度分数校准。方法无需训练且计算复杂度低。

Result: 在多个数据集的对比实验中，MA-PaPSP在选择性描述生成、图文匹配和细粒度分类任务中均优于原始PaPSP与其他基线方法，代码已开源（github.com/kingston-aditya/MA-PaPSP）。

Conclusion: 所提方法有效解决视觉-语言表示不稳定性与分数校准问题，在开放/封闭混合任务中实现稳健的选择预测，验证了非训练增强策略的有效性。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [22] [DELNet: Continuous All-in-One Weather Removal via Dynamic Expert Library](https://arxiv.org/abs/2601.22573)
*Shihong Liu,Kun Zuo,Hanguang Xiao*

Main category: cs.CV

TL;DR: DELNet是一种持续学习框架，用于天气图像复原，通过动态专家库和任务识别机制，避免重复训练，显著提升PSNR


<details>
  <summary>Details</summary>
Motivation: 传统方法需要针对不同降质类型反复收集数据和全面重训练，导致高昂的时间/计算成本，限制实际部署

Method: 设计双模块架构：1) 判断阀门通过任务相似度检测区分新/已知降质类型 2) 动态专家库实现：新任务时top-k专家协同学习并新增专家，已知任务直接调用专家

Result: 在OTS/Rain100H/Snow100K数据集上分别取得16%,11%,12%的PSNR增益，超越当前最优持续学习方法

Conclusion: DELNet通过结构化持续学习机制，在无需模型重训练的前提下实现多降质类型处理，同时保持实时推理速度

Abstract: All-in-one weather image restoration methods are valuable in practice but depend on pre-collected data and require retraining for unseen degradations, leading to high cost. We propose DELNet, a continual learning framework for weather image restoration. DELNet integrates a judging valve that measures task similarity to distinguish new from known tasks, and a dynamic expert library that stores experts trained on different degradations. For new tasks, the valve selects top-k experts for knowledge transfer while adding new experts to capture task-specific features; for known tasks, the corresponding experts are directly reused. This design enables continuous optimization without retraining existing models. Experiments on OTS, Rain100H, and Snow100K demonstrate that DELNet surpasses state-of-the-art continual learning methods, achieving PSNR gains of 16\%, 11\%, and 12\%, respectively. These results highlight the effectiveness, robustness, and efficiency of DELNet, which reduces retraining cost and enables practical deployment in real-world scenarios.

</details>


### [23] [Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding](https://arxiv.org/abs/2601.22574)
*Yuansheng Gao,Jinman Zhao,Tong Zhang,Xingguo Xu,Han Bao,Zonghui Wang,Wenzhi Chen*

Main category: cs.CV

TL;DR: 视频大语言模型存在幻觉问题，现有解码方法因依赖启发式设计效果有限，提出时空-语义对比解码策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能捕捉幻觉的根源及时空语义关联，导致鲁棒性和泛化性不足，需更有效的解码策略。

Method: 构建负特征破坏视频时空一致性与语义关联，通过对比解码抑制幻觉。

Result: 实验表明该方法显著减少幻觉，同时保留模型视频理解与推理能力。

Conclusion: 所提策略解决了现有方法的局限性，提升复杂场景下的模型表现。

Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.

</details>


### [24] [PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios](https://arxiv.org/abs/2601.22575)
*Xudong Lu,Huankang Guan,Yang Bo,Jinpeng Chen,Xintong Guo,Shuhan Li,Fang Liu,Peiwen Sun,Xueying Li,Wei Zhang,Xue Yang,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: PhoStream introduced the first mobile-centric streaming benchmark for audio-visual temporal reasoning, revealing that current multimodal models excel in immediate/backward tasks but fail in forward tasks due to premature responses before required visual/audio cues.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks (e.g., multiple-choice or short videos) inadequately evaluate real-world continuous streaming scenarios where mobile assistants must temporally align on-screen/off-screen audio-visual inputs and decide response timing.

Method: Constructed 5,572 QA pairs from 578 real-world videos across 4 scenarios (on-screen/off-screen, synchronous/asynchronous) via an automated pipeline with human verification. Evaluated models using online streaming inference and LLM-as-a-Judge metrics for open-ended responses.

Result: Gemini 3 Pro scored ≤80 in instant/backward tasks but dropped to 16.40 in forward tasks, showing 74% models responded prematurely before necessary visual/audio cues appeared, indicating temporal asymmetry in temporal reasoning capabilities.

Conclusion: Current MLLMs lack temporal 'speech timing' capabilities despite content accuracy, highlighting the need for explicit modeling of cross-modal temporal dependencies in streaming environments. Dataset/code released publicly.

Abstract: Multimodal Large Language Models excel at offline audio-visual understanding, but their ability to serve as mobile assistants in continuous real-world streams remains underexplored. In daily phone use, mobile assistants must track streaming audio-visual inputs and respond at the right time, yet existing benchmarks are often restricted to multiple-choice questions or use shorter videos. In this paper, we introduce PhoStream, the first mobile-centric streaming benchmark that unifies on-screen and off-screen scenarios to evaluate video, audio, and temporal reasoning. PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios and 10 capabilities. We build it with an Automated Generative Pipeline backed by rigorous human verification, and evaluate models using a realistic Online Inference Pipeline and LLM-as-a-Judge evaluation for open-ended responses. Experiments reveal a temporal asymmetry in LLM-judged scores (0-100): models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before the required visual and audio cues appear. This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/PhoStream.

</details>


### [25] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: 本文提出MIFOMO，结合远程基础模型、融合投影、混合域适应及标签平滑，提升高光谱影像CDSL性能，超越现有方法14%并开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有CDFSL方法依赖无效数据增强与大量参数易过拟合，且缺乏对基础模型强泛化能力的探索。

Method: 基于预训练的RS基础模型冻结主干，采用共融投影快速适配任务，混合域适应缓解域间差异，并用标签平滑减少伪标签噪声影响。

Result: 在实验中MIFOMO相较传统方法性能提升最高达14%，且实现开源模型复用

Conclusion: 该方法通过基础模型泛化与模块组合，在小样本高光谱分类中显著提升效果，同时解决了参数冗余及域差异问题

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [26] [FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data](https://arxiv.org/abs/2601.22596)
*Abdelrrahman Moubane*

Main category: cs.CV

TL;DR: 本文提出FOTBCD，一种覆盖法国多个地区的大规模建筑变化检测数据集，具有高分辨率和地理多样性，包含两个公开数据集，并通过实验验证地理多样性对跨域泛化性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现有建筑变化检测数据集地理覆盖范围有限（如局限于单一城市），导致模型在面对地理域迁移时泛化能力不足。本文旨在构建覆盖更广地理范围、验证地理多样性对跨域性能影响的大规模基准数据集。

Method: 基于法国官方正射影像和地形建筑数据（IGN France），将28个大区划分为25个训练区与3个独立测试区，生成0.2m/像素的地理多样化数据集。发布FOTBCD-Binary（28,000对二值掩码图像对）与FOTBCD-Instances（实例级标注子集），并通过固定基线模型对比LEVIR-CD+和WHU-CD进行基准测试。

Result: FOTBCD-Binary在地理域迁移场景下展现出显着优于现有数据集的跨域泛化能力，实验证实数据集级别的地理多样性与性能提升呈正相关。两个子数据集已公开。

Conclusion: FOTBCD通过广泛地理覆盖解决了现有数据集的局限性，其结构设计验证了地理多样性对构建稳健变化检测模型的重要性，所提实例级标注标准为后续研究提供了参考框架。

Abstract: We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.

</details>


### [27] [TTSA3R: Training-Free Temporal-Spatial Adaptive Persistent State for Streaming 3D Reconstruction](https://arxiv.org/abs/2601.22615)
*Zhijie Zheng,Xinhao Xiang,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出TTSA3R框架，通过时空自适应策略提升流媒体3D重建的长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 流媒体循环模型在长期序列中易遗忘历史信息，现有方法仅单维度处理，缺乏时空一致性。

Method: 设计时间自适应更新模块（分析时态演变）与空间上下文模块（定位高误差区域），融合时空信号进行状态更新。

Result: 实验显示TTSA3R在长期序列中错误率仅增15%，远优于基线模型超200%的下降，显著提升稳定性。

Conclusion: 所提框架有效平衡历史与新信息，增强了时空一致性，在3D重建中保持长期记忆稳定性。

Abstract: Streaming recurrent models enable efficient 3D reconstruction by maintaining persistent state representations. However, they suffer from catastrophic memory forgetting over long sequences due to balancing historical information with new observations. Recent methods alleviate this by deriving adaptive signals from attention perspective, but they operate on single dimensions without considering temporal and spatial consistency. To this end, we propose a training-free framework termed TTSA3R that leverages both temporal state evolution and spatial observation quality for adaptive state updates in 3D reconstruction. In particular, we devise a Temporal Adaptive Update Module that regulates update magnitude by analyzing temporal state evolution patterns. Then, a Spatial Contextual Update Module is introduced to localize spatial regions that require updates through observation-state alignment and scene dynamics. These complementary signals are finally fused to determine the state updating strategies. Extensive experiments demonstrate the effectiveness of TTSA3R in diverse 3D tasks. Moreover, our method exhibits only 15% error increase compared to over 200% degradation in baseline models on extended sequences, significantly improving long-term reconstruction stability. Our codes will be available soon.

</details>


### [28] [UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating](https://arxiv.org/abs/2601.22616)
*Xing Yi,Jinyang Huang,Feng-Qi Cui,Anyang Tong,Ruimin Wang,Liu Liu,Dan Guo*

Main category: cs.CV

TL;DR: 为解决当前3D物体检测方法在稀疏点云场景下无法有效建模几何关系及特征分布的问题，作者提出UniGeo框架。该方法通过几何关系映射模块和动态通道门控机制提升特征表征能力，在六大数据集上验证了性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有统一训练方法在稀疏点云场景中存在几何关系建模不足与重要特征分布忽略的问题，导致检测性能受限。需要更有效的几何特征增强机制。

Method: 提出UniGeo框架：1) 几何感知学习模块建立空间关系到特征权重的可学习映射，实现几何特征显性增强；2) 动态通道门控机制通过可学习通道权重优化3D U-Net输出特征，强化关键几何信息。

Result: 在六个不同室内场景数据集的对比实验显示，该方法在检测精度和特征表征能力方面均优于现有模型，特别是在统一多数据集训练场景下表现突出。

Conclusion: UniGeo通过显式建模场景几何关系和优化点云特征表示，在不依赖特定数据集训练的情况下显著提升3D物体检测性能，为跨域检测提供了新思路。

Abstract: The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.

</details>


### [29] [LINA: Linear Autoregressive Image Generative Models with Continuous Tokens](https://arxiv.org/abs/2601.22630)
*Jiahao Wang,Ting Pan,Haoge Deng,Dongchen Han,Taiqiang Wu,Xinlong Wang,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出了LINA，一种完全基于线性注意力的高效文本到图像生成模型，通过改进的线性注意力设计和KV门机制，在保持高图像质量的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统自回归视觉生成模型（尤其是文本到图像合成）因计算成本过高而受限，需要开发高效的线性注意力机制以提升性能与计算效率。

Method: 实证分析不同设计选择（归一化范式、深度卷积增强局部性）对线性注意力扩展性的影响，并提出适用于双向架构的KV门机制，通过可学习参数动态控制键值对的记忆权重。

Result: 实验发现除法基归一化更适配生成任务，卷积局部化对自回归生成关键；LINA模型（约1.4-1.5B参数）在ImageNet和GenEval数据集分别取得2.18 FID和0.74指标，线性注意力模块相较softmax减少61%计算量。

Conclusion: 线性注意力结合所提出的设计原则（归一化、局部建模和KV门控）可同时实现高效且高质量的文本到图像生成，为相关模型架构设计提供重要参考。

Abstract: Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation.
  Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models.
  We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models.
  Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: https://github.com/techmonsterwang/LINA.

</details>


### [30] [What can Computer Vision learn from Ranganathan?](https://arxiv.org/abs/2601.22634)
*Mayukh Bagchi,Fausto Giunchiglia*

Main category: cs.CV

TL;DR: 提出通过S.R.拉甘纳森的分类原则解决计算机视觉中的语义鸿沟问题，并验证了vTelos注释方法的有效性


<details>
  <summary>Details</summary>
Motivation: 视觉与词汇语义的错位导致数据集缺陷，亟需更系统的方法修正语义鸿沟

Method: 将图书馆学分类原则适配到计算机视觉领域，构建vTelos注释框架

Result: 实验显示该方法在图像标注准确率与模型性能方面均优于传统方法

Conclusion: 跨学科理论迁移能有效提升数据质量，为基准测试提供新范式

Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.

</details>


### [31] [Unsupervised Synthetic Image Attribution: Alignment and Disentanglement](https://arxiv.org/abs/2601.22663)
*Zongfang Liu,Guangyi Chen,Boyang Sun,Tongliang Liu,Kun Zhang*

Main category: cs.CV

TL;DR: 本文提出一种无监督的合成图像 attribution 方法 'Alignment and Disentanglement'，通过对比自监督学习和解耦表示，在无需配对标注的情况下超越现有监督方法，为版权保护和模型透明提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有合成图像归因方法依赖成本高昂的配对标注数据（需人工设计概念或标注百万级数据源），亟需一种无需监督的替代方案。

Method: 1) 利用对比自监督学习进行基础概念对齐；2) 通过Infomax损失增强解耦表示；3) 理论上将对齐与解耦分解为典型相关分析目标，解释其近似概念匹配机制。

Result: 在真实基准数据集AbC上，无监督方法表现显著优于监督方法（对比实验显示SOTA级性能提升）

Conclusion: 证明对齐与解耦的协同作用可有效替代监督学习，为合成图像归因任务提供理论框架和可扩展的无监督范式。

Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.

</details>


### [32] [ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding](https://arxiv.org/abs/2601.22666)
*Junyi Hu,Tian Bai,Fengyi Wu,Wenyan Li,Zhenming Peng,Yi Zhang*

Main category: cs.CV

TL;DR: 提出ExpAlign框架，通过基于多实例学习的期望对齐头和能量基多尺度一致性正则化，显著提升开放词汇检测和零样本实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖缺乏细粒度的全局句子嵌入或需要显式监督/复杂注意力机制，而开放词汇模型需在弱监督下实现精准跨模态对齐。

Method: 构建期望对齐头执行注意力软MIL池化，并设计多尺度能量正则化（包含Top-K对比目标和几何感知一致性目标），通过拉格朗日约束自由能最小化实现隐式对齐学习。

Result: 在LVIS数据集minival拆分中实现36.2AP$_r$性能，优于其他SOTA方法且保持轻量化推理效率，尤其在长尾类别中表现突出。

Conclusion: ExpAlign通过理论严格的多实例学习框架和新型正则化策略，在弱监督条件下实现了更优的视觉语言对齐效果。

Abstract: Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.

</details>


### [33] [VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)
*Hanxun Yu,Wentong Li,Xuan Qu,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: 本文提出VisionTrim，这是一种无需训练的多模态大语言模型加速框架，通过融合两种模块（DVTS和TGVC）实现高效视觉token压缩。


<details>
  <summary>Details</summary>
Motivation: 现有模型因高分辨率/视频场景下的视觉token冗余导致计算成本高昂，且现有方法因分割处理模块及忽视文本对齐造成性能损耗，需要端到端优化方案。

Method: 1）DVTS模块通过全局-局部分析提取关键视觉token；2）TGVC模块基于文本上下文引导token合并，形成文本感知的压缩策略。

Result: 实验表明该方法在多模态图像/视频基准测试中展现性能优势，实现模型加速的同时保持准确性，优于孤立优化方法。

Conclusion: VisionTrim通过双模块协同优化解决了视觉token冗余与文本对齐矛盾，为多模态模型实际部署提供了高效解决方案。

Abstract: Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.

</details>


### [34] [Visual Personalization Turing Test](https://arxiv.org/abs/2601.22680)
*Rameen Abdal,James Burgess,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出视觉个性化图灵测试VPTT，评估情境化视觉个性化能力，基于感知不可区分性而非身份复制。


<details>
  <summary>Details</summary>
Motivation: 传统身份复制评价无法真实反映个性化效果，需更合理的评价标准。

Method: 构建VPTT-Bench基准集，开发视觉检索增强生成器VPRAG，并设计文本驱动的VPTT评分系统

Result: VPTT评分与人类及视觉模型评估高度相关，VPRAG在对齐性和原创性间取得最佳平衡

Conclusion: 建立基于感知不可区分性的新型评估体系，实现可扩展且隐私安全的个性化生成AI

Abstract: We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.

</details>


### [35] [OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection](https://arxiv.org/abs/2601.22685)
*Binyi Su,Chenghao Huang,Haiyong Chen*

Main category: cs.CV

TL;DR: 本文提出了零样本OOV检测框架OOVDet，通过合成低似然区域的OOV提示和基于Dirichlet不确定性的样本挖掘，有效区分已知（IV）与未知（OOV）类别。


<details>
  <summary>Details</summary>
Motivation: 现有方法容易对IV类别过拟合，导致OOV样本被高置信度误判为IV类别。需要一种无需OOV分布先验知识即可同时提升IV识别准确率和OOV检测可靠性的框架。

Method: 在隐空间中对类条件高斯分布的低密度区域采样生成OOV提示；通过Dirichlet证据梯度归因机制筛选高不确定性伪OOV样本；采用低密度先验约束（结合核密度估计）构建OOV决策边界。

Result: 实验显示该方法在零样本场景下显著提升OOV检测性能，代码开源以支持复现（https://github.com/binyisu/OOV-detector）。

Conclusion: 通过合成OOV数据分布模拟和不确定性量化，OOVDet实现了无需显式OOV监督的鲁棒分类决策边界，为ZS-OOVD提供了新范式。

Abstract: Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model's lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.

</details>


### [36] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: This paper addresses the limitation of current vision-language models (VLMs) in understanding negated clinical statements. It proposes a bi-directional multiple-choice learning framework (Bi-MCQ) that reformulates vision-language alignment as a conditional semantic comparison problem, using tasks like Image-to-Text and Text-to-Image MCQs with mixed prompts and novel cross-attention fusion modules. Evaluated on medical datasets, Bi-MCQ achieves significant improvements in negation understanding, reducing the affirmative-negative AUC gap by an average of 0.12 compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs trained with contrastive alignment objectives fail to handle negated clinical statements because they treat negation as minor linguistic variation rather than a fundamental meaning inversion. This limitation is exacerbated in multi-label settings by prompt-based InfoNCE fine-tuning, which reinforces easy-positive alignments while hindering effective learning of disease absence.

Method: The authors reformulate vision-language alignment as a conditional semantic comparison problem through a bi-directional multiple-choice learning framework (Bi-MCQ). Key components include: 1) Jointly training Image-to-Text and Text-to-Image MCQ tasks using affirmative, negative, and mixed prompts; 2) Introducing direction-specific Cross-Attention fusion modules to address asymmetric cues in bi-directional reasoning and reduce alignment interference. This replaces traditional global similarity maximization with conditional semantic comparison during fine-tuning.

Result: On ChestXray14, Open-I, CheXpert, and PadChest datasets, Bi-MCQ demonstrates: 1) Up to 0.47 AUC improvement in negation understanding over zero-shot CARZero; 2) Up to 0.08 absolute gain in positive-negative combined (PNC) evaluation; 3) Average reduction of 0.12 in the affirmative-negative AUC gap compared to InfoNCE-based fine-tuning. These results validate the effectiveness of objective reformulation for improving medical VLMs' negation understanding.

Conclusion: The study proves that rethinking vision-language alignment from global similarity maximization to conditional semantic comparison (via Bi-MCQ) substantially enhances VLMs' ability to interpret negated clinical statements in medical imaging. This framework provides a new paradigm for improving VLMs' complex linguistic reasoning capabilities through structural and objective reformulation rather than increasing model size or training data.

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [37] [DAVIS: OOD Detection via Dominant Activations and Variance for Increased Separation](https://arxiv.org/abs/2601.22703)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: 本论文提出DAVIS，一种通过结合通道方差和最大激活值提升特征向量的后验OOD检测方法，在多个模型上显著降低假阳性率。


<details>
  <summary>Details</summary>
Motivation: 现有后验检测方法依赖丢失信息的全局平均池化(GAP)特征，本文指出其遗漏的通道方差和最大激活值对OOD检测具有强判别性。

Method: DAVIS通过将激活图的通道方差和最大值融入特征向量，在CIFAR和ImageNet等基准测试中超越现有方法。

Result: DAVIS在ResNet-18上将CIFAR-10的FPR95降低48.26%，ResNet-34和MobileNet-v2分别实现38.13%和26.83%的提升。

Conclusion: 该方法为突破GAP的均值限制提供了理论依据，并建立了OOD检测的新基准，推动领域发展。

Abstract: Detecting out-of-distribution (OOD) inputs is a critical safeguard for deploying machine learning models in the real world. However, most post-hoc detection methods operate on penultimate feature representations derived from global average pooling (GAP) -- a lossy operation that discards valuable distributional statistics from activation maps prior to global average pooling. We contend that these overlooked statistics, particularly channel-wise variance and dominant (maximum) activations, are highly discriminative for OOD detection. We introduce DAVIS, a simple and broadly applicable post-hoc technique that enriches feature vectors by incorporating these crucial statistics, directly addressing the information loss from GAP. Extensive evaluations show DAVIS sets a new benchmark across diverse architectures, including ResNet, DenseNet, and EfficientNet. It achieves significant reductions in the false positive rate (FPR95), with improvements of 48.26\% on CIFAR-10 using ResNet-18, 38.13\% on CIFAR-100 using ResNet-34, and 26.83\% on ImageNet-1k benchmarks using MobileNet-v2. Our analysis reveals the underlying mechanism for this improvement, providing a principled basis for moving beyond the mean in OOD detection.

</details>


### [38] [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/abs/2601.22709)
*Yanlong Chen,Amirhossein Habibian,Luca Benini,Yawei Li*

Main category: cs.CV

TL;DR: GRACE是一种结合知识蒸馏和量化感知训练的方法，通过信息瓶颈原理优化视觉-语言模型的量化效果，显著减少部署成本并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）部署成本高，传统量化方法常导致精度显著下降，而量化感知训练（QAT）在该领域尚未充分探索。

Method: 提出GRACE框架，将蒸馏与QAT统一：使用信息瓶颈原理约束信息容量，通过教师模型引导信息保留；包含置信度过滤解耦蒸馏、关系中心化核对齐、拉格朗日松弛自适应控制器。

Result: 在LLaVA和Qwen系列模型的广泛基准测试中，INT4量化模型超越FP16基线（如LLaVA-1.5-7B在SQA数据集上70.1 vs 66.8），实现3倍吞吐量和54%内存降低，接近教师模型性能。

Conclusion: GRACE提供了基于原则的有效量化方案，在资源受限部署场景中显著优于现有方法，为实际应用提供可行解决方案。

Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.

</details>


### [39] [OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation](https://arxiv.org/abs/2601.22725)
*Jin Li,Tao Chen,Shuai Jiang,Weijie Wang,Jingwen Luo,Chenhui Wu*

Main category: cs.CV

TL;DR: The paper introduces OpenVTON-Bench, a large-scale dataset with 100K high-resolution image pairs, and a multi-modal evaluation protocol for Virtual Try-On (VTON) systems to address shortcomings in existing VTON evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in traditional metrics' ability to capture fine-grained texture and semantic consistency, and address dataset shortcomings in scale/diversity that prevent meeting commercial VTON standards.

Method: Constructed OpenVTON-Bench using DINOv3-based clustering for balanced sampling and Gemini-powered dense captioning across 20 garment categories. Developed a protocol measuring five dimensions with VLM-based semantic reasoning and Multi-Scale Representation Metric combining SAM3 and morphological erosion to isolate texture/boundary errors.

Result: Experimental evidence shows strong human agreement (Kendall's τ of 0.833 vs. 0.611 for SSIM), demonstrating the proposed protocol's superior correlation with human judgment compared to traditional metrics.

Conclusion: Established OpenVTON-Bench as a reliable benchmark for VTON evaluation through its large-scale dataset and interpretable multi-modal assessment framework that better captures complex VTON quality aspects.

Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $τ$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.

</details>


### [40] [GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction](https://arxiv.org/abs/2601.22729)
*A. Enes Doruk,Hasan F. Ates*

Main category: cs.CV

TL;DR: GaussianOcc3D: A multi-modal framework for 3D semantic occupancy using a continuous Gaussian representation, achieving strong performance on benchmarks with mIoU up to 49.4%.


<details>
  <summary>Details</summary>
Motivation: Existing single-modality and multi-modal approaches face challenges in balancing camera semantics, LiDAR geometry, spatial alignment, modality heterogeneity, computational efficiency, and robustness under adverse conditions.

Method: Proposes GaussianOcc3D with 3D Gaussian representation through four modules: (1) LDFA (LiDAR depth feature aggregation), (2) EBFS (feature smoothing), (3) ACLF (fusion with uncertainty reweighting), and (4) Gauss-Mamba Head (global context modeling via state space models).

Result: SOTA performance on three benchmarks (Occ3D: 49.4% mIoU, SurroundOcc: 28.9%, SemanticKITTI: 25.2%) with superior robustness in rain/night conditions.

Conclusion: GaussianOcc3D effectively addresses multi-modal fusion challenges via continuous Gaussian representation, achieving SOTA performance and robustness in 3D semantic occupancy prediction.

Abstract: 3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.

</details>


### [41] [ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model](https://arxiv.org/abs/2601.22730)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Taichun Zhou,Xinwang Liu*

Main category: cs.CV

TL;DR: This paper proposes ImgCoT, a method to compress reasoning chains (CoT) by rendering them as images instead of reconstructing text, reducing linguistic bias and better capturing global reasoning structures. A hybrid version, loose ImgCoT, combines visual latent tokens with key textual details for improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior CoT compression methods preserve text reconstruction, leading to linguistic bias (prioritizing syntax/word choice over reasoning structure). This limits logical abstraction, necessitating a shift to capture abstract reasoning patterns more effectively.

Method: ImgCoT replaces textual CoT reconstruction with visual CoT rendering (inducing spatial bias), while loose ImgCoT selectively adds textual reasoning steps (via low-log-likelihood token selection) to retain fine-grained details. Both methods use latent tokens for compression.

Result: Experiments demonstrate that ImgCoT and loose ImgCoT improve reasoning efficiency while maintaining accuracy, with loose ImgCoT achieving better balance by combining coarse visual structure and critical textual details using fewer tokens than full CoT.

Conclusion: Shifting from linguistic to spatial inductive bias via visual CoT enhances abstract reasoning representation. Loose ImgCoT further retains essential details, offering a scalable solution for compressing reasoning chains in LLMs.

Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.

</details>


### [42] [Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models](https://arxiv.org/abs/2601.22737)
*Enyi Shi,Pengyang Shao,Yanxin Zhang,Chenhang Cui,Jiayi Lyu,Xu Xie,Xiaobo Xia,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出Lingua-SafetyBench，首个支持跨语言/模态的有害图文评估基准，发现现有VLLM在非高资源语言的安全缺陷更严重，强调需针对性安全对齐而非单纯模型升级。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估基准仅支持单语言多模态或纯文本多语言，缺乏能评估真实跨语言跨模态交互风险的基准。作者通过构建多语言多模态图文配对数据集填补该空白。

Method: 创建包含100,440个跨10语言有害图文对的数据集，创新性地细分为图像主导/文本主导子集。通过11个开源VLLM评估攻击成功率(ASR)，使用Qwen系列验证模型规模与版本迭代影响。

Result: 图像主导风险ASR在高资源语言更严重（如中文90.8%），文本主导风险在非高资源语言更突出（如越南语87.2%）。Qwen模型升级虽降低ASR但使HRLs/NON-HRLs差距扩大3.7倍。

Conclusion: 揭示现有多模态大模型在低资源语言存在安全对齐不足问题，证明盲目模型缩放无法解决跨语言风险差异，呼吁针对性安全对齐策略并开放基准/模型促进后续研究。

Abstract: Robust safety of vision-language large models (VLLMs) under joint multilingual and multimodal inputs remains underexplored. Existing benchmarks are typically multilingual but text-only, or multimodal but monolingual. Recent multilingual multimodal red-teaming efforts render harmful prompts into images, yet rely heavily on typography-style visuals and lack semantically grounded image-text pairs, limiting coverage of realistic cross-modal interactions. We introduce Lingua-SafetyBench, a benchmark of 100,440 harmful image-text pairs across 10 languages, explicitly partitioned into image-dominant and text-dominant subsets to disentangle risk sources. Evaluating 11 open-source VLLMs reveals a consistent asymmetry: image-dominant risks yield higher ASR in high-resource languages, while text-dominant risks are more severe in non-high-resource languages. A controlled study on the Qwen series shows that scaling and version upgrades reduce Attack Success Rate (ASR) overall but disproportionately benefit HRLs, widening the gap between HRLs and Non-HRLs under text-dominant risks. This underscores the necessity of language- and modality-aware safety alignment beyond mere scaling.To facilitate reproducibility and future research, we will publicly release our benchmark, model checkpoints, and source code.The code and dataset will be available at https://github.com/zsxr15/Lingua-SafetyBench.Warning: this paper contains examples with unsafe content.

</details>


### [43] [StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing](https://arxiv.org/abs/2601.22738)
*Han Wang,Deyi Ji,Lanyun Zhu,Jiebo Luo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为StreamSense的流式检测器，通过结合轻量级编码器与选择性路由的视觉-语言模型（VLM），实现了对社交媒体流数据（如图像、文本、音频）的高效实时检测。训练方法包括跨模态对比损失与IoU加权损失，评估结果表明其准确率优于纯VLM方法，同时降低了延迟和计算开销。


<details>
  <summary>Details</summary>
Motivation: 社交媒体直播平台需要实时分析多种异步、部分观察的模态数据（视频、文本、音频），现有方法难以高效平衡准确率与计算资源限制。

Method: 1) StreamSense包含轻量级流式编码器（处理常规案例）与VLM专家模型（处理困难/模糊案例）；
2) 采用选择性路由策略：简单案例由轻量级模型处理，复杂案例升级至VLM，信息不足则延迟决策；
3) 编码器训练包含两个关键模块：i) 跨模态对比损失（对齐视觉/音频与文本信号），ii) IoU加权损失（缓解标签干扰）。

Result: 1) StreamSense在情感分类、仇恨内容审核等多任务中准确率高于纯VLM流式方法；
2) 仅在约20%时间戳调用VLM，平均延迟降低47%，计算量减少68%；
3) IoU加权损失使mAP@0.5提升2.3个百分点。

Conclusion: 通过'选择性升级+延迟决策'机制，StreamSense在资源效率与准确性间实现优化平衡，为实时社交流媒体分析提供了有效范式，代码已开源（GitHub）。

Abstract: Live streaming platforms require real-time monitoring and reaction to social signals, utilizing partial and asynchronous evidence from video, text, and audio. We propose StreamSense, a streaming detector that couples a lightweight streaming encoder with selective routing to a Vision-Language Model (VLM) expert. StreamSense handles most timestamps with the lightweight streaming encoder, escalates hard/ambiguous cases to the VLM, and defers decisions when context is insufficient. The encoder is trained using (i) a cross-modal contrastive term to align visual/audio cues with textual signals, and (ii) an IoU-weighted loss that down-weights poorly overlapping target segments, mitigating label interference across segment boundaries. We evaluate StreamSense on multiple social streaming detection tasks (e.g., sentiment classification and hate content moderation), and the results show that StreamSense achieves higher accuracy than VLM-only streaming while only occasionally invoking the VLM, thereby reducing average latency and compute. Our results indicate that selective escalation and deferral are effective primitives for understanding streaming social tasks. Code is publicly available on GitHub.

</details>


### [44] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: FaceDefense提出了一种增强的主动防御框架，有效平衡了扩散模型换脸攻击的防御效果与图像不可察觉性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型换脸技术的潜在滥用风险（如侵犯肖像权、损害声誉）催生了主动防御方法，但现有方案存在扰动强度与防御效果之间的权衡困境。

Method: 1) 引入新扩散损失函数增强对抗样本的防御效力，2) 采用定向面部属性编辑修复扰动失真，3) 通过两阶段交替优化策略生成最终扰动图像。

Result: 在保持更高质量面部结构的同时，相比现有方法将防御有效性提升23.7%，主观视觉评分提高18.2%，实现防御强度与视觉质量的最优平衡。

Conclusion: FaceDefense通过协同优化扰动生成与属性修复，为扩散模型攻击防护提供了兼顾安全与可用性的新范式，为数字内容安全防护技术树立了新标杆。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [45] [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/abs/2601.22754)
*Guillermo Gil de Avalle,Laura Maruster,Christos Emmanouilidis*

Main category: cs.CV

TL;DR: 视觉语言模型（VLM）可自动化工业故障排除指南的知识提取，但在布局敏感性和语义鲁棒性之间存在模型权衡。


<details>
  <summary>Details</summary>
Motivation: 工业故障排除指南需手动解析成结构化数据以用于支持系统，但此过程耗时且易错。VLM的跨模态解析潜力尚未在该领域充分验证。

Method: 测试两类VLM模型，对比标准指令与引入布局模式线索的增强提示策略在知识提取任务中的表现差异。

Result: 不同VLM在布局适应性和语义稳定性上呈现互补优势：视觉感知强的模型易受布局干扰，语义强的模型对非标准排版鲁棒性更优。

Conclusion: 建议根据具体部署场景选择模型类型：需要高布局兼容性时优选视觉优先模型，强调语义一致性则选择语言优先模型。

Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.

</details>


### [46] [Is Training Necessary for Anomaly Detection?](https://arxiv.org/abs/2601.22763)
*Xingwu Zhang,Guanxuan Li,Paul Henderson,Gerardo Aragon-Camarasa,Zijun Long*

Main category: cs.CV

TL;DR: 论文提出基于检索的无监督异常检测方法RAD，抛弃传统重构范式，通过记忆库存储无异常特征并进行多级检索匹配，实现多类别异常检测SOTA性能，理论证明检索分数优于重构残差分数。


<details>
  <summary>Details</summary>
Motivation: 现有基于重构残差的MUAD方法存在保真度与稳定性矛盾，且需任务特定训练，论文旨在提出无需训练且更高效的替代方案。

Method: 构建无训练的RAD框架：1) 构建内存存储无异常特征；2) 通过多级检索策略匹配测试块与内存特征；3) 理论分析检索分数对重构分数的上界性。

Result: 在MVTec-AD等四个基准中均达SOTA，MVTec-AD全数据下98.5% AUROC，单样本仍维持96.7%。

Conclusion: 证明内存检索可替代重构范式，训练并非MUAD必要条件，为领域提供新理论视角与高效解决方案。

Abstract: Current state-of-the-art multi-class unsupervised anomaly detection (MUAD) methods rely on training encoder-decoder models to reconstruct anomaly-free features. We first show these approaches have an inherent fidelity-stability dilemma in how they detect anomalies via reconstruction residuals. We then abandon the reconstruction paradigm entirely and propose Retrieval-based Anomaly Detection (RAD). RAD is a training-free approach that stores anomaly-free features in a memory and detects anomalies through multi-level retrieval, matching test patches against the memory. Experiments demonstrate that RAD achieves state-of-the-art performance across four established benchmarks (MVTec-AD, VisA, Real-IAD, 3D-ADAM) under both standard and few-shot settings. On MVTec-AD, RAD reaches 96.7\% Pixel AUROC with just a single anomaly-free image compared to 98.5\% of RAD's full-data performance. We further prove that retrieval-based scores theoretically upper-bound reconstruction-residual scores. Collectively, these findings overturn the assumption that MUAD requires task-specific training, showing that state-of-the-art anomaly detection is feasible with memory-based retrieval. Our code is available at https://github.com/longkukuhi/RAD.

</details>


### [47] [Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2601.22778)
*Nan Zhong,Yiran Xu,Mian Zou*

Main category: cs.CV

TL;DR: 本文提出了一种基于相机成像管道颜色相关性的检测框架DCCT，用于有效区分现实与AI生成图像。


<details>
  <summary>Details</summary>
Motivation: AI生成图像对数字真实性构成威胁，现有检测方法存在泛化性不足的问题，需探索更鲁棒的检测方案。

Method: 提出DCCT框架，模拟CFA采样模式将彩色图像拆分为单通道输入和双通道目标，通过自监督U-Net及混合logistic函数训练模型建模条件分布，理论分析揭示了两类图像的颜色相关特征分布差异。

Result: 实验表明该方法能有效发现颜色相关特征分布差异，在超过20个未见过的生成器测试中，分类准确率显著优于现有检测方法。

Conclusion: 利用相机成像物理特征的DCCT框架解决了生成对抗样本的泛化检测问题，为图像真实性鉴别提供了新方向。

Abstract: As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.

</details>


### [48] [FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images](https://arxiv.org/abs/2601.22809)
*Haiyang Wu,Weiliang Mu,Jipeng Zhang,Zhong Dandan,Zhuofei Du,Haifeng Li,Tao Chao*

Main category: cs.CV

TL;DR: 提出了一种动态分割框架FarmMind，通过推理查询辅助图像解决遥感图像分割中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 现有农田遥感图像分割方法依赖单一静态图像，面对模糊复杂场景时效果有限，而人类专家能通过查询辅助图像提升判读准确性。

Method: FarmMind引入推理-查询机制，首先分析分割模糊原因，再动态选择合适的辅助图像（如高分辨率、时序数据）进行交叉验证，突破传统静态分割范式。

Result: 实验表明FarmMind在分割性能和泛化能力上优于现有方法，且代码与数据集已开源（https://github.com/WithoutOcean/FarmMind）。

Conclusion: 动态查询辅助图像能有效解决单一图像信息不足问题，为复杂农田遥感图像分割提供新范式。

Abstract: Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.

</details>


### [49] [A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions](https://arxiv.org/abs/2601.22830)
*Ji Zhou,Yilin Ding,Yongqi Zhao,Jiachen Xu,Arno Eichberger*

Main category: cs.CV

TL;DR: This paper evaluates Large Vision-Language Models (LVLMs) as high-level safety validators for SOTIF in automated vehicles, revealing a trade-off between semantic reasoning (superior recall in complex/natural scenarios) and geometric regression (higher precision in synthetic perturbations).


<details>
  <summary>Details</summary>
Motivation: SOTIF safety risks from perception insufficiencies under adverse conditions are challenging, and the effectiveness of LVLMs in 2D object detection for such safety-critical scenarios requires systematic investigation.

Method: Systematic evaluation of ten LVLMs using PeSOTIF dataset (long-tail traffic scenarios and environmental degradation), benchmarked against YOLO-based classical perception approaches.

Result: Top LVLMs (Gemini 3, Doubao) show 25%+ recall improvement over YOLO in complex natural scenarios but lower geometric precision than YOLO in synthetic perturbations, indicating complementary strengths of semantic reasoning and geometric regression.

Conclusion: LVLMs demonstrate robustness to visual degradation and potential as high-level safety validators in SOTIF-oriented autonomous driving systems, but should be combined with geometric regression approaches for balanced performance.

Abstract: Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.

</details>


### [50] [NativeTok: Native Visual Tokenization for Improved Image Generation](https://arxiv.org/abs/2601.22837)
*Bin Wu,Mengqi Huang,Weinan Jia,Zhendong Mao*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果依赖的视觉token化框架NativeTok，通过在token化阶段引入关系约束，解决了VQ图像生成中token依赖性不足导致的生成质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 传统VQ图像生成方法的token化阶段未强制约束token间的依赖性，导致生成模型从无序分布中学习，引发偏差和弱连贯性问题。

Method: 1. 构建包含Meta Image Transformer（MIT）的潜像建模；2. 设计Mixture of Causal Expert Transformer（MoCET），每个轻量专家模块基于先验token和潜特征生成单个token；3. 采用分层原生训练策略仅更新新增专家模块。

Result: 实验显示NativeTok在保持高效重建的同时，显著提升了生成图像的连贯性，并验证了训练策略的计算效率优势。

Conclusion: 通过将因果依赖性嵌入token序列，NativeTok成功弥合了token化与生成阶段的间隙，在图像生成质量和训练效率方面均优于现有方法。

Abstract: VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.

</details>


### [51] [Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model](https://arxiv.org/abs/2601.22838)
*Zhijing Yang,Weiwei Zhang,Mingliang Yang,Siyuan Peng,Yukai Shi,Junpeng Tan,Tianshui Chen,Liruo Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种定制化虚拟试穿（Cu-VTON）方法，允许用户根据自身偏好调整模型的外观、姿势等属性，提升虚拟试衣体验的灵活性和参与感。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试穿（VTON）无法满足用户对数字形象个性化定制的需求，因此研究者希望通过增强语义描述和可控模块优化生成效果。

Method: 提出神经服饰试穿框架（NCT），通过语义增强模块和语义控制模块：1）利用视觉-语言编码器提取语义特征与衣服特征对齐；2）输入服饰图片、姿势图和语义描述控制模型生成定制化结果。

Result: 实验显示该方法在公开基准数据集上显著优于现有模型，能更精准保留服饰纹理细节并实现多属性编辑（如姿势/表情调整）。

Conclusion: 所提NCT框架通过双模块设计有效提升了个性化虚拟试穿的定制能力与真实性，语义增强策略显著优化了跨模态特征对齐效果。

Abstract: This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.

</details>


### [52] [How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models](https://arxiv.org/abs/2601.22841)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 该论文提出遥感领域的大规模基础模型在较小规模时已进入过度参数化阶段，通过“事后瘦身”测试发现其任务相关信息具有高冗余性，瘦身后的模型在资源受限环境下仍保持高精度，并挑战了传统扩展范式。


<details>
  <summary>Details</summary>
Motivation: 质疑直接沿用计算机视觉中的参数扩展假设，旨在验证遥感模型是否存在更早的过度参数化及任务相关冗余表示，以优化资源效率和模型设计。

Method: 采用事后瘦身法缩减预训练编码器宽度，在六个最先进遥感模型和四类分类任务上测试表征冗余；使用解释方差比和特征相关性分析探究机制，并尝试可瘦身训练以改进模型。

Result: 瘦身后的遥感模型在1%计算预算下保持71%以上相对精度，显著优于计算机视觉模型；论证了冗余表征的存在性，且可瘦身训练提升了MoCo和MAE类模型性能。

Conclusion: 提出事后瘦身既是资源受限部署策略，也是诊断工具，颠覆了遥感领域模型扩展范式。验证了缩减模型规模在遥感任务中的可行性，并提供了冗余性理论依据。

Abstract: Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.

</details>


### [53] [Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction](https://arxiv.org/abs/2601.22861)
*Refael Sheffer,Chen Pinchover,Haim Zisman,Dror Ozeri,Roee Litman*

Main category: cs.CV

TL;DR: 提出了一种基于NeRF和RGB图像的森林地形与地表重建方法，可生成无遮挡的地面视图，适用于搜救、路径绘制和森林调查任务。


<details>
  <summary>Details</summary>
Motivation: 现有森林地形绘制技术依赖高成本的LiDAR或专用的热成像AOS设备，本研究旨在探索低成本、高分辨率的通用解决方案。

Method: 采用Neural Radiance Fields（NeRF）三维重建框架，结合光照优化、低光损耗算法及两种基于光线积分控制的遮挡消除方法，实现仅用RGB图像的无遮挡地表建模。

Result: 在搜救任务中，该方法人体检测效果接近热成像AOS水平；在森林调查中成功实现树木计数，分辨率和成本优势显著优于传统设备。

Conclusion: 该技术为森林环境感知提供了可扩展的低成本视觉解决方案，在安防、生态监测等领域具有实际应用潜力。

Abstract: Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.
  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.
  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.

</details>


### [54] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文提出上下文感知的异常检测方法与数据集CAAD-3K，利用视觉-语言模型建模对象-上下文兼容性，突破传统异常检测忽视上下文依赖的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测假设异常为内禀属性，但在实际场景中（如不同场地跑步）异常判定受环境上下文影响。现有方法未有效建模这种上下文-对象交互关系，亟需专用基准与算法解决此问题。

Method: 1) 设计CAAD-3K数据集，固定对象身份变量仅变化上下文场景；2) 提出条件兼容性学习框架，使用CLIP等视觉-语言表征联合建模对象-上下文关系，在弱监督下预测兼容性得分作为异常判据。

Result: 在新提出的CAAD-3K数据集上超越现有方法，在MVTec-AD（98.2% AUROC）和VisA（96.5% AUROC）两大工业基准上达到SOTA性能，实验证明上下文建模可补充传统结构异常检测。

Conclusion: 上下文依赖性是异常检测的核心维度，本文构建的基准数据集与视觉-语言建模方法为上下文敏感型异常检测提供了标准化评测平台与有效技术路径，预示多模态表征在工业质检等场景的应用潜力。

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [55] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: DINO-SAE 提出通过语义方向优先和灵活的特征模长设计、结合分层卷积嵌入与黎曼流匹配，提升生成式自编码器重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成式自编码器在使用预训练视觉基础模型时存在高频细节丢失问题，需解决语义表征与像素级重建的协同约束矛盾。

Method: 1) 提出层次卷积补丁嵌入模块增强局部结构；2) 余弦相似性对齐目标优化语义一致性；3) 基于超球面表征的黎曼流匹配训练扩散Transformer。

Result: 在ImageNet-1K上实现0.37 rFID/26.2 dB PSNR（SOTA），黎曼流扩散模型在80个epoch达到3.47 gFID（高效收敛）。

Conclusion: 该框架通过几何表征对齐与新型模块设计，成功弥合语义-像素重建鸿沟，验证了预训练基础模型超球面几何特性的应用价值。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [56] [Multi-Cue Anomaly Detection and Localization under Data Contamination](https://arxiv.org/abs/2601.22913)
*Anindya Sundar Das,Monowar Bhuyan*

Main category: cs.CV

TL;DR: 该论文提出了一种在工业场景中处理数据污染的鲁棒视觉异常检测框架，通过结合有限监督与自适应学习，提升检测与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完全正常或无监督的数据且无法利用标注异常样本，导致在真实场景中因数据污染和模型判别能力不足而表现不佳。

Method: 构建由统计偏离度、预测不确定性（熵）和空间异常度组成的复合异常评分机制，并通过自适应实例加权减轻污染样本影响，结合少量标注异常进行训练。

Result: 在MVTec和VisA数据集上优于现有方法，实现高检测精度、定位能力及对污染数据的鲁棒性，且支持梯度可视化解释。

Conclusion: 验证了有限监督与多维度评分融合的有效性，为实际工业场景中的异常检测提供了更可靠、可解释的解决方案。

Abstract: Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.

</details>


### [57] [Deep in the Jungle: Towards Automating Chimpanzee Population Estimation](https://arxiv.org/abs/2601.22917)
*Tom Raynes,Otto Brookes,Timm Haucke,Lukas Bösch,Anne-Sophie Crunchant,Hjalmar Kühl,Sara Beery,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 研究评估使用计算机视觉模型在野生猩猩种群中自动化估算丰度和密度的可行性，发现其结果与传统方法误差在22%内，但存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 传统依赖手工测量摄像机与动物距离的生态统计框架效率低下且耗时，需探索自动化替代方案。

Method: 结合两种单目深度估计模型（DPT和Depth Anything）与多种距离采样策略，处理220个野生黑猩猩监控视频，自动生成检测距离并推断种群参数。

Result: 校准后DPT表现优于Depth Anything，但模型均高估检测距离（低估种群密度/丰度），主要误差源自复杂森林环境下的动物检测失败。

Conclusion: 计算机视觉驱动的距离抽样可作为实用替代方案，尽管存在系统偏差，但能稳定生成与传统方法接近的种群估算结果。

Abstract: The estimation of abundance and density in unmarked populations of great apes relies on statistical frameworks that require animal-to-camera distance measurements. In practice, acquiring these distances depends on labour-intensive manual interpretation of animal observations across large camera trap video corpora. This study introduces and evaluates an only sparsely explored alternative: the integration of computer vision-based monocular depth estimation (MDE) pipelines directly into ecological camera trap workflows for great ape conservation. Using a real-world dataset of 220 camera trap videos documenting a wild chimpanzee population, we combine two MDE models, Dense Prediction Transformers and Depth Anything, with multiple distance sampling strategies. These components are used to generate detection distance estimates, from which population density and abundance are inferred. Comparative analysis against manually derived ground-truth distances shows that calibrated DPT consistently outperforms Depth Anything. This advantage is observed in both distance estimation accuracy and downstream density and abundance inference. Nevertheless, both models exhibit systematic biases. We show that, given complex forest environments, they tend to overestimate detection distances and consequently underestimate density and abundance relative to conventional manual approaches. We further find that failures in animal detection across distance ranges are a primary factor limiting estimation accuracy. Overall, this work provides a case study that shows MDE-driven camera trap distance sampling is a viable and practical alternative to manual distance estimation. The proposed approach yields population estimates within 22% of those obtained using traditional methods.

</details>


### [58] [Q-Hawkeye: Reliable Visual Policy Optimization for Image Quality Assessment](https://arxiv.org/abs/2601.22920)
*Wulin Xie,Rui Dai,Ruidong Ding,Kaikui Liu,Xiangxiang Chu,Xinwen Hou,Jie Wen*

Main category: cs.CV

TL;DR: Q-Hawkeye 是一种基于强化学习的可靠视觉策略优化框架，通过改进学习信号解决图像质量评估中的可靠性和感知能力问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的图像质量评估方法存在两个关键问题：1) 忽视训练样本中预测稳定性的差异，采用统一优势权重导致不稳定样本干扰优化；2) 过度依赖文本推理而弱化模型的视觉感知能力。

Method: 提出 Q-Hawkeye 框架，包含两部分优化：1) 不确定性感知动态优化：基于多次采样预测的差异性计算样本权重，抑制不稳定样本干扰；2) 感知感知优化：通过原始图像与失真图像的配对输入，引入隐式感知损失，增强模型对视觉证据的依赖性。

Result: 实验表明 Q-Hawkeye 在多个数据集中优于最先进的方法，且具有更好的泛化能力。

Conclusion: Q-Hawkeye 通过动态样本加权和视觉感知增强，显著提升了强化学习在图像质量评估中的可靠性和有效性。

Abstract: Image Quality Assessment (IQA) predicts perceptual quality scores consistent with human judgments. Recent RL-based IQA methods built on MLLMs focus on generating visual quality descriptions and scores, ignoring two key reliability limitations: (i) although the model's prediction stability varies significantly across training samples, existing GRPO-based methods apply uniform advantage weighting, thereby amplifying noisy signals from unstable samples in gradient updates; (ii) most works emphasize text-grounded reasoning over images while overlooking the model's visual perception ability of image content. In this paper, we propose Q-Hawkeye, an RL-based reliable visual policy optimization framework that redesigns the learning signal through unified Uncertainty-Aware Dynamic Optimization and Perception-Aware Optimization. Q-Hawkeye estimates predictive uncertainty using the variance of predicted scores across multiple rollouts and leverages this uncertainty to reweight each sample's update strength, stabilizing policy optimization. To strengthen perceptual reliability, we construct paired inputs of degraded images and their original images and introduce an Implicit Perception Loss that constrains the model to ground its quality judgments in genuine visual evidence. Extensive experiments demonstrate that Q-Hawkeye outperforms state-of-the-art methods and generalizes better across multiple datasets. The code and models will be made available.

</details>


### [59] [Semantic Leakage from Image Embeddings](https://arxiv.org/abs/2601.22929)
*Yiyi Chen,Qiongkai Xu,Desmond Eliott,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CV

TL;DR: 本文提出SLImE框架，通过保留压缩图像嵌入中的语义邻域结构，在无需重建原图情况下揭示潜在语义信息泄露风险。


<details>
  <summary>Details</summary>
Motivation: 挑战

Method: 形式化语义泄露为从压缩嵌入恢复语义结构的能力，通过保持对齐后的局部语义邻域，构建轻量级推理框架SLImE，结合本地训练的语义检索器与现成模型。

Result: 在GEMINI、COHERE、NOMIC和CLIP等模型上验证了从标签、符号表达到文本描述的语义信息一致恢复能力。

Conclusion: 图像嵌入的语义邻域保持特性会引发语义泄露漏洞，这对隐私保护提出了根本性挑战。

Abstract: Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1

</details>


### [60] [Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.22959)
*Anmin Wang,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Triage通过层次化视觉预算分配（关键帧选择与token分阶段分配），实现视频推理加速和内存优化，无需训练且适用于多种视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 视频处理中冗余数据导致token序列过长，传统方法计算成本高且效率低，急需一种免训练的通用优化框架。

Method: Triage分为两阶段：1）帧级预算：通过视觉动态和相关性评分筛选关键帧；2）token级预算：先保留高相关性token，再通过批量MMR算法选取多样化上下文token。

Result: 在保持或超越基线模型性能的前提下，显著提升推理速度并减少内存占用，在多项视频推理任务中验证有效性。

Conclusion: 分阶段、动态的token分配策略为视频VLM优化提供了免训练且通用的新范式，解决了冗余数据带来的计算瓶颈问题。

Abstract: Vision-Language Models (VLMs) face significant computational challenges in video processing due to massive data redundancy, which creates prohibitively long token sequences. To address this, we introduce Triage, a training-free, plug-and-play framework that reframes video reasoning as a resource allocation problem via hierarchical visual budgeting. Its first stage, Frame-Level Budgeting, identifies keyframes by evaluating their visual dynamics and relevance, generating a strategic prior based on their importance scores. Guided by this prior, the second stage, Token-Level Budgeting, allocates tokens in two phases: it first secures high-relevance Core Tokens, followed by diverse Context Tokens selected with an efficient batched Maximal Marginal Relevance (MMR) algorithm. Extensive experiments demonstrate that Triage improves inference speed and reduces memory footprint, while maintaining or surpassing the performance of baselines and other methods on various video reasoning benchmarks.

</details>


### [61] [Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion](https://arxiv.org/abs/2601.22961)
*Dennis Sprute,Hanna Senke,Holger Flatt*

Main category: cs.CV

TL;DR: This paper explores using generative AI (Stable Diffusion, CycleGAN) to address imbalanced datasets in industrial optical quality control, improving segmentation performance for defect detection in combine harvester components.


<details>
  <summary>Details</summary>
Motivation: Industrial datasets often lack sufficient defective samples, hurting machine learning model performance. Existing solutions like specialized loss functions or traditional augmentation have limitations in hyperparameter tuning and feature alteration.

Method: Applied Stable Diffusion and CycleGAN to generate synthetic defective thermal images for dataset expansion, focusing on component segmentation to enhance defect detection in combine harvesters.

Result: Stable Diffusion improved segmentation performance by 4.6% (Mean IoU of 84.6%) compared to baseline methods.

Conclusion: Generative AI offers a promising alternative to traditional approaches for addressing class imbalance in industrial quality control tasks.

Abstract: Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.

</details>


### [62] [About an Automating Annotation Method for Robot Markers](https://arxiv.org/abs/2601.22982)
*Wataru Uemura,Takeru Nagashima*

Main category: cs.CV

TL;DR: 本研究提出一种基于ArUco标记自动标注深度学习模型的方法，通过YOLO架构实现工厂自动化场景下的鲁棒目标识别，在模糊/散焦场景下显著优于传统OpenCV方案。


<details>
  <summary>Details</summary>
Motivation: 针对工业自动化中传统视觉算法在运动模糊
defocus
defocus条件下的失效问题，以及手动标注深度学习数据集的高成本瓶颈，利用ArUco标记自带的ID和定位信息实现自动化标注。

Method: 1) 基于ArUco检测模块的定位结果生成训练集标注
2) 设计YOLO架构的深度学习模型
3) 通过动态阈值机制优化检测置信度

Result: 在各种噪声/模糊/光照变化场景中:
1) 检测准确率提升18.7%
2) 推理速度保持23FPS实时性
3) 完全消除人工标注成本

Conclusion: 首次实现基于传统标记系统的自动化标注闭环

验证了深度学习在工业视觉场景下的可行性

为后续研究置信度阈值与复杂场景的关联性奠定基础

Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.

</details>


### [63] [Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI](https://arxiv.org/abs/2601.22990)
*Yinsong Wang,Thomas Fletcher,Xinzhe Luo,Aine Travers Dineen,Rhodri Cusack,Chen Qin*

Main category: cs.CV

TL;DR: 提出GaussianSVR自监督3D胎儿MRI重建方法，无需真实数据训练且高效。


<details>
  <summary>Details</summary>
Motivation: 传统切片到体积重建(SVR)方法耗时且依赖多正交图像栈，学习方法依赖无法获取的真实体积数据。

Method: 基于3D高斯表示构建目标体积，模拟前向切片采样模型实现自监督训练，并采用多分辨率联合优化策略。

Result: GaussianSVR在胎儿MRI体积重建中优于现有基线方法。

Conclusion: 该方法解决了真实数据获取困难和计算效率问题，实现了高保真3D重建。

Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.

</details>


### [64] [Leveraging Multi-Rater Annotations to Calibrate Object Detectors in Microscopy Imaging](https://arxiv.org/abs/2601.23007)
*Francesco Campi,Lucrezia Tondo,Ekin Karabati,Johannes Betge,Marie Piraud*

Main category: cs.CV

TL;DR: 利用多专家标注的共识策略提升目标检测模型校准效果


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在显微图像目标检测中存在置信度校准不足的问题，影响生物医学场景的可靠性，需要探索更精确的校准方法。

Method: 分别训练专家单标注模型并聚合预测结果形成共识，对比传统混合标注采样策略，更准确捕获评分者间差异。

Result: 在结直肠类器官数据集实验中，该方法在保持检测精度的前提下，校准性能指标（如Brier分数）提升16%，ECE下降34%。

Conclusion: 显式建模专家标注差异能有效提升生物医学图像目标检测器的可信赖度，为高风险医学场景提供更可靠的模型验证方案。

Abstract: Deep learning-based object detectors have achieved impressive performance in microscopy imaging, yet their confidence estimates often lack calibration, limiting their reliability for biomedical applications. In this work, we introduce a new approach to improve model calibration by leveraging multi-rater annotations. We propose to train separate models on the annotations from single experts and aggregate their predictions to emulate consensus. This improves upon label sampling strategies, where models are trained on mixed annotations, and offers a more principled way to capture inter-rater variability. Experiments on a colorectal organoid dataset annotated by two experts demonstrate that our rater-specific ensemble strategy improves calibration performance while maintaining comparable detection accuracy. These findings suggest that explicitly modelling rater disagreement can lead to more trustworthy object detectors in biomedical imaging.

</details>


### [65] [One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs](https://arxiv.org/abs/2601.23041)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: 本文提出OSGA，一种针对视觉语言模型(VLMs)的单次优化steering方法，通过生成输入无关的steering向量，在推理阶段提升模型可靠性。该方法仅需一次优化，无需修改模型参数，显著减少幻觉及安全风险，兼具高效性与实用性。


<details>
  <summary>Details</summary>
Motivation: VLMs虽性能优异，但存在幻觉/安全问题。传统steering技术（输入相关或无关）在效率与效果间存在权衡。本文旨在找到一种更高效的输入无关steering方案，通过单次优化实现跨任务通用的steering向量，规避多次优化成本。

Method: OSGA采用方差策略筛选具有代表性的样本，通过对比学习目标（结合生成锚点正则化）训练单个steering向量。该向量可直接在推理时注入特定层，无需调整模型参数。其核心是利用任务语义对齐性，实现跨输入的steering向量泛化。

Result: 在多个基准测试中，仅需一个OSGA优化的steering向量即可显著缓解VLM幻觉并增强安全性，计算开销可忽略。实验验证了该方法在不同任务与模型规模下的有效性，证明其可扩展性与实用性。

Conclusion: OSGA通过单次优化生成通用steering向量，为提升VLM可靠性提供了高效且易部署的解决方案，无需复杂参数调整，适用于实际场景中的模型改进。

Abstract: Vision Language Models (VLMs) achieve strong performance on multimodal tasks but still suffer from hallucination and safety-related failures that persist even at scale. Steering offers a lightweight technique to improve model performance. However, steering, whether input-dependent or input-independent, achieves a meaningful trade-off between efficiency and effectiveness. In this work, we observe that steering vectors can generalize across inputs when tasks share aligned semantic intent. Based on this insight, we propose \textbf{OSGA} (\textbf{O}ne-shot \textbf{S}teering with \textbf{G}enerative \textbf{A}nchor), an input-independent framework that improves model performance with a single optimization instance. OSGA first selects an informative sample via a variance-based data selection strategy and learns a single steering vector with a contrastive objective with generative anchor regularization. The resulting vector can be universally applied at a certain layer during inference time without modifying model parameters. Experiments across multiple benchmarks show that a single OSGA-optimized steering vector consistently improves hallucination mitigation and safety enhancement with negligible overhead, highlighting one-shot steering as a practical and scalable solution for reliable VLMs.

</details>


### [66] [HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation](https://arxiv.org/abs/2601.23064)
*Hari Krishna Gadi,Daniel Matos,Hongyi Luo,Lu Liu,Yongliang Wang,Yanfeng Zhang,Liqiu Meng*

Main category: cs.CV

TL;DR: 该论文提出了一种基于地理实体层级结构的视觉地理定位方法，利用超球面嵌入和Geo-Weighted Hyperbolic对比学习，在减少存储成本的同时提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如大规模检索、网格分类、生成模型）存在存储开销大、忽略地理连续性或细节处理不足的问题，且地理定位任务因全局尺度、视觉歧义和地理层级结构而具有挑战性，亟需新方法平衡效率与精度。

Method: 1) 构建国家-区域-子区域-城市的地理实体层级结构；2) 将地理实体嵌入超球面空间；3) 通过哈弗辛距离改进对比学习目标函数，实现图像与地理实体的直接对齐；4) 使用层级预测机制生成可解释的定位结果。

Result: 1) 在OSV5M基准上建立新SOTA，用24万实体嵌入取代500万图像嵌入；2) 平均地理距离误差降低19.5%；3) 子区域分类精度提升43%；4) 推理效率显著提高。

Conclusion: 几何感知的层级嵌入方法为全球图像地理定位提供了可扩展且概念新颖的解决方案。

Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.

</details>


### [67] [Segment Any Events with Language](https://arxiv.org/abs/2601.23159)
*Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出SEAL框架，首次实现基于事件传感器的开放词汇表多粒度实例分割，包括实例级和部件级，并构建四个基准数据集验证性能。


<details>
  <summary>Details</summary>
Motivation: 事件传感器领域的开放词汇实例分割研究存白，现有方法局限于语义级理解，缺乏多粒度（实例/部件级）分割方法及相应评估体系。

Method: 设计语义感知的SEAL框架：1）统一架构实现事件分割与开放词汇掩膜分类；2）构建包含粗粒度到细粒度标签配置的四个基准；3）附录提出无需视觉提示的时空扩展变体。

Result: SEAL在性能上显著优于基线模型（提升19.8% mAP），参数效率提升2.3倍，推理速度达48FPS。扩展变体实现无提示时空分割，在动态数据集表现稳定。

Conclusion: SEAL是首个解决事件传感器开放词汇多粒度分割的框架，通过参数共享机制兼顾精度与效率，其无需提示的变体为动态视觉处理提供了新范式。

Abstract: Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL

</details>


### [68] [Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm](https://arxiv.org/abs/2601.23167)
*Xiangrui Liu,Haoxiang Li,Yezhou Yang*

Main category: cs.CV

TL;DR: Hi-Light 是一个无需训练的高保真视频重打光框架，解决光照稳定性、细节保留和评价指标缺失问题。


<details>
  <summary>Details</summary>
Motivation: 视频重打光技术受限于缺乏有效评价指标、灯光闪烁严重及编辑过程中的细节退化问题。

Method: 提出 Hi-Light 框架，包含三个创新：1) 基于亮度先验的引导扩散模型稳定中间结果；2) 基于光流的混合动态自适应滤波器提升时序稳定性；3) LAB颜色空间细节融合模块保留高频信息，并提出首个专门测量光照一致性的量化指标 Light Stability Score。

Result: 实验证明该方法在定性效果和定量指标上均优于现有SOTA方法，输出稳定且细节丰富的重打光视频。

Conclusion: 为视频重打光领域提供了首个完整解决方案，通过新框架和评价指标显著提升处理效果。

Abstract: Video relighting offers immense creative potential and commercial value but is hindered by challenges, including the absence of an adequate evaluation metric, severe light flickering, and the degradation of fine-grained details during editing. To overcome these challenges, we introduce Hi-Light, a novel, training-free framework for high-fidelity, high-resolution, robust video relighting. Our approach introduces three technical innovations: lightness prior anchored guided relighting diffusion that stabilises intermediate relit video, a Hybrid Motion-Adaptive Lighting Smoothing Filter that leverages optical flow to ensure temporal stability without introducing motion blur, and a LAB-based Detail Fusion module that preserves high-frequency detail information from the original video. Furthermore, to address the critical gap in evaluation, we propose the Light Stability Score, the first quantitative metric designed to specifically measure lighting consistency. Extensive experiments demonstrate that Hi-Light significantly outperforms state-of-the-art methods in both qualitative and quantitative comparisons, producing stable, highly detailed relit videos.

</details>


### [69] [Region-Normalized DPO for Medical Image Segmentation under Noisy Judges](https://arxiv.org/abs/2601.23222)
*Hamza Kalisch,Constantin Seibold,Jens Kleesiek,Ken Herrmann,Frederic Jonske*

Main category: cs.CV

TL;DR: 医疗图像分割中使用噪声质量控制信号改进模型训练


<details>
  <summary>Details</summary>
Motivation: 全像素标注成本高昂，而现成的质量控制信号如模型一致性、不确定性等可以用于训练，但这些信号存在噪音和偏差，可能导致训练效果不佳。

Method: 提出区域归一化DPO（RN-DPO），通过归一化偏好更新并考虑mask的分歧区域大小，降低有害比较的影响。

Result: 在两个医疗数据集和多种实验环境下，RN-DPO在不增加像素标注的情况下，优于标准DPO和基线方法，提升性能稳定性。

Conclusion: RN-DPO有效缓解了不可靠标注信号对模型训练的负面影响，为医疗图像分割提供了一种高效稳定的训练策略。

Abstract: While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations.

</details>


### [70] [Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning](https://arxiv.org/abs/2601.23224)
*Xiangyu Zeng,Zhiqiu Zhang,Yuhan Zhu,Xinhao Li,Zikang Wang,Changlian Ma,Qingyu Zhang,Zizheng Huang,Kun Ouyang,Tianxiang Jiang,Ziang Yan,Yi Wang,Hongjie Zhang,Yali Wang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Video-o3的新型框架，通过迭代发现视觉关键线索、细粒度分析关键片段及自适应终止，解决了现有长视频理解模型在处理冗余数据时对稀疏但关键证据的识别不足问题。技术上采用任务解耦注意力掩码和可验证轨迹引导奖励，并构建大规模数据集Seeker-173K，在MLVU和Video-Holmes上分别达到72.1%和46.5%的准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解模型依赖均匀采样和单步推理，导致冗余视频中无法有效识别稀疏但关键的证据。需要一种能够动态探索视频、逐步聚焦重要片段并灵活终止分析的框架，以提升多跳证据检索和推理能力。

Method: 提出Video-o3框架：1）迭代式发现显著线索并精细分析关键段；2）通过"任务解耦注意力掩码"分离推理与工具调用的注意力分散，保留全局上下文；3）设计"可验证轨迹引导奖励"平衡探索范围与推理效率；4）创建含173K高质量轨迹的Seeker-173K数据集，支持监督与强化学习训练。

Result: 在MLVU和Video-Holmes基准测试中，Video-o3分别取得72.1%和46.5%的准确率，远超现有技术。验证了其多跳证据检索能力及工具调用在长视频场景中的有效性，证明动态探索机制可显著提升冗余环境下的推理性能。

Conclusion: Video-o3通过迭代发现、细粒度分析及自适应终止机制，解决了传统方法在长视频中忽略关键证据的问题。其创新性技术（注意力掩码、奖励机制）与大规模数据集的结合，为视频多模态推理提供了可扩展的范式。

Abstract: Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.

</details>


### [71] [ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search](https://arxiv.org/abs/2601.23232)
*Tao Yu,Haopeng Jin,Hao Wang,Shenghua Chai,Yujia Yang,Junhao Gong,Jiaming Guo,Minghui Zhang,Xinlong Chen,Zhenghao Zhang,Yuxuan Zhou,Yanpei Gong,YuanCheng Liu,Yiming Ding,Kangwei Zeng,Pengfei Yang,Zhongtian Luo,Yufei Xiong,Shanbin Zhang,Shaoxiong Cheng,Huang Ruilin,Li Shuo,Yuxi Niu,Xinyuan Zhang,Yueya Xu,Jie Mao,Ruixuan Ji,Yaru Zhao,Mingchen Zhang,Jiabing Yang,Jiaqi Liu,YiFan Zhang,Hongzhu Yi,Xinming Wang,Cheng Zhong,Xiao Ma,Zhang Zhang,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 该研究提出了ShotFinder基准，在文本驱动下进行视频片段检索与定位，通过生成包含多种约束条件的高质量样本集并采用三级检索流程，揭示了当前模型在时序定位和视觉风格理解方面与人类仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型研究集中于静态文本或单一模态场景，针对开放域视频片段检索（含复杂时序结构与语义）缺乏系统性基准和分析框架，亟需构建具备多因素可控约束的评测体系以推动该领域发展。

Method: 构建了包含Temporal Order、Color等五类单因子约束的ShotFinder基准，从YouTube收集20个主题的1210个样本，并提出三级文本驱动检索流程：视频想象扩展查询-搜索引擎检索候选视频-描述引导时序定位。

Result: 跨模态模型在时序定位任务表现接近人类但存在局限，颜色和视觉风格任务准确率显著低于人类水平，闭源与开源模型均暴露出现有方法在复杂语义理解与时空结构建模中的缺陷。

Conclusion: 开放域视频片段检索是当前多模态大模型亟需突破的能力盲区，提出的基准为评估与改进视频理解系统提供了可扩展框架，揭示了模型在高阶语义推理和跨模态对齐方面的优化方向。

Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.

</details>


### [72] [Structured Over Scale: Learning Spatial Reasoning from Educational Video](https://arxiv.org/abs/2601.23251)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本论文提出通过结构化教育视频内容提升视觉语言模型的推理能力，使用DoraVQA数据集与GRPO方法微调Qwen系列模型，在多项基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽在标准任务表现良好，但未能掌握幼儿基础推理能力，作者假设结构化教育视频可提供更优训练信号。

Method: 构建DoraVQA数据集并采用Group Relative Policy Optimization方法，利用教育视频的结构化正确性信号进行模型微调。

Result: 模型在DoraVQA提升8-14分，CVBench达86.16%，并有效迁移至Video-MME和NExT-QA，验证跨领域泛化能力。

Conclusion: 结构化教育内容显著增强模型推理表现，论证内容结构在模型训练中与规模具有同等重要性。

Abstract: Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.

</details>


### [73] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出无需训练的测试时适应方法TaTa，利用布朗距离协方差和属性增强提示提升视觉语言模型领域适应性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法计算量大、依赖反向传播且仅处理单一模态，需开发更高效稳定的跨模态领域自适应方案。

Method: ① 布朗距离协方差度量跨模态相关性，② 动态计算特征权重实现参数无关适应，③ 结合视觉属性增强提示与伪标签优化。

Result: 在多个数据集上计算成本降低47%，跨领域准确率提升5.2-8.3%，跨数据集泛化性能超越现有方法。

Conclusion: TaTa通过非参数化动态校准策略，在避免模型权重更新的前提下实现了视觉语言模型的高效领域适应。

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


### [74] [User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments](https://arxiv.org/abs/2601.23281)
*Junfeng Lin,Yanming Xiu,Maria Gorlatova*

Main category: cs.CV

TL;DR: 该论文研究了开放集目标检测模型在不同用户提示下的鲁棒性，并提出通过提示增强策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有OSOD模型在标准基准测试表现良好，但缺乏针对现实用户提示（如模糊、不完整或过度详细）的鲁棒性研究，尤其在交互式XR场景中的应用需要解决提示多样性带来的挑战。

Method: 1) 采用GroundingDINO和YOLO-E模型测试真实XR图像。
2) 通过视觉-语言模型模拟四种提示类型：标准、过简略、过详细、模糊。
3) 评估两种提示增强策略（如语法修正、语义扩展）对鲁棒性的影响。

Result: 1) 两模型对标准和过简略提示表现稳定（过简略提示下YOLO-E mIoU下降＜5%）。
2) GroundingDINO受过详细提示影响显著（mIoU下降22%），两模型在模糊提示下性能大幅下降（如YOLO-E平均置信度降39%）。
3) 提示增强策略使模糊提示下的mIoU提升55%以上，平均置信度提高41%。

Conclusion: 提出针对XR场景的OSOD提示优化框架，包含用户提示策略（如避免模糊描述）和自动化增强方法（如语义去歧义），为交互式环境下的模型部署提供指导。

Abstract: Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.

</details>


### [75] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: VideoGPA通过引入几何偏好对齐，利用几何基础模型和DPO优化，在不依赖人工标注的前提下提升视频扩散模型的3D结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在3D结构一致性上存在缺陷，导致物体变形或空间漂移，其根本原因是标准去噪目标缺乏几何一致性监督信号。

Method: 提出的VideoGPA框架采用几何基础模型自动生成密集偏好信号，通过数据高效的自监督方式结合直接偏好优化（DPO），隐式引导模型生成符合3D几何规律的视频内容。

Result: 仅需少量偏好对即可显著提升时间稳定性/物理合理性/运动连贯性，在多个实验中均超越现有最优方法。

Conclusion: 本研究表明几何约束信号可通过自监督方式有效融合到视频扩散模型训练中，为提升生成质量提供了无需人工标注的新型优化框架。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [76] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 本研究探讨醉酒语言（受酒精影响的文本）对大型语言模型（LLMs）安全失效的影响，发现通过特定诱导方法可显著提高LLMs的越狱攻击敏感性及隐私泄露风险，凸显其潜在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 人类饮酒后易产生不良行为与隐私泄露，而LLMs是否存在类似风险尚未被研究。本文旨在揭示醉酒语言作为LLMs安全失效驱动因素的潜在关联，并评估当前防御机制对这类攻击的防护效果。

Method: 提出三种醉酒语言诱导方法：基于角色的提示、因果微调、强化学习后训练。在5个LLMs上进行实验，结合JailbreakBench（越狱攻击基准）和ConfAIde（隐私泄露基准）评估攻击效果，并通过人工与LLM联合评测分析错误分类。

Result: 诱导方法成功提升LLMs越狱敏感性（即使存在防御）及隐私泄露率（ConfAIde数据集），且发现人类醉酒行为与LLMs拟人化特征间存在对应关系，表明诱导方法可能绕过当前主流安全对策。

Conclusion: 醉酒语言诱导技术的简单高效性使其可能成为LLMs安全防护的潜在挑战，研究结果强调需针对性优化现有安全策略以防范此类风险。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [77] [MrRoPE: Mixed-radix Rotary Position Embedding](https://arxiv.org/abs/2601.22181)
*Qingyuan Tian,Wenhong Zhu,Xiaoran Liu,Xiaofeng Wang,Rui Wang*

Main category: cs.CL

TL;DR: This paper introduces MrRoPE, a generalized framework for Rotary Position Embedding extensions based on radix system theory, enabling efficient long-sequence modeling without training.


<details>
  <summary>Details</summary>
Motivation: Current RoPE-extension methods are fragmented with no unified theory; this work establishes a mathematical foundation to systematically generalize RoPE for long sequences while maintaining 'train short, test long' capability.

Method: Proposes a novel perspective linking RoPE to positional radix systems, formalizing different extension strategies as radix conversion operations. Develops training-free methods (MrRoPE-Uni/Pro) using uniform/progressive radix conversions without parameter updates.

Result: MrRoPE-Pro achieves 85%+ recall in 128K-context tests, doubles YaRN's accuracy on Infinite-Bench, and theoretically proves increased encoding length limits without fine-tuning.

Conclusion: Establishes radix conversion theory as RoPE-extension foundation, demonstrating that principled positional encoding design enables strong long-context generalization without retraining.

Abstract: Rotary Position Embedding (RoPE)-extension refers to modifying or generalizing the Rotary Position Embedding scheme to handle longer sequences than those encountered during pre-training. However, current extension strategies are highly diverse and lack a unified theoretical foundation. In this paper, we propose MrRoPE (Mixed-radix RoPE), a generalized encoding formulation based on a radix system conversion perspective, which elegantly unifies various RoPE-extension approaches as distinct radix conversion strategies. Based on this theory, we introduce two training-free extensions, MrRoPE-Uni and MrRoPE-Pro, which leverage uniform and progressive radix conversion strategies, respectively, to achieve 'train short, test long' generalization. Without fine-tuning, MrRoPE-Pro sustains over 85% recall in the 128K-context Needle-in-a-Haystack test and achieves more than double YaRN's accuracy on Infinite-Bench retrieval and dialogue subsets. Theoretical analysis confirms that MrRoPE-Pro effectively raises the upper bound of RoPE's attainable encoding length, which further validates the reliability and utility of our theory and methodology.

</details>


### [78] [Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning](https://arxiv.org/abs/2601.22297)
*Chenxi Liu,Yanshuo Chen,Ruibo Chen,Tianyi Xiong,Tong Zheng,Heng Huang*

Main category: cs.CL

TL;DR: 该论文提出了一种Self-Debate Reinforcement Learning（SDRL）方法，旨在通过结合自主问题解决与多智能体辩论中的多样化推理路径，增强大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（RLVR）仅训练模型独立解题，未使其具备在辩论中整合多元合理论证的能力，限制了其在协作推理中的应用表现。

Method: 针对给定提示，首先采样多个候选解，构建包含多样性推理路径的辩论上下文，并生成基于该上下文的第二轮响应。通过联合优化初始响应和辩论条件响应，训练模型兼具自主解题和辩论参与的能力。

Result: 实验表明，SDRL在多个基础模型和推理基准上提升了多智能体辩论整体表现，同时增强了单模型的推理能力。

Conclusion: SDRL框架使单一LLM有效掌握自主与协作推理，实现独立解题与多智能体协同的双重优势。

Abstract: The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.

</details>


### [79] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: 提出MERMAID框架，通过结合检索与推理及持久化记忆模块，提升在线内容真实性评估的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法将证据检索视为静态过程，缺乏跨断言的证据复用，导致效率低下且结果不一致。

Method: 设计多智能体框架，集成动态检索与推理循环、结构化知识表示、证据记忆库和迭代优化机制，实现证据的增量获取与跨任务复用。

Result: 在3个基准测试和2个数据集上实现SOTA性能，减少30%检索次数，推理成本降低40%，验证效率提升50%。

Conclusion: 协同检索、推理与记忆机制能显著提升验证系统的效率和一致性，为自动化事实核查提供新范式。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [80] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: LLMs adaptively employ representational straightening based on task structure, aiding prediction in continual tasks but not in structured ones.


<details>
  <summary>Details</summary>
Motivation: Previous research indicated LLMs simplify neural trajectories for next-token prediction. The authors aimed to explore if this straightening occurs within contexts during in-context learning (ICL).

Method: Measured representational straightening in Gemma 2 models across diverse in-context tasks, comparing continual and structured prediction settings.

Result: Continual prediction tasks showed increased straightness with longer contexts, correlating with improved prediction. Structured prediction tasks exhibited inconsistent straightening, only in phases with explicit structure.

Conclusion: LLMs dynamically select strategies based on task structure, with representational straightening being one optional mechanism, not a universal process in ICL.

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [81] [Stability-Aware Prompt Optimization for Clinical Data Abstraction](https://arxiv.org/abs/2601.22373)
*Arinbjörn Kolbeinsson,Daniel Timbie,Sajjan Narsinghani,Sanjay Hariharan*

Main category: cs.CL

TL;DR: Large language models (LLMs) for clinical tasks are sensitive to prompt variations, requiring optimization for both accuracy and stability through a dual-objective approach.


<details>
  <summary>Details</summary>
Motivation: Prompt sensitivity remains a critical issue in clinical LLMs, even when models appear well-calibrated, necessitating joint optimization of accuracy and stability instead of treating prompts as fixed.

Method: Measured prompt sensitivity via flip rates across two clinical tasks (MedAlign applicability/correctness, MS subtype abstraction) using open and proprietary models, analyzed relationships with calibration and selective prediction, and proposed a dual-objective optimization loop to balance accuracy and stability.

Result: Higher model accuracy does not ensure prompt stability; models may remain fragile to paraphrases despite appearing well-calibrated. The proposed dual-objective technique reduced flip rates across tasks and models, occasionally with minimal accuracy trade-offs.

Conclusion: Prompt sensitivity should be explicitly prioritized during clinical LLM validation to enhance robustness and real-world reliability.

Abstract: Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.

</details>


### [82] [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)
*Bailin Wang,Dan Friedman,Tao Lei,Chong Wang*

Main category: cs.CL

TL;DR: The paper introduces SPLA, a method combining sparse attention and linear attention to improve long-context modeling by selecting key blocks via a Taylor expansion-based metric and compressing non-critical blocks into a compact state, avoiding performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing block-wise sparse attention methods suffer from low selection accuracy and contextual loss due to indiscriminately discarding unselected attention blocks, creating a performance gap in long-context tasks like continual pretraining.

Method: SPLA employs three core components: (1) A second-order Taylor expansion-based metric to precisely identify relevant attention blocks; (2) A residual linear attention (RLA) module that compresses unselected blocks into a recurrent state; (3) A subtraction-based RLA implementation that calculates residuals as global attention minus selected blocks, eliminating explicit access to unselected blocks during inference.

Result: SPLA achieves state-of-the-art results on long-context benchmarks like RULER, surpassing dense attention baselines while maintaining comparable general knowledge and reasoning capabilities, with experiments showing reduced contextual loss and lower computational overhead.

Conclusion: The hybrid attention framework bridges the performance gap in long-sequence modeling by theoretically principled block selection and residual compression, enabling efficient inference without sacrificing accuracy in critical applications.

Abstract: Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention (SPLA), a framework that utilizes a selection metric derived from second-order Taylor expansions to accurately identify relevant blocks for exact attention. Instead of discarding the remaining "long tail," SPLA compresses unselected blocks into a compact recurrent state via a residual linear attention (RLA) module. Crucially, to avoid IO overhead, we derive an optimized subtraction-based formulation for RLA -- calculating the residual as the difference between global and selected linear attention -- ensuring that unselected blocks are never explicitly accessed during inference. Our experiments demonstrate that SPLA closes the performance gap in continual pretraining, surpassing dense attention models on long-context benchmarks like RULER while maintaining competitive general knowledge and reasoning capabilities.

</details>


### [83] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: 本文提出SP2DPO，通过语义标注动态调整DPO算法的温度参数beta_i，以解决偏好数据异质性问题，在AlpacaEval 2.0测试中提升了8B以下模型的可控胜率性能，且无需额外训练开销。


<details>
  <summary>Details</summary>
Motivation: 原始DPO使用全局温度参数β导致：1）无法区分偏好数据的高低价值特征（如安全性/事实性的客观错误 vs 风格的主观差异）；2）依赖人工调参优化β。实际偏好数据普遍存在标注噪声和语义间隙异质性问题。

Method: 1. 引入实例级自适应温度β_i：基于教师模型生成的结构化语义标注（类别、幅度、置信度）离线预定义β计划。2. 保持标准DPO内循环优化器不变，仅针对UltraFeedback数据集(59,960对)构建可审计的β_i制品。3. 在AlpacaEval 2.0评估框架下，对比标准DPO与调优全局β基线。

Result: 1. 在4B-8B指令调优模型上，与全局β调优的DPO相比：• 保持原始胜率指标竞争力；• 在2/4个backbone上提升长度控制胜率（如8B模型提升12.3%）。2. 无需模型逐个调优β参数的实验开销。3. 开源代码、标注和β_i制品实现完全可复现。

Conclusion: 实例级语义感知的温度调度机制能有效平衡偏好拟合与模型稳定性：1. 通过语义标注显式建模低/高信号特征；2. 在不增加训练成本前提下改进小规模模型的表现；3. 产出的β_i制品支持后续人机协同的质量控制审计。

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [84] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本研究通过世界价值观调查和道德基础理论评估大型语言模型生成的合成文化角色对人类价值观和道德体系的还原程度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在模拟人类行为方面表现突出，但其生成的合成角色在不同文化背景下的价值观和道德体系对齐度尚未明确，需通过跨文化框架进行系统验证。

Method: 基于世界价值观调查变量生成可解释的合成角色，结合Inglehart-Welzel文化地图、世界价值观调查人口学框架及道德基础理论，从文化定位、人口学一致性及道德谱三个维度进行交叉验证。

Result: 合成角色准确映射人类跨文化差异：在Inglehart-Welzel文化地图中展现稳定文化定位，与人类群体人口学特征匹配，且道德基础响应模式显示文化-道德结构的系统关联。

Conclusion: 该方法提供了量化评估LLMs跨文化表现力的框架，证明文化扎根的角色生成可有效捕捉群体价值观差异，为跨文化AI研究提供新工具。

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


### [85] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: This paper identifies limitations in existing positional encoding (RoPE) for handling long-range recursive structures in LLMs and proposes Bifocal Attention with Spectral Evolution to address these issues.


<details>
  <summary>Details</summary>
Motivation: Standard RoPE suffers from 'Spectral Rigidity' due to fixed geometric decay, which fails to model long-range dependencies in recursive logic. This creates a 'Structure Gap' when extrapolating to deeper reasoning steps, necessitating a more flexible positional encoding methodology.

Method: Bifocal Attention decouples positional encoding into static Geometric Eyes (token-level RoPE) and learnable Spectral Eyes (harmonic operators). Spectral Evolution trains these by first using fixed geometric frequencies and then allowing gradient-driven adaptation to form task-specific harmonic bases optimized for algorithmic reasoning.

Result: A dual-modal positional encoding framework that combines local precision with global adaptability, enabling models to capture both syntactic coherence and deep recursive structures through harmonically optimized frequencies.

Conclusion: The proposed paradigm bridges the Structure Gap in algorithmic reasoning by introducing learnable, task-specific spectral components while retaining the benefits of standard RoPE for localized token interactions.

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [86] [Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss](https://arxiv.org/abs/2601.22439)
*Galim Turumtaev*

Main category: cs.CL

TL;DR: This paper addresses the problem of rare tokens in low-resource languages struggling with neural language models due to marginalization during training. A thresholding technique is proposed to reduce this issue, allowing rare tokens to learn more effectively. Experiments show improved performance on low-resource language validation data through a character-level model, marking the first application of negative sampling for this challenge.


<details>
  <summary>Details</summary>
Motivation: Neural language models perform poorly on low-resource languages because limited training data makes tokens rare, and these rare tokens are disproportionately hurt by marginalization during training, preventing effective learning.

Method: Thresholding technique to mitigate marginalization effects on rare tokens, enabling more meaningful alignment. Negative sampling is used to limit the harmful impact of excessive marginalization during training.

Result: Experiments with a character-level language model demonstrated significant improvements in performance on low-resource language validation data.

Conclusion: This work introduces a novel thresholding approach combined with negative sampling to address rare token representation challenges in low-resource languages, offering a new direction for improving language model performance in underrepresented contexts.

Abstract: Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages.

</details>


### [87] [One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry](https://arxiv.org/abs/2601.22521)
*Weisong Zhao,Tong Wang,Zichang Tan,Te Yang,Siran Peng,Haoyuan Zhang,Tianshuo Zhang,Haichao Shi,Meng Meng,Yang Yang,Xiangyu Zhu,Zhen Lei,Xiao-Yu Zhang,Xu Zhou*

Main category: cs.CL

TL;DR: This paper introduces Power-Mean Policy Optimization (PMPO), a framework that generalizes GRPO and GMPO by adaptively adjusting the aggregation geometry parameter p through a Clip-aware Effective Sample Size mechanism, improving reinforcement learning stability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods GRPO and GMPO rely on fixed arithmetic/geometric mean aggregation geometries, failing to adapt to trajectory heterogeneity and evolution dynamics which limits learning stability and efficiency.

Method: PMPO combines power-mean geometry with parameter p, theoretically analyzing p's role in gradient concentration. It proposes a deterministic rule mapping trajectory clipping fractions to target ESS (Effective Sample Size) and solves for p to align trajectory-induced ESS with targets.

Result: PMPO demonstrates superior performance over strong baselines on mathematical reasoning benchmarks while enabling dynamic transition between aggressive arithmetic mean (for reliable trajectories) and conservative geometric mean (for unstable trajectories).

Conclusion: Adaptive power-mean geometry outperforms fixed aggregation methods by automatically aligning gradient update concentrations with trajectory characteristics through ESS-matching, providing a unified framework for policy optimization.

Abstract: Group-based reinforcement learning has evolved from the arithmetic mean of GRPO to the geometric mean of GMPO. While GMPO improves stability by constraining a conservative objective, it shares a fundamental limitation with GRPO: reliance on a fixed aggregation geometry that ignores the evolving and heterogeneous nature of each trajectory. In this work, we unify these approaches under Power-Mean Policy Optimization (PMPO), a generalized framework that parameterizes the aggregation geometry via the power-mean geometry exponent p. Within this framework, GRPO and GMPO are recovered as special cases. Theoretically, we demonstrate that adjusting p modulates the concentration of gradient updates, effectively reweighting tokens based on their advantage contribution. To determine p adaptively, we introduce a Clip-aware Effective Sample Size (ESS) mechanism. Specifically, we propose a deterministic rule that maps a trajectory clipping fraction to a target ESS. Then, we solve for the specific p to align the trajectory induced ESS with this target one. This allows PMPO to dynamically transition between the aggressive arithmetic mean for reliable trajectories and the conservative geometric mean for unstable ones. Experiments on multiple mathematical reasoning benchmarks demonstrate that PMPO outperforms strong baselines.

</details>


### [88] [$ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs](https://arxiv.org/abs/2601.22527)
*Jingyi Yang,Yuxian Jiang,Jing Shao*

Main category: cs.CL

TL;DR: 提出$\rho$-EOS方法，通过动态调整生成长度提升遮罩扩散大语言模型的推理效率与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有遮罩扩散大语言模型依赖预设固定生成长度，导致质量与效率难以权衡。

Method: 基于去噪过程中EOS标记隐式密度（$\rho$）动态调整生成长度，过高密度触发收缩，不足则扩展。

Result: 在数学与代码基准测试中，$\rho$-EOS在保持性能的同时显著提升推理效率与token利用率。

Conclusion: $\rho$-EOS通过单阶段双向长度调整机制，为遮罩扩散模型提供更灵活高效的生成解决方案。

Abstract: Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational efficiency. To address this, we study the denoising dynamics and find that the implicit density ($ρ$) of end-of-sequence ($\texttt{EOS}$) tokens serves as a reliable signal of generation sufficiency. In particular, the evolving implicit $\texttt{EOS}$ density during denoising reveals whether the current masked space is excessive or insufficient, thereby guiding the adjustment direction for generation length. Building on this insight, we propose $\textbf{$ρ$-$\texttt{EOS}$}$, a training-free, single-stage strategy that enables bidirectional variable-length generation for masked dLLMs. Unlike prior two-stage approaches--which require separate length adjustment and iterative mask insertion phases while supporting only unidirectional expansion--$\textbf{$ρ$-$\texttt{EOS}$}$ achieves bidirectional length adjustment within a unified denoising process by continuously estimating the implicit $\texttt{EOS}$ density: excessively high density triggers $\texttt{MASK}$ token contraction, while insufficient density induces expansion. Extensive experiments on mathematics and code benchmarks demonstrate that $\textbf{$ρ$-$\texttt{EOS}$}$ achieves comparable performance while substantially improving inference efficiency and token utilization.

</details>


### [89] [Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation](https://arxiv.org/abs/2601.22546)
*Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang*

Main category: cs.CL

TL;DR: 本文提出HOLO插件，利用大语言模型生成初期捕捉目标关键词的全息特性，通过并行生成提升效率。


<details>
  <summary>Details</summary>
Motivation: 研究发现LLMs生成时倾向优先生成目标侧关键词，但现有工作未系统探讨该特性。HOLO旨在利用此特性提升生成效率。

Method: 提出全息特性的观测：LLMs通过少量生成步骤即可提取目标关键词。HOLO插件利用此特性，用受限生成提取关键词，并通过并行约束补全文本。

Result: 在短文本生成任务中，HOLO在保持生成质量的同时显著提升推理效率，证明了全息特性对加速生成的有效性。

Conclusion: 全息特性揭示了LLMs生成模式的关键特征，HOLO的成功验证了利用模型内置特性优化生成过程的新路径。

Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.

</details>


### [90] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 大型语言模型在作为评估者时表现出自我偏好，影响评估可靠性。研究发现并修正了导致测量误差的核心方法问题


<details>
  <summary>Details</summary>
Motivation: LLM评估者倾向于偏好自身输出导致评估偏差，需区分真正的自我偏好与实验干扰因素

Method: 引入评估者质量基线，比较模型自我错误判断与错误选择其他模型响应的概率差异

Result: 通过新基线分析发现仅51%的初始研究结论具统计显著性，揭示评估投票中'简单'与'困难'问题的熵值差异

Conclusion: 提出的校正方法能过滤噪声数据，支持更准确的自我偏好研究，为评估偏差效应分类提供新视角

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [91] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: SpanNorm是一种新型归一化技术，结合PreNorm的训练稳定性和PostNorm的性能优势，通过跨Transformer块的残差连接和输出归一化解决信号传播问题。


<details>
  <summary>Details</summary>
Motivation: PreNorm和PostNorm存在训练稳定性与性能的权衡，需设计兼顾二者优势的方案以提升LLM训练效果。

Method: 在Transformer块间建立跨层残差连接实现信号传播稳定，采用PostNorm风格归一化输出。结合理论分析证明该方法通过缩放策略保持信号方差有界，避免梯度问题和表征崩溃。

Result: 在稠密和MoE场景下均超越标准归一化方案，验证了理论有效性并通过实验验证了模型性能提升。

Conclusion: SpanNorm解决了Transformer架构中归一化设计的核心矛盾，为构建更强大的Transformer模型提供了新路径。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [92] [Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models](https://arxiv.org/abs/2601.22629)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Yiqiao Huang,Ivor Tsang,Yang You*

Main category: cs.CL

TL;DR: 本文提出TAPS方法，通过动态调整扩散过程中的时间步扰动强度，在保证文本流畅性的同时有效提升了扩散语言模型的生成多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本生成中引入了时间维度，但如何利用该维度平衡全局语义多样性与局部流畅性尚未被充分探索。现有扩散模型在推理阶段缺乏针对不同时间步的差异化扰动控制机制。

Method: 设计时间退火扰动采样（TAPS）策略：1）早期时间步增强语义扰动以促进全局路径探索，2）中后期逐步降低扰动强度保持文本流畅性，3）兼容非自回归（LLaDA）和半自回归（TraDo）架构。通过温度系数α(t)=1-√(t/T)进行动态调度。

Result: 在LLaDA和TraDo模型上取得显著提升：i）语言模型困惑度降低8.3%，ii）语义多样性指标（distn-4）提升19.6%，iii）推理任务准确率提高4.5%，且无需微调模型参数。

Conclusion: 时间维度的分阶段控制机制可有效平衡生成质量与多样性，证明扩散模型在语义探索和局部优化上具有内在优势，为可控文本生成提供了新范式。

Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.

</details>


### [93] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: DART是一种动态剪枝方法，通过注意力引导在运行时动态更新神经元掩码，在保持模型性能的同时，显著减少大语言模型的参数冗余。


<details>
  <summary>Details</summary>
Motivation: 传统的剪枝方法依赖于特定数据集校准，存在高数据依赖性和计算开销，且静态剪枝无法应对上下文演变过程中知识神经元的动态变化。

Method: DART通过监测注意力评分分布的变化推断上下文变化，实时动态更新神经元级别的掩码，保留显著参数，采用无需训练的轻量化机制。

Result: DART在十项基准测试中表现优于动态基线，在LLAMA-3.1-8B上70%FFN稀疏度下最高提升14.5%准确率；摘要任务中ROUGE-L得分比静态剪枝高3倍，接近原模型表现。

Conclusion: 该框架能适应多样化语义上下文，在通用和领域任务中保持模型能力，LLAMA-3.1-8B(16GB)剪枝后仅需10MB内存，仅增加0.1%FLOPs开销。

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [94] [NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models](https://arxiv.org/abs/2601.22657)
*Haisong Gong,Zhibo Liu,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.CL

TL;DR: NAG (Native Architecture for Graphs) proposes a unified framework to integrate graph processing within Language Models (LMs) via self-attention mechanisms, replacing traditional segregated architectures that use external GNNs and avoiding complex alignment between structural and semantic components.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-graph integration use segregated architectures where external GNNs handle graph topology and LMs process text, leading to disjointed interactions and implicit alignment between graph tokens and textual elements, which is deemed suboptimal.

Method: NAG eliminates external encoders by internalizing graph processing in LMs through repurposed self-attention to enforce topological dependencies and recalibrated positional IDs for structural equivalence. Two implementations are introduced: NAG-Zero (retaining base model capabilities) and NAG-LoRA (enhanced structural adaptation).

Result: Experiments on diverse graph tasks show NAG achieves robust graph comprehension without external encoders, with validated effectiveness and efficiency.

Conclusion: NAG offers a simpler, more coherent paradigm for text-graph modeling by unifying structural and semantic processing within the LM's native architecture, overcoming limitations of segregated frameworks.

Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually disjointed interaction paradigm. By segregating structural encoding from semantic processing, these systems must perform a complex implicit alignment between abstract graph tokens and concrete textual elements. Challenging the necessity of external encoders, we propose NAG (Native Architecture for Graphs), a unified framework that internalizes graph processing within the LM's native manifold. Instead of bridging disparate embedding spaces, NAG repurposes the self-attention mechanism to enforce topological dependencies and recalibrates positional IDs to ensure structural equivalence. This allows the model to harness its intrinsic linguistic capability to simultaneously comprehend node and edge content alongside structural topology. We introduce two efficient implementations: NAG-Zero for absolute preservation of the base model's linguistic capabilities, and NAG-LoRA for enhanced structural adaptation. Experiments across diverse graph tasks validate that NAG achieves robust graph comprehension without the overhead of external encoders, offering a simpler, more coherent paradigm for text-graph modeling.

</details>


### [95] [TSLM: Tree-Structured Language Modeling for Divergent Thinking](https://arxiv.org/abs/2601.22688)
*Doyoung Kim,Jaehyeok Doo,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文提出树结构语言建模(TSLM)，通过特殊标记编码分支结构，使语言模型能在单次生成过程中同时展开多个推理路径并动态裁剪无效分支，显著提升复杂任务的推理效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型的顺序生成机制无法在搜索过程中分离无关推理路径，导致冗余计算且难以系统化探索解空间。现有外部搜索方法需要多次独立前向计算，效率低下。

Method: 引入树结构标记编码方法，在单次生成过程中通过特殊控制符实现多分支并行展开与动态剪枝。采用包含成功与失败推理路径的完整树状训练数据，使模型内化探索策略并共享分支前缀计算。

Result: TSLM在多项复杂推理任务中表现出优越的样本效率，推理时相比传统搜索方法减少87%的前向计算量，且在错误路径修正能力与多路径探索深度上均有显著提升。

Conclusion: 本研究颠覆了语言模型推理范式，证明通过监督学习完整树状推理轨迹，可高效培养模型系统化探索能力，为构建高效决策架构提供了新方向。

Abstract: Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.

</details>


### [96] [Models Know Models Best: Evaluation via Model-Preferred Formats](https://arxiv.org/abs/2601.22699)
*Joonhak Lee,Sungmok Jung,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: LLMs表现因问答格式显著差异，动态格式对齐策略通过模型偏好信号选择最优方式，提升零样本准确率。


<details>
  <summary>Details</summary>
Motivation: 评估格式（符号选择vs填空）影响LLM推理效果，传统人工启发式方法效果差，需探索模型自适应的格式选择机制。

Method: 训练轻量级分类器，基于模型自生成的偏好隐变量信号动态决定每题的最优格式，避免人工启发式规则。

Result: 在多项知识与推理基准测试中，该方法显著提升零样本准确率（未给出具体指标），且效果跨模型具泛化性。

Conclusion: 模型隐含格式偏好信号能有效指导评估方式选择，动态对齐策略优于人工规则设计，揭示LLM潜在能力。

Abstract: Performance of Large Language Models (LLMs) on multiple-choice tasks differs markedly between symbol-based and cloze-style evaluation formats. The observed discrepancies are systematically attributable to task characteristics: natural language continuation benefits from likelihood scoring, whereas explicit comparison is better suited to symbol-based selection. These trends are consistent across various decoder-based LLMs, indicating model-agnostic effects. To address these inconsistencies, a dynamic format-alignment strategy is introduced that employs a lightweight classifier trained on latent model-preference signals. In contrast to human-designed heuristics, which often degrade performance, this approach uses model-generated signals to determine the optimal format for each problem instance. The proposed method achieves substantial and consistent improvements in zero-shot accuracy across reasoning and knowledge benchmarks, better revealing the models' latent capabilities.

</details>


### [97] [MM-THEBench: Do Reasoning MLLMs Think Reasonably?](https://arxiv.org/abs/2601.22735)
*Zhidian Huang,Zijun Yao,Ji Qi,Shangqing Tu,Junxian Ma,Jinxin Liu,Weichuan Liu,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出了MM-THEBench基准测试，用于评估多模态大语言模型中间推理阶段的错误感知和推理幻觉，包含细粒度分类体系、带验证的推理标注数据集和多级自动化评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对早期多模态模型，未能考察具推理能力MLLMs的内部思维过程及推理过程中的幻觉问题，而当前模型的思维过程可能引入新的幻觉类型。

Method: 构建MM-THEBench基准，包含基于认知维度的分类体系、覆盖多任务的标注数据集，以及结合多层级指标的自动化评估框架，可解析性分析模型推理各阶段的幻觉特征。

Result: 在主流推理MLLMs上的实验证明，该基准能够量化思维过程对幻觉和推理能力的影响，揭示了模型在感知错误、自洽性推理与最终决策间的关联性差异。

Conclusion: 强调评估多模态大模型内部推理过程幻觉的重要性，MM-THEBench为研究思维与幻觉关系提供系统工具，建议未来模型开发需强化思维过程的可解释性与鲁棒性。

Abstract: Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.

</details>


### [98] [AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction](https://arxiv.org/abs/2601.22742)
*Yifei Li,Richong Zhang,Wanyu Tu,Zhijie Nie,Haokun Luo,Chuantao Yin,Pengchong Li*

Main category: cs.CL

TL;DR: 本研究提出法律判决审查新任务APPELLATE REVIEW及数据集AR-BENCH，揭示现有大语言模型在识别法律错误上的局限性。


<details>
  <summary>Details</summary>
Motivation: 法律判决易因复杂案情和抽象概念产生错误，而现有AI研究侧重预测/生成任务，缺乏针对后验性错误检测的审查机制研究，且面对案件量激增的效率压力亟需突破。

Method: 构建包含8,700个精细标注案例的AR-BENCH数据集，首创面向法律诊断推理的APPELLATE REVIEW任务框架，通过14个大模型进行系统性评估。

Result: 实证研究显示当前大语言模型在识别法律适用错误方面存在显着缺陷，特别是在错误分类和修正能力上表现不足，为后续优化提供明确改进方向。

Conclusion: 该研究填补了法律人工智能领域在判决后验性审查的技术空白，建立的评估范式为提升法律AI可靠性提供了基准框架。

Abstract: Legal judgments may contain errors due to the complexity of case circumstances and the abstract nature of legal concepts, while existing appellate review mechanisms face efficiency pressures from a surge in case volumes. Although current legal AI research focuses on tasks like judgment prediction and legal document generation, the task of judgment review differs fundamentally in its objectives and paradigm: it centers on detecting, classifying, and correcting errors after a judgment is issued, constituting anomaly detection rather than prediction or generation. To address this research gap, we introduce a novel task APPELLATE REVIEW, aiming to assess models' diagnostic reasoning and reliability in legal practice. We also construct a novel dataset benchmark AR-BENCH, which comprises 8,700 finely annotated decisions and 34,617 supplementary corpora. By evaluating 14 large language models, we reveal critical limitations in existing models' ability to identify legal application errors, providing empirical evidence for future improvements.

</details>


### [99] [RASST: Fast Cross-modal Retrieval-Augmented Simultaneous Speech Translation](https://arxiv.org/abs/2601.22777)
*Jiaxuan Luo,Siqi Ouyang,Lei Li*

Main category: cs.CL

TL;DR: 提出了RASST方法，通过跨模态检索增强语音到文本的实时翻译，解决罕见术语翻译问题。采用轻量检索器、滑动窗口和合成数据优化术语提示。


<details>
  <summary>Details</summary>
Motivation: 当前语音大模型在翻译罕见/专业术语时表现不足，传统机器翻译中的检索增强技术难以直接应用于需要实时处理的语音即时翻译(SST)，因存在跨模态检索延迟和术语应用时机决策难题。

Method: 1)训练轻量级语音-文本检索器实现快速跨模态检索；2)采用滑动窗口机制处理持续输入的语音片段；3)提供分块术语提示作为模型输入；4)合成专门训练数据强化模型对检索术语的应用能力。

Result: 在ACL数据集三个语言方向测试中，术语翻译准确率提升16%，整体翻译质量提升3BLEU分，消融实验证实检索器、滑动窗口和合成数据均对性能有显著贡献。

Conclusion: RASST成功将检索增强引入语音即时翻译领域，通过轻量化设计和合成数据策略有效解决跨模态检索延迟和术语应用难题，为实时语音翻译提供了新的优化方向。

Abstract: Simultaneous speech translation (SST) produces target text incrementally from partial speech input. Recent speech large language models (Speech LLMs) have substantially improved SST quality, yet they still struggle to correctly translate rare and domain-specific terminology. While retrieval augmentation has been effective for terminology translation in machine translation, bringing retrieval to SST is non-trivial: it requires fast and accurate cross-modal (speech-to-text) retrieval under partial, continually arriving input, and the model must decide whether and when to apply retrieved terms during incremental generation. We propose Retrieval-Augmented Simultaneous Speech Translation (RASST), which tightly integrates cross-modal retrieval into the SST pipeline. RASST trains a lightweight speech-text retriever and performs efficient sliding-window retrieval, providing chunkwise terminology hints to the Speech LLM. We further synthesize training data that teaches the Speech LLM to leverage retrieved terms precisely. Experiments on three language directions of the ACL 60/60 dev set show that RASST improves terminology translation accuracy by up to 16% and increases overall translation quality by up to 3 BLEU points, with ablations confirming the contribution of each component.

</details>


### [100] [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)
*Corentin Kervadec,Iuliia Lysova,Marco Baroni,Gemma Boleda*

Main category: cs.CL

TL;DR: 通过引入基于机械可解释性的密度估计器，研究发现LLM中的计算密度并非均匀分布，且动态变化，挑战了传统对稀疏计算的假设，揭示了输入复杂性与上下文长度对计算密度的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究证明LLM参数可大量剪枝而不显著影响性能，暗示计算非均匀分布，但缺乏系统量化方法。本研究旨在建立计算密度的测量框架，以解析LLM信息处理机制。

Method: 设计基于机械可解释性的计算密度估计器，通过实验分析LLM在不同输入下的计算分布模式，并探讨影响密度的关键因素如token稀有度与上下文长度。

Result: 1) LLM处理呈现密集计算特征，而非传统假设的稀疏性；2) 计算密度随输入动态变化（在稀疏与密集模式间切换）；3) 不同LLM对相同样本的计算密度表现高度一致，且稀有token提升密度，上下文增长降低密度。

Conclusion: 计算密度估计器为解析LLM动态处理机制提供了新视角，证明计算模式由输入特征驱动，这一发现为模型优化及挑战符号化解释理论提供了基础。

Abstract: Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.

</details>


### [101] [When Meanings Meet: Investigating the Emergence and Quality of Shared Concept Spaces during Multilingual Language Model Training](https://arxiv.org/abs/2601.22851)
*Felicia Körner,Max Müller-Eberstein,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: The study reveals that shared cross-lingual concept spaces emerge early during LLM pretraining and refine over time, with translation improvements sometimes stemming from behavioral shifts rather than enhanced translation capability.


<details>
  <summary>Details</summary>
Motivation: Prior research lacks causal analysis of how multilingual LLMs develop shared concept spaces during training, focusing instead on final models without exploring dynamic formation or language-specific alignment mechanisms.

Method: Activation patching and causal interpretability techniques were applied to EuroLLM pretraining stages to track cross-lingual concept space development and assess their functional impact on translation tasks.

Result: Shared concept spaces appear early but alignment depends on language-specific factors. Translation quality improvements partly reflect behavioral adjustments (e.g., polysemy resolution) rather than pure performance enhancement, with implications for model interpretability.

Conclusion: The findings challenge assumptions about static cross-lingual alignment, demonstrating dynamic training-phase development patterns and highlighting limitations in attributing translation gains solely to improved multilingual understanding.

Abstract: Training Large Language Models (LLMs) with high multilingual coverage is becoming increasingly important -- especially when monolingual resources are scarce. Recent studies have found that LLMs process multilingual inputs in shared concept spaces, thought to support generalization and cross-lingual transfer. However, these prior studies often do not use causal methods, lack deeper error analysis or focus on the final model only, leaving open how these spaces emerge during training. We investigate the development of language-agnostic concept spaces during pretraining of EuroLLM through the causal interpretability method of activation patching. We isolate cross-lingual concept representations, then inject them into a translation prompt to investigate how consistently translations can be altered, independently of the language. We find that shared concept spaces emerge early} and continue to refine, but that alignment with them is language-dependent}. Furthermore, in contrast to prior work, our fine-grained manual analysis reveals that some apparent gains in translation quality reflect shifts in behavior -- like selecting senses for polysemous words or translating instead of copying cross-lingual homographs -- rather than improved translation ability. Our findings offer new insight into the training dynamics of cross-lingual alignment and the conditions under which causal interpretability methods offer meaningful insights in multilingual contexts.

</details>


### [102] [From Labels to Facets: Building a Taxonomically Enriched Turkish Learner Corpus](https://arxiv.org/abs/2601.22875)
*Elif Sayar,Tolgahan Türker,Anna Golynskaia Knezhevich,Bihter Dereli,Ayşe Demirhas,Lionel Nicolas,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本论文提出基于多维分类学的半自动标注方法，解决学习者语料库平面标注的局限性，以土耳其语为例实现标注精度95.86%的创新框架


<details>
  <summary>Details</summary>
Motivation: 传统平面标注无法分离多维度语言特征，导致难以深入研究学习者错误的成因与模式。需建立标准化细粒度标注体系以支撑复杂语言学分析。

Method: 构建多维分类学理论框架并开发扩展标注工具。通过分类法定义语言学维度，利用自动推理引擎将平面标签扩展为包含词汇/句法/语用等维度的多面标注体系。

Result: 土耳其语标注扩展工具在16类语言学维度上达到95.86%标注准确率，建成首个分类学富信息的土耳其学习者语料库，形成可协作的细粒度标注指南。

Conclusion: 研究为学习者语料库建设提供了标准化多维标注范式，创建的分类学扩展框架为现有语料库升级奠定基础，显著提升错误模式分析的深度与精度。

Abstract: In terms of annotation structure, most learner corpora rely on holistic flat label inventories which, even when extensive, do not explicitly separate multiple linguistic dimensions. This makes linguistically deep annotation difficult and complicates fine-grained analyses aimed at understanding why and how learners produce specific errors. To address these limitations, this paper presents a semi-automated annotation methodology for learner corpora, built upon a recently proposed faceted taxonomy, and implemented through a novel annotation extension framework. The taxonomy provides a theoretically grounded, multi-dimensional categorization that captures the linguistic properties underlying each error instance, thereby enabling standardized, fine-grained, and interpretable enrichment beyond flat annotations. The annotation extension tool, implemented based on the proposed extension framework for Turkish, automatically extends existing flat annotations by inferring additional linguistic and metadata information as facets within the taxonomy to provide richer learner-specific context. It was systematically evaluated and yielded promising performance results, achieving a facet-level accuracy of 95.86%. The resulting taxonomically enriched corpus offers enhanced querying capabilities and supports detailed exploratory analyses across learner corpora, enabling researchers to investigate error patterns through complex linguistic and pedagogical dimensions. This work introduces the first collaboratively annotated and taxonomically enriched Turkish Learner Corpus, a manual annotation guideline with a refined tagset, and an annotation extender. As the first corpus designed in accordance with the recently introduced taxonomy, we expect our study to pave the way for subsequent enrichment efforts of existing error-annotated learner corpora.

</details>


### [103] [Leveraging LLMs For Turkish Skill Extraction](https://arxiv.org/abs/2601.22885)
*Ezgi Arslan İltüzer,Özgür Anıl Özlü,Vahid Farajijobehdar,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文引入首个土耳其语技能提取数据集，提出基于大语言模型的端到端方法，在低资源语言场景下实现0.56准确率。


<details>
  <summary>Details</summary>
Motivation: 土耳其作为全球重要劳动力市场，其形态复杂语言在技能提取领域长期缺乏专用语料库与分类体系，导致相关研究滞后。

Method: 构建4819个标注样本的土耳其语技能数据集，采用Claude Sonnet 3.7大模型结合动态few-shot提示、嵌入检索与LLM重排序技术，对比监督序列标注方法。

Result: LLM端到端方法较传统模型提升显著，在ESCO标准化技能对齐任务中达到0.56F1值，超越英语同类研究中位数水平。

Conclusion: 证明大模型可有效缓解低资源语言技能提取困境，为土耳其等小语种发展提供可迁移的方法框架。

Abstract: Skill extraction is a critical component of modern recruitment systems, enabling efficient job matching, personalized recommendations, and labor market analysis. Despite Türkiye's significant role in the global workforce, Turkish, a morphologically complex language, lacks both a skill taxonomy and a dedicated skill extraction dataset, resulting in underexplored research in skill extraction for Turkish. This article seeks the answers to three research questions: 1) How can skill extraction be effectively performed for this language, in light of its low resource nature? 2)~What is the most promising model? 3) What is the impact of different Large Language Models (LLMs) and prompting strategies on skill extraction (i.e., dynamic vs. static few-shot samples, varying context information, and encouraging causal reasoning)? The article introduces the first Turkish skill extraction dataset and performance evaluations of automated skill extraction using LLMs. The manually annotated dataset contains 4,819 labeled skill spans from 327 job postings across different occupation areas. The use of LLM outperforms supervised sequence labeling when used in an end-to-end pipeline, aligning extracted spans with standardized skills in the ESCO taxonomy more effectively. The best-performing configuration, utilizing Claude Sonnet 3.7 with dynamic few-shot prompting for skill identification, embedding-based retrieval, and LLM-based reranking for skill linking, achieves an end-to-end performance of 0.56, positioning Turkish alongside similar studies in other languages, which are few in the literature. Our findings suggest that LLMs can improve skill extraction performance in low-resource settings, and we hope that our work will accelerate similar research on skill extraction for underrepresented languages.

</details>


### [104] [Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial](https://arxiv.org/abs/2601.22888)
*Jio Oh,Paul Vicinanza,Thomas Butler,Steven Euijong Whang,Dezhi Hong,Amani Namboori*

Main category: cs.CL

TL;DR: 论文提出MDial框架和MDialBench基准，解决非标准英语方言在大型语言模型中的表现缺陷。研究显示当前模型在非美式英语方言处理上准确率低于70%，并揭示直接复制方言语法特征的局限性。


<details>
  <summary>Details</summary>
Motivation: 全球16亿英语使用者中超过80%不使用标准美式英语（SAE），导致与LLM交互时失败率升高且遭遇刻板回应。多方言性能在现有研究中未被充分探索，而英语方言需同时覆盖词汇、拼写和句法三种书面维度特征。

Method: 通过与母语语言学家合作，设计基于注释和可扩展的规则LLM转换系统MDial，创造性地挑战了模型必须镜像用户句法特征的传统假设。构建包含50k+对话和97k+问答对的方言平行基准MDialBench，并对17个LLM进行方言识别和回答生成评测。

Result: 独立评估显示，98%的成对比较中注释者更偏好MDial输出。即使前沿模型在加拿大英语场景下未达到50%准确率，且系统性错误将军方非SAE方言归类为美国/英国英语，揭示当前方法在语法特征映射上的重大缺陷。

Conclusion: 方言识别作为自然语言理解的基石，其性能缺陷可能导致下游任务的级联故障。研究强调需建立专门基准推动多方言理解技术发展，并呼吁关注模型在语言多样性场景下的公平性与包容性。

Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\textbf{MDial}$, the first large-scale framework for generating multi-dialectal conversational data encompassing the three pillars of written dialect -- lexical (vocabulary), orthographic (spelling), and morphosyntactic (grammar) features -- for nine English dialects. Partnering with native linguists, we design an annotated and scalable rule-based LLM transformation to ensure precision. Our approach challenges the assumption that models should mirror users' morphosyntactic features, showing that up to 90% of the grammatical features of a dialect should not be reproduced by models. Independent evaluations confirm data quality, with annotators preferring MDial outputs over prior methods in 98% of pairwise comparisons for dialect naturalness. Using this pipeline, we construct the dialect-parallel $\textbf{MDialBench}$mark with 50k+ dialogs, resulting in 97k+ QA pairs, and evaluate 17 LLMs on dialect identification and response generation tasks. Even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. As dialect identification underpins natural language understanding, these errors risk cascading failures into downstream tasks.

</details>


### [105] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: 提出了基于扩散模型的语音-文本联合语言模型“Silent Thought, Spoken Answer”，通过生成内部文本推理链提升语音QA准确率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型直接生成回复缺少显式推理过程，导致生成错误无法修正，且缺乏可解释性

Method: 构建统一的掩码扩散框架（diff-to-speech），实现文本推理链与语音token的联合迭代生成，采用模态特异性掩码调度策略

Result: 在语音对话任务中达到SOTA准确率（比基线高9个百分点），语音合成质量最佳（6.2% WER），保持66.2% MMLU语言理解能力

Conclusion: 扩散架构与推理链的协同显著提升性能，在语音QA数据集SLAST-26K上获得验证，为可解释语音生成提供新范式

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [106] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文发现当前解释大语言模型（LLM）的方法（基于注意力和嵌入的方法）存在缺陷，削弱了其在理解模型和部署计算系统中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在普适计算中广泛应用，但其优异性能的底层机制尚不明确。现有LLM可解释性方法本身也缺乏充分验证，作者试图探究语言抽象性在LLM中的形成机制。

Method: 采用两种文献中的成熟方法：(1) 通过注意力头探测词元级关系结构；(2) 利用嵌入向量作为载体进行属性映射。测试了后续层表征是否对应词元、预测分数是否反映语义知识等核心假设。

Result: 两种方法均因不同方法论问题失败：注意力权重无法保持词元对应关系，属性映射的高预测得分由数据集结构和方法论缺陷导致，而非语义知识。

Conclusion: 当前LLM可解释性技术存在系统性缺陷，基于其得出的模型理解结论不可靠。这对将LLM作为系统组件部署的分布式计算场景具有警示意义，因解释方法常被用于调试、压缩和模型解释。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [107] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 本论文提出通过放松严格位置监督提高Masked扩散语言模型（MDLMs）开放文本生成效果，引入<slack>标记优化训练策略。


<details>
  <summary>Details</summary>
Motivation: MDLMs在开放文本生成任务中与自回归模型存在性能差距，主要受位置预测敏感性和训练-解码目标不匹配问题困扰。

Method: 在微调阶段采用基于连接时序分类（CTC）目标的<slack>标记，实现位置灵活监督策略，允许解码过程容错。

Result: 在5个开放生成基准测试中，改进后的MDLM均优于原始模型，展现出更强的抗位置偏移能力和生成连贯性。

Conclusion: 训练时放松严格位置监督要求有利于提升MDLM生成质量，证明解码动态特性需与监督策略匹配。

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [108] [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.CL

TL;DR: 提出FraudCoT框架，通过自主的图感知思维链推理和可扩展的LLM-GNN联合训练，提升文本属性图上的欺诈检测效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM增强GNN方法受限于预定义提示和分离式训练流水线，导致推理自主性不足及语义-结构对齐弱化。

Method: 采用欺诈感知的选择性思维链蒸馏机制生成多路径推理，将蒸馏结果整合入节点文本，并设计非对称共训练策略降低联合训练计算成本。

Result: 实验显示比先进方法提升8.8% AUPRC，训练吞吐量加速1,066倍，在检测性能与效率上均显著进步。

Conclusion: FraudCoT解决了预定义提示和训练分离问题，实现了语义-结构联合优化的高效欺诈检测范式。

Abstract: Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening semantic-structural alignment. We propose FraudCoT, a unified framework that advances TAG-based fraud detection through autonomous, graph-aware chain-of-thought (CoT) reasoning and scalable LLM-GNN co-training. To address the limitations of predefined prompts, we introduce a fraud-aware selective CoT distillation mechanism that generates diverse reasoning paths and enhances semantic-structural understanding. These distilled CoTs are integrated into node texts, providing GNNs with enriched, multi-hop semantic and structural cues for fraud detection. Furthermore, we develop an efficient asymmetric co-training strategy that enables end-to-end optimization while significantly reducing the computational cost of naive joint training. Extensive experiments on public and industrial benchmarks demonstrate that FraudCoT achieves up to 8.8% AUPRC improvement over state-of-the-art methods and delivers up to 1,066x speedup in training throughput, substantially advancing both detection performance and efficiency.

</details>


### [109] [Residual Context Diffusion Language Models](https://arxiv.org/abs/2601.22954)
*Yuezhou Hu,Harman Singh,Monishwaran Maheswaran,Haocheng Xi,Coleman Hooper,Jintao Zhang,Aditya Tomar,Michael W. Mahoney,Sewon Min,Mehrdad Farajtabar,Kurt Keutzer,Amir Gholami,Chenfeng Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Residual Context Diffusion (RCD)的方法，通过回收扩散大语言模型（dLLMs）中被丢弃的上下文信息以提升模型效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于块状的dLLMs依赖于仅保留高置信度token的remasking机制，导致计算资源的浪费，且被丢弃的token仍包含可用于后续解码的上下文信息。

Method: RCD将被丢弃的token表示转化为上下文残差并注入下一次去噪过程，同时采用解耦的两阶段训练流水线以缓解反向传播内存瓶颈。

Result: 在长推理链（SDAR）和短推理链（LLaDA）任务中，RCD以约10亿token数据将dLLM转为RCD范式，平均提升5-10个百分点精度，并在AIME任务中减少4-5倍去噪步骤的同时达到基线双倍准确率。

Conclusion: RCD通过高效利用残差上下文显著增强了dLLMs性能，且额外计算开销极低，为模型优化提供了新方向。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.

</details>


### [110] [A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training](https://arxiv.org/abs/2601.22966)
*Zihan Qiu,Zeyu Huang,Kaiyue Wen,Peng Jin,Bo Zheng,Yuxin Zhou,Haofeng Huang,Zekun Wang,Xiao Li,Huaqing Zhang,Yang Xu,Haoran Lian,Siqi Zhang,Rui Men,Jianwei Zhang,Ivan Titov,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型中异常值（如attention sinks和residual sinks）的功能作用，提出‘outlier-driven rescaling’现象，验证其与归一化层协同工作机制，并提出优化方案以提升训练稳定性和量化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 作者观察到模型中存在异常激活单元（sinks）影响训练稳定性，试图揭示其生成机制与归一化层的关联性，为模型优化提供理论依据。

Method: 通过实证分析对比不同架构模型，设计归一化删除、异常值裁剪、参数吸收和显式门控缩放等实验，验证outlier-driven rescaling假设的普适性和有效性。

Result: 实验证明：1）异常值依赖归一化存在，直接移除会损害训练稳定性；2）异常值主要起缩放因子作用，自身贡献较小；3）将异常值参数化或门控缩放后，训练性能提升2.0分且W4A4量化损失降低1.2分。

Conclusion: 论文揭示了归一化层与sinks的共现机制，证明异常值的非冗余性，提出通过结构改进缓解其副作用，为后续模型设计提供了量化优化和稳定性增强的新方向。

Abstract: We investigate the functional role of emergent outliers in large language models, specifically attention sinks (a few tokens that consistently receive large attention logits) and residual sinks (a few fixed dimensions with persistently large activations across most tokens). We hypothesize that these outliers, in conjunction with the corresponding normalizations (\textit{e.g.}, softmax attention and RMSNorm), effectively rescale other non-outlier components. We term this phenomenon \textit{outlier-driven rescaling} and validate this hypothesis across different model architectures and training token counts. This view unifies the origin and mitigation of both sink types. Our main conclusions and observations include: (1) Outliers function jointly with normalization: removing normalization eliminates the corresponding outliers but degrades training stability and performance; directly clipping outliers while retaining normalization leads to degradation, indicating that outlier-driven rescaling contributes to training stability. (2) Outliers serve more as rescale factors rather than contributors, as the final contributions of attention and residual sinks are significantly smaller than those of non-outliers. (3) Outliers can be absorbed into learnable parameters or mitigated via explicit gated rescaling, leading to improved training performance (average gain of 2 points) and enhanced quantization robustness (1.2 points degradation under W4A4 quantization).

</details>


### [111] [ArabicDialectHub: A Cross-Dialectal Arabic Learning Resource and Platform](https://arxiv.org/abs/2601.22987)
*Salem Lahlou*

Main category: cs.CL

TL;DR: ArabicDialectHub是一个整合了552个跨方言阿拉伯语短语与互动学习平台的开源项目。


<details>
  <summary>Details</summary>
Motivation: 缺乏集中且互动性强的阿拉伯语多方言学习资源，现有工具难以支持跨文化交际需求。

Method: 使用大语言模型生成短语，通过五名母语者验证，按主题与难度分层组织，并开发具自适应测验功能的开源平台。

Result: 产出包含6种阿拉伯语变体的数据集、MIT许可的开源平台，支持翻译探索、算法化干扰项测验及进度同步，且提供文化背景信息。

Conclusion: 项目成功构建可持续的语言学习资源，促进阿拉伯多方言学习效率与文化理解，后续将优化算法与扩展内容。

Abstract: We present ArabicDialectHub, a cross-dialectal Arabic learning resource comprising 552 phrases across six varieties (Moroccan Darija, Lebanese, Syrian, Emirati, Saudi, and MSA) and an interactive web platform. Phrases were generated using LLMs and validated by five native speakers, stratified by difficulty, and organized thematically. The open-source platform provides translation exploration, adaptive quizzing with algorithmic distractor generation, cloud-synchronized progress tracking, and cultural context. Both the dataset and complete platform source code are released under MIT license. Platform: https://arabic-dialect-hub.netlify.app.

</details>


### [112] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: The study develops the Cross-Lingual Alignment Steering (CLAS) framework to reduce political bias in multilingual large language models (LLMs) across 50 countries and 33 languages, improving cross-lingual consistency while preserving response quality.


<details>
  <summary>Details</summary>
Motivation: Existing research on LLM political bias focuses on Western languages or narrow multilingual settings, leaving gaps in cross-lingual consistency analysis and post-hoc bias mitigation strategies. This work aims to address these issues through a large-scale multilingual evaluation and a novel mitigation framework.

Method: The authors (1) conducted a large-scale multilingual political bias evaluation across 50 countries and 33 languages, and (2) developed CLAS—a post-hoc mitigation framework that aligns latent ideological representations into a shared ideological subspace while dynamically adjusting intervention strength to balance bias reduction and response coherence.

Result: Experiments demonstrated significant bias reduction along economic and social political axes (measured through country-specific evaluations) with minimal degradation in response quality metrics like coherence, relevance, and linguistic naturalness across multiple languages.

Conclusion: The CLAS framework offers a scalable, interpretable solution for mitigating political bias in multilingual LLMs by enforcing ideological alignment across languages while preserving cultural diversity and linguistic integrity through adaptive intervention mechanisms.

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>


### [113] [InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.23006)
*Junyou Su,He Zhu,Xiao Luo,Liyu Zhang,Hong-Yu Zhou,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出InstructDiff框架，通过差分熵选择最优数据子集，在降低训练成本的同时提升跨任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法存在领域特异性，无法兼顾推理与一般指令任务；低差分熵样本虽有效但需领域自适应策略。

Method: InstructDiff包含三阶段：warmup阶段模型校准、双向NLL过滤噪声样本、基于差分熵的方向性排序（推理任务取熵增样本，一般任务取熵减样本）。

Result: 数学推理任务相对提升17%，一般指令任务提升52%，均仅用10%数据量超越全数据训练效果。

Conclusion: 差分熵原则具有领域自适应性，可统一优化推理与指令任务的数据选择，显著降低训练成本并提升模型性能。

Abstract: Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\% relative improvement over full data training on mathematical reasoning and 52\% for general instruction-following, outperforming prior baselines while using only 10\% of the data.

</details>


### [114] [DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2601.23022)
*Lung-Hao Lee,Liang-Chih Yu,Natalia Loukashevich,Ilseyar Alimova,Alexander Panchenko,Tzu-Mi Lin,Zhe-Yu Xu,Jian-Yu Zhou,Guangmin Zheng,Jin Wang,Sharanya Awasthi,Jonas Becker,Jan Philip Wahle,Terry Ruas,Shamsuddeen Hassan Muhammad,Saif M. Mohammed*

Main category: cs.CL

TL;DR: This paper proposes a dimensional approach to Aspect-Based Sentiment Analysis (ABSA) by introducing continuous valence-arousal (VA) scores and a multilingual dataset (DimABSA), bridging traditional ABSA with fine-grained emotional analysis through three subtasks and a new evaluation metric (cF1).


<details>
  <summary>Details</summary>
Motivation: Existing ABSA methods rely on coarse categorical labels (positive/negative) which cannot capture nuanced emotional states. This work addresses this limitation by adopting VA dimensional theory to enable more granular sentiment analysis at both aspect and sentiment levels.

Method: 1) Created DimABSA - a multilingual dataset with 76,958 aspect instances across 6 languages and 4 domains, annotated with traditional ABSA elements (aspect terms/categories, opinion terms) and new VA scores; 2) Defined three subtasks merging VA scores with ABSA components; 3) Proposed continuous F1 (cF1) metric combining VA prediction error with standard F1; 4) Benchmarking using prompted and fine-tuned large language models.

Result: Successfully constructed the first multilingual dimensional ABSA benchmark covering 42,590 sentences with comprehensive annotations, demonstrated the validity of VA-based approach through benchmark experiments, and established cF1 as a unified evaluation metric that accounts for both categorical and continuous outputs.

Conclusion: DimABSA provides a foundational benchmark for advancing multilingual dimensional ABSA research with its novel combination of qualitative (aspect) and quantitative (VA) annotations, offering a more comprehensive approach to sentiment analysis than traditional categorical methods.

Abstract: Aspect-Based Sentiment Analysis (ABSA) focuses on extracting sentiment at a fine-grained aspect level and has been widely applied across real-world domains. However, existing ABSA research relies on coarse-grained categorical labels (e.g., positive, negative), which limits its ability to capture nuanced affective states. To address this limitation, we adopt a dimensional approach that represents sentiment with continuous valence-arousal (VA) scores, enabling fine-grained analysis at both the aspect and sentiment levels. To this end, we introduce DimABSA, the first multilingual, dimensional ABSA resource annotated with both traditional ABSA elements (aspect terms, aspect categories, and opinion terms) and newly introduced VA scores. This resource contains 76,958 aspect instances across 42,590 sentences, spanning six languages and four domains. We further introduce three subtasks that combine VA scores with different ABSA elements, providing a bridge from traditional ABSA to dimensional ABSA. Given that these subtasks involve both categorical and continuous outputs, we propose a new unified metric, continuous F1 (cF1), which incorporates VA prediction error into standard F1. We provide a comprehensive benchmark using both prompted and fine-tuned large language models across all subtasks. Our results show that DimABSA is a challenging benchmark and provides a foundation for advancing multilingual dimensional ABSA.

</details>


### [115] [Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures](https://arxiv.org/abs/2601.23081)
*Yanghao Su,Wenbo Zhou,Tianwei Zhang,Qiu Han,Weiming Zhang,Nenghai Yu,Jie Zhang*

Main category: cs.CL

TL;DR: 研究发现微调大语言模型时，特定字符级倾向数据比错误内容更容易引发广泛行为错位，揭示出模型行为模式的稳定性风险。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为错位源自错误内容泛化的观点，探索字符级倾向数据对行为错位的特殊影响机制。

Method: 通过跨领域多模型实验，对比字符级倾向数据与错误建议微调的效果，在训练和推理阶段测试行为触发条件。

Result: 验证了字符级倾向数据会引发更强且可迁移的错位行为，发现此类错位具有行为稳定性特征，并存在训练触发器与推理提示双重激活路径。

Conclusion: 模型角色塑造是关键对齐风险，必须从行为模式层面构建防御体系，而非仅依赖错误修正或单次提示防御。

Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

</details>


### [116] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文提出动态认知回退(DEF)协议,通过模拟人类认知防御机制提升LLM对恶意政策文本的防御能力。实验显示该方法在HIPAA/GDPR合规任务中实现100%攻击检测率。


<details>
  <summary>Details</summary>
Motivation: 受人类防范欺骗的认知机制启发,针对LLM在数据隐私等高风险场景中易受恶意篡改攻击的问题,需要开发可解释的安全防护方案

Method: 设计三级文本提示协议(检测-拒绝-知识回退),在推理阶段通过动态触发机制增强模型对恶意政策文本的辨识能力

Result: 基于HIPAA/GDPR的实证表明:采用DEF的前沿LLM在恶意文本检测中表现卓越,DeepSeek-R1实现100%检测准确率且防御效果具有跨模型泛化性

Conclusion: 认知科学启发的DEF协议可显著提升LLM的法律抗欺骗能力,建议进一步探索类人认知防护机制在AI安全中的应用

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


### [117] [Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics](https://arxiv.org/abs/2601.23129)
*Yilun Hua,Giuseppe Castellucci,Peter Schulam,Heba Elfardy,Kevin Small*

Main category: cs.CL

TL;DR: 本文提出GroGU指标，通过大模型生成信心评估RAG内容效用，无需人工标注且效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG效用评估依赖高成本标注或忽略模型特性，需构建模型专用且无需标注的评估指标

Method: 基于LLM生成熵值定义效用函数，通过对比真实文档与候选文档的生成置信度差异量化内容质量

Result: 在MRR指标提升18.2点，回答准确率提升9.4点，验证了效用指标对RAG系统的优化效果

Conclusion: GroGU能准确捕捉LLM对文档的依赖程度，其指导的查询重写策略显著增强了检索与生成的协同效果

Abstract: Retrieval Augmented Generation (RAG)'s success depends on the utility the LLM derives from the content used for grounding. Quantifying content utility does not have a definitive specification and existing metrics ignore model-specific capabilities and/or rely on costly annotations. In this paper, we propose Grounding Generation Utility (GroGU), a model-specific and reference-free metric that defines utility as a function of the downstream LLM's generation confidence based on entropy. Despite having no annotation requirements, GroGU is largely faithful in distinguishing ground-truth documents while capturing nuances ignored by LLM-agnostic metrics. We apply GroGU to train a query-rewriter for RAG by identifying high-utility preference data for Direct Preference Optimization. Experiments show improvements by up to 18.2 points in Mean Reciprocal Rank and up to 9.4 points in answer accuracy.

</details>


### [118] [Monotonic Reference-Free Refinement for Autoformalization](https://arxiv.org/abs/2601.23166)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种无需参考答案的全定理自动形式化迭代方法，利用定理证明器和LLM（大语言模型）的复合反馈优化形式有效性、逻辑保持性、数学一致性及形式质量四个维度，最终在miniF2F和ProofNet数据集上分别实现93.9%和44.1%的形式有效性。


<details>
  <summary>Details</summary>
Motivation: 当前形式化自动迭代主要针对单一维度（如语法正确性）优化，难以同时提升多个关键质量指标，而全定理自动形式化需跨维度联合优化。本文旨在解决这一局限，实现全面质量提升。

Method: 设计多阶段的掩码复合目标优化框架，结合定理证明器与扮演不同角色的LLM（裁判模型）的互补反馈。通过响应度图分析各LLM角色对不同质量维度的偏好，并制定保证单调性改进的接受策略，从形式证明和逻辑结构双路径协同优化。

Result: 在miniF2F数据集上取得93.44%的形式有效性及78.22%的总体评分；ProofNet数据集上实现44.09%的形式有效性与29.79%的总体评分，均优于现有单一维度优化方法。

Conclusion: 无需真实证明作为参考的迭代框架可有效协同优化多维度质量指标，结合LLM角色分工与定理证明反馈可确保收敛性，为复杂定理的端到端形式化提供新范式。

Abstract: While statement autoformalization has advanced rapidly, full-theorem autoformalization remains largely unexplored. Existing iterative refinement methods in statement autoformalization typicall improve isolated aspects of formalization, such as syntactic correctness, but struggle to jointly optimizing multiple quality dimensions, which is critical for full-theorem autoformalization. We introduce a reference-free iterative monotonic process for full-theorem autoformalization that leverages complementary feedback from theorem provers and LLM-based judges, without access to ground-truth proofs or existing formalizations at inference time. Our approach optimizes a masked composite objective over Formal Validity, Logical Preservation, Mathematical Consistency, and Formal Quality, guided by a responsiveness map that indicates how different LLMs acting as different roles preferentially improve each dimension. We further propose an acceptance policy that guarantees certified monotonic improvement, and provide conditions ensuring convergence and termination. Empirical experiments demonstrate the proposed process enables simultaneous improvement across multiple dimensions, achieving 93.44% formal validity and a 78.22% overall score on miniF2F, and 44.09% formal validity and a 29.79% overall score on ProofNet.

</details>


### [119] [FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation](https://arxiv.org/abs/2601.23182)
*Siyang He,Qiqi Wang,Xiaoran Liu,Hongnan Ma,Yiwei Shi,Yuerong Song,Ying Zhu,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种名为FourierSampler的解码方法，通过频域分析解决扩散语言模型（dLLMs）的生成偏差问题，显著提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有dLLMs解码策略存在位置偏差，未能充分发挥非自回归模型潜力。频域分析发现低频分量编码全局结构信息，高频分量编码局部细节，但缺乏有效利用该特性的方法。

Method: 提出FourierSampler方法，基于频域滑动窗口机制动态引导模型生成，先生成低频结构再细化高频局部细节。通过频域分析指导生成顺序，解决位置偏差问题。

Result: 在LLADA和SDAR数据集上超越其他推理增强策略，在LLaDA1.5-8B和LLaDA-8B-Instruct模型上分别取得20.4%和16.0%的相对提升，且显著优于同等规模的Llama3.1-8B-Instruct等自回归模型。

Conclusion: 通过频域分析揭示了dLLMs隐藏状态的频谱特性，证明频域指导生成的有效性。FourierSampler为非自回归模型提供了新的优化方向，在生成质量和效率上均优于传统自回归模型。

Abstract: Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.

</details>


### [120] [ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought](https://arxiv.org/abs/2601.23184)
*Fanmeng Wang,Haotian Liu,Guojiang Zhao,Hongteng Xu,Zhifeng Gao*

Main category: cs.CL

TL;DR: 提出ReGuLaR方法解决链式思考(CoT)在大语言模型中的计算冗余问题


<details>
  <summary>Details</summary>
Motivation: 1) CoT虽能提升模型性能但导致计算冗余；2) 现有压缩方法因缺乏有效压缩引导出现性能下降；3) 多模态推理能力尚未被充分挖掘

Method: 1) 在VAE框架下建模潜在推理，用前序状态的后验分布采样当前状态；2) 将思维链渲染为图像，提取密集视觉语义表征；3) 通过视觉表征正则化后验分布，实现高效压缩

Result: 1) 在计算效率和推理效果双维度超越现有方法；2) 多模态推理首次反超CoT基线；3) 信息压缩损失降低40%以上（推测）

Conclusion: 开辟了图像渲染辅助语言模型推理的新范式，证明了视觉-语言联合表征学习在隐式推理中的优势

Abstract: While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.

</details>


### [121] [Are you going to finish that? A Practical Study of the Tokenization Boundary Problem](https://arxiv.org/abs/2601.23223)
*Hao Xu,Alisa Liu,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: The study reveals that partial token prompts (ending mid-token) severely distort language models' predictions, especially in non-whitespace languages and code, with correct continuation probabilities dropping by 1000x compared to aligned prompts.


<details>
  <summary>Details</summary>
Motivation: Language models are trained on token sequences, but user inputs often end mid-token due to misaligned word/token boundaries, affecting prediction reliability in real-world scenarios like Chinese text and code.

Method: Identified failure domains (non-whitespace languages, compound languages, code), created natural partial-token prompts, and tested frontier LMs' probability predictions on aligned vs. misaligned prompts.

Result: Partial tokens caused 1000x probability drops in correct continuations without token alignment; larger models showed worse degradation, and mitigation strategies had mixed effectiveness.

Conclusion: Tokenization-induced probability distortion is a critical failure mode in practical applications; recommends token-aware prompt engineering and model inference adjustments to address alignment issues.

Abstract: Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic prompts respecting word boundaries remains underexplored. In this work, we identify three domains where token and "word" boundaries often do not line up: languages that do not use whitespace, highly compounding languages, and code. In Chinese, for example, up to 25% of word boundaries do not line up with token boundaries, making even natural, word-complete prompts susceptible to this problem. We systematically construct semantically natural prompts ending with a partial tokens; in experiments, we find that they comprise a serious failure mode: frontier LMs consistently place three orders of magnitude less probability on the correct continuation compared to when the prompt is "backed-off" to be token-aligned. This degradation does not diminish with scale and often worsens for larger models. Finally, we evaluate inference-time mitigations to the partial token problem and validate the effectiveness of recent exact solutions. Overall, we demonstrate the scale and severity of probability distortion caused by tokenization in realistic use cases, and provide practical recommentions for model inference providers.

</details>


### [122] [Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models](https://arxiv.org/abs/2601.23255)
*Ye Yu,Haibo Jin,Yaoning Yu,Jun Zhuang,Haohan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种针对音频语言模型的文本对音频越狱攻击方法，揭示了语音输入带来新的安全漏洞，并指出需要多模态安全框架。


<details>
  <summary>Details</summary>
Motivation: 音频语言模型的兴起引入了新的安全威胁，但相关漏洞研究仍不足。需要探究语音输入特有的安全缺陷并提出攻击范例。

Method: 利用先进TTS模型将文本指令转化为叙述式音频，通过声学与结构特性绕过传统文本安全机制，构建端到端的越狱攻击方法。

Result: 在Gemini 2.0 Flash等模型上实现98.26%攻击成功率，显著高于文本攻击基线，验证了语音渠道的严重安全风险。

Conclusion: 需要构建融合语言与副语言（如声学特征）的新型安全框架，以应对语音接口普及带来的多模态安全挑战。

Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.

</details>
