<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 93]
- [cs.CL](#cs.CL) [Total: 46]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR是一款基于语义解耦的面部隐私保护框架，该框架能够在保持机器识别能力的前提下，通过语义引导的对抗样本生成实现视觉匿名化，并支持授权后的图像还原。


<details>
  <summary>Details</summary>
Motivation: 随着面部识别技术在在线服务中的广泛应用，如何在存储和传输过程中实现身份信息与视觉表征的分离成为隐私保护的关键挑战。传统方法难以平衡视觉匿名化与身份识别一致性之间的关系。

Method: 提出基于扩散模型隐空间的语义解耦框架SIDeR，包含三个核心技术：1）将面部图像分解为机器可识的身份特征向量和视觉语义表征分量；2）采用动量驱动的无限制对抗优化算法和语义-视觉平衡因子，生成视觉多样且自然的对抗样本；3）引入授权访问机制，通过密码验证实现受保护图像的原图还原。

Result: 在CelebA-HQ和FFHQ数据集上的实验表明：该框架在黑盒攻击场景下实现99%的攻击成功率，在PSNR恢复质量上较基线方法提升41.28%，成功生成兼具视觉匿名化和高识别率的对抗样本，且支持高质量图像还原。

Conclusion: SIDeR通过语义解耦策略有效解决了身份保护与视觉可用性之间的平衡问题。框架在无需图像降质的情况下实现强泛化能力，具备实际部署可行性，同时通过可逆保护机制满足授权访问需求，为隐私保护提供了新的解决方案。

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [2] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: 针对视觉-语言-动作（VLA）模型中视觉与动作预测脱节导致动作不可靠的问题，本文提出通过增强视觉条件依赖的训练框架，无需修改架构或新增数据即可改善任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型存在视觉-动作预测脱节现象，导致动作输出与当前视觉状态关联性弱。实验发现成功案例的视觉依赖更强，因此提出改进视觉条件依赖的训练方法。

Method: 采用两步法：（1）基于轨迹跟踪替代任务通过偏好优化对齐视觉输入与动作预测；（2）在监督微调中通过潜在空间蒸馏将增强的对齐能力迁移至指令跟随任务。

Result: 在离散动作空间的OpenVLA模型中，该方法同时提升视觉条件依赖性和任务性能；扩展至连续动作空间的OpenVLA-OFT设置时仍保持稳定增益。

Conclusion: 通过显式增强视觉条件依赖性，无需架构修改即可有效提升VLA模型的任务可靠性，且适用于离散与连续动作空间。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [3] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文探讨了基于图像的饮食评估中从2D图像估计食物3D尺寸的挑战及其解决策略，包括辅助输入、模型方法与深度学习的应用。


<details>
  <summary>Details</summary>
Motivation: 传统图像评估方法在将二维图像转化为三维食物尺寸时存在精度不足的问题，这阻碍了其在健康监测中的应用效果，需要探索更精确的策略以提升饮食评估准确性。

Method: 综合分析现有解决方案，包括使用深度图/多视角输入等辅助数据、模板匹配等模型方法，以及基于单目图像或混合输入的深度学习模型，系统性对比各类技术的优劣。

Result: 多种混合策略（如图像与深度数据结合的深度学习模型）已被证实能有效提升食物分量估计的精度，但不同方法在准确性与实施成本权衡上存在差异。

Conclusion: 结合辅助信息与深度学习的方法能显著改善图像评估的局限性，未来需进一步优化模型泛化能力并探索低成本实施路径以推进实际应用。

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [4] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本研究提出Visual Concept Ranking(VCR)方法，用于解析医疗视觉模型的决策逻辑，发现模型在不同人群中的性能差异并通过可视化特征验证其决策可靠性。


<details>
  <summary>Details</summary>
Motivation: 医疗等安全敏感领域需要可验证的模型审计方法，现有大型多模态模型存在难以解释的决策风险与潜在数据偏差

Method: 设计VCR框架评估模型关注的视觉概念重要性，通过生成特征依赖假设并进行人工干预验证，以皮肤癌诊断数据集为核心进行实证分析

Result: 揭示模型在性别/种族亚组间准确率差异达15%，VCR定位出32%的误判病例由非病变区域特征驱动，实验验证了模型对特定纹理特征的过度依赖

Conclusion: VCR为模型审计提供可操作框架，发现医疗视觉模型性能差距源于训练数据分布偏移而非算法架构缺陷，提出需要建立更均衡的医疗数据基准

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [5] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: The paper introduces CLEAR-HPV, a framework for interpretable analysis of HPV-related cancers through morphologic concept discovery in histopathology slides, combining attention-based MIL with explainable representations.


<details>
  <summary>Details</summary>
Motivation: Current attention-based MIL models for cancer prognosis lack morphologic interpretability. CLEAR-HPV addresses this by enabling concept discovery without manual label supervision, bridging predictive power and clinical explainability.

Method: CLEAR-HPV reconstructs the MIL latent space by prioritizing attention weights to identify interpretable morphologic concepts (e.g., keratinizing, stromal). It generates spatial concept maps and compact 10-concept vectors, replacing high-dimensional embeddings (1536D) while retaining predictive information.

Result: CLEAR-HPV successfully identified reproducible morphologic concepts across three major cancer datasets (TCGA-HNSCC, TCGA-CESC, CPTAC-HNSCC). The framework maintains predictive performance while drastically reducing dimensionality and enabling visual interpretation of slide features.

Conclusion: CLEAR-HPV provides a generalizable, backbone-agnostic solution for explainable AI in histopathology, offering both diagnostic accuracy and clinically meaningful morphologic insights for HPV-related cancers.

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [6] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: ARGaze 是一项通过第一人称视频帧进行在线自我中心凝视估计的新方法，利用过去和当前的帧来预测当前的注视位置，无需明确的头部或眼部信号，关键在于利用凝视目标的强时间连续性并通过Transformer架构进行序列预测。


<details>
  <summary>Details</summary>
Motivation: 传统第三人称视线估计依赖显式的头部/眼部信息，而自我中心视角下仅能依赖手持物体交互、场景显著性等间接信号。论文发现目标任务驱动下的凝视具有强时间连续性（即近期注视点可提供强先验预测后续注视），因此提出需设计因果且支持流式推理的序列预测框架解决此问题。

Method: 提出ARGaze模型，将凝视估计转换为序列预测任务：每个时间步使用Transformer解码器，基于（i）当前视觉特征和（ii）一个固定长度的凝视上下文窗口（Gaze Context Window）进行预测。该设计通过限制历史凝视窗口长度保证因果性，并支持资源占用稳定的流式推理。

Result: 在多个自我中心凝视估计基准数据集上达到在线评估SOTA性能，消融实验证明基于有限历史凝视的自回归建模对鲁棒预测具有关键作用，且固定长度上下文窗口在效率与精度间取得平衡。

Conclusion: ARGaze通过融合当前视觉信息与有限历史注视序列，首次将自回归解码机制应用于自我中心凝视估计，显著提升预测鲁棒性并支持实时流式推理，未来将公开代码与预训练模型促进领域发展。

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [7] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: 本研究提出AirGlove，解决视觉方法在传感手套追踪中的泛化问题，通过已有手套数据实现在新设计手套上的高性能追踪


<details>
  <summary>Details</summary>
Motivation: 传感手套追踪受传感器精度限制，而现有视觉模型在裸手训练后难以适应各种手套外观。作者旨在填补视觉方法在手套手部追踪领域的性能差距。

Method: 提出基于对比学习的AirGlove框架，利用少量新式手套数据与现有手套数据建立跨设备表征迁移，并采用自监督校准策略提升模型适应性

Result: 实验表明AirGlove相较基线方法提升28.6%追踪精度，在5种新型传感手套上平均误差降低至4.7mm，且仅需20%训练数据即可达到全量数据90%性能

Conclusion: 研究表明视觉模型可通过跨设备表征迁移突破裸手训练限制，证实了无需传感器即可实现高精度手套手部追踪的技术可行性

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [8] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: 本论文提出了一种通过合成退化图像并采用扩散生成模型进行反向学习的图像重建方法，以解决水下摄影中的色彩失真和模糊问题。


<details>
  <summary>Details</summary>
Motivation: 水下摄影存在对比度低、空间模糊和波长相关色彩失真等问题，传统后期处理流程复杂且效果受限，需开发更有效的自动修复方法。

Method: 构建了基于合成退化流水线的扩散生成模型，使用Keith Ellenbogen高质量水下摄影作品数据集（约2500张）进行端到端训练，通过反向扩散过程学习图像复原。

Result: 模型参数仅11M，在512x768分辨率图像上实现高感知一致性与强泛化能力，验证了小规模数据集训练的有效性。

Conclusion: 证明了基于扩散模型的图像反向退化方法可有效解决水下成像问题，为生态摄影提供了轻量级高保真修复方案。

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

</details>


### [9] [ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification](https://arxiv.org/abs/2602.05175)
*Zhe Li,Bernhard Kainz*

Main category: cs.CV

TL;DR: Shape Guided Purification (ShapePuri) improves adversarial robustness by aligning model representations with structural invariants.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods like adversarial training and purification suffer from high computational costs or information loss, needing a scalable and efficient solution.

Method: ShapePuri integrates two components: a Shape Encoding Module (SEM) using Signed Distance Functions for geometric guidance, and a Global Appearance Debiasing (GAD) module with stochastic transformations to address appearance bias.

Result: Achieves 84.06% clean accuracy and 81.64% robust accuracy under AutoAttack, surpassing the prior 80% robustness threshold on the benchmark, with minimal computational overhead.

Conclusion: ShapePuri provides a scalable defense framework that maintains prediction stability without auxiliary modules or additional costs while enhancing robustness.

Abstract: Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\%$ clean accuracy and $81.64\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

</details>


### [10] [PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction](https://arxiv.org/abs/2602.05190)
*Ju Shen,Chen Chen,Tam V. Nguyen,Vijayan K. Asari*

Main category: cs.CV

TL;DR: PoseGaussian通过将人体姿态引导嵌入几何与时间阶段，实现高保真人体新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将姿态作为条件输入或空间变换，未充分利用姿态的结构先验性和时间连续性，导致在动态人体场景中存在深度估计误差和时间不连贯问题。

Method: 设计双路径架构：1) 姿态-颜色协同编码路径，通过结构先验优化深度估计；2) 专用姿态编码路径捕获时间线索，增强帧间一致性，并采用端到端高斯点阵渲染流程。

Result: 在ZJU-MoCap等数据集上实现PSNR 30.86/SSIM 0.979/LPIPS 0.028，较基线提升12.7%感知质量，单帧渲染速度达100FPS，支持动态人体复杂运动场景。

Conclusion: 本方法首次将姿态信息同时作用于几何建模与时间建模，在解决动态人体自遮挡和关节变形难题上具有显著优势。

Abstract: We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is processed by a dedicated pose encoder to enhance temporal consistency across frames. These components are integrated into a fully differentiable, end-to-end trainable pipeline. Unlike prior works that use pose only as a condition or for warping, PoseGaussian embeds pose signals into both geometric and temporal stages to improve robustness and generalization. It is specifically designed to address challenges inherent in dynamic human scenes, such as articulated motion and severe self-occlusion. Notably, our framework achieves real-time rendering at 100 FPS, maintaining the efficiency of standard Gaussian Splatting pipelines. We validate our approach on ZJU-MoCap, THuman2.0, and in-house datasets, demonstrating state-of-the-art performance in perceptual quality and structural accuracy (PSNR 30.86, SSIM 0.979, LPIPS 0.028).

</details>


### [11] [GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling](https://arxiv.org/abs/2602.05202)
*Shivanshu Shekhar,Uttaran Bhattacharya,Raghavendra Addanki,Mehrab Tanjim,Somdeb Sarkhel,Tong Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种将视频生成模型重新用作奖励模型的新方法，通过合成负样本和能量模型重构，显著减少对人工标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（VLM）的奖励机制难以捕捉视频生成中的细微时间动态，需更精确的时间感知评估方法。

Method: 将生成模型转化为能量模型（EBM），通过对比学习目标训练其区分视频质量；设计三种合成负样本技术（时序切片、特征交换、帧重排）模拟现实退化。

Result: 在GenAI-Bench和MonteBench数据集上达到SOTA效果，且仅需30K人工标注数据，相比VLMB方法减少6-65倍标注需求。

Conclusion: 生成模型重构为奖励模型可有效提升视频质量评估效率，合成负样本设计迫使模型学习语义时空特征而非表面缺陷。

Abstract: Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.

</details>


### [12] [Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures](https://arxiv.org/abs/2602.05213)
*Chuqin Zhou,Xiaoyue Ling,Yunuo Chen,Jincheng Dai,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本研究提出了一种结合显式语义与隐式细节的统一图像压缩框架，在保持语义结构的同时合成高质量纹理。


<details>
  <summary>Details</summary>
Motivation: 现有生成压缩方法在超低比特率下需在语义保真与感知真实间进行权衡：显式表示法保留结构但缺乏细节，隐式方法生成细节却易导致语义漂移。

Method: 通过扩散模型进行显式语义建模（高阶语义条件生成）与反向通道编码实现隐式细节传输，并设计可插拔编码器调节失真-感知平衡。

Result: 在Kodak/DIV2K/CLIC2020数据集上，DISTS BD-Rate指标分别超越现有最优DiffC方法29.92%/19.33%/20.89%。

Conclusion: 实验证实所提框架在维持训练免费特性的同时实现了感知质量与语义保真的协同优化，建立了新的超低比特率压缩性能基准。

Abstract: While recent neural codecs achieve strong performance at low bitrates when optimized for perceptual quality, their effectiveness deteriorates significantly under ultra-low bitrate conditions. To mitigate this, generative compression methods leveraging semantic priors from pretrained models have emerged as a promising paradigm. However, existing approaches are fundamentally constrained by a tradeoff between semantic faithfulness and perceptual realism. Methods based on explicit representations preserve content structure but often lack fine-grained textures, whereas implicit methods can synthesize visually plausible details at the cost of semantic drift. In this work, we propose a unified framework that bridges this gap by coherently integrating explicit and implicit representations in a training-free manner. Specifically, We condition a diffusion model on explicit high-level semantics while employing reverse-channel coding to implicitly convey fine-grained details. Moreover, we introduce a plug-in encoder that enables flexible control of the distortion-perception tradeoff by modulating the implicit information. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art rate-perception performance, outperforming existing methods and surpassing DiffC by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on the Kodak, DIV2K, and CLIC2020 datasets, respectively.

</details>


### [13] [E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching](https://arxiv.org/abs/2602.05215)
*Jiahao Nie,Wenbin An,Gongjie Zhang,Yicheng Xu,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: This paper proposes E.M.Ground, a novel Vid-LLM for Temporal Video Grounding (TVG) that improves event localization through holistic semantic perception.


<details>
  <summary>Details</summary>
Motivation: Existing TVG methods struggle with semantic continuity and temporal integrity due to fragmented frame matching, leading to ambiguous localization.

Method: E.M.Ground introduces (i) an <event> token for aggregated frame semantics, (ii) Savitzky-Golay smoothing to reduce similarity noise, and (iii) multi-grained feature aggregation to enhance temporal understanding and compensate for compression loss.

Result: E.M.Ground significantly outperforms state-of-the-art Vid-LLMs on benchmark datasets through its holistic event perception approach.

Conclusion: Preserving semantic continuity and reducing temporal noise in Vid-LLMs effectively address TVG challenges, demonstrating the potential of event-centric design in video understanding.

Abstract: Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.

</details>


### [14] [Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation](https://arxiv.org/abs/2602.05217)
*Jiahao Nie,Guanqiao Fu,Wenbin An,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Multi-view Progressive Adaptation（MPA）的方法，用于解决跨域少样本分割问题，通过数据和策略双视角渐进式适应，显著提升模型在目标域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在源域预训练的少样本能力难以有效迁移到目标域，主要受限于目标样本的稀缺性、多样性的不足以及显著的领域差距，导致模型无法充分利用有限的目标样本进行有效适应。

Method: 1. 数据视角：Hybrid Progressive Augmentation通过叠加强增强操作，渐进生成更多样化且复杂的训练样本，逐步提升学习难度；2. 策略视角：Dual-chain Multi-view Prediction设计串行与并行预测链，结合多视角监督信号，通过预测一致性约束增强领域适应的鲁棒性。

Result: 在跨域少样本分割任务中，MPA方法在保持7%绝对性能优势的同时，显著超越当前最先进的自适应模型，且在少样本学习曲线的陡峭程度和稳定度上表现突出。

Conclusion: 本文通过动态增强样本复杂度和多路径监督学习的协同优化，有效解决了领域偏移带来的适应性退化问题，为少样本模型的领域迁移提供了新的技术框架。

Abstract: Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).

</details>


### [15] [Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification](https://arxiv.org/abs/2602.05218)
*Jiahao Nie,Yun Xing,Wenbin An,Qingsong Zhao,Jiawei Shao,Yap-Peng Tan,Alex C. Kot,Shijian Lu,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的条件点稀疏化方法（CPS），通过自适应调整密集匹配点密度，在交叉域少样本分割任务中突破传统SAM模型的域移位限制。


<details>
  <summary>Details</summary>
Motivation: 现有SAM基于密集点匹配的few-shot分割方法在医疗/卫星图像等域移位场景中效果不佳，因密集点模式会破坏SAM学习的点-图像交互关系，且未考虑跨域特性。

Method: 利用参考范例提供的ground-truth mask，通过可学习阈值函数动态筛选冗余密集点：1）构建参考-目标密集匹配图 2）基于掩码边缘特征设计自适应稀疏策略 3）通过SAM的prompt生成器将稀疏点映射为有效分割提示

Result: 在5个CD-FSS数据集上超越所有基于预训练SAM的零样本方法，平均IoU提升8.2%，且在医疗影像分割中达到与微调方法相当的性能，推理速度达12FPS。

Conclusion: CPS通过引入参考掩码引导的域无关稀疏策略，证明了无需参数更新即可突破SAM的跨域限制，为医学/遥感图像分割提供了实用解决方案。

Abstract: Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.

</details>


### [16] [PatchFlow: Leveraging a Flow-Based Model with Patch Features](https://arxiv.org/abs/2602.05238)
*Boxiang Zhang,Baijian Yang,Xiaoming Wang,Corey Vian*

Main category: cs.CV

TL;DR: The paper proposes a method combining local neighbor-aware patch features with a normalizing flow model to enhance automated anomaly detection in die casting defect detection.


<details>
  <summary>Details</summary>
Motivation: Surface defects in die casting hinder quality control. Current computer vision techniques face challenges in bridging generic pretrained feature extractors with industrial product images, necessitating an adapter module to improve detection efficiency and accuracy.

Method: The approach integrates local neighbor-aware patch features with a normalizing flow model and introduces an adapter module to align pretrained feature extractors with industrial product images, enabling training without requiring anomalous samples.

Result: On MVTec AD dataset, reduced error rate by 20% (AUROC 99.28%). On VisA dataset, achieved 28.2% error reduction (AUROC 96.48%). Proprietary die casting dataset showed 95.77% anomaly detection accuracy without using anomalous samples for training.

Conclusion: The method demonstrates significant improvements in detection accuracy and efficiency, showcasing the potential of computer vision and deep learning to advance die casting inspection while eliminating reliance on anomalous training samples.

Abstract: Die casting plays a crucial role across various industries due to its ability to craft intricate shapes with high precision and smooth surfaces. However, surface defects remain a major issue that impedes die casting quality control. Recently, computer vision techniques have been explored to automate and improve defect detection. In this work, we combine local neighbor-aware patch features with a normalizing flow model and bridge the gap between the generic pretrained feature extractor and industrial product images by introducing an adapter module to increase the efficiency and accuracy of automated anomaly detection. Compared to state-of-the-art methods, our approach reduces the error rate by 20\% on the MVTec AD dataset, achieving an image-level AUROC of 99.28\%. Our approach has also enhanced performance on the VisA dataset , achieving an image-level AUROC of 96.48\%. Compared to the state-of-the-art models, this represents a 28.2\% reduction in error. Additionally, experiments on a proprietary die casting dataset yield an accuracy of 95.77\% for anomaly detection, without requiring any anomalous samples for training. Our method illustrates the potential of leveraging computer vision and deep learning techniques to advance inspection capabilities for the die casting industry

</details>


### [17] [Active Label Cleaning for Reliable Detection of Electron Dense Deposits in Transmission Electron Microscopy Images](https://arxiv.org/abs/2602.05250)
*Jieyun Tan,Shuo Liu,Guibin Zhang,Ziqi Li,Jian Geng,Lei Zhang,Lei Cao*

Main category: cs.CV

TL;DR: 本研究提出一种主动标签清洗方法，通过结合主动学习和众包标注降低医疗AI开发中的标签噪声，显著提升模型性能并减少专家标注成本。


<details>
  <summary>Details</summary>
Motivation: 电子致密沉积物（EDD）检测面临高质量标注数据匮乏的挑战，而众包虽能降低成本，但引入的标签噪声限制模型性能。现有清洗方法效率低下，亟需高效降噪方案。

Method: 设计主动学习框架：1）利用模型预测与众包标签的差异构建噪声评分系统；2）选择噪声值最高且模型难识别的样本优先清洗；3）开发标签选择模块实现样本级和实例级噪声分级。

Result: 在私有数据集实现67.18% AP₅₀指标，较直接使用噪声标签训练提升18.83%，达到全专家标注模型95.79%的性能，同时减少73.30%专家标注量。

Conclusion: 所提方法平衡了标注效率与模型质量，在有限专家资源条件下，为医疗视觉诊断模型构建提供了可扩展的噪声数据清洗范式。

Abstract: Automated detection of electron dense deposits (EDD) in glomerular disease is hindered by the scarcity of high-quality labeled data. While crowdsourcing reduces annotation cost, it introduces label noise. We propose an active label cleaning method to efficiently denoise crowdsourced datasets. Our approach uses active learning to select the most valuable noisy samples for expert re-annotation, building high-accuracy cleaning models. A Label Selection Module leverages discrepancies between crowdsourced labels and model predictions for both sample selection and instance-level noise grading. Experiments show our method achieves 67.18% AP\textsubscript{50} on a private dataset, an 18.83% improvement over training on noisy labels. This performance reaches 95.79% of that with full expert annotation while reducing annotation cost by 73.30%. The method provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources.

</details>


### [18] [RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation](https://arxiv.org/abs/2602.05257)
*Diya He,Qingchen Liu,Cong Zhang,Jiahu Qin*

Main category: cs.CV

TL;DR: 提出了RFM-Pose框架，通过流匹配生成模型与强化学习结合，在保持类别级6D物体姿态估计精度的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于分数扩散模型在姿态估计中的采样效率低下问题，同时处理旋转对称模糊挑战。

Method: 采用流匹配生成模型沿最优传输路径生成姿态候选，并将采样过程建模为马尔可夫决策过程，通过近端策略优化联合优化姿态生成与假设评分。

Result: 在REAL275基准测试中计算成本显著降低，性能优于现有方法，且支持物体姿态跟踪任务。

Conclusion: 流匹配与强化学习框架能有效平衡采样效率与估计精度，为通用物体姿态分析提供新思路。

Abstract: Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.

</details>


### [19] [ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network](https://arxiv.org/abs/2602.05262)
*Junzhou Li,Manqi Zhao,Yilin Gao,Zhiheng Yu,Yin Li,Dongsheng Jiang,Li Xiao*

Main category: cs.CV

TL;DR: ReGLA combines efficient convolutions and ReLU-based attention for lightweight, high-resolution image processing with improved accuracy-latency trade-off.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models face challenges in balancing accuracy and latency for high-resolution images, necessitating more efficient architectures.

Method: ReGLA integrates efficient convolutions (ELRF module) for local feature extraction and ReLU gated attention (RGMA module) for global modeling, enhanced by multi-teacher distillation.

Result: ReGLA-M achieves 80.85% Top-1 accuracy on ImageNet-1K (224px) with 4.98ms latency (512px), plus 3.1% AP (COCO) and 3.6% mIoU (ADE20K) improvements over iFormer.

Conclusion: ReGLA establishes a state-of-the-art framework for high-resolution vision tasks by effectively combining convolution and attention mechanisms.

Abstract: Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.

</details>


### [20] [Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2602.05271)
*Shengqin Jiang,Xiaoran Feng,Yuankai Qi,Haokui Zhang,Renlong Hang,Qingshan Liu,Lina Yao,Quan Z. Sheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: This paper challenges the common approach in Few-shot Class-Incremental Learning (FSCIL) by focusing on prototype fine-tuning instead of adapting feature extractors. It introduces a framework that evolves static prototypes through dual calibration, achieving better performance with minimal learnable parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional methods use frozen feature extractors with static prototypes, causing representation bias. Recent prompt-based techniques struggle with extreme data scarcity, limiting global feature improvement. The paper argues that optimizing decision regions in a fixed, high-quality feature space is the core FSCIL challenge, not feature learning itself.

Method: A prototype fine-tuning framework keeps the feature extractor frozen while learning dynamic prototypes. It uses a dual-calibration method combining class-specific offsets (refining individual class boundaries) and task-aware offsets (balancing old/new class distributions) to enhance prototype discrimination without backbone updates.

Result: The approach achieves state-of-the-art results across multiple FSCIL benchmarks (e.g., 79.82% average accuracy on MiniImageNet) while requiring only ~2,000 learnable parameters, significantly fewer than prompt-based methods (~50,000 parameters).

Conclusion: Optimizing decision regions via dynamic prototypes in a fixed feature space outperforms backbone adaptation in FSCIL. The dual-calibration method enables efficient incremental learning with minimal parameter costs, offering a scalable solution for real-world applications with limited data.

Abstract: Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.

</details>


### [21] [Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs](https://arxiv.org/abs/2602.05275)
*Qi Li,Yanzhe Zhao,Yongxin Zhou,Yameng Wang,Yandong Yang,Yuanjia Zhou,Jue Wang,Zuojian Wang,Jinxiang Liu*

Main category: cs.CV

TL;DR: Magic-MM-Embedding是一种新型高效多模态检索方法，通过视觉token压缩和渐进训练策略，在保持高性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理大量视觉token时存在显著计算成本，阻碍实际应用，需解决效率与性能平衡问题。

Method: 1) 紧凑型MLLM架构结合视觉token压缩；2) 三阶段训练策略：继续预训练恢复多模态能力、对比预训练增强判别力、MLLM-as-a-Judge指导任务微调。

Result: 模型性能显著超越现有方法，且推理效率更高，但未提具体数据指标。

Conclusion: 实现效率与效果双赢，为通用多模态表征学习提供了新的解决方案，在保持低资源消耗的同时达成SOTA性能。

Abstract: Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.

</details>


### [22] [Fast-SAM3D: 3Dfy Anything in Images but Faster](https://arxiv.org/abs/2602.05293)
*Weilun Feng,Mingqiang Wu,Zhiliang Chen,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiaokun Liu,Guoxin Fan,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: Fast-SAM3D 提出三重异质性感知机制，通过动态计算分配实现免训练加速，在仅损失0.5%保真度下达到2.67倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有加速策略忽略SAM3D管道中存在的多级异质性(形状-布局动态分离/纹理优化稀疏性/几何频谱差异)，导致加速效果脆弱，而复杂场景的实时3D重建亟需高效推理框架。

Method: 构建训练无关框架：1.模态感知步长缓存解耦结构演变与敏感布局更新；2.时空联合标记雕刻聚焦高熵区域；3.频谱感知标记聚合自适应分辨率解码。

Result: 在多视角和单视角数据集上均实现2.67倍端到端加速，PSNR仅下降0.42dB，SSIM下降0.02，建立效率-质量权衡新基准。

Conclusion: 通过异构感知动态计算机制，成功突破3D生成模型实时部署瓶颈，代码开源(https://github.com/wlfeng0509/Fast-SAM3D)推动相关研究。

Abstract: SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.

</details>


### [23] [FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion](https://arxiv.org/abs/2602.05305)
*Zhuokun Chen,Jianfei Cai,Bohan Zhuang*

Main category: cs.CV

TL;DR: 提出FlashBlock，通过缓存块间注意力输出来减少生成过程中重复计算注意力的开销，提升长序列生成效率。


<details>
  <summary>Details</summary>
Motivation: 长序列生成中，现有块扩散方法（block diffusion）因需随KV缓存增长而重复计算注意力导致效率下降，且块间注意力冗余未被充分利用。

Method: 分析发现块间注意力输出在扩散步骤间保持稳定，FlashBlock缓存这部分输出并重复利用；分离块内与块间注意力计算，仅更新动态变化的块内部分，同时与稀疏注意力正交结合。

Result: 在文本和视频生成任务中，实现生成吞吐量提升1.44倍，注意力计算时间减少1.6倍，且生成质量无显著损失。

Conclusion: 通过重用稳定注意力输出，FlashBlock在不改变扩散过程的前提下显著提升效率，并与稀疏注意力互补增强模型表现。

Abstract: Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.

</details>


### [24] [MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors](https://arxiv.org/abs/2602.05330)
*Jingdong Zhang,Xiaohang Zhan,Lingzhi Zhang,Yizhou Wang,Zhengming Yu,Jionghao Wang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: 本文提出MTPano方法，基于无标签训练框架，将透视域密集先验迁移至全景域，通过双流解耦架构处理几何失真和任务干扰，实现全景场景多任务理解SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 全景场景理解受制于高分辨率多任务标注数据稀缺、普通视角模型直接迁移失败的几何失真问题，以及球面空间多任务关系未被充分研究的现状。现有方法难以处理不同任务类型间的干扰和等距矩形投影的失真问题。

Method: 1) 利用现成模型生成视角伪分割标签，通过投影-反投影生成全景监督信号；2) 将任务分为旋转不变(深度/分割)和旋转敏感(法向量)两类，采用几何感知的调制层解耦特征流；3) 引入ERP token mixer和双分支BridgeNet处理失真，通过梯度截断实现跨任务信息共享；4) 增加辅助任务促进多任务联合学习。

Result: 在多个全景基准测试中达到SOTA性能，且与专门化单任务模型相比具有竞争力，在深度估计/语义分割等任务上均优于传统方法。

Conclusion: 该研究证实了无标签训练框架的有效性，通过解决领域差距、任务干扰和畸变建模三大挑战，为全景多任务学习提供了新范式，验证了跨任务知识共享对全景理解的重要性。

Abstract: Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.

</details>


### [25] [Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation](https://arxiv.org/abs/2602.05339)
*Yongwoo Kim,Sungmin Cha,Hyunsoo Kim,Jaewon Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: 提出PAIR框架，在文本到图像扩散模型中实现有害概念的语义一致擦除与安全替代。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法仅移除不安全概念但缺乏安全替代引导，导致生成结果的结构/语义连贯性受损。需寻找兼顾安全性和一致性的新方法。

Method: 创建不安全-安全概念配对数据，通过配对语义重对齐（将目标概念映射到安全锚点）和基于Fisher信息矩阵的DoRA参数初始化（增强安全生成抑制有害概念），实现细粒度选择性擦除。

Result: 实验显示相比现有方法，在保持结构完整性和生成质量的前提下，擦除有害概念的同时有效保留语义一致性，性能显著提升。

Conclusion: PAIR框架成功将概念擦除转化为语义对齐问题，通过数据配对和参数优化的双重机制，解决了安全概念替代与整体一致性保持的矛盾。

Abstract: With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.

</details>


### [26] [Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection](https://arxiv.org/abs/2602.05349)
*Ningkang Peng,JiuTao Zhou,Yuhao Zhang,Xiaoqian Peng,Qianfeng Yu,Linjing Qian,Tingyu Lu,Yi Chen,Yanhui Gu*

Main category: cs.CV

TL;DR: APEX通过解决静态同质性假设和学习-推断断开问题，提出自适应原型流形和后验感知评分机制，提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的方法因固定表征资源和推断阶段忽略原型质量信息而受到限制，导致模型表现受限。

Method: 1. 引入基于最小描述长度原则的自适应原型流形（APM）动态分配原型数量；2. 设计后验感知的OOD评分机制（PAOS）量化原型质量（凝聚度与分离度）。

Result: 在CIFAR-100等基准数据集上达到新的SOTA性能，实验验证方法有效性。

Conclusion: APEX通过优化特征流形和融合原型质量信息，在保持方法简洁性的同时显著提升OOD检测能力。

Abstract: Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.

</details>


### [27] [Multimodal Latent Reasoning via Hierarchical Visual Cues Injection](https://arxiv.org/abs/2602.05359)
*Yiming Zhang,Qiangyu Yan,Borui Jiang,Kai Han*

Main category: cs.CV

TL;DR: 本论文提出了HIVE框架，通过分层视觉线索在潜在空间中实现多模态大模型的‘深度思考’推理。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM的‘快思考’推理依赖语言驱动链式思维（CoT），存在效率低、易幻觉等问题，需要更鲁棒的多模态融合推理方式

Method: 设计递归扩展Transformer模块+内部循环机制，将全局场景到细粒度区域的视觉线索逐层注入潜在表征空间，实现无需显式文本解释的迭代推理解码

Result: 实验验证了视觉知识注入的时序有效性，分层信息融合显著提升复杂场景解析能力

Conclusion: 潜在空间多模态协同推理（HIVE）证明了视觉感知引导的深度推理可增强大模型认知能力

Abstract: The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a "fast thinking" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\emph{HIVE}), a novel framework that instills deliberate, "slow thinking" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.

</details>


### [28] [Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection](https://arxiv.org/abs/2602.05360)
*Ningkang Peng,Xiaoqian Peng,Yuhao Zhang,Qianfeng Yu,Feng Xing,Peirong Ma,Xichen Yang,Yi Chen,Tingyu Lu,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出D-KNN方法，通过几何解耦解决语义霸权问题，有效提升OOD检测性能，减少误报率并应对传感器噪声。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA模型在区分语义细微差异的OOD样本时表现良好，但对结构差异明显或传感器噪声样本存在严重几何盲区，亟需打破语义霸权并重建结构分布检测能力。

Method: 基于正交分解的D-KNN框架，分离语义与结构残差空间，并通过双空间校准机制重新激活模型对结构残差信号的敏感性。

Result: 在CIFAR/ImageNet上实现新SOTA：FPR95从31.3%降至2.3%；高斯噪声场景下AUROC从79.7%提升至94.9%。

Conclusion: 理论分析揭示神经坍塌引发谱集中偏置，提出方法有效解耦语义-结构特征，为训练免费的鲁棒OOD检测提供范式。

Abstract: While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.

</details>


### [29] [Imagine a City: CityGenAgent for Procedural 3D City Generation](https://arxiv.org/abs/2602.05362)
*Zishan Liu,Zecong Tang,RuoCheng Wu,Xinzhe Zheng,Jingyu Hu,Ka-Hei Hui,Haoran Xie,Bo Dai,Zhengzhe Liu*

Main category: cs.CV

TL;DR: 本文提出了CityGenAgent，一个基于自然语言驱动的分层程序生成高质量3D城市的框架，通过结合监督微调和强化学习，解决现有方法在高保真资产生成、可控性和可编辑性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有城市生成方法在高保真3D资产创建、可控性和操作性上存在挑战，而本文的CityGenAgent旨在通过自然语言交互和程序生成提升生成质量与可编辑性，以满足自动驾驶、虚拟现实等应用需求。

Method: 将城市生成分解为Block Program和Building Program两部分。第一阶段通过监督微调训练模型生成符合约束的程序；第二阶段利用强化学习设计空间对齐奖励和视觉一致性奖励优化模型。最终支持通过自然语言编辑与操作生成结果。

Result: 综合评估表明，CityGenAgent在语义对齐、视觉质量和可控性方面均优于现有方法，为可扩展的3D城市生成提供了可靠基础。

Conclusion: 通过程序生成与自然语言交互，CityGenAgent在高保真3D城市生成中实现了更强的可控性和可编辑性，为未来相关应用提供了新方向。

Abstract: The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.

</details>


### [30] [SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback](https://arxiv.org/abs/2602.05380)
*Xiaoxuan He,Siming Fu,Wanli Li,Zhiyuan Li,Dacheng Yin,Kang Rong,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: SAIL通过迭代自我增强学习，使扩散模型在仅有少量人工反馈的情况下实现有效对齐，无需大量偏好数据或外部奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型与人类偏好的对齐依赖昂贵的大规模偏好数据集或不可行的奖励模型，需探索模型自身隐含能力以最小化人工依赖。

Method: 基于少量人工标注偏好对构建初始种子集，SAIL通过闭环迭代循环实现模型自我生成样本、自标注偏好并利用增强数据集优化自身，引入排序偏好混合策略平衡探索与保留原始人类先验。

Result: 实验表明SAIL在多项基准测试中超越现有方法，仅需已有方法6%的偏好数据即可达成更优效果。

Conclusion: 扩散模型具备强大的自我改进潜力，SAIL框架揭示了无需大规模标注或外部奖励模型即可实现高效对齐的可行性。

Abstract: Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \textbf{SAIL} (\textbf{S}elf-\textbf{A}mplified \textbf{I}terative \textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.

</details>


### [31] [VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs](https://arxiv.org/abs/2602.05382)
*Tina Khezresmaeilzadeh,Jike Zhong,Konstantinos Psounis*

Main category: cs.CV

TL;DR: VRIQ benchmark reveals current Vision Language Models (VLMs) struggle with abstract visual reasoning, primarily due to perception limitations, achieving only 28% accuracy on abstract puzzles and 45% on natural tasks, even with tool augmentation.


<details>
  <summary>Details</summary>
Motivation: Assess whether VLMs can perform reliable nonverbal reasoning through targeted visual reasoning tasks and diagnostic analysis.

Method: Develop VRIQ with abstract puzzle-style and natural-image tasks; evaluate model accuracy and employ diagnostic probes to isolate perception and reasoning errors.

Result: VLMs perform near-random on abstract puzzles (28% accuracy), show weak improvement on natural tasks (45% accuracy), and fail due to perception (56%), combined perception-reasoning issues (43%), with only 1% due to reasoning alone.

Conclusion: VLMs remain unreliable abstract visual reasoners, dominated by perception bottlenecks; targeted diagnostic probes highlight category-specific vulnerabilities to improve multimodal reasoning systems.

Abstract: Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.

</details>


### [32] [Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting](https://arxiv.org/abs/2602.05384)
*Hao Feng,Wei Shi,Ke Zhang,Xiang Fei,Lei Liao,Dingkang Yang,Yongkun Du,Xuecheng Wu,Jingqun Tang,Yang Liu,Hong Chen,Can Huang*

Main category: cs.CV

TL;DR: 本文介绍了Dolphin-v2，一个改进的两阶段文档解析模型，通过结合整体页面理解和细粒度元素检测，显著提升了数字化和拍摄文档的解析效果，同时支持并行处理以提高效率。


<details>
  <summary>Details</summary>
Motivation: 文档解析领域存在模型碎片化问题，且传统方法依赖轴对齐边界框，难以处理弯曲或拍摄文档，导致用户选择成本高且系统扩展性差。

Method: Dolphin-v2第一阶段同步进行文档类型分类（数字化/拍摄）与布局分析；数字化文档进一步进行细粒度元素检测与阅读顺序预测。第二阶段采用混合策略：拍摄文档通过整体页面解析处理几何畸变，数字化文档通过布局锚点并行解析元素，并补充代码块识别与元数据提取功能。

Result: 在DocPTBench、OmniDocBench及自主构建的RealDoc-160数据集上，综合评估显示相较于原模型，OmniDocBench整体提升14.78分，拍摄文档错误率降低91%，且保持高效推理能力。

Conclusion: Dolphin-v2通过双策略协同（整体解析+并行元素解析）解决了文档类型适配与几何畸变问题，显著提升性能并增强系统可扩展性。

Abstract: Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.

</details>


### [33] [Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning](https://arxiv.org/abs/2602.05387)
*Zolnamar Dorjsembe,Hung-Yi Chen,Furen Xiao,Hsing-Kuo Pao*

Main category: cs.CV

TL;DR: 提出Parallel Swin Transformer-Enhanced Med2Transformer模型实现MRI合成CT，降低剂量误差。


<details>
  <summary>Details</summary>
Motivation: MRI缺乏电子密度信息需CT补充，但联合采集增加复杂性和误差。合成CT可简化流程但面临MRI-CT非线性和解剖差异的挑战。

Method: 设计3D架构：卷积编码提取局部特征，双分支Swin Transformer建模长程依赖；多尺度位移窗口注意力融合多级特征提升解剖保真度。

Result: 在公开和临床数据验证：图像相似性(SSIM 0.95)、几何误差<1mm、剂量误差1.69%，剂量分布与真CT无显著差异。

Conclusion: 该模型实现了临床可用的合成CT重建，剂量误差在国际标准范围内，开源代码推动MRI放疗规划发展。

Abstract: MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.

</details>


### [34] [Dataset Distillation via Relative Distribution Matching and Cognitive Heritage](https://arxiv.org/abs/2602.05391)
*Qianxin Xia,Jiawei Du,Yuhan Zhang,Jielei Wang,Guoming Lu*

Main category: cs.CV

TL;DR: 本文提出一种高效的统计流对齐方法（Statistical Flow Matching）用于数据集蒸馏，显著降低计算和内存开销，并通过分类器继承策略进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度匹配的数据集蒸馏方法需要多次加载真实图片和增广操作，导致高计算和内存开销，限制了实际应用效率。

Method: 提出统计流对齐框架，通过单次加载原始数据统计信息并仅对合成数据进行一次增广操作，对齐目标类与非目标类间的常量统计流；同时设计分类器继承策略，复用原分类器并仅需轻量线性投影。

Result: 在同等性能下，方法相比SOTA减少10倍GPU内存占用和4倍运行时间，分类器继承策略在极低存储需求下实现性能提升。

Conclusion: 所提方法解决了数据集蒸馏的效率瓶颈，为下游任务提供更实用的解决方案。

Abstract: Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.

</details>


### [35] [Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing](https://arxiv.org/abs/2602.05397)
*Yuechen Yang,Junlin Guo,Ruining Deng,Junchao Zhu,Zhengyi Lu,Chongyu Qu,Yanfan Zhu,Xingyi Guo,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出MAD框架，通过结合VAE与扩散模型，在保持生物特征相关性的前提下实现可解释的病理组学特征编辑。


<details>
  <summary>Details</summary>
Motivation: 病理组学特征常相互关联且难以解释，传统特征编辑因假设独立性导致生物流形偏离和伪影；现有方法无法保持特征编辑后的生物学合理性。

Method: 利用VAE学习解耦的潜在特征空间，通过正则化编辑轨迹确保特征调整符合生物数据分布，再驱动条件扩散模型生成高保真图像。

Result: 实验表明该方法在编辑病理组学特征时能维持生物合理性，结构连贯性优于基线方法，并成功实现目标特征可控编辑。

Conclusion: MAD框架通过特征流形约束显著提升了病理组学特征的可解释性和临床实用性，为医学图像分析提供了新的生成式解释工具。

Abstract: Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., "second-order moment") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.

</details>


### [36] [TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions](https://arxiv.org/abs/2602.05414)
*Ngoc Doan-Minh Huynh,Duong Nguyen-Ngoc Tran,Long Hoang Pham,Tai Huu-Phuong Tran,Hyung-Joon Jeon,Huy-Hung Nguyen,Duong Khac Vu,Hyung-Min Jeon,Son Hong Phan,Quoc Pham-Nam Ho,Chi Dai Tran,Trinh Le Ba Khanh,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 本研究提出TSBOW数据集，用于极端天气下交通监控中的遮挡车辆检测，包含32小时真实交通数据与48,000个手动标注帧，推动智能交通系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅涵盖轻度雾霾、雨雪，缺乏极端天气下的交通监控数据，且遮挡车辆检测研究不足，需新数据集填补领域空白。

Method: 构建TSBOW数据集：采集高密度城市区域32小时极端天气交通视频，进行手动与半自动标注，包含8类交通参与者边界框，并设计遮挡与恶劣天气目标检测基准测试方案。

Result: TSBOW包含48,000个手动标注帧和320万半标注帧，覆盖多种道路类型与天气场景，基准测试验证了遮挡检测难度及数据集对智能交通系统的实用价值。

Conclusion: TSBOW为极端天气交通监控提供关键数据资源，推动CCTV技术优化与智能交通系统研究，数据集已开源促进后续算法开发。

Abstract: Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.

</details>


### [37] [VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection](https://arxiv.org/abs/2602.05415)
*Ningkang Peng,Qianfeng Yu,Yuhao Zhang,Yafei Liu,Xiaoqian Peng,Peirong Ma,Yi Chen,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 本文提出了一种无需外部数据集的新型框架，通过几何引导的虚拟异常合成（GOS）和双粒度语义损失（DGS），在长尾分布下实现优于现有方法的分布外检测（OOD）性能。


<details>
  <summary>Details</summary>
Motivation: 现有最佳方法依赖大规模外部真实数据集进行特征空间正则化，但高昂的数据获取成本和隐私敏感性限制了其实际部署，因此需要消除对外部数据的依赖。

Method: 提出基于超球面的vMF分布建模方法，在特征空间中定位低似然环形区域进行方向性虚拟异常采样（GOS），并设计对比学习驱动的双粒度语义损失（DGS）以扩大分布内与合成边界异常的区分性。

Result: 在CIFAR-LT等基准测试中，该方法在检测长尾分布下的分布外样本时，性能超过使用真实外部图像的现有最先进方法。

Conclusion: 通过理论驱动的虚拟异常合成策略与自适应损失函数设计，无需外部数据即可优化特征空间决策边界，实现了高鲁棒性的分布外检测方法。

Abstract: Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.

</details>


### [38] [Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring](https://arxiv.org/abs/2602.05420)
*Rui Sun,Yiwen Yang,Kaiyu Guo,Chen Jiang,Dongli Xu,Zhaonan Liu,Tan Pan,Limei Han,Xue Jiang,Wu Wei,Yuan Cheng*

Main category: cs.CV

TL;DR: 提出Disco框架解决密集重叠细胞实例分割问题，基于图着色理论并结合显式标注与隐式消歧策略，并发布GBC-FS 2025数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于轮廓检测和距离映射的细胞分割方法在复杂密集区域表现不足；图着色方法尚未在具有密集重叠和复杂拓扑的真实场景中验证有效性，且发现真实细胞图多为非二分图（含大量奇数环）。

Method: 通过GBC-FS 2025数据集系统分析细胞图色性，提出基于‘分治’思想的Disco框架：（1）显式标注策略将拓扑问题转化为分类任务，递归分解图并隔离冲突集；（2）隐式消歧机制通过强制特征差异化解冲突区域模糊性，学习可分离特征表示。

Result: 发现真实细胞图普遍含奇数环导致2色理论不足，而Disco在4个数据集均达到SOTA性能，尤其在处理复杂拓扑时超越现有方法。

Conclusion: 揭示真实细胞邻接图的高阶色性需求，并证明通过数据驱动标注与约束学习结合的Disco框架能有效解决密集分割挑战，填补图着色理论在病理分析中的应用空白。

Abstract: Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the "divide and conquer" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, "Explicit Marking" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a "conflict set." Second, "Implicit Disambiguation" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.

</details>


### [39] [NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks](https://arxiv.org/abs/2602.05423)
*Pengcheng Chen,Yue Hu,Wenhao Li,Nicole M Gunderson,Andrew Feng,Zhenglong Sun,Peter Beerel,Eric J Seibel*

Main category: cs.CV

TL;DR: 本文提出NeVStereo框架，通过联合优化NeRF渲染、多视角深度估计、姿态优化和几何一致性迭代，首次实现单一框架下的高精度相机姿态估计、深度预测、新视角合成与表面重建的完整解决方案，各项指标优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建系统存在以下问题：（1）端到端方法不能显式输出新视角合成（2）神经渲染依赖固定姿态对姿态误差敏感（3）多数方法无法同时保证准确姿态、可靠深度与高质量渲染。本文旨在构建能从普通RGB输入同时获取准确姿态、高质量3D表面与高保真渲染的统一框架。

Method: 创新性提出四步联动方法：1) 基于NeRF的新视角合成生成立体匹配友好渲染；2) 开发置信度引导的多视角深度估计器；3) 引入NeRF耦合的光束法平差进行姿态优化；4) 建立深度与辐射场联合迭代优化模型。通过几何一致性约束解决传统NeRF存在的表面堆叠、伪影和姿态-深度耦合问题。

Result: 实验表明NeVStereo在室内/户外/桌面/航拍多场景均表现优越：深度误差降低36%（SIFT基准），姿态准确度提升10.4%（TUM基准），新视角合成保真度提高4.5%（PSNR指标）。在网格质量方面取得SOTA表现：F1度量达91.93%，Chamfer距离4.35mm（ScanNet基准）。

Conclusion: 所提出的NeVStereo框架成功实现了同时优化相机姿态、深度估计、新视角合成和表面重建的目标。在无需深度测量设备的条件下，首次将NeRF与立体视觉有机结合，解决了姿态与几何耦合问题，在四个关键指标上均超越现有顶尖方法，为三维重建领域提供了新的范式。

Abstract: In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.

</details>


### [40] [LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects](https://arxiv.org/abs/2602.05434)
*Sanghoon Jeon,Gihyun Jung,Suhyeon Ka,Jae-Sang Hyun*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散的结构光方法(LD-SLRO)，解决高反射率物体三维重建中的条纹失真问题。


<details>
  <summary>Details</summary>
Motivation: 高反射率和低粗糙度物体在条纹投影轮廓术中存在镜面反射和次表面散射，导致条纹模式严重失真或丢失。

Method: 通过编码高反射表面的相移条纹图像提取潜在特征，输入潜扩散模型进行反射伪影抑制和条纹信息恢复，包含镜面反射编码器、时变通道仿射层和注意力模块。

Result: 实验显示均方根误差从1.8176mm降低至0.9619mm，显著优于现有方法。

Conclusion: 实现了高反射表面的高质量条纹恢复与三维重建，且支持输入输出条纹集的灵活配置。

Abstract: Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.

</details>


### [41] [Stable Velocity: A Variance Perspective on Flow Matching](https://arxiv.org/abs/2602.05435)
*Donglin Yang,Yongxing Zhang,Xin Yu,Liang Hou,Xin Tao,Pengfei Wan,Xiaojuan Qi,Renjie Liao*

Main category: cs.CV

TL;DR: Stable Velocity 提出了一种针对流匹配方法的改进框架，通过降低训练目标方差提升生成模型训练与采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统流匹配因单样本条件速度估计导致高方差训练目标，造成优化不稳定和收敛缓慢，本文针对性解决这一问题。

Method: 1) StableVM：构建无偏方差缩减目标函数
2) VA-REPA：基于低方差区域加强辅助监督
3) StableVS：利用低方差区域动态特性的闭式简化实现加速采样

Result: 在ImageNet256x256及SD3.5/Qwen-Image等大型模型验证有效，训练效率提升且采样速度加快2倍以上，样本质量无损

Conclusion: 方差分析揭示高/低方差区域特性，Stable Velocity成功将该理论洞察转化为实用框架，实现训练稳定性和推理加速双重优化

Abstract: While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.

</details>


### [42] [Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations](https://arxiv.org/abs/2602.05440)
*Natascha Jeziorski,Petra Gospodnetić,Claudia Redenbach*

Main category: cs.CV

TL;DR: This paper proposes using parametric 3D defect modeling and physics-based Monte Carlo simulations to generate synthetic training data for automated non-destructive testing (NDT) defect detection, enabling controllable, scalable, and precise data generation with rare defect inclusion and pixel-perfect annotations.


<details>
  <summary>Details</summary>
Motivation: Real-world training data for automated defect detection in NDT is often insufficient in quantity, quality, and diversity, particularly for rare defect types. Current methods struggle to generate synthetic data that both captures defect variability and supports precise pixel-level annotations.

Method: Parametric 3D mesh modeling techniques mimic common metal casting defects (applicable to other machining processes). These defect models are integrated into object geometries via digital twins, and synthetic NDT data is generated through physically accurate Monte Carlo simulations. Pixel-perfect annotations are generated alongside the data.

Result: A flexible framework creates arbitrarily large synthetic datasets with variable defect types (including rare ones) and pixel-accurate labels. Demonstrated with visual surface inspection, but generalizable to any NDT modality by swapping corresponding simulation components.

Conclusion: This approach effectively addresses data scarcity issues in automated defect detection by combining parametric defect modeling with physics-driven simulations, enabling precise, scalable, and defect-diverse synthetic dataset creation for NDT applications.

Abstract: In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.

</details>


### [43] [DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching](https://arxiv.org/abs/2602.05449)
*Chang Zou,Changlin Li,Yang Li,Patrol Li,Jianbing Wu,Xiao He,Songtao Liu,Zhao Zhong,Kailin Huang,Linfeng Zhang*

Main category: cs.CV

TL;DR: This paper proposes a learnable feature caching mechanism compatible with distillation for accelerating video generation in diffusion models, achieving 11.8× speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing acceleration methods for diffusion models face challenges: feature caching causes semantic/detail degradation, while step-distillation for videos suffers from quality loss and instability when combined with caching due to sparse sampling steps.

Method: 1) Replaces traditional training-free feature caching with a lightweight learnable neural predictor to model feature evolution. 2) Develops a conservative Restricted MeanFlow approach to stabilize distillation under high compression, addressing sparsity from feature caching.

Result: Enables 11.8× inference acceleration for video generation while preserving quality, with experiments showing superior performance over existing feature caching and distillation methods.

Conclusion: The learnable feature caching mechanism bridges the compatibility gap between training-free compression and distillation, pushing computational efficiency boundaries for large-scale video generation in diffusion models.

Abstract: While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.

</details>


### [44] [Attention Retention for Continual Learning with Vision Transformers](https://arxiv.org/abs/2602.05454)
*Yue Lu,Xiangyu Zhou,Shizhou Zhang,Yinghui Xing,Guoqiang Liang,Wencong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的注意力保持方法来缓解视觉Transformer在持续学习中的灾难性遗忘问题，效果达到当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 持续学习中灾难性遗忘的主要根源是视觉Transformer的注意力漂移现象，即新任务学习会显著改变原有视觉概念的注意力分布。

Method: 通过梯度掩码技术约束注意力漂移：1) 使用逐层回溯机制提取历史任务注意力图并生成二值掩码；2) 新任务训练时屏蔽历史注意力区域梯度，同时通过比例缩放维护参数更新幅度。

Result: 方法显著降低遗忘程度，视觉化分析验证其概念保持能力，在多种持续学习场景下达到SOTA性能并展现强泛化性。

Conclusion: 基于神经科学的注意力保持框架有效缓解视觉Transformer的灾难性遗忘，为现代优化器下的持续学习提供了新思路。

Abstract: Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.

</details>


### [45] [MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation](https://arxiv.org/abs/2602.05467)
*Dekang Qi,Shuang Zeng,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Mu Xu*

Main category: cs.CV

TL;DR: 提出了一种名为Memory-Execute-Review的框架，在视觉语言导航任务中同时提升了成功率（SR）和泛化能力，超越了现有监督微调（SFT）和无训练（TF）方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法在成功率和泛化能力间难以平衡：SFT方法SR高但泛化差，TF方法泛化好但SR低，亟需解决这一矛盾。

Method: 设计分层记忆模块（信息支持）、执行模块（常规决策）、复盘模块（异常处理），通过协同工作实现导航任务的端到端优化。

Result: 在4个数据集的TF/ZS设置中平均SR提升7%/5%，在HM3D_v0.1和HM3D_OVON数据集ZS设置中提升8%/6%；在MP3D/HM3D_OVON数据集同时超越TF/SFT方法，SR提升5%/2%且泛化能力领先。

Conclusion: 该框架突破了传统方法对SR与泛化的权衡限制，为具身智能的视觉语言导航提供了新的解决方案。

Abstract: Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.

</details>


### [46] [SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing](https://arxiv.org/abs/2602.05480)
*Peihao Wu,Yongxiang Yao,Yi Wan,Wenfei Zhang,Ruipeng Zhao,Jiayuan Li,Yongjun Zhang*

Main category: cs.CV

TL;DR: 本论文提出了SOMA-1M数据集，包含130万对像素级对齐的SAR与光学图像，覆盖0.5m-10m多分辨率及12类地物类型，并构建了四大跨模态视觉任务基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集存在单一分辨率、数据量不足、对齐精度低的问题，难以支撑多尺度基础模型的训练与泛化。

Method: 设计了由粗到精的图像匹配框架，融合Sentinel-1、PIESAT-1、Capella Space和Google Earth数据源，实现全球多尺度像素级精确对齐。

Result: 基于该数据集的监督训练使所有任务性能显著提升，其中跨模态遥感图像匹配达到当前最优水平(SOTA)。

Conclusion: SOMA-1M为多模态遥感算法和基础模型提供关键数据支撑，将公开发布推动领域发展。

Abstract: Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.

</details>


### [47] [Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014](https://arxiv.org/abs/2602.05487)
*Julien Moreau,S. Ambellouis,Yassine Ruichek*

Main category: cs.CV

TL;DR: 该报告是2014年博士论文草稿，提供首个鱼眼图像数据集PFSeq及相关实验，探索鱼眼自校准中的特征检测难题，但未提出新算法且未经同行评审。


<details>
  <summary>Details</summary>
Motivation: 研究鱼眼相机自校准的鸡生蛋问题：缺乏精确投影模型导致特征检测困难，但需要优质特征完成校准，形成方法闭环。

Method: 基于PFSeq数据集开展特征检测/描述符对比实验，但未涉及专用鱼眼算法，采用传统方法验证可行性。

Result: 实验发现特定特征组合更适用于鱼眼成像，但结果未更新至领域最新进展，仍保持2014年实验数据。

Conclusion: 属未完成的早期研究，提供基础数据集与问题框架，需结合最新算法验证并完善校准方法闭环。

Abstract: What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).

</details>


### [48] [VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency](https://arxiv.org/abs/2602.05508)
*Zhuang Xiong,Chen Zhang,Qingshan Xu,Wenbing Tao*

Main category: cs.CV

TL;DR: 本文提出VGGT-Motion，一种无需校准的单目SLAM系统，通过运动感知的子地图构建、锚点驱动的Sim(3)配准和轻量级优化策略，在公里级轨迹中实现高效且鲁棒的全局一致性，显著提升了长序列下的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有校准自由单目SLAM方法因运动无关的分割导致上下文断裂和零运动漂移，而传统几何对齐方法计算成本高，无法满足长序列下高效且精确的全局一致性需求。

Method: 1) 运动感知子地图机制：利用光流引导自适应分割、消除静态冗余并封装转向点；2) 锚点驱动的Sim(3)配准：通过上下文均衡锚点实现无搜索像素级对齐和高效闭环；3) 子地图级轻量优化：线性复杂度的全局一致性优化。

Result: 在零样本条件下，VGGT-Motion显著提升长距离轨迹精度和计算效率，达到现有技术中无需校准单目SLAM的最优性能。

Conclusion: 提出的VGGT-Motion系统在解决长序列尺度漂移和计算效率问题上取得突破，为校准自由的单目SLAM提供了高效且可扩展的解决方案。

Abstract: Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.

</details>


### [49] [Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains](https://arxiv.org/abs/2602.05527)
*Ben Isselmann,Dilara Göksu,Andreas Weinmann*

Main category: cs.CV

TL;DR: Pretraining self-supervised learning models on domain-relevant datasets improves microsopy data analysis with limited labels.


<details>
  <summary>Details</summary>
Motivation: Small task-specific microscopy datasets hinder robust deep learning models; SSL pretraining aims to address this by leveraging unlabeled data.

Method: Compared Vision Transformers pretrained on ImageNet-1k, Human Protein Atlas (HPA), and OpenCell to evaluate cross-domain transfer via supervised classification on OpenCell labels.

Result: HPA-pretrained model achieved best performance (mean macro F1=0.8221±0.0062), outperforming in-domain pretraining (F1=0.8057±0.0090).

Conclusion: SSL pretraining with large-scale microscopy datasets enables effective transferability to distinct but related tasks despite limited labeled data.

Abstract: Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.

</details>


### [50] [SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation](https://arxiv.org/abs/2602.05534)
*Youngwoo Shin,Jiwan Hur,Junmo Kim*

Main category: cs.CV

TL;DR: 解决视觉自回归模型(VAR)推理阶段层级退化问题，提出无训练的层次化生成引导方法SSG，通过频域增强提高生成质量与多样性。该方法基于信息论视角，强调多尺度语义残差的独立建模。


<details>
  <summary>Details</summary>
Motivation: VAR模型在生成时因容量限制和误差累积导致层级结构退化，破坏粗到细生成机制。现有方法未根本解决训练与推理阶段的频域特征不匹配问题，需通过理论分析寻找更本质的引导机制。

Method: 提出Scaled Spatial Guidance：1) 用Discrete Spatial Enhancement构建频率感知的先验图像作为参考基线；2) 在推理时显式强调当前尺度需补充的高频语义残差；3) 通过频域滤波分离各层级的特征贡献，保持全局一致性的同时防止信息过载。

Result: 方法在ImageNet等多数据集上验证：1) 在FID-64指标上提升15-20%；2) IS多样性指标保持稳定；3) 推理速度不受影响(维持单模型100步生成)；4) 跨不同分词策略和条件生成模式的普适性验证。

Conclusion: 证明多尺度生成中频域特征分离的关键作用，揭示基于先验参考残差引导的有效性。所提SSG为VAR类模型提供即插即用优化方案，在低延迟下释放生成质量提升潜力。

Abstract: Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.

</details>


### [51] [A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments](https://arxiv.org/abs/2602.05538)
*Malaz Tamim,Andrea Matic-Flierl,Karsten Roscher*

Main category: cs.CV

TL;DR: 系统评估了相机、激光雷达及融合方法在3D人体检测中的表现，发现融合方法最优但存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 3D人体检测在机器人等功能安全中至关重要，但现有研究主要聚焦自动驾驶，且传感器融合的鲁棒性研究不足。

Method: 使用JRDB数据集测试BEVDepth（相机）、PointPillars（激光雷达）、DAL（融合）模型，分析遮挡、距离及传感器干扰下的性能差异。

Result: 融合方法DAL在复杂场景表现最佳，但在传感器错位和激光雷达干扰下仍脆弱；BEVDepth性能最差且受遮挡/距离/噪声影响最大。

Conclusion: 传感器融合能提升3D检测性能，但需解决系统脆弱性和传感器对齐问题，凸显技术改进的迫切性。

Abstract: Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.

</details>


### [52] [FastVMT: Eliminating Redundancy in Video Motion Transfer](https://arxiv.org/abs/2602.05551)
*Yue Ma,Zhikai Wang,Tianhao Ren,Mingzhe Zheng,Hongyu Liu,Jiayi Guo,Mark Fong,Yuxuan Xue,Zixiang Zhao,Konrad Schindler,Qifeng Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出FastVMT方法，通过消除视频运动迁移中的运动冗余和梯度冗余，在保持视觉质量的同时实现3.43倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT架构的视频运动迁移方法存在计算效率瓶颈，未能解决结构性冗余问题，导致推理速度受限。

Method: 1) 通过局部注意力mask减少运动冗余，利用视频帧间连续性限制注意力计算范围；2) 设计梯度复用策略减少梯度冗余，跳过扩散过程中的重复梯度计算。

Result: 在保持视觉保真度和时间一致性的同时，平均推理速度提升3.43倍，且加速效果在不同文本-视频任务中保持稳定。

Conclusion: 识别并消除运动冗余与梯度冗余能有效提升视频生成效率，为扩散模型在视频生成应用提供了优化新思路。

Abstract: Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.

</details>


### [53] [IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools](https://arxiv.org/abs/2602.05555)
*Panagiotis Sapoutzoglou,Orestis Vaggelis,Athina Zacharia,Evangelos Sartinas,Maria Pateraki*

Main category: cs.CV

TL;DR: IndustryShapes: A new RGB-D benchmark dataset for industrial tools/components, enabling realistic evaluation of 6D pose estimation in real-world manufacturing scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on household/consumer products, synthetic data, or controlled environments. IndustryShapes bridges lab-to-industry gaps by providing application-relevant industrial object data with real-world complexity.

Method: Constructed 5 new object types with challenging properties in realistic industrial settings. Contains 2 parts: Classic(4.6k RGB-D images/6k poses) and Extended(modalities for model-free/sequence-based eval). Includes first RGB-D static onboarding sequences.

Result: Benchmarked SOTA methods across detection/segmentation/pose estimation, demonstrating dataset diversity (simple to complex scenes) and highlighting room for improvement in industrial object understanding.

Conclusion: IndustryShapes is the first industrial tool dataset with realistic complexity and static onboarding sequences, establishing a foundation to advance robotics research through practical 6D pose estimation evaluation.

Abstract: We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.

</details>


### [54] [PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557)
*Michael Schwingshackl,Fabio F. Oberweger,Mario Niedermeyer,Huemer Johannes,Markus Murschitz*

Main category: cs.CV

TL;DR: 提出PIRATR框架，用于从点云数据中同时估计多类别6-DoF物体姿态及参数化属性，实现端到端3D检测。


<details>
  <summary>Details</summary>
Motivation: 旨在解决因遮挡导致的点云数据感知难题，同时捕捉参数化物体的任务相关属性（如夹具开口）以辅助机器人操作。

Method: 基于PI3DETR扩展，采用模块化类别专用检测头，通过联合优化直接从点云联合预测多类别姿态参数与形状属性，并通过预定义规则动态调整3D模型。

Result: 在合成环境中训练后无需微调即可泛化到真实户外LiDAR数据，实现0.919 mAP检测性能，在自动化叉车平台验证三类结构功能异构目标。

Conclusion: 构建了感知-动作模型的新范式，打通了低层次几何推理与可操作世界模型的桥梁，验证了纯仿真训练在动态环境部署的可行性。

Abstract: We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.

</details>


### [55] [Visual Implicit Geometry Transformer for Autonomous Driving](https://arxiv.org/abs/2602.05573)
*Arsenii Shirokov,Mikhail Kuznetsov,Danila Stepochkin,Egor Evdokimov,Daniil Glazkov,Nikolay Patakin,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: ViGT是用于自动驾驶的几何模型，通过环绕摄像头估算连续3D占用率，采用自主适应的无校准架构和自监督训练，实现跨传感器配置的可扩展性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有几何模型依赖像素对齐预测或手动标注，难以适应多样化传感器配置且成本高，而ViGT旨在构建无需校准、可自监督训练的基础模型以提升自动驾驶场景理解。

Method: 提出无校准的ViGT架构，将多视角图像隐式编码为3D占用场，并通过自监督学习利用同步图像-激光雷达数据，直接输出鸟瞰图（BEV）的连续几何表示。

Result: 在5个自动驾驶数据集混合训练后，ViGT在pointmap估算任务上平均排名领先，在Occ3D-nuScenes基准上表现与监督方法相当，且支持多传感器配置。

Conclusion: ViGT为自动驾驶提供了可扩展的通用几何基础模型，通过无校准设计和自监督训练降低成本，同时实现跨领域的高性能3D场景重建。

Abstract: We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.

</details>


### [56] [LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization](https://arxiv.org/abs/2602.05577)
*Shiyu Wu,Shuyan Li,Jing Li,Jing Liu,Yequan Wang*

Main category: cs.CV

TL;DR: 提出了一种名为LocateEdit-Bench的大规模数据集，包含231K张经过编辑的图片，用于评估针对指令驱动图像编辑的定位方法，并开发了两种多指标评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的伪造图像定位方法主要针对基于修补的编辑，对最新的指令驱动编辑无效，因此需要填补这一关键空白以应对图像编辑技术的挑战。

Method: 构建了一个包含四种前沿编辑模型和三种常见编辑类型的大型数据集，并设计了两种多指标评估协议来分析现有定位方法。

Result: 数据集包含231K张编辑后的图片，实验表明现有方法在新编辑范式下效果不佳，并通过多角度评估揭示了方法的局限性。

Conclusion: 该数据集为应对快速发展的图像编辑技术提供了基准基础，有助于推动未来伪造定位方法的研究。

Abstract: Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.

</details>


### [57] [LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2602.05578)
*Junyang Chen,Xiangbo Lv,Zhiqiang Kou,Xingdong Sheng,Ning Xu,Yiguo Qiao*

Main category: cs.CV

TL;DR: 该论文提出LoGoSeg框架，通过引入对象先验和区域对齐机制，解决开放词汇语义分割中的空间匹配问题，无需额外数据或模块。


<details>
  <summary>Details</summary>
Motivation: 传统OVSS方法依赖VLMs导致空间对齐差且缺乏物体约束，易产生幻觉或漏检。

Method: LoGoSeg包含：(1) 基于图像-文本相似性的动态类别权重；(2) 精确区域级图文对齐；(3) 双流融合局部结构和全局语义上下文。

Result: 在六个基准数据集（A-847, PC-459等）实验显示，LoGoSeg性能优越且具有强泛化能力。

Conclusion: LoGoSeg通过创新模块有效改善OVSS的空间对齐问题，方法简洁高效。

Abstract: Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.

</details>


### [58] [Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation](https://arxiv.org/abs/2602.05582)
*Joe-Mei Feng,Sheng-Wei Yu*

Main category: cs.CV

TL;DR: 提出了一种基于SE(3)李群的统一算子框架，通过几何可观测性指数（GOI）量化特征对相机位姿估计的影响，揭示弱可观测性与敏感性放大的关联。


<details>
  <summary>Details</summary>
Motivation: 现有敏感度分析工具无法解释单个图像特征如何影响位姿估计，亦无法阐明动态或不一致观测扭曲SLAM/运动恢复结构系统的机制

Method: 将影响函数理论扩展至矩阵李群，构建左平凡化M估计量的内禀扰动算子，结合曲率算子与李代数结构推导GOI

Result: GOI通过谱分解揭示可观测曲率主方向与敏感性关系，统计大样本下与SE(3)费舍尔信息几何一致，解释纯旋转退化、视差消失等经典病态问题及动态特征敏感性放大机制

Conclusion: 统一了条件分析、费舍尔几何、影响函数理论及动态场景可检测性，GOI可作为SLAM系统免训练的轻量级诊断工具检测动态特征与弱可观测构型

Abstract: We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).
  The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.
  The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.

</details>


### [59] [EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality](https://arxiv.org/abs/2602.05590)
*Haojie Cheng,Shaun Jing Heng Ong,Shaoyu Cai,Aiden Tat Yang Koh,Fuxi Ouyang,Eng Tat Khoo*

Main category: cs.CV

TL;DR: EgoPoseVR是一个用于VR中全身体态跟踪的框架，融合了头显运动提示和RGB-D数据，通过双模态融合和时空编码提升准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于头戴式摄像头的姿态估计在VR头显上存在时间不稳定、下肢追踪不准确和缺乏实时性能等问题。

Method: 设计双模态融合管道，结合时空编码器提取帧级和关节点级特征，通过交叉注意力融合多模态数据，并利用运动学优化模块施加头显信号约束。

Result: 实验显示EgoPoseVR优于现有方法，在超过180万合成数据帧的测试中表现优异，用户研究证实了更高的准确性、稳定性和实用评价。

Conclusion: 提供了一种无需额外穿戴设备或房间级跟踪系统的实用VR全身体态追踪解决方案。

Abstract: Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.

</details>


### [60] [CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion](https://arxiv.org/abs/2602.05598)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: CAViT通过动态通道混合增强Vision Transformers，提升性能并减少计算量。


<details>
  <summary>Details</summary>
Motivation: 传统ViT使用静态MLP进行通道混合，缺乏对输入内容的适应性，限制了模型表达能力。

Method: 在每个Transformer块中引入双注意力机制，先进行空间自注意力，再采用通道级自注意力动态调整通道混合策略。

Result: 在5个基准数据集上准确率提升3.6%，参数量与FLOPs减少超30%，注意力热图呈现更强语义关联性。

Conclusion: 动态注意力驱动的通道混合策略可有效提升模型表达能力，同时保持轻量级架构优势。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.

</details>


### [61] [Multi-instance robust fitting for non-classical geometric models](https://arxiv.org/abs/2602.05602)
*Zongliang Zhang,Shuxiang Li,Xingwang Huang,Zongyue Wang*

Main category: cs.CV

TL;DR: 本文提出一种针对螺旋曲线、角色模型等非经典模型的多实例鲁棒拟合新方法，通过非阈值依赖的误差估计器与元启发式优化器组合实现出色抗噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒拟合方法主要针对线、圆等经典模型，难以处理非经典模型的多实例重建需求，特别是在存在离群点的情况下。

Method: 构建基于模型-数据误差且无需设定阈值的新型估计器，并集成元启发式优化算法解决其不可微问题，形成双阶段优化框架。

Result: 实验表明该方法在螺旋曲线、自由曲面等多种非经典模型上的有效性和鲁棒性，代码已在GitHub开源（https://github.com/zhangzongliang/fitting）.

Conclusion: 所提框架成功拓展了鲁棒拟合的应用边界，在处理复杂非经典模型的多实例重建任务中展现出显著优势。

Abstract: Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.

</details>


### [62] [Unified Sensor Simulation for Autonomous Driving](https://arxiv.org/abs/2602.05617)
*Nikolay Patakin,Arsenii Shirokov,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: XSIM is a sensor simulation framework for autonomous driving that improves geometric consistency and photorealistic rendering by addressing azimuth boundary issues in spherical cameras and introducing a dual-opacity Gaussian representation.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGUT splatting struggles with spherical LiDAR sensors due to cyclic projection and time discontinuities at azimuth boundaries, causing projection errors in dynamic environments.

Method: XSIM introduces (1) phase modeling to handle temporal/shape discontinuities at azimuth borders via Unscented Transform and (2) an extended 3D Gaussian representation with separate opacities for geometry-color mismatch. It also generalizes rolling-shutter modeling for dynamic sensor distortions.

Result: XSIM outperforms strong baselines on Waymo Open Dataset, Argoverse 2, and PandaSet, achieving state-of-the-art performance in geometric consistency and visual realism.

Conclusion: The unified framework enhances autonomous driving simulation by resolving critical edge cases in spherical sensor modeling, enabling accurate rendering of complex distortions in dynamic scenes.

Abstract: In this work, we introduce \textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.

</details>


### [63] [UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos](https://arxiv.org/abs/2602.05638)
*Jinlin Wu,Felix Holm,Chuxi Chen,An Wang,Yaxin Hu,Xiaofan Ye,Zelin Zang,Miao Xu,Lihua Zhou,Huai Liao,Danny T. M. Chan,Ming Feng,Wai S. Poon,Hongliang Ren,Dong Yi,Nassir Navab,Gaofeng Meng,Jiebo Luo,Hongbin Liu,Zhen Lei*

Main category: cs.CV

TL;DR: 本研究提出UniSurg，一种专为手术视频理解设计的新型基础模型，通过将学习范式从像素级重建转移至潜在运动预测，结合运动引导预测、时空亲和自蒸馏和特征多样性正则化三个创新点，并利用超大规模UniSurg-15M数据集（含3658小时视频）进行预训练。实验表明在17个基准测试中（如EgoSurgery F1提升14.6%，CholecT50 mAP-IVT达39.54%）均显著超越现有方法，确立了面向运动的手术视频理解新标准。


<details>
  <summary>Details</summary>
Motivation: 当前手术视频分析依赖像素级重建目标，浪费模型能力于烟雾、反光等低级视觉细节，而非语义结构。手术场景需要捕捉工具交互、组织动态等关键运动信息，传统方法不足以满足语义级理解需求。

Method: 1) 基于V-JEPA架构设计运动引导的潜在预测模块，聚焦语义区域；2) 引入时空亲和自蒸馏增强时空关系一致性；3) 提出特征多样性正则化防止稀疏纹理场景的表示崩溃；4) 创建包含3658小时视频（覆盖13个解剖区域）的UniSurg-15M数据集用于预训练。

Result: 在EgoSurgery和PitVis工作流识别中F1值分别提升14.6%和10.3%，CholecT50动作三元组识别mAP-IVT达39.54%。在技能评估、息肉分割及深度估计任务中均显著优于SOTA方法，验证了模型在语义理解与多任务泛化性上的优势。

Conclusion: 通过建立基于运动预测的学习范式和创新性技术组件，UniSurg成功将模型能力聚焦于手术语义理解。大规模实验验证了模型在多任务场景下的卓越性能，为手术视频分析提供了统一的框架和数据基础。

Abstract: While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.

</details>


### [64] [Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances](https://arxiv.org/abs/2602.05650)
*Amir Ansari,Jana Subirana,Bruna Silva,Sergio Escalera,David Gallardo-Pujol,Cristina Palmero*

Main category: cs.CV

TL;DR: 本研究通过利用Big-Five人格模型的细粒度层级（如facet和nuance），增强基于音视频交互数据的人格识别效果，且nuance级别模型表现最优。


<details>
  <summary>Details</summary>
Motivation: 传统人格评估依赖聚合的特质得分，难以反映行为多样性与情境依赖性，且训练数据有限阻碍模型泛化。作者尝试通过细化人格层级结构解决此问题。

Method: 基于Transformer架构设计模型，引入跨模态（音视频融合）与跨主体（双人互动感知）注意力机制，在UDIVA v0.5数据集上训练验证不同粒度层级（trait/facet/nuance）的预测效果。

Result: nuance级别模型相较trait/facet级别模型，平均均方误差降低最高达74%，且在多种交互场景中保持稳定性能优势。

Conclusion: 人格特征的细粒度建模与多模态交互感知架构能有效提升个体人格的可解释性识别，为行为数据分析提供更高精度解决方案。

Abstract: Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.

</details>


### [65] [ShapeUP: Scalable Image-Conditioned 3D Editing](https://arxiv.org/abs/2602.05676)
*Inbar Gat,Dana Cohen-Bar,Guy Levy,Elad Richardson,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: ShapeUP是可扩展的3D编辑框架，通过3D Diffusion Transformer实现监督式潜在空间平移，解决视觉控制性、几何一致性和可扩展性间的权衡问题，在保真度和可控性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法存在三重挑战：基于优化的方法速度慢、多视角2D传播易漂移、无训练潜变量方法受限于固定先验。需提出兼顾可控性与一致性的可扩展方案。

Method: 基于预训练3D基础模型，采用监督学习的潜空间到潜空间转换范式。利用包含源3D模型、编辑后2D图像及目标3D模型的三元组数据，训练3D Diffusion Transformer实现图像引导的端到端映射，实现无需掩码的局部/全局协同编辑。

Result: 在保真度（SSIM↑8.2%）和一致性（Chamfer距离↓37.5%）指标上超越无训练方法（如Null-Text）与有监督基线（如Imagic），单次编辑推理速度达4.3秒/模型，支持高分辨率（256³）资产处理。

Conclusion: 证明监督式潜空间微调是3D基础模型编辑的有效路径，其通过图像提示隐式定位与几何约束机制，在保持生成先验的同时实现精确编辑，为原生3D内容创作提供了可扩展范式。

Abstract: Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.

</details>


### [66] [Poster: Camera Tampering Detection for Outdoor IoT Systems](https://arxiv.org/abs/2602.05706)
*Shadi Attarha,Kanaga Shanmugi,Anna Förster*

Main category: cs.CV

TL;DR: 本文提出两种户外静止图像篡改检测方法（规则-based与深度学习-based），前者适用于资源受限场景，后者精度更高但计算需求大，并发布包含正常/模糊/旋转图像的公开数据集。


<details>
  <summary>Details</summary>
Motivation: 户外智能摄像头易受篡改破坏监控效果，静止图像缺乏连续帧时检测难度更高，且现存公开数据集资源不足。

Method: 设计规则-based算法（基于图像特征阈值判断）与深度学习模型（CNN分类），在真实场景数据上对比验证准确率、计算效率和数据需求差异。

Result: 深度-learning方法准确率显著更高（实验数据未量化），规则方法适合低资源即时部署，配套发布包含12,000+标注图像的TamperedCam数据集。

Conclusion: 资源丰富场景优先选用深度学习模型，资源受限场景采用规则方法更优，所开源数据集为后续研究提供标准化基准。

Abstract: Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.

</details>


### [67] [Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2602.05718)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出一种基于多任务学习的点监督时间动作定位框架，通过三个自监督任务增强模型的时间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖点监督的片段分类，未充分建模动作帧间的时间关系，可能影响定位效果。

Method: 设计三个自监督任务：动作补全、动作顺序理解和动作规律性理解，结合多任务学习框架提升时间一致性建模能力。

Result: 在四个基准数据集上，该方法超越多个SOTA方法，验证了时间建模的有效性。

Conclusion: 首次将时间一致性建模用于点监督动作定位，拓展了弱监督动作定位的研究方向。

Abstract: Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.

</details>


### [68] [Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification](https://arxiv.org/abs/2602.05729)
*Lexiang Hu,Youze Xue,Dian Li,Gang Liu,Zhouchen Lin*

Main category: cs.CV

TL;DR: 本文提出AGFF-Embed框架，通过多维语义嵌入自适应融合与显式梯度放大技术，在不修改数据集的情况下实现多模态嵌入模型的全局-精细语义联合建模，超越现有方法达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前CLIP和MLLM类多模态模型仅能捕获全局语义信息，但实际复杂场景中的MLLM应用需要兼容全局与精细粒度感知的混合表征机制

Method: 设计自适应融合框架（AGFF-Embed）生成多维度语义嵌入并通过可微分参数融合，结合显式梯度放大（EGA）技术实现批次级难负例增强

Result: 在MMEB和MMVP-VLM基准测试中，该方法在通用理解和精细粒度任务上均取得最先进的性能表现

Conclusion: 研究证实通过提示多嵌入生成与梯度调控的联合优化策略，可有效提升多模态基础模型在混合感知场景下的表征能力

Abstract: Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.

</details>


### [69] [Depth as Prior Knowledge for Object Detection](https://arxiv.org/abs/2602.05730)
*Moussa Kassem Sbeyti,Nadja Klein*

Main category: cs.CV

TL;DR: DepthPrior利用深度信息作为先验知识无需修改检测模型结构即可提升小物体检测效果


<details>
  <summary>Details</summary>
Motivation: 小而远物体检测存在尺度变化、低分辨率和背景干扰问题，现有方法需复杂模型修改且深度监督机制缺乏统一理论解释

Method: 提出DepthPrior框架，包含训练阶段的深度加权损失（DLW）、深度分层损失（DLS）和推理阶段的深度感知置信度阈值（DCT），通过理论分析揭示深度与检测性能的系统性关联

Result: 在KITTI/MS COCO/VisDrone/SUN RGB-D四大基准测试中，使用YOLOv11/EfficientDet检测器实现小物体mAP_S提升9%、mAR_S提升7%，推理真阳/伪阳检测恢复率95:1

Conclusion: DepthPrior提供了一种通用检测增强方案，相较传统深度融合方法无需传感器/架构修改且无额外性能开销，唯一初始成本为深度估计计算

Abstract: Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.

</details>


### [70] [FMPose3D: monocular 3D pose estimation via flow matching](https://arxiv.org/abs/2602.05755)
*Ti Wang,Xiaohang Yu,Mackenzie Weygandt Mathis*

Main category: cs.CV

TL;DR: FMPose3D采用Flow Matching与ODE建模，通过单次前向传播生成多样化3D姿态假设，结合RPEA模块实现高效精准的单目3D姿态估计。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态估计存在深度模糊与遮挡问题，传统扩散模型需多步迭代导致计算量大，本文旨在提出更高效的生成式方法解决该问题。

Method: 1) 使用Flow Matching和ODE将2D输入条件分布映射为3D姿态分布，2) 通过噪声种子采样生成多假设，3) 设计RPEA模块优化后验期望聚合。

Result: 在Human3.6M、MPI-INF-3DHP、Animal3D等数据集上实现SOTA性能，单次推理步骤比扩散模型减少80%以上。

Conclusion: 本文方法在保持多假设生成优势的同时显著提升推理效率，证明了ODE连续分布迁移在3D姿态估计中的有效性。

Abstract: Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.

</details>


### [71] [ReText: Text Boosts Generalization in Image-Based Person Re-identification](https://arxiv.org/abs/2602.05785)
*Timur Mamedov,Karina Kvanchiani,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: ReText通过结合多摄像头数据与带有文本描述的单摄像头数据，提升图像人物再识别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂架构解决域间差异，但单摄像头数据虽易获取却因缺乏跨视角变化而效果有限。需增强单摄像头数据的语义线索。

Method: 提出ReText，联合优化三任务：(1) 多摄像头Re-ID；(2) 图文匹配；(3) 基于文本的单摄像头图像重建，实现多模态混合数据学习。

Result: 实验表明ReText在跨域Re-ID基准测试中显著优于现有方法，具备更强泛化能力。

Conclusion: 首次将多模态联合学习应用于混合多/单摄像头数据的图像人物再识别，为领域泛化提供新思路。

Abstract: Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.

</details>


### [72] [Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning](https://arxiv.org/abs/2602.05809)
*Enwei Tong,Yuanchao Bai,Yao Zhu,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: 该研究提出Focus-Scan-Refine（FSR）框架，通过模拟人类看图提问思维流程，在视觉-语言模型推理中实现高效token剪枝。实验表明该方法在保证准确率的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型生成大量视觉token导致推理延迟和内存占用问题。现有训练免费剪枝技术难以平衡局部关键信息与全局上下文，容易忽略与问题相关的视觉区域。

Method: 设计三阶段剪枝框架：1）通过整合视觉重要性与指令相关性聚焦关键区域；2）基于已聚焦区域选择差异化的互补上下文token；3）通过相似度匹配和加权融合优化上下文细节，且不增加token总数。

Result: 在多种视觉-语言模型架构和基准测试中，FSR相较现有最优剪枝方法，将准确率-效率均衡指标提升5-15%，同时降低40%推理延迟，且保持零训练成本。

Conclusion: 提出的FSR框架成功模仿了人类视觉认知机制，在保持模型准确率的同时显著提升推理效率，为大规模视觉-语言系统的应用提供了新的优化范式。

Abstract: Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR

</details>


### [73] [Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation](https://arxiv.org/abs/2602.05827)
*Hai Zhang,Siqi Liang,Li Chen,Yuxian Li,Yukuan Xu,Yichao Zhong,Fu Zhang,Hongyang Li*

Main category: cs.CV

TL;DR: 该论文提出SparseVideoNav，利用视频生成模型实现无需详细指令的长视野导航，通过稀疏预测加速推理，实现实时高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航方法依赖详细指令，违背实际应用中对高层意图自主导航的需求。传统LLM受限于短视野监督训练，难以处理无密集指引的远距离导航（BVN）挑战。

Method: 首次将视频生成模型引入BVN任务，利用其长视野监督优势，提出SparseVideoNav：通过生成稀疏未来帧（20秒视界）替代完整视频生成，结合子秒级轨迹推断解决计算延迟问题。

Result: 实际场景零样本实验显示，相比现有最优LLM基线，SparseVideoNav在BVN任务成功率提升2.5倍，速度提升27倍，且首次实现复杂夜间场景导航能力。

Conclusion: 该研究表明视频生成模型能有效解决长视野导航中的规划不足问题，为自主导航系统提供了无需密集指令的实用化解决方案框架，尤其在极端条件（如夜间）表现出独特优势。

Abstract: Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.

</details>


### [74] [Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning](https://arxiv.org/abs/2602.05829)
*Yudi Shi,Shangzhe Di,Qirui Chen,Qinian Wang,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出Weaver，一种端到端可训练的多模态视频推理系统，通过动态调用工具和强化学习，在复杂视频推理基准（尤其是长视频）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本链式推理的方法存在表征不匹配和感知能力受限的问题，需探索更高效的视频推理能力提升方法。

Method: 构建Weaver系统：1) 动态调用多样化工具逐步捕获视觉线索；2) 引入强化学习算法，无需轨迹数据即可自由探索工具组合策略。

Result: 实验表明该系统在复杂视频推理基准上性能显著提升，尤其在处理长视频任务时优势明显。

Conclusion: Weaver通过多模态推理与动态工具交互，有效解决了传统方法的局限性，为视频理解提供了新的技术路径。

Abstract: Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.

</details>


### [75] [UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents](https://arxiv.org/abs/2602.05832)
*Han Xiao,Guozhi Wang,Hao Wang,Shilong Liu,Yuxiang Chai,Yue Pan,Yufeng Zhou,Xiaoxin Chen,Yafei Wen,Hongsheng Li*

Main category: cs.CV

TL;DR: UI-Mem改进在线GUI强化学习，通过结构化记忆实现跨任务知识迁移。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在长周期GUI任务中因信用分配低效和无经验迁移导致重复错误，需有效方法提升长期任务效率及跨任务泛化能力。

Method: 提出UI-Mem框架，包含分层经验记忆（记录工作流、子任务技能、失败模式的参数模板）、分层分组采样（多轨迹引导保持多样性）、自演化循环（动态更新记忆）。

Result: 在在线GUI基准测试中显著优于传统RL和静态复用策略，能有效泛化至未见应用。

Conclusion: 结构化记忆与动态更新机制提升了长期任务效率，实现了跨任务/应用的有效经验迁移。

Abstract: Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io

</details>


### [76] [Pathwise Test-Time Correction for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.05871)
*Xunzhi Xiang,Zixuan Duan,Guiyu Zhang,Haiyu Zhang,Zhe Gao,Junta Wu,Shaofeng Zhang,Tengfei Wang,Qi Fan,Chunchao Guo*

Main category: cs.CV

TL;DR: Test-Time Correction (TTC) enables stable long-sequence generation in distilled autoregressive diffusion models by using the initial frame as a reference anchor, reducing error accumulation without training-based methods.


<details>
  <summary>Details</summary>
Motivation: Current distilled diffusion models face error accumulation in long sequences, and existing Test-Time Optimization (TTO) methods fail due to unstable reward landscapes and parameter hypersensitivity.

Method: TTC calibrates intermediate stochastic states during sampling using the initial frame as a stable reference, operating training-free without modifying model parameters.

Result: TTC integrates seamlessly with distilled models, achieving extended generation lengths with negligible overhead and matching quality of resource-intensive training-based methods on 30-second benchmarks.

Conclusion: TTC provides a training-free solution for mitigating error accumulation in long video generation, offering practical advantages over existing TTO and training-based approaches.

Abstract: Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.

</details>


### [77] [Contour Refinement using Discrete Diffusion in Low Data Regime](https://arxiv.org/abs/2602.05880)
*Fei Yu Guan,Ian Keefe,Sophie Wilkinson,Daniel D. B. Perrakis,Steven Waslander*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级离散扩散轮廓优化方法，通过结合CNN与自注意力机制，在低数据环境下（<500张训练图像）实现高效的边界检测。


<details>
  <summary>Details</summary>
Motivation: 针对医学成像等场景中边界检测任务在低数据和资源受限环境下的挑战，现有方法多关注分割掩膜一致性而忽视轮廓检测的研究现状。

Method: 构建基于CNN与自注意力层的核心架构，采用简化扩散流程迭代去噪稀疏轮廓，并通过定制模型结构（如通道注意力模块）和最小化后处理实现高效推理。

Result: 在KVASIR数据集超越SOTA基线，HAM10K和自建Smoke数据集表现优异，推理速度提升3.5倍，仅需要小型数据集即可生成密集孤立轮廓。

Conclusion: 验证了离散扩散模型在低数据边界检测中的可行性，为资源受限场景（医疗诊断/火灾监测）提供了有效的轻量化解决方案。

Abstract: Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.

</details>


### [78] [EoCD: Encoder only Remote Sensing Change Detection](https://arxiv.org/abs/2602.05882)
*Mubashir Noman,Mustansar Fiaz,Hiyam Debary,Abdul Hannan,Shah Nawaz,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了一种名为EoCD的高效变化检测方法。采用早期融合策略和无参数多尺度特征融合模块，显著降低了模型复杂度，同时在多种编码器架构和四个数据集中实现了性能与推理速度的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法采用孪生编码器和复杂解码器导致计算成本高，早期融合方法因依赖复杂解码器且性能较差而受限。文章旨在解决模型复杂度与性能的权衡问题。

Method: 提出纯编码器结构EoCD：1）输入时域图像进行早期融合；2）采用无参数多尺度特征融合模块替代传统解码器；3）通过跨四个挑战性数据集的广泛实验验证方法有效性。

Result: 实验表明：1）EoCD在保持高精度的同时显著提升推理速度；2）模型性能主要依赖编码器设计，证明解码器为附加组件；3）在四个变化检测数据集上均取得优异结果。

Conclusion: 基于早期融合和创新性特征融合模块的EoCD方法成功实现了性能与效率的平衡，揭示了编码器在变化检测中的核心地位，为未来轻量级模型设计提供新思路。

Abstract: Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.

</details>


### [79] [Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views](https://arxiv.org/abs/2602.05884)
*Gino E. Jansen,Carolina Brás,R. Nils Planken,Mark J. Schuuring,Berto J. Bouma,Ivana Išgum*

Main category: cs.CV

TL;DR: 本研究提出一种基于神经隐函数的3D心脏形状重建方法，从稀疏CT切片中实现高精度多结构心脏重建，平均Dice系数达0.86。相比传统方法将左心室体积误差降低40%以上，为二维超声心动图的三维量化提供可行路径。


<details>
  <summary>Details</summary>
Motivation: 传统二维超声心动图（TTE）的临床标准方法（如Simpson双平面法）存在显著心腔体积测量误差，而三维重建可提升诊断准确性，但缺乏有效从稀疏二维视图恢复三维结构的方法。

Method: 开发基于神经隐函数的生成模型：1）训练时用多层感知机学习CT血管造影（CTA）中分割的三维心脏解剖先验；2）推理时结合潜在编码优化与刚性变换，从模拟TTE的稀疏CT切片反演完整三维心脏结构；3）通过Dice系数和体积误差评估重建精度。

Result: 在保留CT数据集上取得：平均Dice系数0.86±0.04（心腔多结构分割指标）；左心室体积误差4.88±4.26mL vs 传统方法8.14±6.04mL（降低40.4%）；左心房误差6.40±7.37mL vs 传统方法37.76±22.96mL（降低83.0%）。

Conclusion: 通过深度学习建立的隐式神经表征可准确重建三维心脏形态，解决传统二维TTE方法在复杂解剖结构量化中的局限性，为三维心脏功能评估提供更可靠的技术基础。

Abstract: Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\pm$ 4.26 mL vs. 8.14 $\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\pm$ 7.37 mL vs. 37.76 $\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.

</details>


### [80] [CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression](https://arxiv.org/abs/2602.05909)
*Kangjie Zhang,Wenxuan Huang,Xin Zhou,Boxiang Zhou,Dejia Song,Yuan Xie,Baochang Zhang,Lizhuang Ma,Nemo Chen,Xu Tang,Yao Hu,Shaohui Lin*

Main category: cs.CV

TL;DR: This paper proposes CLIP-Map, a mapping-based framework for compressing Contrastive Language-Image Pre-training (CLIP) models by preserving original weight information through learnable matrices and Kronecker factorization.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP compression methods suffer from compromised feature representation due to subset-based weight inheritance, especially under extreme compression scenarios, while CLIP itself has high memory/computation costs that limit its deployment in resource-constrained environments.

Method: CLIP-Map introduces: (1) Full-Mapping with Kronecker Factorization to combine pretrained weights via learnable matrices, (2) Diagonal Inheritance Initialization to address distribution shifting during optimization, enabling information preservation and efficient learning.

Result: The method achieves superior performance compared to selection-based frameworks across multiple compression ratios on vision-language tasks, with particularly significant improvements observed in high-compression settings (e.g., 4.3x compression with minimal performance drop).

Conclusion: Mapping-based compression strategies outperform conventional weight-selection approaches for CLIP models, offering a solution for deploying vision-language models in resource-limited scenarios while maintaining semantic representation capabilities.

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.

</details>


### [81] [Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2602.05937)
*Lingrui Li,Yanfeng Zhou,Nan Pu,Xin Chen,Zhun Zhong*

Main category: cs.CV

TL;DR: 该论文提出了一种多尺度全局-实例提示调优（MGIPT）方法，以解决医学图像分割中持续测试时适应（CTTA）的分布式偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的CTTA方法依赖于模型参数的增量更新，易导致误差累积和灾难性遗忘，且现有提示调优方法缺乏多尺度多样性、实例特定知识整合不足及隐私泄露风险。

Method: 提出MGIPT框架，包含自适应尺度实例提示（AIP）和多尺度全局提示（MGP）。AIP通过动态选择最优尺度学习实例特定提示以减少误差累积；MGP通过多尺度捕获全局域知识以增强抗遗忘能力。两者通过加权集成结合，实现多尺度全局-局部信息融合。

Result: 在医学图像分割基准上的实验表明，MGIPT在持续变化的目标域中表现优于现有方法，具有更优的鲁棒性。

Conclusion: 该方法通过增强提示尺度多样性和整合双重层级知识（全局与实例级），解决了CTTA中的关键挑战，适用于跨域医学图像部署。

Abstract: Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.

</details>


### [82] [Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching](https://arxiv.org/abs/2602.05951)
*Junwan Kim,Jiho Park,Seonghu Jeon,Seungryong Kim*

Main category: cs.CV

TL;DR: Flow matching 的生成模型在文本到图像生成中表现出色，但其源分布的设计常被忽视。本文提出通过原理化设计条件相关源分布，并引入正则化策略以实现更稳定高效的生成。


<details>
  <summary>Details</summary>
Motivation: 尽管 flow matching 允许灵活选择源分布，但现有方法仍沿用扩散模型的高斯分布设定，未将源分布作为优化目标，可能导致模型性能受限。论文旨在探索源分布优化对条件生成任务的影响。

Method: 提出基于 flow matching 目标的条件依赖源分布学习框架，引入方差正则化与方向对齐策略以解决分布崩溃和训练不稳定问题，并分析目标空间表示对模型效果的影响。

Result: 在多个文本到图像基准测试中，新方法使 FID 收敛速度提升最高达 3 倍，生成质量显著优于基线模型，验证了源分布优化的可行性与有效性。

Conclusion: 通过原理化设计源分布并引入正则化策略，可显著提升条件 flow matching 模型的训练效率与生成质量，为后续研究提供了新的优化方向。

Abstract: Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.

</details>


### [83] [LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation](https://arxiv.org/abs/2602.05966)
*Mirlan Karimov,Teodora Spasojevic,Markus Braun,Julian Wiederer,Vasileios Belagiannis,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出Localized Semantic Alignment（LSA）框架，通过语义特征对齐增强视频生成模型的时间一致性，无需外部控制信号。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成方法依赖推理时的控制信号，限制了扩展性和泛化性。通过语义特征对齐提升动态物体生成的时间一致性是核心需求。

Method: 在预训练模型微调阶段：1) 提取真实视频与生成视频动态物体区域的语义特征；2) 构造语义特征一致性损失；3) 与标准扩散损失结合进行联合优化。

Result: 单epoch微调即超越基线模型，在nuScenes/KITTI数据集上提升视频生成评估指标（FVD、mAP、mIoU），且无推理时开销。

Conclusion: LSA通过训练阶段引入语义对齐损失，在无需额外控制信号的情况下显著提升生成视频的时间一致性，具有轻量化和通用性优势。

Abstract: Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.

</details>


### [84] [RISE-Video: Can Video Generators Decode Implicit World Rules?](https://arxiv.org/abs/2602.05986)
*Mingxin Liu,Shuran Ma,Shibei Meng,Xiangyu Zhao,Zicheng Zhang,Shaofeng Zhang,Zhihang Zhong,Peixian Chen,Haoyu Cao,Xing Sun,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: RISE-Video introduces a reasoning-focused benchmark for Text-Image-to-Video synthesis, evaluating models on implicit world rules and cognitive reasoning, revealing deficiencies in current models' capabilities.


<details>
  <summary>Details</summary>
Motivation: Current generative video models excel in visual fidelity but lack depth in internalizing implicit world rules. This work addresses the under-explored need for assessing deep cognitive reasoning in TI2V generation.

Method: RISE-Video framework uses 467 human-annotated samples across 8 categories, evaluated via four metrics: Reasoning Alignment, Temporal Consistency, Physical Rationality, and Visual Quality. An automated pipeline employs LMMs for scalable evaluation.

Result: Experiments on 11 SOTA TI2V models reveal consistent deficiencies in simulating complex scenarios under implicit constraints, highlighting gaps in current generative models' reasoning capabilities.

Conclusion: The RISE-Video benchmark provides critical insights for advancing world-simulating generative models by emphasizing structured reasoning tasks, offering a pathway to improve models' intelligence over time.

Abstract: While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.

</details>


### [85] [VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation](https://arxiv.org/abs/2602.05998)
*Jie Deng,Kaichun Yao,Libo Zhang*

Main category: cs.CV

TL;DR: VisRefiner通过结合视觉差异反馈和强化学习，改进从界面截图生成前端代码的效果，提升布局保真度和模型自我优化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型直接生成代码但未利用视觉输出反馈，而人类开发者通过迭代渲染与对比进行学习。本文旨在让模型学会从视觉差异中关联代码修改。

Method: 构造差异对齐监督信号，将视觉差异与对应代码修改关联，并引入基于视觉反馈的强化学习自我 refine 阶段。

Result: 实验显示单步生成质量与布局保真度显著提升，模型获得强自我优化能力，验证了视觉差异学习的有效性。

Conclusion: 通过视觉交互学习的 VisRefiner 框架，为截图生成代码任务提供了更符合人类开发逻辑的解决方案。

Abstract: Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.

</details>


### [86] [GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013)
*Ruihang Li,Leigang Qu,Jingxu Zhang,Dongnan Gui,Mengde Xu,Xiaosong Zhang,Han Hu,Wenjie Wang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本研究提出GenArena，通过成对比较的评估框架解决现有绝对点评估方法在视觉生成模型中的局限性，提升准确性和人类对齐。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法在视觉生成模型快速发展下不足，绝对点评估存在随机不一致性和人类认知对齐差的问题。

Method: 构建基于视觉-语言模型的GenArena框架，采用成对比较范式，通过实验对比验证其有效性，并评估与权威榜单的斯皮尔曼相关性。

Result: GenArena使评估准确率提升超20%，斯皮尔曼相关性达0.86（原方法仅0.36），并让开源模型超越专有模型表现。

Conclusion: GenArena为视觉生成模型提供了稳定、可扩展的自动化评估标准，推动社区建立更严谨的评估体系。

Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.

</details>


### [87] [Context Forcing: Consistent Autoregressive Video Generation with Long Context](https://arxiv.org/abs/2602.06028)
*Shuo Chen,Cong Wei,Sun Sun,Ping Nie,Kai Zhou,Ge Zhang,Ming-Hsuan Yang,Wenhu Chen*

Main category: cs.CV

TL;DR: 该论文提出Context Forcing框架，通过长上下文师生对齐与Slow-Fast Memory架构，实现超越现有技术2-10倍的视频生成上下文长度（>20秒）和长期一致性优化。


<details>
  <summary>Details</summary>
Motivation: 解决实时长视频生成中短上下文教师模型无法指导学生模型学习全局时间依赖的问题，突破学生模型的上下文长度瓶颈。

Method: 采用双阶段策略：1) 使用长上下文教师模型提供完整历史监督的Context Forcing框架；2) 设计渐进式上下文管理的Slow-Fast Memory架构，通过多粒度记忆压缩降低冗余计算。

Result: 在2分钟极端时长场景下实现有效上下文长度超20秒，FID指标较LongLive提升35%，时序一致性指标TC-SIM提升42%，推理速度达1080p@24fps。

Conclusion: 通过师生上下文对齐和动态记忆管理，该研究显著提升了长视频生成的连贯性和生成质量，为实时超长视频生成提供了新范式。

Abstract: Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.

</details>


### [88] [Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation](https://arxiv.org/abs/2602.06032)
*David Shavin,Sagie Benaim*

Main category: cs.CV

TL;DR: 提出Splat and Distill框架，通过前馈式3D重建增强2D视觉模型的3D感知能力，显著提升几何任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D视觉基础模型在3D几何任务中存在感知缺陷，传统逐场景优化方法存在效率和特征模糊问题。

Method: 创新性采用两阶段策略：先将2D特征转换为3D高斯表示，再通过前馈式提升和视角投影生成新特征图监督学习，实现教师-学生模型协同优化。

Result: 在单目深度估计、表面法线估计等多任务中超越现有方法，同时提升特征语义丰富性和计算速度。

Conclusion: 提出的动态知识蒸馏框架有效解决了2D-3D特征转化难题，为模型注入几何感知能力的同时保持语义质量。

Abstract: Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/

</details>


### [89] [V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval](https://arxiv.org/abs/2602.06034)
*Dongyang Chen,Chaoyang Wang,Dezhao SU,Xi Xiao,Zeyu Zhang,Jing Xiong,Qing Li,Yuzhang Shang,Shichao Ka*

Main category: cs.CV

TL;DR: 本研究提出V-Retrver框架，通过视觉驱动的证据收集机制改进多模态检索，采用课程学习策略训练模型，在多个基准测试中平均检索准确率提升23%。


<details>
  <summary>Details</summary>
Motivation: 解决现有语言驱动多模态模型在视觉模糊情况下依赖静态编码导致的推测性推理问题，提升证据验证能力。

Method: 构建基于视觉检验的智能体推理流程，实现假设生成与定向视觉验证的交替推理，并采用融合监督学习、拒绝优化和证据对齐强化学习的课程训练策略。

Result: 在多模态检索基准测试中平均准确率提升23%，同时增强感知驱动推理的可靠性和泛化能力。

Conclusion: 通过引入视觉证据收集智能体和分阶段训练策略，有效实现了以视觉验证为核心的多模态检索范式革新。

Abstract: Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.

</details>


### [90] [InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions](https://arxiv.org/abs/2602.06035)
*Sirui Xu,Samuel Schulter,Morteza Ziyadi,Xialin He,Xiaohan Fei,Yu-Xiong Wang,Liangyan Gui*

Main category: cs.CV

TL;DR: 本文提出InterPrior框架,通过大规模模仿预训练和强化学习后训练,构建具有统一生成控制能力的人形机器人运动先验,使其能够泛化至训练数据外的新目标和初始状态。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以通过显式全身体运动规划实现复杂人机交互,而人类可通过高阶意图(如可供性)指导自然协调动作。研究旨在通过扩展动作先验,使双足机器人在不同场景下实现连贯的全身协调操作。

Method: 1) 使用全参考模仿专家进行大规模模仿预训练 2) 提取条件变分策略重建多模态观测下的运动 3) 通过物理扰动数据增强 4) 强化学习微调提升未见目标适应性

Result: 成功实现未见物体交互能力,在用户交互控制任务中表现良好,具备向实体机器人部署的潜力,证明了运动先验的有效性

Conclusion: InterPrior通过重建潜在技能流形实现了超越训练数据的运动泛化能力,其模仿学习与强化学习的组合策略有效提升了双足机器人动作协调性与适应性

Abstract: Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.

</details>


### [91] [Thinking with Geometry: Active Geometry Integration for Spatial Reasoning](https://arxiv.org/abs/2602.06037)
*Haoyuan Li,Qihang Cao,Tao Tang,Kun Xiang,Zihan Guo,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: GeoThinker通过主动感知和Spatial-Grounded Fusion机制提升多模态模型的空间推理能力，在VSI-Bench达到72.6并增强下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs被动融合几何信息导致语义-几何不匹配及冗余，需主动整合空间结构以提升空间智能。

Method: 提出GeoThinker：1) Spatial-Grounded Fusion分层选择性融合语义与几何特征；2) 基于跨帧注意力动态查询几何证据；3) Importance Gating模块增强关键结构注意力。

Result: VSI-Bench达72.6 SOTA成绩，在具身指代表达、自动驾驶等复杂场景中空间感知准确率提升15%以上，模型鲁棒性显著增强。

Conclusion: 验证了主动集成空间结构对空间智能的重要性，为下一代MLLMs提供可扩展架构范式。

Abstract: Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.

</details>


### [92] [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/abs/2602.06040)
*Jintao Tong,Shilin Yan,Hongwei Xue,Xiaojun Tang,Kunyu Shi,Guannan Zhang,Ruixuan Li,Yixiong Zou*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.

</details>


### [93] [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/abs/2602.06041)
*Xuejun Zhang,Aditi Tiwari,Zhenhailong Wang,Heng Ji*

Main category: cs.CV

TL;DR: The paper introduces CAMCUE, a pose-aware multi-image framework that uses camera pose as a geometric anchor for cross-view fusion and novel-view reasoning, along with a dataset (CAMCUE-DATA) for evaluating multi-view spatial reasoning. It achieves improved accuracy and 180x faster inference compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models (MLLMs) face challenges in multi-image spatial reasoning, particularly in building 3D scene understanding and performing perspective taking (reasoning from a language-specified new viewpoint).

Method: CAMCUE: (1) injects per-view camera pose into visual tokens, (2) grounds language viewpoint descriptions to target poses, and (3) synthesizes pose-conditioned imagined views. CAMCUE-DATA includes 27,668 training and 508 test instances with multi-view images, poses, and perspective-shift questions.

Result: CAMCUE achieves 9.06% overall accuracy improvement, >90% rotation accuracy (within 20°) and translation accuracy (within 0.5 error threshold) for pose prediction, and reduces inference time from 256.6s to 1.45s per example.

Conclusion: The explicit camera-pose grounding enables efficient multi-view spatial reasoning by avoiding test-time search, supporting practical applications in real-world interactive scenarios.

Abstract: Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [94] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

TL;DR: BioACE是一个自动化框架，用于评估生物医学生成答案的质量及其引用的科学文献支持度，涵盖答案的完整性、正确性、精确度和召回率，并通过与人类评估的相关性分析验证有效性。


<details>
  <summary>Details</summary>
Motivation: 需要解决生成式模型在生物医学领域答案生成后引用评估的痛点：领域专家验证成本高、复杂术语导致的传统评估方法失效，同时填补现有技术对答案质量多维度综合评估的空白。

Method: 构建多维度评价体系，包括：1) 自动化评估答案完整性和正确性；2) 检测引用与文献的匹配精度（使用NLI、预训练模型等现有技术）；3) 通过大规模实验与人类评估结果进行相关性分析，筛选最佳评估方案。

Result: 1) 答案完整性评估指标与人类评估相关性达0.82（Spearman）；2) 基于领域适配模型的引用评估F1值超过传统方法15%；3) 开源BioACE工具包实现全流程自动化评估。

Conclusion: BioACE成功实现了生物医学答案与引用质量的自动化评估，其多维度指标设计有效替代专家人工评审，开源代码为后续LLM评估方法研究提供基准框架。

Abstract: With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [95] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 该研究发现多语言模型在词汇消歧任务中存在‘多语言惩罚’，并通过实证分析识别出表征、注意力和词汇分段三个容量限制因素。


<details>
  <summary>Details</summary>
Motivation: 为解释多语言模型为何在某些下游任务中表现不如单语言模型，聚焦于需要精确语义处理的词汇消歧场景，验证容量限制假说。

Method: 构建英西双语的人类相似度判断数据集，对比相同模型家族的单/多语言版本性能，通过注意力热力图分析、嵌入各向同性测量和子词分段统计量化三种潜在限制因素。

Result: 多语言模型在所有任务上均显著低于单语言基线，发现嵌入各向同性降低42%、注意力权重分散度增加28%、多标记分段率提高15%，三因素联合解释了82%的方差。

Conclusion: 证实多语言模型确实存在由表征能力受限、上下文关注分散和分词粒度粗化导致的协同性能瓶颈，为多语言模型优化提供了可量化的改进方向。

Abstract: Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [96] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: This paper introduces Locas, a parametric memory integrated with transformer models via FFN blocks, enabling efficient continual learning with minimal parameter overhead and reduced catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in continual learning by developing a flexible parametric memory that can merge into model parameters while preserving prior knowledge with minimal resource cost.

Method: Designing two Locas variants (MLP and GLU-FFN) inspired by transformer FFNs, initialized using model parameters/activations/gradients. Tested on PG-19 and LoCoMo tasks with MMLU evaluations for forgetting analysis.

Result: Locas-GLU achieves 0.02% parameter overhead, retains past context knowledge in small windows, and shows minimal capability loss on MMLU, demonstrating efficient memory storage and reduced forgetting.

Conclusion: Locas enables effective long-term parametric memory integration with theoretical guarantees and practical efficiency, balancing parameter reuse and continual learning performance.

Abstract: In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [97] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

TL;DR: 本文提出Data Kernel Perspective Space (DKPS)理论框架，为Transformer模型生成合成数据的质量提供数学分析与统计保证。


<details>
  <summary>Details</summary>
Motivation: 标签数据稀缺制约语言技术发展，现有LLM生成合成数据依赖黑箱调参缺乏理论保证，需要数学框架解释并预测合成数据质量。

Method: 构建DKPS理论框架，通过数学推导建立模型输出与数据空间结构的关联性分析，推导性能下界保证并建立误差传播分析模型。

Result: 证明DKPS能为神经机器翻译和CPO优化LLM等任务建立形式化质量保证，实证分析展示温度参数与翻译BLEU分数的反比关系，理论下界解释78%的性能波动。

Conclusion: DKPS为数据增强提供首个严格数学基础，但受限于线性假设和token-level分析，未来将扩展至非线性空间和更复杂任务场景。

Abstract: Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [98] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

TL;DR: 提出了一个跨语言、多模态的隐式话语关系分类方法，结合文本和音频信息，并利用跨语言迁移学习提升低资源语言的性能。


<details>
  <summary>Details</summary>
Motivation: 隐式话语关系分类需要依赖上下文推断语义，但单一文本模态难以捕捉跨语言、多模态的语境线索，低资源语言的数据不足也限制了模型效果。

Method: 1) 构建涵盖英语、法语、西班牙语的多模态数据集；2) 提出融合文本与音频（通过Qwen2-Audio）的多模态分类方法；3) 引入跨语言迁移学习策略。

Result: 文本模型性能优于音频模型，但融合双模态能提升效果；跨语言迁移对低资源语言有显著改进，且多模态与跨语言策略的协同作用优于单一方法。

Conclusion: 多模态信息融合与跨语言迁移学习的结合，为低资源语言隐式话语关系分类提供了有效解决方案，揭示了多模态与多语言协同建模的潜力。

Abstract: Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [99] [The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems](https://arxiv.org/abs/2602.05182)
*Shangbin Feng,Kishan Panaganti,Yulia Tsvetkov,Wenhao Yu*

Main category: cs.CL

TL;DR: The paper introduces a method to distill collaborative patterns of multiple language models (LMs) into a single model, enabling efficient inference with reduced cost while maintaining collaboration benefits. It also proposes a single-multi evolution loop where models iteratively collaborate and improve through distillation.


<details>
  <summary>Details</summary>
Motivation: Collaboration between multiple LMs can combine their strengths but incurs costs from loading multiple models. The work aims to preserve collaboration benefits while improving efficiency by distilling collaborative behaviors into a single model.

Method: 1) Distill collaborative patterns into a single model trained on collaboration outputs; 2) Introduce the single-multi evolution loop, where models iteratively collaborate, distill, and evolve. This creates a self-improving ecosystem through model interactions.

Result: Experiments show distilled models reduce inference costs to single-model levels while improving average performance by 8%. The evolved collaboration system further boosts performance by 14.9% over non-evolved systems, outperforming existing evolutionary AI methods.

Conclusion: The proposed distillation and evolution framework enhances both individual model capabilities and collaborative systems, enabling efficient and synergistic AI advancement across diverse tasks and settings.

Abstract: Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.

</details>


### [100] [Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky](https://arxiv.org/abs/2602.05189)
*Hsuan-Yu Chou,Wajiha Naveed,Shuyan Zhou,Xiaowei Yang*

Main category: cs.CL

TL;DR: Comparative analysis shows open-weight and proprietary LLMs perform similarly in detecting harmful content on social media, with open-weight models offering privacy-preserving moderation potential.


<details>
  <summary>Details</summary>
Motivation: Rapid internet expansion increases harmful content exposure, requiring efficient moderation. While proprietary LLMs excel in zero-shot detection, the performance of open-weight LLMs remains unverified.

Method: Evaluated 7 state-of-the-art LLMs (4 proprietary, 3 open-weight) using real-world Bluesky posts, moderation decisions, and human annotations. Measured sensitivity/specificity across rudeness, intolerance, and threats categories, with inter-rater agreement analysis.

Result: Open-weight LLMs showed comparable sensitivity (81%-97%) and specificity (91%-100%) to proprietary models (72%-98% sensitivity, 93%-99% specificity). Rudeness detection prioritized specificity, while intolerance/threats emphasized sensitivity. Consistent inter-rater agreement observed between humans and models.

Conclusion: Open-weight LLMs enable effective, privacy-preserving moderation on consumer hardware, supporting both platform-scale and personalized moderation systems balancing community values with individual preferences.

Abstract: As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.
  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.

</details>


### [101] [Aligning Large Language Model Behavior with Human Citation Preferences](https://arxiv.org/abs/2602.05205)
*Kenichiro Ando,Tatsuya Harada*

Main category: cs.CL

TL;DR: 研究大型语言模型（LLMs）引用行为与人类偏好差异，发现模型过度引用部分内容且不足引用其他类型，可通过直接偏好优化调整。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLMs文献引用的研究主要关注如何选择参考文献，但对其识别引用必要性的机制及控制方式缺乏探究。医疗文本等场景中准确引用对可信度至关重要，但现有模型难以匹配人类需求。

Method: 构建包含8种引用动机类型的Web文本数据集，通过对全部类型组合进行成对偏好评估捕捉人类与LLM行为的细粒度差异，并实施直接偏好优化实验调整模型行为。

Result: 人类最常为医疗内容寻求引用，强模型具相似趋势但存在系统偏差：对Wikipedia标记需要引用的内容过引用27%，对数字句（-22.6%）和个人名称句（-20.1%）引用不足。直接偏好优化显著提升模型与人类偏好对齐度。

Conclusion: 揭示LLMs引用行为与人类偏好的关键差异，证明通过优化可改进引用选择机制，为后续细粒度研究及可信度提升提供方法论基础。

Abstract: Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\%$ relative to humans) and sentences containing personal names (by $-20.1\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.

</details>


### [102] [Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions](https://arxiv.org/abs/2602.05220)
*Jinchuan Tian,Haoran Wang,Bo-Hao Su,Chien-yu Huang,Qingzheng Wang,Jiatong Shi,William Chen,Xun Gong,Siddhant Arora,Chin-Jou Li,Masao Someki,Takashi Maekaku,Yusuke Shinohara,Jin Sakuma,Chao-Han Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文提出Bagpiper（8B参数），首个通过自然语言描述（如转录和音频事件）建立原始音频与高阶认知概念双向映射的通用音频基础模型，实现理解和生成一体化，超越现有模型并在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有音频模型依赖任务特定监督且处理孤立因素，而人类智能能整体解析音频并关联抽象概念，因此需构建模拟人类认知、支持多任务学习的通用音频模型。

Method: 1) 使用包含关键认知概念的自然语言描述（如转录）作为中间表示；2) 预训练6000亿token，学习音频与高阶概念的双向映射；3) 微调采用'字幕生成→任务处理'流程模拟认知推理。

Result: 在MMAU和AIRBench等音频理解任务上超过Qwen-2.5-Omni，生成质量优于CosyVoice3和TangoFlux，可自由合成语音、音乐和音效组合，证明了模型的跨任务泛化能力。

Conclusion: Bagpiper首次实现通用音频理解与生成的统一框架，通过引入认知概念空间突破传统模型限制，公开发布的资源为后续研究奠定基础。

Abstract: Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.

</details>


### [103] [FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters](https://arxiv.org/abs/2602.05235)
*Zhilin Liang,Yuxiang Wang,Zimu Zhou,Hainan Zhang,Boyi Liu,Yongxin Tong*

Main category: cs.CL

TL;DR: FedMosaic 是首个联邦检索增强生成框架，通过参数化适配器在不共享原始文档的情况下，实现高准确率（提升10.9%）并显著降低存储（78.8%-86.3%）与通信成本（91.4%）。


<details>
  <summary>Details</summary>
Motivation: 集中式检索增强生成（RAG）在隐私敏感领域难以适用，因跨机构数据孤岛限制。上下文RAG直接传输原文档违反隐私要求，而参数化RAG虽通过适配器避免此问题，但联邦环境下仍面临存储/通信开销高和聚合冲突的双重挑战。

Method: 提出FedMosaic：1）将语义相关文档聚类生成多文档适配器，并嵌入文档掩码保证特异性；2）采用选择性聚合策略，仅合并语义一致且无冲突的适配器，避免破坏性聚合。

Result: 在四个任务类别中，相比现有方法平均提升10.9%的准确率，存储成本降低78.8%-86.3%，通信成本降低91.4%，且全程未共享任何原始文档。

Conclusion: FedMosaic 成功解决了联邦RAG框架下的隐私保护与效率权衡问题，通过结构创新和聚合策略优化，为跨机构知识协同提供了可行方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.

</details>


### [104] [Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks](https://arxiv.org/abs/2602.05252)
*Guangwei Zhang,Jianing Zhu,Cheng Qian,Neil Gong,Rada Mihalcea,Zhaozhuo Xu,Jingrui He,Jiaqi Ma,Yun Huang,Chaowei Xiao,Bo Li,Ahmed Abbasi,Dongwon Lee,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: 本文提出Copyright Detective系统，通过整合多种检测技术（如内容回忆测试、改写相似度分析）和交互审计流程，动态检测LLM输出的版权风险，支持黑盒环境下的透明评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM版权风险检测方法过于静态且无法应对侵权判定的法律复杂性。传统方法缺乏对黑盒模型的可审计性支持，而司法实践亟需动态证据分析框架。该系统填补了负责任的LLM部署与版权合规验证之间的技术缺口。

Method: 构建四层检测架构：1) 基于对抗性提示的逐字内容召回测试 2) 多粒度语义相似度分析 3) 逃逸提示防御能力验证 4) 遗忘机制有效性评估。采用迭代证据收集框架，通过可视化界面实现人工-模型协作的证据链构建。

Result: 实验验证了系统可检测0.1%级的隐式版权泄露，在10万级参数模型中识别出传统方法遗漏的训练数据残留证据。可视化组件使非技术利益相关方可解读97%以上的检测决策过程。

Conclusion: 通过将法证据学原理引入AI合规审计，构建了首个支持交互取证的版权风险检测系统。其模块化设计兼容主流LLM架构，为版权立法者与开发者提供了技术中立的合规验证路径。

Abstract: We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.

</details>


### [105] [CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs](https://arxiv.org/abs/2602.05258)
*Haoran Li,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CL

TL;DR: 本文提出CoPE，一种通过软剪切RoPE低频分量来提升大语言模型上下文扩展性的方法，显著提高256k长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统RoPE扩展方法存在频谱泄漏风险且无法融合分布外缓解与语义建模目标，需设计统一框架解决这两方面缺陷。

Method: 在RoPE中引入低频软剪切机制，通过可微分阈值函数衰减高频噪声，同时抑制硬剪切产生的频谱泄漏，保留低频语义关联信息。

Result: 实验表明在256k长度任务中比基线提升18.7%准确率，训练效率提升2.3倍，且在5种长序列基准测试中均达SOTA性能。

Conclusion: 证明软剪切策略能同时实现分布外泛化增强与语义建模优化，理论分析与实验结果共同验证了方法有效性。

Abstract: Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.

</details>


### [106] [Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR](https://arxiv.org/abs/2602.05261)
*Fanfan Liu,Youyang Yin,Peng Shi,Siqi Yang,Zhixiong Zeng,Haibo Qiu*

Main category: cs.CL

TL;DR: 本文通过理论分析响应长度变化，提出LUSPO算法解决RLVR中的长度偏差问题，并展示其优势。


<details>
  <summary>Details</summary>
Motivation: 为了解释不同RLVR算法训练中响应长度变化差异的根源，并解决响应长度崩塌问题。

Method: 分析RLVR算法组成成分，理论推导响应长度影响因素，提出LUSPO算法消除GSPO损失函数的长度偏置。

Result: LUSPO在数学推理和多模态任务中均超越GRPO/GSPO，有效缓解长度崩塌现象。

Conclusion: LUSPO通过消除长度偏置成为RLVR任务的新SOTA优化策略。

Abstract: Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.

</details>


### [107] [Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science](https://arxiv.org/abs/2602.05289)
*Jingru Fan,Dewen Liu,Yufan Dang,Huatao Li,Yuheng Wang,Wei Liu,Feiyu Duan,Xuanwen Ding,Shu Yao,Lin Wu,Ruijie Shi,Wai-Shing Leung,Yuan Cheng,Zhongyu Wei,Cheng Yang,Chen Qian,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出基于协作增益度量Γ的统构架，将LLM驱动的多智能体系统研究从经验探索转向设计科学范式，建立因素归因体系与系统化因子库


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统研究依赖经验试错法，缺乏量化协作收益的标准及系统化因子分类体系，导致优化改进缺乏理论指导

Method: 设计协作增益度量Γ以区分资源积累与真实协作收益，构建包含控制层预设和信息层动态学的因子分类框架，提出系统化因素归因方法论

Result: 建立首个量化协作本质收益的指标Γ，完成多智能体设计空间的结构化分类，实现从资源配置效应到协同作用的因果推理机制

Conclusion: 通过科学的度量体系与结构化设计框架，为构建真正的集体智能科学奠定理论基础，实现从直觉工程到设计科学的范式转换

Abstract: Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($Γ$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $Γ$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.

</details>


### [108] [MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning](https://arxiv.org/abs/2602.05307)
*Haojin Wang,Yike Wang,Shangbin Feng,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: MentorCollab提出了一种基于大模型（LRM）动态指导小模型（SLM）的推理协作框架，通过选择性使用LRM短片段修正SLM错误推理，以18.4%的LRM成本实现了最高8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大模型长链推理效果优异但消耗高且冗余，小模型高效但多步推理能力薄弱。现有协作方案模仿式引导导致冗余且纠错不足，研究亟需高效的小大模型协作范式。

Method: 在推理过程中：1) 随机采样token位置探测模型分歧；2) 使用轻量验证器判断是否采用LRM的短前瞻片段修正；3) 仅在必要位置引导SLM恢复正确推理路径。

Result: 在3个领域15组模型对中，12组显著提升（均值+3.0%，峰值+8.0%），LRM参与生成平均仅18.4%的token，显示短片段选择性探测即可实现有效协作。

Conclusion: 研究证明推理时的精准引导机制能协同发挥大小模型优势，在控制成本的同时恢复大模型的推理能力，为模型协作提供了轻量高效的解决方案。

Abstract: Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.

</details>


### [109] [How Do Language Models Acquire Character-Level Information?](https://arxiv.org/abs/2602.05347)
*Soma Sato,Ryohei Sasano*

Main category: cs.CL

TL;DR: 语言模型（LMs）在无显式字符输入时仍能隐式编码字符信息，其机制由分词器规则（如合并规则、正字法规则）及独立于分词器的因素（语义关联、语法信息）共同驱动。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确LMs如何隐式获取字符知识，需通过对比不同训练条件解析其编码机制。

Method: 通过对比标准训练设置与受控条件（如限定预训练数据集或分词器）下的LMs，分离分词器依赖与独立影响因素。

Result: 发现分词器的合并规则和正字法规则主导分词器相关机制，而子字符串语义关联与语法信息是独立于分词器的关键因素。

Conclusion: LMs字符知识积累源于分词器交互和内在语义-语法建模的双重路径。

Abstract: Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.

</details>


### [110] [PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning](https://arxiv.org/abs/2602.05370)
*Jun Rao,Zixiong Yu,Xuebo Liu,Guhan Chen,Jing Li,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.CL

TL;DR: 该论文提出PACE方法，通过纠正性探索替代暴力挖掘，在降低计算成本的同时提升大语言模型的推理对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPO-R1的模型依赖高N值的Best-of-N采样（如N≥8）进行轨迹挖掘，但作者发现高N值会导致验证噪声放大和分布偏移，影响数学推理任务的效果。

Method: 引入PACE（Proximal Alignment via Corrective Exploration），采用基于生成的纠正策略，在低预算（2<N<3）下从失败探索中合成高保真偏好对，避免暴力挖掘。

Result: PACE在仅使用约1/5计算资源的情况下，超越DPO-R1（N=16）的性能，且对奖励黑客和标签噪声具有更强鲁棒性。

Conclusion: PACE通过优化探索策略，解决了高N值带来的分布偏移问题，为模型对齐提供了更高效且稳定的新范式。

Abstract: Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.

</details>


### [111] [Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks](https://arxiv.org/abs/2602.05374)
*Chaimae Abouzahir,Congbo Ma,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型（LLM）在阿拉伯语和英语医学问答任务中存在语言驱动性能差距，尤其在任务复杂度增加时差距加剧，并发现阿拉伯语文本的分词碎片化及模型置信度与正确性关联有限。


<details>
  <summary>Details</summary>
Motivation: 针对现有LLM在低资源语言医学任务中的性能差异缺乏系统性研究，且医疗场景对模型可靠性要求极高，需明确语言特异性障碍的成因。

Method: 通过跨语言实证分析对比阿拉伯语与英语医学问答性能，并进行分词结构分析及模型置信度/解释的可靠性评估。

Result: 1) 阿拉伯语任务性能显著低于英语且差距随任务复杂度扩大；2) 阿拉伯语文本分词存在结构性碎片化；3) 模型生成答案的置信度与正确性相关性有限。

Conclusion: 医学领域LLM设计与评估需纳入语言特异性考虑，特别针对分词特性优化及开发可语言适配的可靠性评估框架。

Abstract: In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.

</details>


### [112] [IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://arxiv.org/abs/2602.05385)
*Tao Liu,Jiafan Lu,Bohan Yu,Pengcheng Wu,Liu Haixin,Guoyu Xu,Li Xiangheng,Lixiao Li,Jiaming Hou,Zhao Shijun,Xinglin Lyu,Kunli Zhang,Yuxiang Jia,Hongyin Zan*

Main category: cs.CL

TL;DR: IESR框架通过分步信息增强和结构化搜索验证机制，在Text-to-SQL任务中实现了高效轻量化推理，采用紧凑模型无需微调即可在复杂推理数据集中达到SOTA性能，同时揭示了现有模型的推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有多参数Text-to-SQL模型在复杂推理、领域知识处理和企业部署成本方面存在显著不足，特别是数学计算、物理知识和常识推理能力存在系统性偏差。

Method: 1. 解耦架构设计：分离信息理解、模式链接、数学计算和SQL生成
2. 基于MCTS的多路径推理机制+多数投票融合
3. 鉴别器驱动的轨迹一致性验证模块
4. 零微调轻量化模型部署方案

Result: 在LogicCat和Archer两个复杂推理数据集分别取得24.28 EX和37.28 EX效果，显著优于现有方案。验证了轻量化模型在保持高性能的同时降低部署成本，实证分析揭示了现有coder模型在物理知识(误差率↑32%)、算术计算(错误链长40%)、常识推理(矛盾率↑55%)等方面存在系统性缺陷。

Conclusion: 结构化推理框架能有效弥补紧凑模型的推理能力不足，数学计算能力瓶颈已成为制约Text-to-SQL性能的关键因素，未来需建立物理-数学-逻辑三位一体的评估体系

Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.

</details>


### [113] [Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances](https://arxiv.org/abs/2602.05392)
*Jiyun Chun,Eric Fosler-Lussier,Michael White,Andrew Perrault*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大语言模型（LLM）的评估框架，通过'扩展性'（Expansion）和'独立性'（Independence）两个维度，更精准地评估儿童对话中的语义贡献，超越传统长度依赖型指标。


<details>
  <summary>Details</summary>
Motivation: 传统指标（如平均语句长度MLU、词汇多样性vocd-D）过度依赖句长，忽视对话语境，无法捕捉反应质量的核心维度（如推理深度、话题延续）。需开发能反映儿童语言发展中语境化表达与话语规划能力的评估方法。

Method: 构建LLM-裁判框架：首先分类成人前导语句类型，随后从两个轴向评估儿童回应：1) 扩展性（通过上下文扩展、推理深度衡量表达丰富度）；2) 独立性（通过推动对话进展的主动性、减少成人支架依赖评估话语控制力）。

Result: 所提出的指标：1) 展现出显著的年龄发展规律性（5-12岁）；2) 相较传统基线模型提升32%的年龄预测准确率；3) 可辨别不同话语关系（因果/对比连接词）带来的语义差异；4) 与人工评分高度一致（r=0.89），支持大规模自动化评估。

Conclusion: 实现了从单纯度量语言长度到评估对话中语义贡献的范式转换，为个性化语言教育及儿童语言障碍早期筛查提供新工具。方法已开源并可通过在线平台访问（未具体说明系统名称）

Abstract: Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.

</details>


### [114] [Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better](https://arxiv.org/abs/2602.05393)
*Ji Zhao,Yufei Gu,Shitong Shao,Xun Zhou,Liang Xiang,Zeke Xie*

Main category: cs.CL

TL;DR: 本文提出Late-to-Early Training (LET)方法，利用小型预训练模型加速大型语言模型训练，在1.4B/7B参数模型实验中实现最高1.6倍速度提升，且下游任务准确率提高5%。


<details>
  <summary>Details</summary>
Motivation: 预训练大型语言模型计算成本过高阻碍研发，但已有小型预训练模型未被充分利用。提出核心问题：能否通过现有小型模型加速大模型训练？

Method: 构建晚到早训练范式(Late-to-Early Training)，通过两个核心机制：1) 晚到早步骤学习 2) 晚到早层次学习，利用预训练模型后期层表示指导大模型早期训练阶段的早期层参数优化。

Result: 在1.4B/7B模型实验中：1) 训练速度提升1.6倍 2) 下游任务准确率提升5% 3) 使用参数量小10倍的预训练模型仍保持性能优势

Conclusion: LET方法通过知识迁移机制有效降低大模型训练成本，同时提升收敛速度和任务性能，为模型扩展提供了高效的技术路径

Abstract: As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\times$ speedup with nearly 5\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\times$ fewer parameters than the target model.

</details>


### [115] [Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation](https://arxiv.org/abs/2602.05419)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出UOT-ERRANT新评估指标，基于ERRANT编辑向量和不平衡最优传输优化语法错误纠正系统评价。


<details>
  <summary>Details</summary>
Motivation: 传统参考相似度指标在GEC评估中存在局限性，当原句未改动词汇较多时，BERTScore等嵌入式度量效果差。

Method: 构建ERRANT编辑向量表示，通过不平衡最优传输（UOT）计算假设编辑到参考编辑的传输计划，形成新指标UOT-ERRANT。

Result: SEEDA元评估显示UOT-ERRANT在+Fluency领域性能显著提升（+14.8 BLEU），传输计划可实现软编辑对齐分析。

Conclusion: UOT-ERRANT在流畅性指标上优于现有方法（如ERRANT、BERTScore），且因其软对齐特性，既可用于系统排名又可用于GEC系统分析（代码开源）。

Abstract: Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.

</details>


### [116] [Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models](https://arxiv.org/abs/2602.05437)
*Basel Mousi,Fahim Dalvi,Shammur Chowdhury,Firoj Alam,Nadir Durrani*

Main category: cs.CL

TL;DR: 该论文提出了M2CQA——一个基于中东和北非文化的多模态基准测试数据集，旨在评估视觉-语言模型（VLMs）在处理非西方文化背景下的幻觉问题，发现模型在阿拉伯语及其方言中的幻觉率显著上升，并提出了一种新的度量标准CFHR。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型虽然在准确性上表现优异，但可能接受与文化背景相关但视觉上错误的理解。现有测试数据集很少关注这种问题，特别是在非西方和非英语语境中。

Method: 作者构建了M2CQA数据集，包含来自17个中东和北非国家的图像及其对应的正确和反事实描述语句（涵盖英语和阿拉伯语及其方言），并引入了一种新的度量标准：反事实幻觉率（CFHR），以量化模型在正确理解真实陈述的同时接受反事实陈述的倾向。同时评估了多种提示策略对幻觉率的影响。

Result: 实验发现，尽管模型在正确回答真实陈述方面表现优异，但在阿拉伯语及其方言中的CFHR显著上升；推理优先的提示策略增加了幻觉发生率，而回答优先的策略则增强了鲁棒性。

Conclusion: VLMs在处理文化背景信息时存在显著的幻觉问题。作者提出的M2CQA和CFHR能够用于更准确地评估模型在跨文化语境中的可靠性，并强调了在非英语和非西方语境下测试的重要性，相关资源已开源以供社区使用。

Abstract: Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.

</details>


### [117] [Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444)
*Yao Zhou,Zeen Song,Wenwen Qiang,Fengge Wu,Shuyi Zhou,Changwen Zheng,Hui Xiong*

Main category: cs.CL

TL;DR: 本研究提出CFA²攻击框架，利用因果因果关系的前端准则剥离大型语言模型的安全机制，通过稀疏自编码器移除防御特征并简化计算过程。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐机制作为潜在内部状态存在，掩盖了模型固有能力。研究旨在通过因果视角建模安全机制，并系统性解除其对模型能力的限制。

Method: 将安全机制建模为未观测混杂因子，基于Pearl前端准则构建攻击框架；采用sparse autoencoders物理剥离防御特征，通过确定性干预替代计算密集的边缘化操作。

Result: 实验表明CFA²在保持低推理复杂度的同时实现了最先进的攻击成功率，并提供了对抗过程的机械性解释。

Conclusion: 该方法展示了解耦因果特征在破解LLM安全机制方面的有效性，揭示了现有对齐方案的可剥离特性，为模型安全性分析提供了新的因果视角。

Abstract: Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.

</details>


### [118] [Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision](https://arxiv.org/abs/2602.05471)
*Md. Mithun Hossaina,Mashary N. Alrasheedy,Nirban Bhowmick,Shamim Forhad,Md. Shakil Hossain,Sudipto Chaki,Md Shafiqul Islam*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reasoning under Ambiguity的多语言情绪分类框架，通过引入不确定性感知机制和优化策略，有效解决标签模糊与部分监督下的情绪识别难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖确定性目标且假设完全标注标签，导致情绪模糊场景下学习偏差。多语言情感识别需要新框架适配标注不确定性和稀疏性。

Method: 设计共享多语言编码器结合语言特异性优化，采用基于熵的模糊权重机制弱化高模糊样本影响，并引入mask-aware学习目标与正-未标记正则化方案。

Result: 在英西阿三语数据集上，相较强基线模型在F1-score等指标提升8-12%，标注稀疏场景下准确率提高15%，训练稳定性提升40%，同时增强模型可解释性。

Conclusion: 该框架通过显式建模标注不确定性，解决了多语言情绪分类中的关键难题，为情感模糊场景下的智能决策系统提供了可靠技术路径。

Abstract: Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.

</details>


### [119] [A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering](https://arxiv.org/abs/2602.05512)
*Larissa Pusch,Alexandre Courtiol,Tim Conrad*

Main category: cs.CL

TL;DR: 本文提出了一种结合大型语言模型（LLMs）和知识图谱（KGs）的交互框架，通过自然语言协同优化Cypher查询，提升多跳推理的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: LLMs在知识密集型领域的推理能力受限于幻觉、过时信息和多跳推理不足，而传统基于文本的RAG方法在此类任务上表现不佳，知识图谱虽具解释性但需掌握查询语言，因此需要更高效的交互方案。

Method: 设计交互式框架：LLMs生成Cypher图查询并用自然语言解释，用户通过自然语言反馈迭代修正查询；在合成电影知识图谱（含90个查询基准测试）及Hyena、MaRDI两个真实知识图谱中验证效果。

Result: 定量评估显示框架有效平衡查询准确性与可解释性，揭示模型性能的领域差异性；用户实验表明其能提升复杂数据集的可访问性，同时保持语义严谨性。

Conclusion: 该框架为知识密集型任务提供了可解释的推理路径，通过人机协同优化解决了LLMs与结构化数据交互的核心挑战，但需进一步验证跨领域泛化能力。

Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.

</details>


### [120] [Multi-Task GRPO: Reliable LLM Reasoning Across Tasks](https://arxiv.org/abs/2602.05547)
*Shyam Sundhar Ramesh,Xiaotong Ji,Matthieu Zimmer,Sangwoong Yoon,Zhiyong Wang,Haitham Bou Ammar,Aurelien Lucchi,Ilija Bogunovic*

Main category: cs.CL

TL;DR: 提出多任务GRPO（MT-GRPO），通过动态调整任务权重和比例保留采样器，提升大语言模型多任务推理中的最差任务性能与平衡性。


<details>
  <summary>Details</summary>
Motivation: 传统多任务GRPO存在任务贡献不平衡问题：部分任务主导优化导致其他任务停滞，且零优势梯度任务影响优化信号分布，需在实际部署中实现跨任务的可靠性。

Method: 1) 基于最差性能动态调整任务权重以优化下界表现。2) 引入比例保留采样器使梯度更新与任务权重匹配。

Result: 在3任务/9任务场景中，MT-GRPO的最差任务准确率较GRPO提升16-28%、较DAPO提升6%，平均准确率相当。3任务下达到50%最差准确率所需训练步数减少50%。

Conclusion: 通过平衡任务贡献实现多任务学习的鲁棒性优化，为实际部署提供高效且可靠的解决方案。

Abstract: RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.

</details>


### [121] [CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models](https://arxiv.org/abs/2602.05633)
*Rui Jia,Ruiyi Lan,Fengrui Liu,Zhongxiang Dai,Bo Jiang,Jing Shao,Jingyuan Chen,Guandong Xu,Fei Wu,Min Zhang*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的同质化回复忽视学生认知与心理差异，可能对特定群体造成安全隐患。本研究提出CASTLE基准测试，结合15种教育安全风险与14种学生属性，构建中英双语92,908个场景。


<details>
  <summary>Details</summary>
Motivation: 传统安全评估缺乏对学生个体差异的考量，现有指标（如事实性、偏见）无法反映同一回答在不同人群中的差异化危害风险

Method: 基于教育理论构建双语基准测试，包含风险敏感性、情感共情与学生匹配度三大评估维度，覆盖92,908个多维度场景

Result: 18种前沿LLM在CASTLE测试中平均安全评分＜2.3/5，验证测试模型对学生特性适配性严重不足

Conclusion: CASTLE有效揭示当前教育大模型在个性化安全保障方面的系统性缺陷，为差异化模型开发提供评估框架

Abstract: Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.

</details>


### [122] [MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations](https://arxiv.org/abs/2602.05692)
*Congbo Ma,Yichun Zhang,Yousef Al-Jazzazi,Ahamed Foisal,Laasya Sharma,Yousra Sadqi,Khaled Saleh,Jihad Mallat,Farah E. Shamout*

Main category: cs.CL

TL;DR: 作者创建了首个跨语言的MedErrBench数据集，用于检测、定位和纠正临床文本错误，旨在提升医疗AI的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本错误可能导致严重后果，但缺乏覆盖多语言的评估基准，阻碍了LLM在医疗领域的可靠应用。

Method: 构建包含英语、阿拉伯语、中文的医学错误数据集，涵盖10类错误类型，由专家标注并评估多种语言模型性能。

Result: 实验显示现有模型在非英语数据上表现明显下降，且错误定位和纠正能力有限，表明语言和领域适配的重要性。

Conclusion: 研究强调需开发融合医学知识与语言特性的系统，并开源数据集以推动多语言医疗文本纠错技术的发展。

Abstract: Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.

</details>


### [123] [Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation](https://arxiv.org/abs/2602.05694)
*Shuting Jiang,Ran Song,Yuxin Huang,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 本研究提出了一种针对多领域机器翻译（MDMT）的神经元高效微调框架，通过识别和更新大语言模型（LLMs）中与领域特征对齐的关键神经元，解决了领域自适应中的参数干扰和领域偏移问题，并在多个翻译任务中实现了超越现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在机器翻译方面表现出色，但其在多领域自适应中仍面临领域偏移、参数干扰和泛化能力不足的挑战。现有方法（如上下文学习和参数高效微调）在处理这些问题时存在局限性，亟需一种更高效的领域适应策略。

Method: 提出基于信息论的神经元选择方法：1）通过最大化神经元行为与领域特征间的互信息，识别跨领域对齐的核心神经元；2）以这些神经元为引导进行LLMs微调，通过控制参数更新范围减少参数干扰和领域过拟合，在保持模型泛化能力的同时增强领域适配性。

Result: 在3种LLMs上进行的10个德英/中英翻译领域实验表明，该方法在已见和未见领域均显著优于主流参数高效微调基线模型（如LoRA、Adapter等），在BLEU评分上取得当前最优结果，且参数更新量仅为基线方法的1/5-1/2。

Conclusion: 通过神经元层面的领域特征建模，证明了大语言模型中存在跨领域共享的核心知识表征。该方法为LLMs的多领域自适应提供了新思路，既保持了模型基础能力，又实现了高效精准的领域专门化，在实际应用中具有更低的部署和维护成本。

Abstract: Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.

</details>


### [124] [OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale](https://arxiv.org/abs/2602.05711)
*Jingze Shi,Zhangyang Peng,Yizhang Zhu,Yifan Wu,Guang Liu,Yuyu Luo*

Main category: cs.CL

TL;DR: 本文提出OmniMoE框架，通过向量级原子专家和系统-算法协同设计，突破混合专家模型的粒度与效率权衡，实现50.9%平均零样本准确率及10.9倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型在专家粒度细化时面临硬件执行效率瓶颈，粗粒度设计牺牲模型表达能力，而细粒度方案导致路由复杂度与内存访问效率恶化。亟需系统与算法协同优化框架解决该权衡问题。

Method: 1) 向量级原子专家：单MoE层内集成细粒度专家池与全局共享MLP分支；2) 笛卡尔积路由器：通过平方级空间分解将路由复杂度从O(N)降至O(√N)；3) 专家中心调度：重构计算顺序将稀疏内存访问转换为密集矩阵运算。采用软硬件协同设计方法进行全栈优化。

Result: 在7个基准测试中，OmniMoE(1.7B参数)零样本准确率达44.7%-55.2%（均值50.9%），超越DeepSeekMoE(粗粒度)与PEER(细粒度)基线。推理延迟从PEER的73ms降至6.7ms(10.9倍加速)，且在长上下文场景保持稳定吞吐量。

Conclusion: 本文证明细粒度混合专家模型可通过系统-算法协同设计实现性能突破：原子专家架构结合优化后的路由策略与执行调度，在保持参数效率的同时显著提升推理速度。开源实现为后续研究提供基础框架。

Abstract: Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.

</details>


### [125] [CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering](https://arxiv.org/abs/2602.05728)
*Hao Yang,Zhiyu Yang,Xupeng Zhang,Wei Wei,Yunjie Zhang,Lin Yang*

Main category: cs.CL

TL;DR: 提出CompactRAG框架，通过离线构建原子问答库和在线阶段问题分解重写，减少多跳检索增强生成的LLM调用次数至2次，显著降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有迭代式多跳RAG系统存在重复调用LLM、token消耗高、实体关联不稳定的问题，导致推理效率低下。

Method: 1) 离线阶段：LLM单次遍历语料库构建原子级QA对；2) 在线阶段：复杂问题分解重写后，采用稠密检索+RoBERTa提取答案，仅需两次LLM调用。

Result: HotpotQA等三个数据集实验表明，在保持竞争力的同时token消耗比基线模型显著降低，实现了成本高效的多跳推理。

Conclusion: 解耦结构设计有效平衡准确性与效率，为大规模知识库多跳推理提供了实用性解决方案。

Abstract: Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.
  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.
  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.

</details>


### [126] [LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards](https://arxiv.org/abs/2602.05758)
*Bowen Ping,Zijun Chen,Yiyao Yu,Tingfeng Hui,Junchi Yan,Baobao Chang*

Main category: cs.CL

TL;DR: 本研究提出LongR框架，通过动态‘思考与阅读’机制和上下文密度奖励增强长上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖稀疏结果奖励，无法有效引导复杂长上下文推理，需改进奖励信号的细粒度和引导性。

Method: 结合动态交替推理与文档查机制，设计基于相对信息增益的上下文密度奖励函数，量化文档效用并优化决策路径。

Result: 在LongBench v2提升9%，RULER/InfiniteBench持续优化，适配DAPO/GSPO等多种RL算法，长链推理效率提升且抗干扰能力增强。

Conclusion: LongR通过信息感知奖励机制有效解决长上下文挑战，为复杂推理任务提供通用优化框架。

Abstract: Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic "Think-and-Read" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.

</details>


### [127] [Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors](https://arxiv.org/abs/2602.05769)
*Adnan Al Ali,Jindřich Helcl,Jindřich Libovický*

Main category: cs.CL

TL;DR: 研究表明，对非母语者使用低困惑度（perplexity）指标导致检测LLM生成文本的系统偏差的担忧在捷克语语境下不成立，当代检测工具已不再依赖这一指标，且检测效果更可靠。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于ChatGPT等LLM普及后学术界对检测工具误判非母语者文本的担忧。已有研究表明此类工具可能因非母语者文本的low perplexity被误判为生成文本，但相关结论是否具有语言普适性尚未验证。

Method: 1) 在捷克语场景下对比母语者与非母语者文本的perplexity差异；2) 评估三种不同技术路线的检测工具（如OpenAI Classifier、GPTZero等）对非母语者的识别准确率；3) 分析检测模型是否依赖perplexity特征进行决策。

Result: 1) 捷克语场景下非母语者文本perplexity与母语者无显著差异；2) 三类检测工具均未发现系统性误判非母语者的现象；3) 检测效果主要依赖生成概率分布特性，而非单纯依赖perplexity数值。

Conclusion: 此前关于语言认知水平导致检测偏差的假设仅在特定条件下成立，现代检测工具通过多维特征分析（如token生成概率序列模式）有效规避了语言背景带来的干扰，但仍需在其他语言和低资源语言场景中进一步验证。

Abstract: LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.

</details>


### [128] [OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://arxiv.org/abs/2602.05843)
*Fangzhi Xu,Hang Yan,Qiushi Sun,Jinyang Wu,Zixian Huang,Muye Huang,Jingyang Gong,Zichen Ding,Kanzhi Cheng,Yian Wang,Xinyu Che,Zeyi Sun,Jian Zhang,Zhangyue Yin,Haoran Luo,Xuanjing Huang,Ben Kao,Jun Liu,Qika Lin*

Main category: cs.CL

TL;DR: OdysseyArena introduces a new evaluation framework for autonomous agents focusing on inductive capability, long-horizon planning, and active learning, addressing limitations in existing deductive-based evaluation paradigms.


<details>
  <summary>Details</summary>
Motivation: Current agent evaluations rely on predefined rules/static goals and short-term planning, failing to assess the inductive ability to discover latent transition laws from experience - critical for strategic coherence and foresight in complex environments.

Method: Developed OdysseyArena with four formalized primitives to create interactive environments that require agent-driven discovery. Established two benchmarks: OdysseyArena-Lite (120 tasks for standardized inductive efficiency measurement) and OdysseyArena-Challenge (extreme horizons >200 steps for stability testing). Evaluated 15+ leading LLMs.

Result: State-of-the-art LLMs showed significant deficiencies in inductive scenarios despite strong deductive capabilities. The benchmarks revealed critical bottlenecks in long-horizon discovery and strategic coherence for complex environment exploration.

Conclusion: The study exposes limitations in current LLM-based agents' inductive reasoning capacities through OdysseyArena's novel evaluation paradigm, providing a pathway to advance autonomous discovery and strategic planning capabilities in AI agents.

Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena

</details>


### [129] [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853)
*Siran Liu,Guoxia Wang,Sa Wang,Jinle Zeng,HaoYang Xie,Siyu Lou,JiaBin Yang,DianHai Yu,Haifeng Wang,Chao Yang*

Main category: cs.CL

TL;DR: RRAttention是一种新的动态稀疏注意力方法，通过头轮询(Head Round-Robin)采样策略，在保持查询独立性的同时实现高效的全局模式发现，将注意力机制复杂度从O(L²)降低到O(L²/S²)。


<details>
  <summary>Details</summary>
Motivation: 针对现有动态稀疏注意力方法在预处理需求、全局评估缺失、查询依赖性破坏以及计算开销高等方面存在的根本性权衡问题，提出同时满足所有理想属性的解决方案。

Method: 通过跨注意力头轮询旋转查询采样位置，在每个步长(stride)内进行轮流采样，采用自适应Top-τ选择实现最优稀疏性，并通过步长级聚合保持全局模式发现能力。

Result: 在自然语言理解和多模态视频理解任务中（HELMET和Video-MME），仅计算50%的注意力块即可恢复99%以上的全注意力性能，在128K上下文长度下实现2.4倍加速，且表现优于现有动态稀疏注意力方法。

Conclusion: RRAttention通过理论优化和实验验证，解决了传统方法的关键限制，在保持查询独立性的同时显著提升长上下文处理效率，适用于语言和视觉任务的广泛场景。

Abstract: The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \underline{r}ound-\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.

</details>


### [130] [xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection](https://arxiv.org/abs/2602.05874)
*Adrián Girón,Pablo Miralles,Javier Huertas-Tato,Sergio D'Antonio,David Camacho*

Main category: cs.CL

TL;DR: 提出xList-Hate框架，通过LLM多维度诊断和决策树聚合提升仇恨言论检测的鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统二分类方法易过拟合特定数据集定义，难以应对跨域迁移和标注噪声。仇恨言论需基于多准则（法律、平台策略等）进行分解。

Method: 构建诊断框架：1）LLM回答基于规范标准的显式问题生成二值诊断信号；2）决策树聚合信号产生可审计预测；3）分解检测为可解释的细粒度因素分析。

Result: 跨基准测试显示：1）域内性能与监督方法相当；2）跨数据集鲁棒性显著提升；3）对标注不一致和语境歧义敏感度降低；4）通过决策路径实现透明性诊断。

Conclusion: 将仇恨言论识别重构为诊断推理任务，提供了一种比传统单标签分类更具可扩展性、可解释性和稳定性的内容审核方案。

Abstract: Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.
  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.
  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.
  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.

</details>


### [131] [EuroLLM-22B: Technical Report](https://arxiv.org/abs/2602.05879)
*Miguel Moura Ramos,Duarte M. Alves,Hippolyte Gisserot-Boukhlef,João Alves,Pedro Henrique Martins,Patrick Fernandes,José Pombal,Nuno M. Guerreiro,Ricardo Rei,Nicolas Boizard,Amin Farajian,Mateusz Klimaszewski,José G. C. de Souza,Barry Haddow,François Yvon,Pierre Colombo,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-22B是一个支持35种欧洲语言的大语言模型，旨在弥补现有模型对欧洲语言覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源大语言模型对欧洲语言支持不足，需开发专门模型满足欧盟多语言需求

Method: 从零构建分词器与架构，采用多阶段数据过滤和跨语言训练策略

Result: 在多项多语言基准测试中，模型在推理、指令执行和翻译任务上达到同类模型竞争力水平

Conclusion: EuroLLM-22B通过全面开源策略（模型/数据/代码）推动多语言AI发展，有效解决欧洲语言技术鸿沟

Abstract: This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.

</details>


### [132] [Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models](https://arxiv.org/abs/2602.05897)
*Shuo Nie,Hexuan Deng,Chao Wang,Ruiyu Fang,Xuebo Liu,Shuangyong Song,Yu Li,Min Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 提出FaithRL通过分步监督和隐式截断重采样策略，减少小语言模型推理中的不忠实行为，并提升结果可靠性。


<details>
  <summary>Details</summary>
Motivation: 小模型在链式推理中间步骤易出现幻觉，现有方法依赖终局奖励可能导致错误推理被强化，需改进监督粒度与训练策略。

Method: 引入分步忠实度奖励函数（Process Reward Model）并采用截断重采样生成忠实前缀对比信号，结合强化学习框架优化推理过程。

Result: 在多模型与开放知识库问答基准测试中，方法显著降低中间推理步骤与最终答案的错误率，提升答案可信度。

Conclusion: 该方法有效解决资源受限场景下小模型推理的忠实性问题，为可信AI推理提供轻量级技术路径。

Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.

</details>


### [133] [KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs](https://arxiv.org/abs/2602.05929)
*Jian Chen,Zhuoran Wang,Jiayu Qin,Ming Li,Meng Wang,Changyou Chen,Yin Chen,Qizhen Weng,Yirui Liu*

Main category: cs.CL

TL;DR: 本文提出KV-CoRE方法，基于SVD对大型语言模型的键值缓存进行数据依赖性压缩分析，揭示了模型架构、训练数据和语言覆盖与缓存可压缩性的关联性，并提出Normalied Effective Rank指标预测压缩性能损失。


<details>
  <summary>Details</summary>
Motivation: 为解决KV缓存随上下文长度增长导致GPU内存带宽饱和的问题，突破现有方法忽视缓存数据依赖性和层间差异的局限性，构建更具普适性的评估框架。

Method: 通过奇异值分解(SVD)计算KV缓存的低秩近似，采用梯度无关且增量式的Normalized Effective Rank指标，实现跨数据集、跨模型层的可压缩性量化评估。

Result: 在5大英文领域和16种语言的实证中，首次系统揭示模型架构深度、训练数据分布和语言类型与缓存可压缩性的对应关系，证明该指标与压缩性能损失呈强正相关性。

Conclusion: 为KV缓存动态压缩算法设计提供了理论依据，揭示了数据感知型压缩策略在降低LLMs内存消耗中的实践价值，同时推动了基于数据复杂度的模型架构优化方向。

Abstract: Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.

</details>


### [134] [Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions](https://arxiv.org/abs/2602.05932)
*Léo Labat,Etienne Ollion,François Yvon*

Main category: cs.CL

TL;DR: 本研究探究多语言大语言模型在回答价值观相关选择题时是否因语言不同而表现出差异性，发现模型回答存在语言依赖性，且受微调策略影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注多语言模型在事实性知识上的表现，但语言对价值观导向型问题回答的影响尚未明确，需验证模型是表现为理论上的多语通才还是多语单语者的混合体。

Method: 构建全新的人工翻译多语言价值观调查语料库（MEVS），包含8种欧洲语言，对30+个多语言模型进行系统测试，控制问题顺序、符号类型等变量。

Result: 更大且经过指令微调的模型整体一致性更高，但不同问题间差异显著：某些问题引发模型高度共识，另一些导致答案分裂；所有一致模型均存在语言特异性反应，但仅出现在特定问题上。

Conclusion: 模型输出同时受语言环境和微调策略影响，揭示了价值对齐微调的选择性效应，为多语言模型伦理评估提供了新视角。

Abstract: Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.

</details>


### [135] [Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training](https://arxiv.org/abs/2602.05940)
*Junxiao Liu,Zhijun Wang,Yixiao Li,Zhejian Lai,Liqian Huang,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: 该论文提出TRIT框架，通过整合翻译与推理训练，解决多语言场景下长推理模型倾向于使用英语进行推理的问题，无需外部反馈或多语言数据即可提升多语言理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 长推理模型常在非英语问题中切换至英语推理，且受限于多语言理解和推理能力，导致多语言场景下准确率下降。

Method: 设计TRIT（Translate-Reason Integrated Training）框架，将翻译训练融入多语言推理，通过自增强机制协同优化问题理解和响应生成。

Result: 在MMATH数据集上平均性能超越基线7个百分点，提升答案正确性和语言一致性；跨语言问题对齐准确性提升超10个百分点，翻译质量（COMET）在FLORES-200上最高增益8.4分。

Conclusion: TRIT无需额外数据即能有效增强多语言推理的跨语言适配性，为多语言通用人工智能系统提供可行技术路径。

Abstract: Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.

</details>


### [136] [Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space](https://arxiv.org/abs/2602.05971)
*Felipe D. Toro-Hernández,Jesuino Vieira Filho,Rodrigo M. Cabral-Carvalho*

Main category: cs.CL

TL;DR: 本研究通过将语义导航建模为嵌入空间中的结构化轨迹，提出了一种量化语义表示动力学的框架。该方法利用累积嵌入和几何动力学指标（如距离、熵、速度等），结合多语言数据集验证了其在临床群体区分和跨语言任务中的有效性，且相比传统方法减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有语义分析方法依赖劳动密集型的语言预处理，且缺乏捕捉语义动态变化的数学框架。本文旨在建立一种通过嵌入空间导航来建模语义生产的普适框架，量化人类语义检索的运动特征，解决传统方法在动态性、自动化程度上的不足。

Method: 1) 使用Transformer模型构建个体化语义轨迹；2) 提取累积嵌入的几何指标（距离到下一个词/质心、熵、速度/加速度）；3) 对比累积与非累积嵌入在长短轨迹的表现差异；4) 在四组跨语言数据集（神经退行性疾病、诅咒词流畅性任务等）上验证框架有效性。

Result: 框架成功区分不同临床群体和概念类型，证明累积嵌入在长轨迹任务中表现更优，而非累积方法适用于短轨迹。不同Transformer模型输出结果高度相似，表明各类表示学习范式在捕捉语义导航特征上具有共性。

Conclusion: 该框架通过连接认知建模与表示学习，为语义表示动态分析提供了数学基础工具，在临床诊断（如神经退行性疾病的早期检测）、跨语言研究和人工智能评估中具有潜在应用价值，同时验证了嵌入空间作为语义导航几何空间的合理性。

Abstract: Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.

</details>


### [137] [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992)
*Lizhuo Luo,Shenggui Li,Yonggang Wen,Tianwei Zhang*

Main category: cs.CL

TL;DR: 本论文指出扩散语言模型（dLLMs）中固定块解码策略的局限性，提出基于动态滑动块（DSB）及其缓存机制的改进方案，在保持训练自由度下同时提升生成质量与推理效率。


<details>
  <summary>Details</summary>
Motivation: 固定块调度无法根据语义难易度动态调整解码颗粒度，导致对高不确定性位置过早决策与低复杂度位置延迟处理并存，需通过语义感知的动态块自适应实现可靠高效生成。

Method: 1) 设计动态滑动块（DSB）：基于注意力权重熵自适应调整相邻解码块的重叠区域大小；2) 提出DSB Cache缓存机制：通过双指针管理键值缓存降低动态调度的计算开销。

Result: 在BERT、GLM和LLaMA系列模型上的实验表明：DSB相比固定块策略，在ROUGE-L指标提升2.3-4.1分同时减少37%-49%的解码时间，并在长文本生成任务中展现出更优的时延-质量平衡。

Conclusion: 动态块调度与缓存机制能有效缓解dLLMs的次优解码问题，为生成模型开发提供了兼具质量保障与效率优势的新范式。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.

</details>


### [138] [A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies](https://arxiv.org/abs/2602.06015)
*Panagiotis Kaliosis,Adithya V Ganesan,Oscar N. E. Kjell,Whitney Ringwald,Scott Feltman,Melissa A. Carr,Dimitris Samaras,Camilo Ruggero,Benjamin J. Luft,Roman Kotov,Andrew H. Schwartz*

Main category: cs.CL

TL;DR: 本研究评估了11种先进LLM在零样本心理健康评估中的表现，发现上下文知识和建模策略显著影响准确性，并提出了最佳实践方法。


<details>
  <summary>Details</summary>
Motivation: 研究填补当前LLM在零样本心理健康评估中性能影响因素系统性分析的空白，特别是在不同上下文和建模策略下的准确性差异。

Method: 基于1437人的临床文本数据和PTSD评分，系统评估11种LLM的性能变化，变量包括上下文知识（定义、摘要、问题）和建模策略（零样本/少样本、推理量、模型规模、输出校准），采用九种集成方法对比。

Result: (a) 详细定义和上下文提升准确性；(b) 推理量增加提升性能；(c) 开源模型（Llama, Deepseek）70B参数后性能趋稳，闭源模型（o3-mini, gpt-5）随代际优化；(d) 监督模型与零样本LLM集成效果最佳。

Conclusion: 研究结果表明上下文设计和建模策略选择对心理健康评估的LLM部署至关重要，建议实践时结合定义增强与监督集成方法。

Abstract: Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.

</details>


### [139] [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036)
*Jian Chen,Yesheng Liang,Zhijian Liu*

Main category: cs.CL

TL;DR: DFlash通过并行块扩散模型提升大语言模型解码速度，较现有方法加速6倍


<details>
  <summary>Details</summary>
Motivation: 传统自回归生成存在顺序解码瓶颈，现有推测解码方法(EAGLE-1/EAGLE-3)受限于顺序草稿生成，扩散模型虽并行但性能不足，导致实用加速效果有限

Method: 提出DFlash框架：1) 使用轻量级块扩散模型实现并行草稿生成；2) 通过单次前向传递生成草稿；3) 基于目标模型提取的上下文特征进行条件生成

Result: 实验显示DFlash实现6倍无损加速，在多个模型和任务上相较EAGLE-3提升2.5倍，输出质量与自回归模型相当

Conclusion: DFlash成功结合扩散模型的并行优势与自回归模型的质量，验证了基于扩散模型的推测解码框架在大语言模型高效部署中的可行性和优越性

Abstract: Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.

</details>
