<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 36]
- [cs.CL](#cs.CL) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: 本文提出了一种名为GRAFNet的生物启发架构，通过模仿人类视觉系统的层次结构，实现了更精确的结肠镜下息肉分割。其核心包括三个关键模块，解决了现有深度学习方法存在的单向处理、弱多尺度融合和缺乏解剖约束问题。


<details>
  <summary>Details</summary>
Motivation: 常规结直肠镜息肉分割因以下原因面临挑战：1）形态多样性（从扁平到突起病变）；2）与褶皱血管等正常结构的视觉相似性；3）多尺度检测需求。现有深度学习方法存在过度分割正常结构（假阳性）和遗漏细微扁平病变（假阴性）问题。

Method: 提出GRAFNet架构，整合三大模块：1）引导非对称注意力模块（GAAM）模仿方向调谐皮层神经元强化边界；2）多尺度视网膜模块（MSRM）模拟视网膜节细胞通路实现并行多特征分析；3）引导皮层注意力反馈模块（GCAFM）通过预测编码进行迭代优化。这些模块通过解剖-语义一致性保障的息肉编码器-解码器模块（PEDM）统一架构实现分辨率自适应反馈。

Result: 在Kvasir-SEG等5个公共基准测试中取得显著成果：Dice指标提升3-8%，泛化能力提高10-20%，且决策路径具有可解释性。代码已开源（https://github.com/afofanah/GRAFNet）。

Conclusion: 通过神经计算原则实现人工智能精度与临床可信推理间的范式突破，确立了将生物视觉机制与深度学习结合的创新路径。

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

</details>


### [2] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文提出了一种解耦框架，通过多模态大语言模型（MLLMs）实现零样本人-物交互（HOI）检测，解决了现有方法依赖特定检测器和粗粒度特征导致的泛化性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将交互识别（IR）与特定检测器紧密耦合且依赖粗粒度视觉-语言模型（VLM）特征，难以泛化到未见交互场景。零样本HOI检测需同时解决目标定位和复杂交互识别挑战。

Method: 1. 提出解耦框架分离目标检测与交互识别；2. 引入确定性生成方法将IR建模为视觉问答任务，实现无训练零样本IR；3. 设计空间感知池化模块整合外观与成对空间信息；4. 开发单次确定性匹配方法实现单次前向传播预测所有交互。

Result: 在HICO-DET和V-COCO数据集上取得优异零样本性能，具备跨数据集强泛化能力，且无需重新训练即可灵活集成任意目标检测器。

Conclusion: 该方法通过MLLMs优势与解耦架构创新，显著提升了零样本HOI检测的性能与适用性，代码已开源。

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [3] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文提出Cumulative Sample Loss (CSL) 方法，通过分析帧级训练损失轨迹检测视频数据集的标注错误（如标签错误/时间错乱），在EgoPER/Cholec80数据集验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 标注错误破坏视频数据集质量，尤其影响时序敏感任务。现有方法依赖真实标签，而CSL通过建模损失动态特征实现无监督检测。

Method: 训练模型并保存各epoch检查点，用每个检查点计算测试帧的损失轨迹。错误标注帧呈现高/不规则损失模式，正确帧损失早期收敛。通过CSL阈值筛选异常帧。

Result: 在EgoPER/Cholec80数据集有效识别出误标签和时序错乱帧，检测表现优于基线，且方法具有跨数据集泛化性。

Conclusion: CSL提供了一种通用、鲁棒的数据集审计工具，可改善视频模型训练可靠性，且无需真实错误标签先验。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [4] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 提出了基于分布学习的超分辨率框架，结合CFD模拟与4D Flow MRI数据，解决医学成像中的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率方法依赖人工降采样数据对，但临床低分辨率图像来自不同采集机制导致的领域偏移问题，需提升模型跨领域泛化能力。

Method: 模型先用CFD模拟的高低分辨率数据预训练，再用少量配对4D Flow MRI与CFD数据微调，通过分布估计器增强鲁棒性。

Result: 在真实数据中显著优于传统方法，验证了分布学习对领域偏移的处理能力。

Conclusion: 结合模拟数据与真实数据的分布学习框架可有效提升医学成像超分辨率性能，尤其适用于临床复杂场景。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [5] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经体积渲染的相机虚拟化方法，针对动态场景（如体育赛事和舞台表演）实现高质量视角合成和高效时间归档。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确的3D点云，难以处理快速非刚性运动和多物体独立运动（如翻转、跳跃、突然互动），且不支持时间回溯和归档。

Method: 通过建模动态场景中的刚性变换（多相机视图同步），结合神经表示学习，重新设计神经体积渲染框架，并支持时间维度的归档功能。

Result: 在测试时获得高视觉质量的渲染效果，可支持任意时间点的视角合成及回溯功能（如回放、分析和赛事存档）。

Conclusion: 解决了当前方法在动态场景中时间一致性和运动处理的不足，提供了可应用于体育转播等场景的实用化新型视角合成方案。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [6] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 该论文首次系统研究了训练长上下文视觉语言模型（最高达344K上下文）的方法，通过持续预训练、监督微调和偏好优化提升长文档视觉问答及长上下文文本性能，发布了MMLBD-C数据集，并提出多项关键技术发现。


<details>
  <summary>Details</summary>
Motivation: 现有开源视觉语言模型（如Qwen3 VL、GLM 4.5/6V）的训练方法不可复制，且缺乏针对长文档与长上下文性能的系统性研究。

Method: 对24B和32B参数模型进行持续预训练、监督微调与偏好优化，结合消融实验进行大规模长上下文评估，提出合成数据生成与页面索引强化等方法。

Result: 在MMLongBenchDoc数据集上达到SOTA性能，发现匹配训练与评估上下文长度、页面索引、视觉长上下文向文本迁移等关键技术有效性，并发布修正后的MMLBD-C数据集。

Conclusion: 系统验证了长上下文视觉语言模型训练策略的有效性，提出可复现的训练方法与数据集修正方案，为后续研究提供了基准与技术基础。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [7] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: 提出E²D方法，在大规模数据集蒸馏中平衡准确率与效率，通过探索-利用策略减少冗余计算，实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有解耦蒸馏方法在准确率与计算效率间存在权衡：优化密集型方法准确率高但速度慢，优化免费方法速度快但损失准确率。需寻求平衡方案。

Method: E²D分两阶段优化：1. 全图初始化保持语义完整性和特征多样性；2. 探索阶段均匀更新并识别高损失区域，利用阶段针对性优化加速收敛。

Result: 在ImageNet-1K上超越SOTA且快18倍，ImageNet-21K精度提升同时快4.3倍。

Conclusion: 针对性减少冗余计算的有效更新策略，可克服大规模数据蒸馏中准确率与效率的权衡问题。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [8] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种基于流匹配视频生成器的联合采样框架，在低样本条件下提升文本到视频生成的跨视频多样性，同时保持时间一致性和颜色自然度。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成成本高昂导致单提示样本量少，需提升每批生成视频的多样性价值，但现有方法存在时间一致性下降和计算成本高的问题。

Method: 设计双阶段框架：1）多样性驱动更新增强跨视频差异；2）通过轻量级潜在空间模型量化并剔除破坏时间一致性的成分，避免解码器反向传播。

Result: 在保留色彩自然度和时间连续性的前提下，多样性指标与主流基线方法持平，但计算效率显著提升。

Conclusion: 该方法通过解耦多样性提升与一致性约束，在低资源场景下实现了更优的文本到视频生成性能平衡。

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [9] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的零样本3D脑部MRI异常检测框架，通过聚合多轴切片构建局部三维token，有效扩展了现有2D方法至三维空间。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像零样本异常检测（ZSAD）方法多局限于2D数据，直接扩展到3D时因依赖逐层特征提取和视觉语言模型，无法捕捉立体结构信息。需要解决3D医学影像中空间上下文缺失和计算复杂度高的问题。

Method: 提出一种完全无需训练的框架：1）通过2D基础模型处理多轴切片并聚合生成局部三维patch token；2）恢复立方体空间上下文；3）集成距离计算和批级异常检测流程。使用标准GPU即可计算且无需微调、提示或监督。

Result: 实验验证了无需训练的批处理ZSAD方法可有效扩展到完整3D MRI体数据，在保持计算可行性的同时实现了立体空间异常检测。三维表征在检测精度和鲁棒性上优于现有基于切片的方法。

Conclusion: 该研究证明通过多轴切片聚合生成的局部三维token能有效保持立体上下文信息，为医学影像提供了一种简化的、可扩展的零样本异常检测解决方案。

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [10] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: Sparrow框架通过解决注意力稀释和视觉噪声问题，显著加速了视频大语言模型（Vid-LLMs）的推理过程。


<details>
  <summary>Details</summary>
Motivation: 推测解码在加速视觉语言模型（VLMs）上有效，但在视频大语言模型（Vid-LLMs）上易因注意力稀释和视觉增益下降导致性能崩溃，且需要缓解视觉输入在深层推理中的冗余性。

Method: 提出Sparrow框架，包含三个核心方法：1）基于隐藏状态复用的视觉感知文本锚定窗口注意力；2）中间层视觉状态衔接；3）多token预测策略，分别用于卸载视觉计算、过滤低层噪声、弥合训练-推理分布差异。

Result: 实验表明Sparrow在25k视觉token输入下实现平均2.82倍加速，有效解决长序列性能退化问题。

Conclusion: Sparrow为长视频实时任务提供了可行解决方案，验证了隐藏状态复用与中间层衔接在Vid-LLMs加速中的有效性。

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [11] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 该论文提出EventMemAgent框架，通过分层记忆模块和双层策略处理在线视频理解的长程上下文与细粒度细节冲突问题。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解因MLLM的上下文窗口限制和现有方法在长程上下文与细节捕捉间的权衡而受到挑战，需要主动式处理框架。

Method: 设计双层策略：短时记忆通过事件粒度采样动态处理视频帧，长时记忆结构化存储历史事件；集成多粒度感知工具并通过Agentic RL实现推理与工具使用的端到端训练。

Result: 实验表明EventMemAgent在在线视频基准测试中表现优异，代码已公开。

Conclusion: EventMemAgent有效结合主动感知与强化学习，为长视频流处理提供了结构化记忆与自适应决策的解决方案。

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [12] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: 本研究创建了CREMD数据集，通过多模态实验分析犬类情感识别的影响因素，发现视觉上下文、专业背景可提高标注一致性，音频增强特定情感识别信心。


<details>
  <summary>Details</summary>
Motivation: 解决犬类情感识别中主观评估偏差与缺乏标准化数据的问题，通过多模态数据集研究不同呈现模式及标注者属性对情感标注的影响。

Method: 采集923个视频片段构建数据集，设计三种视听模式并开展跨群体标注实验，包含犬主、专业人士及不同性别群体，采用统计分析方法探究标注一致性与相关变量。

Result: 1) 视觉上下文显著提升标注一致性，音频设计因实验限制结论不明确；2) 非犬主与男性标注者一致性更高，专业人士符合预期；3) 音频显著增强对愤怒/恐惧的识别信心。

Conclusion: CREMD数据集为犬体情感分析提供了标准化框架，证明标注一致性受视觉线索、专业经验及性别因素影响，建议未来改进音频条件设计及扩展数据维度。

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [13] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: 该论文提出DAV-GSWT框架，通过扩散先验与主动视角采样结合，在3D高斯泼溅中实现仅需少量输入即可生成高质量Wang Tiles，大幅减少数据需求且保持渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有Wang Tiles方法依赖密集采样数据，在扩展大规模场景时面临数据量爆炸与重构质量下降问题，需解决数据效率与场景连贯性矛盾。

Method: 提出层次化不确定性量化机制，结合扩散生成模型动态选择关键视角；通过主动学习策略补全结构细节，并利用扩散先验优化视角规划，实现低数据输入下的无缝Tile生成。

Result: 实验表明在保持4K分辨率与120FPS交互性能前提下，输入数据量较传统方法减少76%，Tile拼接缝隙降低至视觉不可察水平。

Conclusion: DAV-GSWT成功突破3D高斯泼溅对密集采样的依赖，为大规模虚拟场景生成提供高效解决方案，兼顾渲染质量与计算资源消耗。

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [14] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: GMAIL 是一种新型框架，通过将生成图像作为独立模态与真实图像对齐，提升视觉-语言任务性能。


<details>
  <summary>Details</summary>
Motivation: 生成图像与真实图像存在模态差异，直接替代使用会导致模式崩溃，需区分利用生成数据优势。

Method: 1. 使用跨模态对齐损失在生成图像上微调模型；2. 在潜在空间中桥接真实与合成模态，结合多模态学习训练视觉-语言模型。

Result: 在图像描述生成、零样本检索等任务中显著提升性能，生成数据规模扩大时表现持续增强，LLaVA大模型的描述能力也获改进。

Conclusion: GMAIL框架有效结合生成图像与真实数据优势，无需直接替代即可提升多模态模型训练效果，具有广泛适用性。

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [15] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文提出了一种抑制语义幻觉的无配对图像翻译框架，在昼夜图像转换中显著提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 昼夜图像翻译缺乏像素级监督且存在显著外观差异，现有方法易产生语义幻觉（如错误生成交通标志/灯光效果），严重影响下游任务性能。

Method: 基于Schrödinger Bridge翻译模型，设计双头判别器检测背景区域幻觉内容，并通过目标域标注对象特征构建的类原型进行语义锚定，在特征空间迭代优化抑制幻觉。

Result: 在BDD100K数据集上，该方法使昼夜域适应的mAP提升15.5%，其中易幻觉的'交通灯'类性能提升达31.7%。

Conclusion: 通过显式抑制语义特征空间的幻觉，有效保持翻译过程中的物体语义一致性，显著优于现有方法。

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [16] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: ASBM通过两阶段方法优化扩散模型的轨迹生成，在高维数据中实现更高效和稳定的采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因前向过程无记忆且独立数据-噪声耦合，导致轨迹弯曲和噪声目标。ASBM旨在恢复高维最优轨迹以提升效率和稳定性。

Method: 第一阶段通过数据-能量采样视角构建Schrodinger Bridge向前动态并传输到先验；第二阶段用最优耦合监督的简单损失训练反向生成动态。

Result: 非记忆化模式使采样路径更平直高效，在图像生成任务中减少采样步数同时提高保真度，并能通过蒸馏实现一步生成。

Conclusion: ASBM在高维数据上显著优于现有方法，为生成模型提供了更稳定高效的轨迹优化框架。

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [17] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: This paper evaluates open-source multimodal large language models (MLLMs) for zero-shot morphing attack detection (MAD) in biometric verification, finding that models like LLaVA1.6-Mistral-7B achieve state-of-the-art performance without task-specific training, surpassing conventional MAD systems by 23% in equal error rate.


<details>
  <summary>Details</summary>
Motivation: Current MAD systems require task-specific training and poorly generalize to unseen attack types, whereas MLLMs may inherently capture facial inconsistencies through multimodal pretraining, enabling zero-shot detection without domain adaptation.

Method: A standardized protocol assessed open-source MLLMs' zero-shot MAD performance on diverse morphing techniques using pre-trained weights, measuring discriminative ability via equal error rate (EER).

Result: LLaVA1.6-Mistral-7B achieved the best performance, exceeding task-specific MAD baselines by 23% in EER; other MLLMs also showed non-trivial discriminative capacity without fine-tuning, indicating multimodal pretraining encodes morphing-related visual patterns.

Conclusion: Open-source MLLMs provide reproducible, interpretable foundations for biometric security via zero-shot MAD capabilities, with potential improvements through lightweight adaptation. This establishes a new direction for forensic image analysis using off-the-shelf vision-language models.

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [18] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: 本文提出了一种用于红外图像超分辨率的区域先验注意力Transformer模型（RPT-SR），通过引入区域先验令牌和局部令牌的双令牌框架，解决了固定视角场景下通用超分辨率模型的空间先验利用不足问题。


<details>
  <summary>Details</summary>
Motivation: 通用型超分辨率模型（尤其是视觉Transformer）在红外监控和自动驾驶等固定视角场景中存在效率低下问题，因其未能有效利用此类场景中强而持久的空间先验信息，导致冗余学习和次优性能。

Method: 提出区域先验注意力机制，采用双令牌架构：1）可学习的区域先验令牌作为场景全局结构的持久记忆；2）局部令牌捕捉当前输入的帧特异性内容。两者在注意力机制中融合，使先验信息动态调节局部重建过程。

Result: 在多光谱红外数据集（包括长波和短波红外）上取得SOTA性能，验证了模型在不同红外波段的普适性和有效性，而传统方法通常仅针对单一红外波段。

Conclusion: 实验证明通过显式编码场景布局信息到注意力机制中，RPT-SR能显著提升固定视角下的红外超分辨率性能，为监控和自动驾驶等应用提供了更高效的解决方案。

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [19] [LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction](https://arxiv.org/abs/2602.15493)
*Raffaele Cappelli,Matteo Ferrara*

Main category: cs.CV

TL;DR: 本文提出了LEADER，一个轻量级端到端深度学习模型，用于将原始指纹图像直接映射为包含位置、方向和类型的细节描述，参数仅需0.9M。


<details>
  <summary>Details</summary>
Motivation: 传统指纹细节提取依赖复杂的预处理和后处理步骤，严重限制效率。现有深度学习方法未实现完整的端到端处理，且模型参数量过大。

Method: 提出'城堡-护城河-围墙'真值编码策略，结合双编码器结构与注意力门控机制；通过整合非极大值抑制和角度解码模块实现真正端到端推理。

Result: 在NIST SD27数据集上F1-score提升34%，平均排名2.07（第一占比47%），15ms GPU/322ms CPU的推理速度超越商业软件，且模型参数量仅为现有方法的1/10。

Conclusion: LEADER在保持计算效率的同时实现跨域鲁棒性，其内部表征与传统指纹特征（分割掩码、方向场等）高度对齐，代码开源将推动领域发展。

Abstract: Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel "Castle-Moat-Rampart" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.

</details>


### [20] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: This paper proposes a semantic filtering framework to remove transient objects in 3D Gaussian Splatting by leveraging vision-language models and CLIP similarity scores for category-aware transient removal.


<details>
  <summary>Details</summary>
Motivation: Transient objects in multi-view captures cause ghosting artifacts in 3DGS reconstruction. Existing solutions face memory constraints with scene decomposition or struggle with parallax ambiguity in motion heuristics, necessitating a robust, memory-efficient alternative.

Method: A semantic filtering framework uses CLIP similarity scores between rendered views and text prompts to identify transients per-Gaussian. Opacity regularization and pruning are applied to Gaussians exceeding a threshold, with semantic classification resolving parallax ambiguity via category-based object identification.

Result: Experiments on RobustNeRF showed improved reconstruction quality over vanilla 3DGS across four sequences, with minimal memory overhead, real-time rendering, and validated effectiveness through threshold calibration and baseline comparisons.

Conclusion: Semantic guidance offers a practical, efficient strategy for transient removal in scenarios with predictable distractor categories, outperforming motion-based methods and scene decomposition approaches.

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [21] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: 本文提出了一种针对手势生物特征评分的多维度评估体系，通过整合排序、趋势补偿和身份解缠等要素构建综合评价指标，并在多个数据集和模型上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生物特征评估依赖错误率指标，无法反映评分质量。手势生物特征评分的质量评估需考虑排序合理性、趋势一致性和身份特征解耦性。

Method: 构建包含排序偏离惩罚、评分相关趋势补偿、身份特征解缠折扣的评估框架，通过加权集成生成高级接纳评分，并在三个数据集与五个SOTA模型上进行验证。

Result: 实验表明该指标相较于现有方法能更准确选择最优评分，并与传统评估指标存在显著相关性，证明了其可靠性。

Conclusion: 提出的多维评估体系为手势生物特征分析提供了全面的质量诊断工具，开源代码库为领域发展提供了基准框架。

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [22] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: 该研究提出一种动态、无需训练的LoRA融合框架，结合特征级选择与指标引导调整，实现无需重新训练的主题与风格协同生成。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法依赖静态权重合并，偏离自适应特征调整初衷且忽略输入随机性，需提出动态机制解决这一问题。

Method: 前向阶段动态计算KL散度选择最优融合权重；反向阶段基于CLIP/DINO指标动态应用梯度校正，全流程整合双机制实现端到端生成引导。

Result: 跨多主题-风格组合实验显示，该方法在生成质量（FID/CLIP分数）和视觉一致性上全面超越当前最优LoRA融合技术（如LORA++/DoRA）。

Conclusion: 动态融合机制有效平衡模型权重与外部指标，首次实现无需微调的多LoRA协同推理框架，为个性化图像生成提供轻量级解决方案。

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [23] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: PADE是一种无需训练的注意力干预方法，通过增强内部正注意力动态（PAD）来减少多模态推理中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言大模型（LVLMs）因注意力失衡和静态增强方法易导致幻觉，现有训练无关方法计算成本高且存在干扰。

Method: 提出PADE方法：构建PAD映射捕捉视觉核心区域，采用中位数绝对偏差缩放动态控制干预强度，并利用系统令牌补偿保持用户指令关注。

Result: 在多个LVLMs和基准测试中验证了PADE在提升视觉语义锚定能力和降低幻觉发生率上的有效性。

Conclusion: 基于内部注意力动态的干预策略可有效提升多模态推理的可靠性。

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [24] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的全自动化OCT图像血管分割与分类方法，整合预处理、导丝伪影去除、特征提取及SVM/逻辑回归分类，实现高精度(99.68%)且低计算复杂度的血管边界检测。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉OCT图像存在噪声、成像伪影及复杂组织结构，现有方法在自动分割和分类方面存在挑战，需要一种高效准确且低人工干预的解决方案。

Method: 提出多阶段自动化流程：1) 图像预处理与导丝伪影去除；2) 极坐标转笛卡尔坐标；3) K-means聚类分割；4) 局部特征提取；5) 使用逻辑回归和SVM进行像素级分类。

Result: 实验显示最高精确率、召回率和F1分数均为1.00，整体分类准确率达99.68%，相比现有方法计算复杂度降低且无需手动标注。

Conclusion: 该方法为OCT图像分析提供了可靠高效的自动化解决方案，具备临床决策支持和实时医学图像处理的应用潜力。

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [25] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: The paper introduces IRIS-v2, a comprehensive dataset for aligning industrial schematics with real-world 2D/3D data to improve digital twin creation. It experimentally combines segmentation and graph matching in a case study to reduce manual alignment time.


<details>
  <summary>Details</summary>
Motivation: Manual alignment of industrial schematics with real-world data is inefficient and error-prone for large-scale facilities. Inconsistencies between schematics and reality, combined with the lack of public industrial datasets, necessitate an automated approach for scalable digital twin development.

Method: The authors constructed IRIS-v2, which includes multi-modal data (images, point clouds, CAD, P&ID) and developed a case study combining semantic segmentation of real-world scenes with graph matching between schematics and 3D models to automate alignment.

Result: While specific quantitative results are not provided, the IRIS-v2 dataset provides a standardized evaluation benchmark for alignment algorithms. The case study demonstrates a methodological framework for reducing manual effort in industrial digital twin construction.

Conclusion: IRIS-v2 addresses the critical need for industrial alignment research by offering a realistic dataset with ground truth annotations. The integration of segmentation and graph matching shows potential for improving the scalability of industrial digital twin workflows.

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [26] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 本文提出CEMRAG框架，结合可解释临床概念与多模态检索增强技术，解决放射学报告生成中的可解释性与准确性冲突问题，实验证明该方法在多个医疗数据集上有效提升诊断准确性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告生成模型存在可解释性缺失与生成结果与影像证据不符的矛盾，亟需通过结构化方法将视觉概念透明化与事实推理统一，打破可解释性与性能的对立困境。

Method: 提出概念增强的多模态RAG框架：1）通过可解释视觉概念编码分解图像特征；2）构建包含知识库的多模态上下文感知推理架构；3）采用双路径条件约束（视觉透明化+结构化语言建模）实现生成可追踪性。

Result: 在MIMIC-CXR/IU X-Ray数据集验证中，CEMRAG在临床准确率提升12.7%/8.2%，生成文本与影像证据一致性提高9.5%/6.3%，且保持了与传统VLM相当的诊断准确率，证明可解释性增强不损害核心性能。

Conclusion: 通过视觉概念解耦和多模态RAG协同，成功实现医学报告生成的『准确可解释』，为医疗AI的临床可信任部署提供了模块化技术路径。

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [27] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: This study introduces a new public strawberry ripeness dataset with 566 images and 1,201 labeled objects, collected under varied agricultural conditions in Turkey. Experiments using YOLOv8, YOLOv9, and YOLO11 models show that YOLOv8s achieves the highest mAP@50 (86.09%), while smaller models balance performance and efficiency for smart agriculture.


<details>
  <summary>Details</summary>
Motivation: Traditional strawberry ripeness classification based on visual assessment is subjective and error-prone, causing producer losses and inconsistent consumer quality. Existing methods lack standardized benchmarks due to sparse public datasets, necessitating a comprehensive, accessible dataset and comparative evaluation of detection models.

Method: A strawberry ripeness dataset was generated under variable lighting/conditions across two Turkish greenhouses. The dataset was evaluated with YOLOv8, YOLOv9, and YOLO11 models to benchmark precision, recall, and mAP@50 scores for detecting strawberries.

Result: The YOLOv9c model achieved 90.94% precision, YOLO11s achieved 83.74% recall, and YOLOv8s achieved 86.09% mAP@50—a 7.25% improvement over its recall. Small-to-medium models demonstrated superior balance between accuracy and computational efficiency.

Conclusion: Small YOLO models enable efficient strawberry detection in varied agricultural environments. The release of this dataset fills a critical gap for standardized smart agriculture research, particularly for cost-effective, real-time harvesting systems.

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [28] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: 提出了一种自动化管道（3D数据分析优化管道），通过双阶段贝叶斯优化实现3D生物医学图像分割与分类模型的设计及参数优化。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在3D生物医学图像分析中面临模型选型和参数调优瓶颈，人工干预效率低下。为解决此问题，需构建自动化优化框架。

Method: 第一阶段：基于领域适应句法基准数据集和自主设计的分割质量指标，通过贝叶斯优化选择最优分割模型及后处理参数；第二阶段：优化分类器架构（编码器/分类头设计、先验知识融合、预训练策略），并集成自动实例提取标注流程，通过分割结果驱动分类实例的交互式标注。

Result: 四个实际案例表明该管道能自动识别针对特定3D生物医学数据集的最优模型配置与参数组合，显著降低人工标注工作量。

Conclusion: 该双阶段优化管道有效解决3D生物医学图像分析中模型选型与参数调优难题，通过量化指标和交互式工作流提升跨场景应用的通用性与效率。

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [29] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: 提出了一种'标准优先、语义其次'的图像分析框架，通过可通用的结构发现解决传统标签驱动方法在开放科学发现、跨传感器/跨站点数据比较和长期监测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有语义优先范式在科学图像分析中面临三大挑战：开放科学发现中的不可预见性、跨设备/跨机构数据的可比性，以及本体论随时间演变的长期监测问题。这促使了需要一种非语义依赖的结构发现方法论。

Method: '标准定义的非语义结构提取'与'下游语义映射'分离的双阶段框架：1) 通过显式最优性标准(非局部本体论)生成稳定结构；2) 建立结构产物到领域本体的显式语义映射，该框架基于控制论、区分观测理论和信息论基础。

Result: 跨领域实证显示当标签系统不可扩展时，'标准优先'组件呈现规律性复现。框架成功分离了信息发现(结构提取)与意义赋予(语义映射)，实现了结构产物的FAIR特性(可发现、可访问、可互操作、可重用)。

Conclusion: 为验证体系提供了超越分类准确率的新范式，并为数字孪生和长期环境监测构建了通用的AI就绪型数字对象，同时通过显式语义映射支持多元解释体系的互操作性。

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [30] [ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT](https://arxiv.org/abs/2602.15720)
*Hyunchan Moon,Cheonjun Park,Steven L. Waslander*

Main category: cs.CV

TL;DR: ViTs计算成本高，现有解决方案存在优化问题，ToaSt框架通过模块专用策略实现高效压缩，提升性能。


<details>
  <summary>Details</summary>
Motivation: 结构化剪枝和token压缩方法存在训练时间长、优化困难，需开发更高效解决方案。

Method: 提出ToaSt框架，多头注意力模块采用coupled head-wise剪枝，利用FFN的TCS方法提高压缩比并避免全局传播。

Result: 在九个ViT模型上验证，ViT-MAE-Huge准确率88.52%（+1.64%），FLOPs降39.4%，目标检测mAP提升至52.2。

Conclusion: ToaSt在不同ViT架构上均有效，兼顾准确性和效率，适合部署应用。

Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\% accuracy (+1.64 \%) with 39.4\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.

</details>


### [31] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: This paper proposes a retrieval-augmented framework to enhance LLM-based vision-and-language navigation efficiency by integrating episode-level exemplar retrieval and step-level candidate pruning.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based VLN methods suffer from inefficient decision-making due to repetitive instruction interpretation and noise in navigable candidates, requiring a more stable and scalable approach.

Method: Introduces two retrieval modules: (1) episode-level instruction embedding retriever for providing successful trajectory exemplars; (2) step-level candidate retriever for pruning irrelevant directions, both operating independently from the LLM.

Result: Demonstrated consistent improvements in VLN metrics (Success Rate, Oracle Success Rate, SPL) on seen/unseen environments, with ablation studies confirming complementary benefits of both retrieval levels.

Conclusion: Retrieval-augmented decision support proves effective for LLM-based VLN by reducing action ambiguity while maintaining modularity and scalability.

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [32] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: 提出基于语言与几何的稀疏体素表示统一框架，综合建模三维场景外观、语义与几何。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖将2D语言特征蒸馏到3D，但忽视场景外观/语义/几何的协同作用，导致场景理解脱离几何结构且与重建过程脱节。

Method: 采用3D稀疏体素为基元，构建外观场、密度场、特征场和置信场；设计特征调制模块融合语言特征，并通过深度相关正则和模式一致性正则实现几何蒸馏，将几何知识从2D模型迁移到3D表示。

Result: 在整体场景理解与重建任务中，方法性能优于现有最先进方法。

Conclusion: 通过统一框架实现语言和几何基础的稀疏体素表示，有效协同建模三维场景的三大核心要素。

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [33] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: 本文提出一种名为R3（Reason-Reflect-Refine）的多模态框架，通过将生成任务分解为'生成-理解-再生成'的多步骤过程，缓解生成能力与理解能力之间的优化矛盾。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型存在生成能力与理解能力相互制约的问题，增强一方通常以牺牲另一方为代价，研究旨在解决这种优化困境。

Method: 设计Reason-Reflect-Refine（R3）算法，将单步生成任务重构为包含推理（生成）、反思（理解）、精炼（再生成）的循环流程，显式利用模型理解能力指导生成过程。

Result: 成功缓解模型内部竞争动态，在保持理解能力的同时提升了生成效果，并获得了关于生成过程与理解能力关联性的新洞察。

Conclusion: R3框架为构建下一代统一多模态模型提供了可行路径，其循环优化机制有效平衡了生成质量和语义理解需求，开源代码加速了后续研究。

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [34] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出NeRFscopy，一种基于神经渲染的自监督内窥镜视频动态3D重建方法，通过可变形模型和SE(3)变换参数化实现无需模板的高精度3D重建。


<details>
  <summary>Details</summary>
Motivation: 传统内窥镜3D重建受限于软组织变形、单目相机成像缺陷、光照变化及未知相机轨迹等挑战，需探索无需预训练模板的鲁棒动态建模方法。

Method: 构建可变形模型，包含规范辐射场与时间依赖的SE(3)变形场参数化，并通过引入复杂颜色项直接从单目视频数据中学习隐式3D结构，完全摆脱模板依赖。

Result: NeRFscopy在多个复杂内窥镜场景中实现当前最优的新视角合成效果，3D重建精度超过现有方法，且无需任何先验模型。

Conclusion: 该方法为内窥镜动态3D重建提供了新的自监督框架，解决了变形软组织建模难题，具备提升临床诊疗可视化的应用潜力。

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [35] [Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers](https://arxiv.org/abs/2602.15783)
*Lucas Sancéré,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 使用可扩展图变压器模型对全切片细胞图进行健康与肿瘤细胞分类，准确率优于传统图像方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像分析方法因依赖局部图像块丢失组织层级上下文，难以区分形态相似的健康与肿瘤细胞。

Method: 构建全切片细胞图，采用SGFormer和DIFFormer等图变压器模型进行分类；对比图模型与图像模型性能；通过融合形态、纹理及细胞类型组合优化节点特征。

Result: 图模型SGFormer/DIFFormer在单一WSI分类中平衡准确率85.2/85.1，高于图像方法81.2；多WSI扩展后DIFFormer达83.6，优于图像模型78.1。

Conclusion: 图模型利用组织级上下文提升分类精度，节点特征需结合多维度信息，验证了图结构在大规模病理分析中的潜力。

Abstract: Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.

</details>


### [36] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: 本文提出了一种结合大型语言模型（LLM）和视频扩散模型的数据高效方法，用于生成具有时序结构的连续草图。通过分阶段训练和文本引导的笔画顺序控制，仅需少量人工草图即可生成高质量、可控的连续绘画过程。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型将草图视为静态图像，忽略其时序特性，导致无法真实还原创作过程。研究旨在通过建模笔画顺序与视觉细节的协同关系，实现可控且高质量的连续草图生成。

Method: 1) 将草图定义为动态视频轨迹，用文本指令约束笔画时序；2) 两阶段训练：先用合成数据学习笔画顺序（LLM规划），再用少量人工数据提炼视觉细节（视频扩散模型渲染）；3) 支持笔刷风格条件控制和自回归生成。

Result: 即使仅使用7个手绘样本训练，仍能生成遵循文本指令顺序、具毫米级笔画精度的连贯动画，且支持交互式协作绘画与风格迁移。

Conclusion: 证明LLM与视频扩散模型的协同可有效解决草图生成的时序建模难题，为小样本条件生成和可控创意工具开发提供了新范式。

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research](https://arxiv.org/abs/2602.15034)
*Houping Yue,Zixiang Di,Mei Jiang,Bingdong Li,Hao Hao,Yu Song,Bo Jiang,Aimin Zhou*

Main category: cs.CL

TL;DR: 本文提出了EduResearchBench，一个用于教育学术写作的细粒度评估平台，并展示了其通过任务分解和课程学习策略训练出的专用模型EduWrite，该模型在学术写作任务中显著优于更大的通用模型。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在社会科学中的应用缺乏针对学术写作流程的细粒度能力评估，现有基准测试过于侧重单次生成任务，无法反映复杂的学术研究需求。

Method: 构建基于层级化原子任务分解（HATD）框架的EduResearchBench平台，将研究流程分解为6个模块24个细粒度任务；采用课程学习策略分阶段训练教育学术写作模型，并使用11K高质量指令对数据集进行训练。

Result: 实验表明EduWrite（30B参数）在多项核心指标上显著超越72亿参数的通用模型，证明垂直领域中数据质量密度和层级化训练策略比参数规模更重要。

Conclusion: 细粒度任务分解评估框架和课程学习策略能有效提升学术写作模型的专业能力，领域专用模型在数据质量和训练设计优化下可超越参数规模更大的通用模型。

Abstract: While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.

</details>


### [38] [Indic-TunedLens: Interpreting Multilingual Models in Indian Languages](https://arxiv.org/abs/2602.15038)
*Mihir Panchal,Deeksha Varshney,Mamta,Asif Ekbal*

Main category: cs.CL

TL;DR: 本研究提出针对印度多语言大模型的解释框架Indic-TunedLens，通过共享仿射变换对齐隐藏状态，显著提升跨语言可解释性，尤其对低资源语言效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型解释工具以英语为中心，难以适配印度等语言多样性区域的多语言模型解释需求。

Method: 构建共享仿射变换框架，对每个目标语言调整隐藏层状态以匹配其输出分布，并在MMLU基准测试中验证效果。

Result: 在10种印度语言测试中，相比当前最优方法显著提升解释性能，低资源语言和形态复杂语言提升最明显。

Conclusion: 该框架为多语言Transformer的分层语义编码提供了重要见解，填补了跨语言可解释性研究的技术空白。

Abstract: Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/AnonymousAccountACL/IndicTunedLens. Our code is available at https://github.com/AnonymousAccountACL/IndicTunedLens.

</details>


### [39] [CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding](https://arxiv.org/abs/2602.15139)
*Tahir Hussain,Saddam Hussain Khan*

Main category: cs.CL

TL;DR: 研究提出了CGRA DeBERTa模型，专为提升经典伊斯兰文本问答准确率，通过概念引导机制与定制化架构，在42,591个圣训问答对上取得97.85 EM分，显著超越传统模型。


<details>
  <summary>Details</summary>
Motivation: 经典伊斯兰文本QA面临领域语义复杂、长上下文依赖及概念敏感推理挑战，传统BERT与DeBERTa性能有限，需更高效的领域适配模型解决教育与神学应用场景需求。

Method: 构建定制化DeBERTa变压器主干，集成轻量LoRA参数微调与残差概念门控机制，利用12核心术语的伊斯兰概念词典注入神学先验，并通过重要性加权注意力（缩放系数1.04-3.00）强化语义关键token的表征。

Result: 在布哈里及穆斯林圣训数据集测试显示：CGRA DeBERTa EM得分为97.85，较BERT（75.87）和DeBERTa（89.77）提升8.08，推理开销仅增加8%，模型效率与准确性达到平衡。

Conclusion: CGRA DeBERTa为高效可解释的圣训QA系统提供解决方案，兼顾计算效率与神学语义精准度，适用于生成具神学深度的教育内容，推动领域特定NLP技术发展。

Abstract: Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.

</details>


### [40] [AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking](https://arxiv.org/abs/2602.15190)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 论文提出了一种基于检索增强生成（RAG）和反向图像搜索（RIS）的低成本事实核查系统，在AVerImaTeC任务中取得第三名，单次核查成本仅0.013美元。


<details>
  <summary>Details</summary>
Motivation: 通过结合去年的RAG流程与RIS模块，设计一个高效且经济的系统以应对视觉事实核查任务，并提供可复制的实验基线。

Method: 采用三个解耦模块：基于相似度的文本检索、API调用的RIS图像检索及GPT5.1生成模块，仅需单次多模态LLM调用完成核查。

Result: 系统在保持竞争力的同时显著降低成本（单次约0.013美元），代码、提示语、向量库及成本分析已公开，实验证验证可行性。

Conclusion: 证明简单解耦架构在低成本事实核查中的有效性，建议作为后续研究的基准，并指出通过模块优化进一步提升的潜在方向。

Abstract: In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.

</details>


### [41] [Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement](https://arxiv.org/abs/2602.15312)
*Stephan Ludwig,Peter J. Danaher,Xiaohao Yang,Yu-Ting Lin,Ehsan Abedin,Dhruv Grewal,Lan Du*

Main category: cs.CL

TL;DR: 提出LX语言模型，用于分析消费者情感和评价，超越现有模型并揭示情感对购买行为的影响。


<details>
  <summary>Details</summary>
Motivation: 准确从非结构化文本中测量消费者情感和评价是营销研究的核心难题。

Method: 训练基于消费者文本数据的LX精细调优大语言模型，标注16种情绪和4种评价维度，对比GPT-4等模型进行测试。

Result: 在开放问答数据达81%宏F1值，第三方标注数据超95%准确率，发现部分情绪直接驱动购买行为。

Conclusion: 验证LX在消费者认知测量中的有效性，证明大语言模型可提升营销研究并提供可扩展分析工具。

Abstract: Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.

</details>


### [42] [Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory](https://arxiv.org/abs/2602.15313)
*Zihao Tang,Xin Yu,Ziyu Xiao,Zengxuan Wen,Zelin Li,Jiaxi Zhou,Hualei Wang,Haohua Wang,Haizhen Huang,Weiwei Deng,Feng Sun,Qi Zhang*

Main category: cs.CL

TL;DR: 本文提出Mnemis框架，结合System-1相似性检索和System-2全局选择机制，通过基础图和分层图实现语义与结构相关记忆混合检索，在长时记忆基准测试中取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 已有RAG/Graph-RAG方法依赖相似性检索（System-1），但在全局推理或全面信息覆盖任务中存在局限性，需补充结构化检索能力。

Method: 构建双路径内存框架：1) 基础图实现相似性检索；2) 分层语义图进行自顶向下语义层级遍历，组合两种机制实现混合检索。

Result: 基于GPT-4.1-mini模型，在LoCoMo和LongMemEval-S基准测试中分别获得93.9和91.6的最先进性能指标。

Conclusion: 混合式内存架构有效提升大模型记忆检索能力，尤其在需要全局推理和多级信息关联的任务中表现突出。

Abstract: AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.

</details>


### [43] [NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.15353)
*Rong Fu,Yang Li,Zeyu Zhang,Jiekai Wu,Yaohua Liu,Shuaishuai Cao,Yangchen Zeng,Yuhang Zhang,Xiaojing Du,Chuang Zhao,Kangning Cui,Simon Fong*

Main category: cs.CL

TL;DR: NeuroSymActive结合神经符号推理与主动探索控制器，提升知识图谱问答的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理知识密集型多跳推理任务时面临效率低下和模型调用频繁的问题，且难以在符号结构与神经模型间有效整合。

Method: 提出NeuroSymActive框架，融合可微神经-符号推理层与基于价值评估的主动探索策略，通过soft-unification模块、神经路径评估器和蒙特卡洛式路径扩展优先级机制协同工作。

Result: 在标准KGQA数据集上，方法相比基线模型减少48%的图检索次数和35%的模型调用，同时保持92.7%的准确率。

Conclusion: 该方法实现了知识推理精度与计算资源消耗的平衡，为神经符号系统整合提供了新范式。

Abstract: Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.

</details>


### [44] [Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language](https://arxiv.org/abs/2602.15378)
*Prathamesh Devadiga,Paras Chopra*

Main category: cs.CL

TL;DR: 通过结构化提示和辅助技术，大型语言模型可在无微调情况下实现对低资源语言Tulu的基本对话能力，降低跨语言词汇污染并提升语法准确性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在无训练数据的语言（如Tulu）中是否具备零样本对话能力，解决资源匮乏语言因数据缺乏导致的传统微调方法失效问题。

Method: 结合显式语法文档、负约束抑制邻近语言token、罗马化标准化、基于自博弈的合成数据生成，并在Gemini/GPT-4o/Llama 70B上进行评估。

Result: 实现85%语法准确率，跨语言词汇污染从80%降至5%，负约束提升12-18个百分点，语法文档效果因模型架构差异提升8-22个百分点。

Conclusion: 结构化提示结合多模态技术可为低资源语言在无数据状态下提供可行对话路径，负约束具普适有效性，但语法指导效果受模型架构影响，为欠资源语言应用提供范式参考。

Abstract: Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).

</details>


### [45] [The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems](https://arxiv.org/abs/2602.15382)
*Xiaoze Liu,Ruowang Zhang,Weichen Yu,Siheng Xiong,Liu He,Feijie Wu,Hoin Jung,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.CL

TL;DR: 该论文提出Vision Wormhole框架，通过视觉语言模型的视觉接口实现无需文本的通信，解决多智能体系统因离散文本传输导致的效率低下和信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统受限于离散文本通信的高运行时开销和信息量化丢失，而现有潜在状态传输方法需依赖同构架构或成对学习的翻译器，难以扩展到异构模型族。

Method: 1) 构建通用视觉编解码器将异构推理轨迹映射至共享连续潜在空间；2) 采用星型拓扑结构将配对对齐复杂度从O(N²)降至O(N)；3) 通过无标签的师生蒸馏目标对齐视觉通道与文本通道的推理模式。

Result: 在Qwen-VL、Gemma等异构模型族上的实验证明，相比传统文本通信，端到端壁钟时间显著减少，同时保持了与标准文本通信相当的推理保真度。

Conclusion: Vision Wormhole通过将视觉编码器抽象为通用通信端口，有效解决了异构多智能体系统的高带宽通信难题，为模块化系统设计提供了新范式。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas

</details>


### [46] [Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs](https://arxiv.org/abs/2602.15436)
*Joonatan Laato,Veera Schroderus,Jenna Kanerva,Jenni Kauppi,Virpi Lummaa,Filip Ginter*

Main category: cs.CL

TL;DR: The paper presents a method to categorize social participation data from historical interviews using a framework and large language models, enabling scalable analysis of social integration post-WWII in Finland.


<details>
  <summary>Details</summary>
Motivation: Historical text archives lack structured data needed for quantitative analysis of social life questions posed by historians and sociologists, requiring methods to transform unstructured text into analyzable categories.

Method: Developed a categorization framework for activity/organization attributes (type, sociality, regularity, physical demand), created gold-standard annotations, and tested LLMs with a multi-run voting approach for scalable classification.

Result: Voting-aggregated open-weight LLM outputs closely matched expert judgments, enabling full categorization of 350K entities into a structured resource for analyzing social participation patterns.

Conclusion: Combining annotation frameworks, LLMs, and ensemble methods enables large-scale quantitative analysis of historical social participation data, offering new insights into post-conflict social integration processes.

Abstract: Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly.
  We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.

</details>


### [47] [TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models](https://arxiv.org/abs/2602.15449)
*Chansung Park,Juyong Jiang,Fan Wang,Sayak Paul,Jiasi Shen,Jing Tang,Jianguo Li*

Main category: cs.CL

TL;DR: This paper introduces TAROT, a curriculum-based reinforcement fine-tuning method for code generation. It addresses LLMs' challenges in synthesizing robust code by designing adaptive test suites and decoupling curriculum progression from reward signals, improving functional correctness through capability-conditioned training.


<details>
  <summary>Details</summary>
Motivation: Existing RFT methods for code generation ignore the diverse difficulty levels in test cases, creating imbalanced reward signals that bias gradient updates. This work aims to systematically address this by designing difficulty-aware curricula that enhance reasoning capability development in LLMs.

Method: TAROT creates four-tier test suites (basic/intermediate/complex/edge) for controlled difficulty progression. It decouples curriculum learning from raw reward scores, instead selecting optimal training policies based on model capability through adaptive evaluation metrics. This enables dynamic adjustment between easy-to-hard or hard-first learning patterns.

Result: Experiments show curriculum effectiveness correlates with model capability: less capable models benefit most from gradual progression, while advanced models excel with hard-first approaches. TAROT achieves stable optimization and measurable improvements in code correctness/robustness through its adaptive framework.

Conclusion: TAROT provides a reproducible method for capability-adaptive curriculum design in code generation. By aligning training difficulty with model maturity, it systematically enhances LLMs' ability to produce functionally correct and robust code, particularly benefiting models at different capability levels through customized progression strategies.

Abstract: Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.

</details>


### [48] [In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations](https://arxiv.org/abs/2602.15456)
*Mohammad Aflah Khan,Mahsa Amani,Soumi Das,Bishwamittra Ghosh,Qinyuan Wu,Krishna P. Gummadi,Manish Gupta,Abhilasha Ravichander*

Main category: cs.CL

TL;DR: 本文发现大语言模型（LLMs）在筛选信息时存在系统性来源偏好，这些偏好独立于内容本身且受上下文影响，可能导致信息呈现偏差（如新闻推荐的左倾偏见），建议研究其根源并建立用户控制机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLMs生成信息的偏见，但缺乏对LLMs主动筛选信息过程中来源偏好（source preferences）的研究，尤其是当信息被标注明确来源（如出版社、期刊）时的选择性呈现问题。

Method: 通过控制实验测试12个来自6家厂商的LLMs，涵盖合成任务和现实场景，系统性分析模型对信息源（如不同媒体、期刊）的优先选择模式及其影响因素（如上下文框架、显式提示干预）。

Result: 1) 多数LLMs存在显著且可预测的信息源偏好；2) 偏好受上下文表征影响，且可能覆盖内容质量本身的权重；3) 显式消除偏见的提示无法阻止偏好持续存在；4) 发现左倾信息推荐偏好的可能来源。

Conclusion: LLMs的信息筛选机制存在潜在来源偏好风险，需深入研究其形成机制（如训练数据、框架效应），并设计提高透明度与用户控制权的技术干预措施以降低偏差影响。

Abstract: Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.

</details>


### [49] [LuxMT Technical Report](https://arxiv.org/abs/2602.15506)
*Nils Rehlinger*

Main category: cs.CL

TL;DR: LuxMT基于Gemma 3 27B开发，专精卢森堡语多语言翻译任务，通过构建新基准测试与过滤技术实现性能超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 低资源语言卢森堡语缺乏高效翻译方案，现有语料库需优化清洗，人工评估的多语言基准测试数据缺失。

Method: 联合微调Gemini 3 27B模型，使用LuxAlign新闻语料+议会记录+Google翻译数据，应用LuxEmbedder句向量过滤低等效语料段。

Result: 卢森堡语至德语翻译超预期提升（零样本迁移），LuxEmbedder与传统指标强相关，翻译质量经人工验证显著优化。

Conclusion: 多模态数据增强+动态过滤有效提升翻译质量，LuxEmbedder展现指标潜力，建议持续验证其跨语言适用性。

Abstract: We introduce LuxMT, a machine translation system based on Gemma 3 27B and fine-tuned for translation from Luxembourgish (LB) into French (FR) and English (EN). To assess translation performance, we construct a novel benchmark covering LB-FR, LB-EN, and LB-FR using human-translated data from Luci, a tourist magazine about Luxembourg. Training data stems from LuxAlign, a parallel corpus of multilingual Luxembourgish news articles, and LB parliamentary transcripts augmented with Google Translate. We filter the data using LuxEmbedder, LB sentence embeddings, to remove low-equivalence segment-pairs. Overall, LuxMT's results suggest strong improvements over the Gemma 3 baseline, even for translating LB to German (DE), despite the training data not containing any DE. We also explore LuxEmbedder's potential to be used as a quality estimation metric and find strong correlations with other reference-based metrics. However, we call for further research to fully assess the metric's utility and advise using it with caution.

</details>


### [50] [Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination](https://arxiv.org/abs/2602.15509)
*Xiangyan Chen,Yujian Gan,Matthew Purver*

Main category: cs.CL

TL;DR: 本研究提出Fine-Refine框架，通过细粒度分解对话响应、利用外部知识验证事实并迭代纠错，在保留对话流畅性的同时提升事实性，对话事实评分提升最高达7.63分。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话系统中易产生事实性错误（幻觉），现有方法仅在响应层面修正，忽略单条响应中可能存在多个可验证/不可验证事实的特性，需开发更精细的事实纠错机制。

Method: 1) 将对话响应分解为原子单元；2) 基于外部知识验证每个单元的准确性；3) 通过困惑度评估流畅性；4) 迭代修正局部错误，实现多层次优化。

Result: 在HybriDialogue和OpendialKG数据集上的实验表明：事实准确性提升显著（最高7.63分），事实覆盖率（NEI比例）优化，但对话质量存在轻微下降（流畅度与事实性的权衡）。

Conclusion: Fine-Refine通过细粒度纠错有效解决对话系统的事实性问题，验证了事实性与生成质量之间的平衡关系，为低错误率对话系统提供新思路。

Abstract: The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.

</details>


### [51] [ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling](https://arxiv.org/abs/2602.15537)
*Nicol Visser,Simon Malan,Danel Slabbert,Herman Kamper*

Main category: cs.CL

TL;DR: ZeroSyl是一种无需训练的语音语言模型音节分割方法，基于冻结的WavLM模型提取音节边界和嵌入，简化了传统多阶段流程。


<details>
  <summary>Details</summary>
Motivation: 纯语音语言模型需解决自监督语音编码器离散符号序列过长的问题，但现有音节单元方法（如Sylber/SyllableLM）依赖复杂多阶段训练，而ZeroSyl旨在通过简单方法直接从预训练模型提取音节单元。

Method: 冻结WavLM模型中间层特征的L2范数检测音节边界，对分割结果进行平均池化、K均值离散化生成音节单元，并用于训练语言模型。

Result: 在音节分割性能与基线模型相当，但基于ZeroSyl的音节单元在语言模型训练中于词汇、句法和叙事任务均超越现有方法，且更精细单元适合词法任务而音节单元更优句法建模。

Conclusion: ZeroSyl通过免训练策略高效提取音节单元，在降低复杂性的同时实现了语言模型训练的跨任务性能权衡。

Abstract: Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.

</details>


### [52] [Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite](https://arxiv.org/abs/2602.15540)
*Tim Fischer,Chris Biemann*

Main category: cs.CL

TL;DR: 本论文介绍了Perspectives，一个交互式话语分析工具套件的扩展，用于帮助数字人文学者探索和组织大型非结构化文档集合。


<details>
  <summary>Details</summary>
Motivation: 传统处理大型非结构化文本文档的方法效率低，需要人工介入进行精细调整，而现有工具缺乏灵活性。研究旨在通过结合人类专家知识与自动化嵌入模型，提升数字人文研究中的文档分析效率。

Method: 开发Perspectives工具，包含：1) 基于文档重写提示和指令嵌入的分析视角定义模块；2) 交互式聚类细化工具；3) 嵌入模型微调机制。用户可通过调整嵌入输入和聚类参数，动态优化文档组织结果。

Result: 演示了典型工作流应用效果：通过交互式文档地图，研究者可直观发现文本集合中的主题、情感等潜在分类，并生成结构化数据集用于后续分析，显著降低人工预处理成本。

Conclusion: Perspectives通过嵌入式人机协作机制，提供了更灵活、可解释的文档分析框架，有效支持数字人文领域的大规模文本探索需求。

Abstract: This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.

</details>


### [53] [jina-embeddings-v5-text: Task-Targeted Embedding Distillation](https://arxiv.org/abs/2602.15547)
*Mohammad Kalim Akram,Saba Sturua,Nastia Havriushenko,Quentin Herreros,Michael Günther,Maximilian Werk,Han Xiao*

Main category: cs.CL

TL;DR: 提出结合模型蒸馏与任务专用对比损失的训练方案，训练出支持多语言长文本的高性能小型嵌入模型jina-v5。


<details>
  <summary>Details</summary>
Motivation: 解决现有小规模嵌入模型训练方法（纯对比学习/纯蒸馏）性能不足的问题，探索两种范式融合的优化空间

Method: 采用分阶段训练策略：1）通过教师模型进行知识蒸馏预训练 2）在特定任务中引入动态对比损失函数进行参数微调

Result: jina-embeddings-v5在相同参数规模下超越SOTA水平，支持32K tokens长文本处理，经二值量化和截断后保持>95%性能保持率

Conclusion: 模型蒸馏与对比学习的协同训练能有效提升小型嵌入模型性能边界，开源模型为社区提供低资源场景下的优质选择

Abstract: Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.

</details>


### [54] [Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL](https://arxiv.org/abs/2602.15564)
*Yihan Wang,Peiyu Liu,Runyu Chen,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出SquRL框架，通过强化学习实现动态工作流程构建，解决Text-to-SQL任务中的分布外场景适应性问题。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL任务依赖静态工作流程导致实际应用中扩展性不足，需探索动态政策以适应复杂查询场景。

Method: 构建基于强化学习的SquRL框架，包含规则奖励函数、动态actor掩码机制及伪奖励训练策略，实现推理时自适应工作流程构建。

Result: 理论证明动态政策优于静态方法，实验显示SquRL在基准数据集的复杂/分布外查询中显著优于最佳静态方法（提升率达15.6%）。

Conclusion: 通过证明异构工作流程的动态组合能突破静态架构限制，验证了强化学习框架在复杂任务自适应推理中的有效性。

Abstract: Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL

</details>


### [55] [STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens](https://arxiv.org/abs/2602.15620)
*Shiqi Liu,Zeyu He,Guojian Zhan,Letian Tao,Zhilong Zheng,Jiang Wu,Yinuo Wang,Yang Guan,Kehua Sheng,Bo Zhang,Keqiang Li,Jingliang Duan,Shengbo Eben Li*

Main category: cs.CL

TL;DR: This paper proposes 	extbf{Spurious-Token-Aware Policy Optimization (STAPO)} to address training instability in reinforcement learning fine-tuning for large language models by identifying and masking harmful spurious tokens that cause gradient explosions.


<details>
  <summary>Details</summary>
Motivation: Despite RL's success in enhancing large model reasoning, existing methods suffer from late-stage performance collapse due to spurious tokens (0.01% of tokens) that receive abnormal reward amplification despite their minimal impact on reasoning.

Method: The authors prove the negative correlation between token probability/local entropy and gradient magnitude, then design STAPO to selectively mask updates from spurious tokens while renormalizing loss over valid tokens.

Result: On 6 math reasoning benchmarks, STAPO achieves 7.13	extsuperscript{a} higher performance and significantly better entropy stability than GRPO, 20-Entropy, and JustRL across Qwen 1.7B/8B/14B models.

Conclusion: Training instability in RL fine-tuning originates from rare spurious tokens, and STAPO effectively improves both stability and reasoning quality in large-scale model refining.

Abstract: Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% over GRPO, 20-Entropy and JustRL.

</details>


### [56] [LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models](https://arxiv.org/abs/2602.15675)
*Ahmed Khaled Khamis,Hesham Ali*

Main category: cs.CL

TL;DR: 本文介绍NileTTS：首个面向埃及阿拉伯语的公开语音合成数据集，包含38小时多领域语音及文本数据，采用大语言模型生成内容结合合成语音技术建立，并提供可复现的合成数据管道与微调后的开源模型。


<details>
  <summary>Details</summary>
Motivation: 埃及阿拉伯语作为阿拉伯世界最广泛使用的方言，长期面临语音资源匮乏问题，现有阿拉伯语TTS研究主要集中在MSA和海湾方言，导致该方言在医疗、商业等应用场景中存在服务空白。

Method: 1) 通过大语言模型生成多领域埃及阿拉伯语文本
2) 使用语音合成工具将其转为自然语音
3) 自动转录结合说话人分离技术
4) 人工质量审查保证数据可靠性
5) 微调XTTS v2模型并在方言语音数据上评估

Result: 1) 发布首个埃及阿拉伯语TTS数据集（2说话人/38小时）
2) 开发自动化合成数据生成系统
3) 预训练模型在方言语音合成中优于基线模型
4) 所有数据与代码均已开源

Conclusion: 本研究填补低资源方言语音合成空白，通过合成数据生成方法为资源匮乏语言提供解决方案，开源资源将推动埃及阿拉伯语语音技术发展，为其他方言合成系统建立范式。

Abstract: Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.

</details>


### [57] [Revisiting Northrop Frye's Four Myths Theory with Large Language Models](https://arxiv.org/abs/2602.15678)
*Edirlei Soares de Lima,Marco A. Casanova,Antonio L. Furtado*

Main category: cs.CL

TL;DR: 本论文结合诺思罗普·弗莱叙事类型理论与荣格原型理论，提出一个角色功能分析框架，并通过大规模语言模型（LLMs）验证了该框架在叙事学计算分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 弗莱的叙事类型理论影响深远，但现有计算研究多聚焦叙事模式而非角色功能；该研究旨在填补角色功能在计算叙事学中的理论与验证空白。

Method: 基于荣格原型理论提取四种普遍角色功能（主角、导师、反派、同伴），并根据典型文本细化为十六种类型专属角色；通过6种SOTA LLMs对40部文本的160个正样本和30个负样本进行角色-功能对应验证。

Result: LLMs平均平衡准确率达82.5%（Fleiss κ=0.600），验证框架有效性；各体裁表现差异反映叙事特性（如讽刺体裁的颠覆性原型使用），角色级别最低准确率52.5%（讽刺反派）与最高99.2%（浪漫伴侣）并存。

Conclusion: 该角色功能框架验证了LLMs在计算叙事学的潜力，为叙事生成与交互故事创作技术奠定了方法论基础。

Abstract: Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $κ$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.

</details>


### [58] [Rethinking Metrics for Lexical Semantic Change Detection](https://arxiv.org/abs/2602.15716)
*Roksana Goworek,Haim Dubossarsky*

Main category: cs.CL

TL;DR: 该论文提出了用于词汇语义变化检测的新指标AMD和SAMD，实验证明AMD在降维和非专用编码器下表现更稳健，而SAMD在专用编码器下效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前语义变化检测主要依赖APD和PRT两类指标，但其性能受限于表示空间和编码器类型，需要探索更鲁棒的替代方法。

Method: 提出基于时间切片间词用法局部对应关系的AMD指标及其对称版本SAMD，通过跨语言、模型和表示空间的实验验证其有效性。

Result: AMD在降维和非专用编码器下显著优于现有方法，SAMD在专业编码器场景下效果最佳，表明新型指标可提升语义变化检测性能。

Conclusion: 建议语义变化检测领域突破APD和PRT的限制，根据编码器类型选择AMD或SAMD作为更稳健的替代方案。

Abstract: Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.

</details>


### [59] [Causal Effect Estimation with Latent Textual Treatments](https://arxiv.org/abs/2602.15730)
*Omri Feldman,Amar Venugopal,Jann Spiess,Amir Feder*

Main category: cs.CL

TL;DR: 本论文提出一种基于稀疏自编码器和协变量残差化的新方法，用于文本作为处理变量的因果效应估计，通过端到端的流水线解决传统方法中混杂信息导致的估计偏差问题，并实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 文本作为处理变量进行因果推断时，传统方法因文本同时包含处理特征与协变量信息而产生显著偏差。需要一种能分离混杂因素并稳健估计因果效应的新框架。

Method: 提出分两阶段流程：1) 使用稀疏自编码器提取并解耦文本特征，生成可控干预变量；2) 通过协变量残差化消除混杂因素，结合稳健统计方法估计因果效应。配套完整的假设生成与验证步骤。

Result: 实验证明该流程能有效诱导目标特征变化（如文本情感强度变化达0.83倍标准差），将因果效应估计误差降低至传统方法的12%，显著提升估计可靠性。

Conclusion: 为文本因果分析提供了可推广的框架，系统化解构了文本数据的因果效应异质性，为后续实际应用（如政策文本设计、教育材料优化）提供了可解释的干预工具。

Abstract: Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.

</details>


### [60] [Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac](https://arxiv.org/abs/2602.15753)
*Chahan Vidal-Gorène,Bastien Kindt,Florian Cafiero*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLM）在低资源语言词形还原和词性标注任务中的少样本和零样本能力，重点关注古代希腊语、古典亚美尼亚语、古格鲁吉亚语和叙利亚语四种语言。


<details>
  <summary>Details</summary>
Motivation: 低资源语言因数据稀缺和复杂形态结构在自然语言处理任务中长期面临挑战，研究如何利用无需微调的LLM解决这些问题具有重要现实意义。

Method: 通过构建包含对齐训练数据和域外测试语料的新基准数据集，对比GPT-4变体、Mistral模型与专用RNN基线模型PIE在少样本/零样本设置下的表现。

Result: LLM在多数语言的少样本任务中表现优于或接近基线模型，但在复杂形态和非拉丁文字语言（如古格鲁吉亚语）中仍存在显著挑战，证明LLM可作为数据匮乏场景的有效标注辅助工具。

Conclusion: 未经微调的LLM为低资源语言处理提供了可行起点，尤其适用于数据缺失场景，但需进一步优化以应对形态复杂性和文字系统差异。

Abstract: Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.

</details>


### [61] [Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos](https://arxiv.org/abs/2602.15757)
*Laura De Grazia,Danae Sánchez Villegas,Desmond Elliott,Mireia Farrús,Mariona Taulé*

Main category: cs.CL

TL;DR: 本文提出了FineMuSe，一个包含细粒度标注的西语多模态性别歧视检测数据集，并评估了大型语言模型在识别复杂性别歧视类型时的表现及局限性。


<details>
  <summary>Details</summary>
Motivation: 现有性别歧视检测工具多采用粗粒度二分类，难以捕捉讽刺、幽默等隐性表达形式，需通过多模态细粒度标注提升检测精度。

Method: 构建包含图像与文本跨模态数据集，提出包含性别歧视类型/非歧视类别/修辞手法的分层分类体系，系统性评估多模态LLM在二分类和细粒度分类任务中的表现。

Result: 多模态大模型在识别人类标注的隐性性别歧视内容上与人工表现相当，但在视觉线索主导的复合性别歧视类型识别上存在明显缺陷。

Conclusion: 细粒度多模态标注能有效提升性别歧视检测能力，但需改进模型对视觉隐喻和多模态协同分析能力，未来研究应强化跨模态特征融合机制。

Abstract: Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.

</details>


### [62] [ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models](https://arxiv.org/abs/2602.15758)
*Manav Nitin Kapadnis,Lawanya Baghel,Atharva Naik,Carolyn Rosé*

Main category: cs.CL

TL;DR: 论文提出了ChartEditBench基准测试，揭示了多模态大语言模型在多轮交互式图表编辑中的表现局限性，尤其是在数据相关操作上的错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单次图表生成，但真实场景中用户依赖多轮交互优化可视化。该研究旨在填补MLLMs在持续性、上下文感知的图表编辑能力评估空白。

Method: 构建了包含5000个渐进式编辑链的ChartEditBench数据集，开发了融合执行保真度验证、像素级视觉相似度分析和逻辑代码校验的混合评估框架。

Result: SOTA级MLLM在多轮编辑中出现显著性能退化，风格化编辑成功率高但数据关联操作常失败，存在上下文维护和错误累积的瓶颈。

Conclusion: ChartEditBench为多模态编程提供了更真实的测试环境，证明现有模型在连续交互场景下的根本性缺陷，为未来研究指明方向。

Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.

</details>


### [63] [*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation](https://arxiv.org/abs/2602.15778)
*Quentin Lemesle,Léane Jourdan,Daisy Munson,Pierre Alain,Jonathan Chevelu,Arnaud Delhay,Damien Lolive*

Main category: cs.CL

TL;DR: *-PLUIE是一种基于ParaPLUIE的改进型LLM-judge指标，通过任务特定提示词优化文本质量评估效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-judge方法计算成本高且依赖后处理，ParaPLUIE虽通过困惑度解决部分问题但缺乏灵活性，需开发更高效的个性化评估方案。

Method: 基于ParaPLUIE设计任务定制化提示词模板（*-PLUIE），在无需生成文本的前提下直接评估Yes/No类问题的置信度，并测试其与人工评分的相关性。

Result: *-PLUIE在保持低计算成本的同时，与人工评分的皮尔逊相关系数达0.89，显著优于传统BLEU、ROUGE等指标。

Conclusion: 通过任务适配的提示词工程，成功提升了无生成式LLM-judge指标的准确性与普适性，为自动化文本评估提供新范式。

Abstract: Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.

</details>


### [64] [Avey-B](https://arxiv.org/abs/2602.15814)
*Devang Acharya,Mohammad Hammoud*

Main category: cs.CL

TL;DR: 提出了基于Avey架构的改进编码器，结合自回归无注意力机制与结构创新，在工业NLP场景下实现更高效的长文本处理。


<details>
  <summary>Details</summary>
Motivation: BERT等双向编码器依赖自注意力机制带来的高质量上下文建模，但在长文本和资源受限场景下计算效率不足。需要一种兼顾性能与效率的新型架构。

Method: 对Avey架构进行编码器范式重构，核心创新包括：1) 解耦静态参数（结构约束）与动态参数（特征生成） 2) 稳定性导向的归一化设计 3) 神经压缩机制提升计算密度。

Result: 新型架构在四个主流Transformer编码器（如BERT、RoBERTa）上实现token分类和检索任务性能超越，长文本扩展效率提升2.3倍，内存占用降低37%。

Conclusion: 证明了无注意力架构在工业NLP场景中的可行性，通过架构创新平衡了自回归模型的计算效率与双向编码的表达能力。

Abstract: Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.

</details>
