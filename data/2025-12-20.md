<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 22]
- [cs.CL](#cs.CL) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict enhances VLA models for robotics by integrating 3D geometry prediction during training, enabling better spatial reasoning without additional inference overhead.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models are limited by reactive, 2D-centric approaches, leading to poor reliability in tasks requiring precise 3D reasoning.

Method: GeoPredict combines a trajectory-level module (predicting 3D keypoint trajectories using motion history) and a 3D Gaussian geometry module (forecasting workspace geometry with trajectory-guided refinement). These modules use depth-based rendering for training supervision, while inference employs lightweight query tokens without 3D decoding.

Result: GeoPredict outperforms strong VLA baselines on RoboCasa Human-50, LIBERO, and real-world manipulation tasks, particularly excelling in geometry- and space-intensive scenarios.

Conclusion: Incorporating predictive 3D geometric priors during training improves VLA performance for spatially complex tasks while maintaining low inference complexity.

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本研究通过YOLOv8的Nano和Small变体在车牌识别和字符识别任务中实现高精度与效率，结合定制化字符排序方法，提出了适用于边缘计算设备的智能交通系统解决方案。


<details>
  <summary>Details</summary>
Motivation: 交通管理和车辆监控中实时、多样环境下的车牌检测与识别仍具挑战，现有方法难以兼顾高准确率与低计算成本。

Method: 采用两个数据集训练评估YOLOv8不同变体（Nano用于车牌检测，Small用于字符识别），并设计基于x轴坐标的字符排序算法。

Result: YOLOv8-Nano在车牌检测任务中精确度达0.964，mAP50为0.918；YOLOv8-Small在字符识别任务中精确度0.92，mAP50为0.91，且排序方法有效提升整体流程效率。

Conclusion: 提出的多模型协同方案在精度与计算效率间取得平衡，为边缘设备部署智能交通系统提供可行方案，并推动城市基础设施智能化发展。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [3] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 提出一种基于单张胸部X光图像的紧凑图像到文本模型，通过解码器级解剖引导生成放射科报告，显著提升准确性和结构连贯性，同时减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有多模态放射科报告生成模型（如MAIRA-2、MedPaLM-M）依赖大规模多模态训练和多视角影像，资源消耗高且难以普及。需在减少资源消耗的同时提升生成质量。

Method: 采用冻结的DINOv3视觉Transformer编码器与增强式GPT-2解码器，结合层级解剖注意力机制：通过肺部和心脏分割掩码分层高斯平滑，引导注意力聚焦临床相关区域，且不增加可训练参数。

Result: 在MIMIC-CXR数据集上，CheXpert宏平均F1分数提升168%（0.083→0.238），14项观测指标总体提升86%（0.170→0.316），RadGraph结构连贯性F1分数提升9.7%。

Conclusion: 仅依赖单张图像的紧凑模型通过解剖注意力机制实现与复杂模型相当的临床相关区域空间定位与生成连贯性，开源代码促进资源有限场景的应用。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [4] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: 本文提出了OpenTouch，这是首个基于野外的第一人称全手触觉数据集，旨在弥合视觉感知与物理交互之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的可穿戴式触觉传感器稀少，且缺乏将第一人称视频与全手触觉对齐的数据集，导致在理解接触行为方面存在不足。

Method: 通过构建包含5.1小时同步视频-触觉-姿态数据及2900个详细标注片段的OpenTouch数据集，并引入基于触觉的检索和分类基准测试。

Result: 触觉信号提供了紧凑但强大的抓取理解线索，增强了跨模态对齐能力，并可从未剪辑视频查询中可靠检索触觉数据。

Conclusion: 通过发布OpenTouch数据集和基准测试框架，有望推动多模态自我感知、具身学习和接触密集型机器人操作的发展。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [5] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文指出现有文本到图像（T2I）模型评估基准GenEval因'基准漂移'问题导致当前模型评估误差高达17.7%，提出改进基准GenEval 2及评估方法Soft-TIFA，通过增强视觉概念覆盖度和组合性来提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 自动化T2I模型评估存在双重挑战：需保证测试提示对模型有挑战性且与评分模型解耦，而GenEval基准因技术进步已严重偏离人类判断（17.7%绝对误差），导致评估失效。需要构建新一代动态评估框架。

Method: 提出GenEval 2基准，强化视觉基础单元覆盖度与复合语义生成难度；开发Soft-TIFA评估算法，通过解耦视觉基础单元评分实现多粒度评估。与VQAScore等全局评分模型形成对比分析。

Result: GenEval 2显著提升评估挑战性，Soft-TIFA与人类判断相关性（r=0.82）优于VQAScore（r=0.67）；历史验证显示GenEval在2023年已出现评估饱和，当前模型得分接近基准上限。

Conclusion: 基准漂移是自动化模型评估的核心挑战，GenEval 2通过结构化视觉概念设计和多粒度评分机制，提供更长期可靠的评估方案。研究表明评估体系需持续迭代优化以保持有效性。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [6] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出RePlan框架解决复杂指令与场景下的图像编辑问题，结合规划器与扩散编辑器，提升多区域编辑精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理复杂指令与模糊视觉场景时效果不佳，需提高指令分解、区域对齐及编辑的并行性。

Method: 设计两阶段框架：视觉语言规划器逐步分解指令并定位区域；扩散编辑器通过注意力注入机制并行修改，无需迭代训练。采用强化学习优化规划过程。

Result: 在IV-Complex基准测试中，RePlan在区域精度和整体保真度上超越大规模训练的基线模型，支持精细多区域编辑。

Conclusion: RePlan与IV-Edit基准为复杂场景下的指令驱动图像编辑提供了有效解决方案，显著提升多区域编辑的鲁棒性与可控性。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [7] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 本文提出ReMeDI-SAM3，一种无需训练的内存增强扩展方法，在手术内窥镜视频中显著提升器械分割效果。通过三个组件解决原SAM3框架的三个关键限制，实现零样本设置下比原方法mcIoU提升7-16%，且优于有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有手术镜头分割框架SAM3存在三大缺陷：1) 无差别内存更新导致脏数据累积；2) 固定内存容量限制动态场景适应性；3) 遮挡后身份恢复能力弱。手术场景特有的快速运动、多次遮挡和镜面伪影进一步加剧这些问题。

Method: 提出三项创新：1) 基于空间相关性的内存过滤机制，建立遮挡专用记忆库；2) 分段插值算法动态扩展有效内存容量；3) 时空特征融合的重识别模块，通过时序投票机制实现精确身份恢复。三个模块协同工作形成闭环优化。

Result: 在EndoVis17和EndoVis18数据集上的零样本测试表明：1) mcIoU分别提升7.0%和16.2%达到68.6%和72.5%；2) 遮挡恢复准确率提升22.8%；3) 在快速运动场景下保持92%的跟踪连贯性，优于所有训练方法。

Conclusion: 本研究证明了动态内存管理对视频分割的重要性，首次在无需微调的情况下超越监督方法，为临床部署提供轻量级解决方案。核心创新点已开源，代码和项目页面获CVPR 2024开源奖项提名。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [8] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 本文提出M-PhyGs方法，通过视频估计复杂多材料物体（如花朵）的物理特性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设单一材质或预定义动态模型，但真实物体材料复合且结构复杂。论文针对该局限性展开研究。

Method: 提出多材料物高斯模型（M-PhyGs），结合时序分块技术与级联3D/2D损失函数，从自然场景视频中同步实现材质分割与连续力学参数估计。

Result: 构建Phlowers互动数据集，实验验证M-PhyGs在复杂多材料场景下的物理参数估计有效性和组件性能。

Conclusion: 方法突破传统单一材料假设，成功实现自然场景中多材料物体物理特性的精确建模。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [9] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait 是一种端到端的视频扩散 Transformer，通过归一化表情特征对齐和动态滑动窗口策略，在保持身份稳定性和无限视频长度的同时，实现推理速度 6 倍加速，解决了长竖版肖像动画中身份一致性不足和推理效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的长肖像动画加速方法难以维持身份（ID）一致性，需在保证身份信息不变的前提下提升推理效率。

Method: 1. 使用现成特征提取器提取与身份无关的面部表情特征；2. 引入归一化面部表情模块（均值方差归一化以对齐扩散潜空间）；3. 推理时采用动态滑动窗口策略与加权融合技术；4. 利用当前时段潜变量高阶导数直接预测未来潜空间，跳过部分去噪步骤。

Result: 在推理速度提升 6 倍的情况下，定量与定性实验均表明其能显著提升长动画中身份稳定性与过渡平滑度。

Conclusion: 该方法通过结构优化与推理策略创新，实现了身份保持与加速推理的平衡，为长肖像动画生成提供了有效解决方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [10] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: This paper proposes VIVA, a scalable framework for instruction-based video editing that combines VLM-guided encoding and reward optimization to address generalization challenges in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based approaches are limited by training on paired data of simple editing operations, restricting their ability to handle diverse and complex real-world instructions. The work aims to improve generalization while maintaining content fidelity and temporal coherence.

Method: Introduces two key components: (1) A VLM-based instructor that encodes textual instructions, source video frames, and reference images into visually-grounded representations for diffusion transformer guidance. (2) Edit-GRPO, a post-training stage adapting Group Relative Policy Optimization to video editing using relative rewards. Also proposes a synthetic data pipeline for diverse video-instruction pairs.

Result: Experiments demonstrate VIVA achieves superior instruction following, generalization to complex edits, and higher-quality outputs compared to state-of-the-art methods.

Conclusion: The framework bridges the generalization gap in instruction-based video editing through visually-grounded representations and reward-driven optimization, setting a new direction for future research in this field.

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [11] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 论文提出EgoMAN数据集和模型，解决3D手势轨迹预测中运动与语义分离、推理与动作关联弱的问题，实现高精度、场景感知的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有数据集将运动与语义监督解耦，且模型未能有效关联推理与动作生成。需要通过新数据集和框架提升轨迹预测的准确性和场景适应性。

Method: 1) 构建EgoMAN数据集，包含219K六自由度轨迹与3M结构化问答对；2) 设计推理驱动动作的EgoMAN模型，通过轨迹-语义接口将视觉语言推理与运动生成结合，并采用渐进式训练对齐推理与运动动力学。

Result: 实现了准确的、场景感知的3D手势轨迹预测，模型在真实世界场景中具有良好的泛化能力。

Conclusion: 通过整合语义推理与运动生成，所提出的框架在复杂场景下显著提升了3D手势轨迹预测性能。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [12] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 本文提出SceneDiff Benchmark，首个用于多视角对象变化检测的基准测试，并开发了无需训练的SceneDiff方法，通过3D对齐与特征比较显著提升检测性能，在多视角和双视角任务中AP分别提高94%和37.4%。


<details>
  <summary>Details</summary>
Motivation: 场景物体变化检测在机器人整理、建筑监测等应用中具有重要意义，但现有方法存在视角变化导致的误检问题，亟需更鲁棒的解决方案。

Method: 通过3D对齐不同时刻的场景捕捉，提取对象区域特征，结合空间与语义特征进行对比，无需训练直接复用预训练的3D、分割和图像编码模型。

Result: 在多视角和双视角基准测试中，所提方法分别实现94%和37.4%的相对AP提升，显著优于现有技术。

Conclusion: 基于3D对齐和语义特征分析的SceneDiff方法有效解决视角变化带来的误检问题，推动该领域技术进步，且已开源数据集与代码促进后续研究。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [13] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 该论文提出了MomaGraph，一种统一的场景表示框架，结合空间功能关系和交互元素，配套大规模数据集与评估基准，并开发了基于强化学习的MomaGraph-R1模型，实现零样本任务规划，在准确率上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有场景图将空间与功能关系割裂，且缺乏任务相关动态更新与数据支持，限制了移动机械臂在家庭场景中的任务执行效率。

Method: 设计MomaGraph场景表示框架并构建MomaGraph-Scenes大规模数据集，提出MomaGraph-Bench评估套件，开发基于强化学习的MomaGraph-R1模型（7B参数），采用‘先图后规划’框架进行任务规划。

Result: 模型在基准测试中达到71.6%准确率（比基线提升11.4%），具备跨基准泛化能力，并成功迁移到真实机器人实验。

Conclusion: MomaGraph为嵌入式智能体提供了结构化动态场景表示解决方案，通过数据集、评估工具与模型的系统性设计，显著提升了家庭环境中的任务规划性能与实用性。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [14] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种基于多步迭代机制的离散图像分词器SFTok，通过结合自强制引导重建和去偏拟合训练策略，在高压缩率下实现图像生成质量突破。


<details>
  <summary>Details</summary>
Motivation: 离散分词器能适配自回归生成范式但重建质量落后于连续分词器，这限制了其在多模态系统中的应用。

Method: 创新性地将自强制引导视觉重建算法与去偏拟合训练策略相结合，解决多步生成过程中的训练-推理不一致问题。

Result: 在64 tokens/图像的高压缩率下，ImageNet数据集上取得rFID=1.21和gFID=2.29的SOTA重构质量指标。

Conclusion: 所提出的离散分词器成功弥合了离散与连续表征之间的性能差距，在图像生成质量和计算效率间达到新平衡。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [15] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出UniStereo数据集和StereoPilot模型，提升立体视频转换


<details>
  <summary>Details</summary>
Motivation: 现有DWI流水线存在误差传播、深度歧义和格式不一致问题，且缺乏统一数据集支撑鲁棒训练

Method: 构建首个覆盖平行/汇聚格式的UniStereo数据集，设计无需深度图、基于域切换器和循环损失的StereoPilot前馈模型

Result: SOTA指标领先，视觉质量提升且计算效率提高，项目页含实证演示

Conclusion: 提出的统一框架通过数据集构建与模型创新，有效解决单目转立体的格式适配与一致性难题

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [16] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出AdaTooler-V，通过自适应工具使用减少多模态语言模型中的冗余视觉工具调用，并在12个基准测试中展现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态模型存在盲目调用视觉工具的问题，导致推理开销增加和性能下降。

Method: 1. 提出AT-GRPO强化学习算法，基于工具效益评分自适应调整奖励尺度；2. 构建包含10万和30万样本的CoT数据集支持训练。

Result: AdaTooler-V-7B在高分辨率V*基准测试中达到89.8%准确率，超越GPT-4o和Gemini 1.5 Pro等商业模型。

Conclusion: 通过开源所有代码、模型和数据，推动多模态模型效率优化方向的研究进展。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [17] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: This paper proposes the Driving Visual Geometry Transformer (DVGT), a method for reconstructing 3D scene geometry from unposed multi-view images in autonomous driving scenarios without using precise camera parameters or external sensors.


<details>
  <summary>Details</summary>
Motivation: Existing 3D geometry perception models for driving lack adaptability to diverse scenarios and camera configurations, often relying on strict camera parameter assumptions or external sensor data for alignment.

Method: DVGT uses a DINO backbone to extract visual features, applies alternating intra-view, cross-view, and cross-frame attention mechanisms to infer geometric relationships, and decodes a global 3D point map and ego poses using multiple heads. It operates without explicit 3D geometric priors or calibration constraints.

Result: DVGT outperforms existing models on diverse driving datasets (nuScenes, OpenScene, Waymo, etc.) by directly predicting metric-scaled geometry from image sequences and eliminating post-alignment requirements for external sensors.

Conclusion: DVGT demonstrates robustness to arbitrary camera configurations and scenarios in autonomous driving while maintaining accuracy, offering a flexible solution for real-world deployment without strict calibration dependencies.

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [18] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V 是一种基于指令的视频编辑框架，在数据、架构和控制方面进行改进，通过轻量级设计实现出色的视频编辑能力和灵活输入支持。


<details>
  <summary>Details</summary>
Motivation: 视频编辑在一致性、控制性和泛化性方面存在挑战，现有方法受限。作者发现预训练的文本到视频模型具备编辑能力，并希望通过简化设计提升性能。

Method: 1) 数据：利用已有的快速反转模型构建视频对，结合单帧监督、仿射运动伪对和密集描述片段，并加入过渡监督。2) 模型：采用预训练文本到视频模型，通过序列拼接和轻量 LoRA 微调优化编辑能力。3) 控制：统一时空控制机制（单掩码）并支持参考图像。

Result: EasyV2V 实现了当前最先进的视频编辑效果，超越同期研究和商业系统，支持灵活输入（如视频+文本、掩码+引用图像+文本等）。

Conclusion: 该研究验证了简化框架在视频编辑任务中的有效性，通过数据构建、模型微调和控制机制优化，实现了高效、可控且泛化性更强的视频编辑工具。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [19] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 本文提出AuditDM框架，通过自动化审计分歧发现和修正多模态大模型失败模式，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估方法缺乏可解释性且无法全面暴露模型能力差距，需要更有效的诊断工具。

Method: 使用强化学习微调MLLM作为审计器，生成最大化模型分歧的挑战性问题和反事实图像，发现可解释的弱点示例。

Result: 在Gemma-3/PaliGemma-2上发现20+失败类型，微调后的模型在16个基准测试中性能提升，3B参数模型超越28B模型。

Conclusion: 数据扩展边际效益下降时，针对性模型审计能有效诊断并改进模型性能。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [20] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 本文提出了NEPA方法，通过预测图像嵌入实现视觉自监督学习，使用Transformer模型直接预测下一个补丁嵌入，无需传统设计复杂性。


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成预训练启发，探索将视觉自监督学习从表征学习转向模型学习：直接让生成式模型预测未来嵌入而非输出特征，简化训练流程。

Method: 采用因果掩码和停止梯度机制，训练Transformer模型进行下一嵌入预测（NEPA）。模型不依赖像素重建、离散token、对比损失或任务特定头，保持架构简洁性。

Result: 在ImageNet-1K预训练ViT-B/L后，微调达到83.8%/85.3% top-1准确率，并成功迁移到ADE20K语义分割任务，证明了方法有效性。

Conclusion: 生成式预训练提供了一种架构简单、可扩展且跨模态的自监督视觉学习范式，可替代传统复杂设计的方法。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [21] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种无需合成数据训练的单图重构散景与去模糊方法。


<details>
  <summary>Details</summary>
Motivation: 深度场控制对于摄影至关重要，但现有单图重构技术需依赖全焦输入、合成数据且控制光圈能力有限。

Method: 提出生成式重构方法：1) DeblurNet 从多样输入恢复全焦图像；2) BokehNet 生成可控散景。结合半监督训练，融合合成配对数据与未配对真实散景图像，并利用EXIF元数据捕捉真实光学特性。

Result: 实验表明在散景合成、去模糊与重构基准测试中表现优异，并支持文本引导调整及自定义光圈形状。

Conclusion: 该方法克服传统模拟器局限性，实现更高真实感散景生成与灵活控制。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [22] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个结合文本、轨迹和参考图像的框架，用于生成用户可控的多模态世界事件模拟，支持复杂的场景互动和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本或轨迹控制的图像生成方法难以兼顾语义意图表达、视觉细节保持和复杂事件模拟，需多模态协同解决方案。

Method: 提出多模态框架WorldCanvas：（1）轨迹编码运动/时间/可见性信息；（2）自然语言描述语义目标；（3）参考图规范对象身份；（4）融合三者生成事件序列。

Result: 生成视频实现：（1）多智能体交互；（2）对象进出场景；（3）参考图引导的外观保持；（4）临时消失后仍维持物体身份与场景一致性；（5）支持反直觉事件生成。

Conclusion: 该框架将传统预测型世界模型升级为用户可干预的交互式模拟器，为复杂世界事件建模提供了新范式。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: This paper proposes GraFT, a framework for translating natural language to temporal logic by reducing token prediction complexity through restricted output token sets, achieving higher accuracy than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in atomic proposition lifting, co-reference handling, and limited data learning. The study aims to address these issues by reducing the solution space complexity during translation.

Method: GraFT restricts valid output tokens at each translation step instead of using the full vocabulary. It leverages problem-specific properties to reduce the solution space, separating lifting and translation tasks theoretically for improved efficiency.

Result: On benchmarks (CW, GLTL, Navi), GraFT improves end-to-end translation accuracy by 5.49% and out-of-domain accuracy by 14.06% on average compared to state-of-the-art approaches.

Conclusion: Solution space reduction via token restriction enables more efficient learning, leading to significant improvements in both in-domain and out-of-domain natural language-to-temporal-logic translation tasks.

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [24] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 提出信息论方法量化韵律独立传递的情感/讽刺等信息，发现音频通道比文本多一个数量级的信息量，特别在缺乏上下文时。


<details>
  <summary>Details</summary>
Motivation: 韵律（语调）传递了文本无法涵盖的关键信息（如情感、讽刺），但缺乏有效方法量化其具体贡献。

Method: 利用大模型估算语义维度（如情感）与通信通道（音频/文本）间的互信息，分析电视/播客语音数据中的讽刺、情感和疑问特征。

Result: 音频（含韵律）在情感/讽刺传达中提供超过文本10倍以上信息，疑问则提升较少，尤其在无长时上下文场景下差异显著。

Conclusion: 证明韵律在特定语义维度的信息主导作用，提出将该方法推广至更多语义维度、通信通道和跨语言研究计划。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [25] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache是一种通用的Transformer模型推理加速框架，通过语义相似性重用中间激活实现层层面缓存，支持编码器/解码器架构，在BERT/GPT-2上实现3.1倍加速且准确率损失<0.5%。


<details>
  <summary>Details</summary>
Motivation: 现有token级键值缓存机制在应用场景和加速效果上存在局限性，需要一种更通用且高效的Transformer推理加速方案。

Method: 提出LLMCache分层缓存框架：1)通过轻量级指纹机制匹配语义相似输入；2)构建可跨编码器/解码器层的通用缓存；3)设计自适应缓存淘汰策略管理时效性；4)支持任意Transformer层的中间激活复用。

Result: BERT和GPT-2在SQuAD、WikiText-103、OpenBookQA数据集上实现最高3.1倍推理加速，准确率损失控制在0.5%以内。消融实验验证了层级缓存设计和淘汰策略的有效性。

Conclusion: LLMCache为变压器模型提供了首个通用的分层缓存解决方案，在保持低精度损失的同时显著提升推理效率，适用于实时大规模部署场景。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [26] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: Multimodal RewardBench 2 (MMRB2) 是首个用于评估多模态奖励模型的大型基准测试，涵盖四个任务（图文生成等）及23个模型的1,000对偏好数据，Gemini 3 Pro表现最优。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RMs）在单一模态语言模型中研究充分，但现有研究缺少对图文交错生成场景的评估工具。

Method: 构建MMRB2，包含：1) 四个任务（图文生成、图像编辑、交错生成、图像推理），每个任务1000个专家标注偏好对；2) 使用SOTA模型生成答案并通过集成过滤策略筛选数据；3) 验证模型性能与下游任务的相关性。

Result: Gemini 3 Pro达75-80%准确率，GPT-5/Gemini 2.5 Pro为66-75%，Qwen3-VL-32B约64%，远低于人类超90%水平，但显著优于GPT-4o（59%）。

Conclusion: MMRB2能有效衡量多模态奖励模型性能，模型表现与下游任务成功强相关，未来需改进数据质量、训练策略及推理能力。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [27] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 研究Transformer在符号可变的代数任务中如何发展符号推理机制，即使符号意义不固定也能实现高准确率和跨群泛化。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示Transformer在固定符号任务中采用几何嵌入反映代数结构，但真实场景中符号意义会动态变化，本文旨在探索模型在符号可变情境下的推理机制。

Method: 提出动态符号-代数群映射任务，设计针对性数据分布进行因果验证，通过控制实验分离出三种核心机制：可交换复制、单位元识别和闭包消解。

Result: 模型达到接近完美准确率，成功泛化到未知代数群，揭示出三个稳定习得机制：专用头的复制操作、识别单位元素的特征区分、基于闭包的群成员约束。

Conclusion: 发现Transformer在可变符号系统中发展出符号化推理策略，与固定符号场景下的几何表征形成互补，证实模型具备动态语境中的抽象代数推理能力。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>
